<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>苹果的味道</title><link href="http://www.qingyuanxingsi.com/" rel="alternate"></link><link href="http://www.qingyuanxingsi.com/feeds/all.atom.xml" rel="self"></link><id>http://www.qingyuanxingsi.com/</id><updated>2014-05-13T00:00:00+08:00</updated><entry><title>机器学习实战(I):手写数字识别</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-shi-zhan-ishou-xie-shu-zi-shi-bie.html" rel="alternate"></link><updated>2014-05-13T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-13:ji-qi-xue-xi-shi-zhan-ishou-xie-shu-zi-shi-bie.html</id><summary type="html">&lt;p&gt;从大三到现在一直在学习机器学习相关的理论,可是隐约间觉得自己对于机器学习算法的理解还只是停留在理论层面,理解还是很浅薄的,并不深刻。此外,最近听到了一些关于机器学习理论与实践关系的说法,觉得挺有道理的,摘录如下:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;只有真正将机器学习算法应用到某个具体问题上的时候,你的学习才刚刚开始。&lt;/li&gt;
&lt;li&gt;机器学习的过程实际上是一个学习理论、实践、学习理论、实践...的过程,只有经过这样一轮又一轮的过滤,你才能慢慢的开始理解机器学习算法。从哲学上来说,其实这就是一个理论联系实际的过程。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Talk is cheap,show me your code&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;When I cannot create,I do not understand&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;总而言之,言而总之,婶婶地觉得我该实践实践了,真正针对实际问题,动手去实现一些机器学习算法,并检验它们的实际效果。&lt;/p&gt;
&lt;p&gt;作为机器学习实战系列的第一篇,本博文选取的问题来自&lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt;,我选取了&lt;a href="https://www.kaggle.com/c/digit-recognizer"&gt;Digit Recognizer&lt;/a&gt;这个问题,于是牛刀霍霍,开始解剖这个问题。&lt;/p&gt;
&lt;p&gt;根据该问主页上的提示,我打算实现Random Forest以及Convolutional Network来检验这两个问题针对手写数字识别这个问题的实际应用效果,以下就记录一下整个过程吧。&lt;/p&gt;
&lt;h1&gt;随机森林&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;随机森林算法是在决策树算法的基础上提出,关于决策树算法的相关知识可参考我之前写的一篇&lt;a href="http://www.qingyuanxingsi.com/Decision%20Tree.html"&gt;机器学习系列(I):决策树算法&lt;/a&gt;,它的核心思想是随机选取训练样本以及随机选取特征,从而针对这些训练样本以及随机选取的特征根据决策树算法构建多棵决策树,最后综合多棵决策树的结果得出须预测样本的分类。由于之前实现过决策树算法,于是这里我就不想实现这个算法了,具体采用&lt;strong&gt;scikit&lt;/strong&gt;工具包中提供的随机森林分类器进行训练。&lt;/p&gt;
&lt;p&gt;具体实现时,由于Kaggle提供的测试集是以csv格式提供的,在一大通搜索之后,我最后采用了&lt;a href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt;库处理文件输入及输出问题,根据其官网上的说明:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可见&lt;strong&gt;pandas&lt;/strong&gt;是针对python开发的一个高性能且易用的数据结构以及数据分析工具库。在处理Kaggle数据的时候用了一下,个人感觉还是挺好用的。&lt;/p&gt;
&lt;p&gt;另外需要说明的一点是,面对MNIST数据集的时候,最先想到的一个问题是如何提取特征以用于手写数字识别的问题中,大概查了一下,一种比较简单的方法是将整个图像分成很多不重叠的2*2的小区域,然后统计每个小区域中不为0的像素值的总数(另,可针对不同的区域设置不同的权重)。在查找的过程中也发现了很多其他方法,有兴趣的朋友可以自行Google之。因为Kaggle提供的训练集每张图片都是一个长度为784的长向量,个人觉得计算2*2小区域包含不为0像素的总数不是很方便(现在看来还是挺简单的昂),所以直接一股脑地把整个784位的向量作为特征扔给随机森林分类器了。(&lt;strong&gt;唉,现在觉得自己太不负责任了,怎么能够这样呢?&lt;/strong&gt;).以下给出采用随机森林进行训练的源代码。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;#-*- coding:utf-8 -*-&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;pandas&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;
&lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;sklearn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ensemble&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;

&lt;span class="err"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;A&lt;/span&gt; &lt;span class="n"&gt;simple&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt; &lt;span class="n"&gt;forest&lt;/span&gt; &lt;span class="n"&gt;program&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;minist&lt;/span&gt; &lt;span class="n"&gt;digit&lt;/span&gt; &lt;span class="n"&gt;recognition&lt;/span&gt;
&lt;span class="err"&gt;@&lt;/span&gt;&lt;span class="n"&gt;author&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;
&lt;span class="err"&gt;@&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;2014&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mo"&gt;05&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;11&lt;/span&gt;
&lt;span class="err"&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;

&lt;span class="n"&gt;csvPath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;home&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;digit_recognizer&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;testPath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;home&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;digit_recognizer&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;destPath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;home&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;digit_recognizer&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;csv&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;n_samples&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42000&lt;/span&gt;

&lt;span class="cp"&gt;#Loading the csv data&lt;/span&gt;
&lt;span class="n"&gt;digitArray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;csvPath&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;as_matrix&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="cp"&gt;#Split the data into two parts:data and tag&lt;/span&gt;
&lt;span class="n"&gt;dataArray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;digitArray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;tagArray&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;digitArray&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_samples&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;

&lt;span class="cp"&gt;#Feed the data and tag into the random forest classifier of the scikit toolkit&lt;/span&gt;

&lt;span class="n"&gt;randomForestClassifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_estimators&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;randomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataArray&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tagArray&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="cp"&gt;#Read the test dat in&lt;/span&gt;
&lt;span class="n"&gt;testDat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testPath&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;as_matrix&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;tags&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testDat&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;tags&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;randomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;testDat&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;

&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tags&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;destPath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;得到分析结果并提交到Kaggle之后,最后的准确率没我想象的那么惨不忍睹,Kaggle选取部分测试集进行测试之后给的SCORE是94%左右,效果也不是灰常灰常差啦。&lt;/p&gt;
&lt;p&gt;当然,这个准确率个人实在不是很满意,所以打算拿Convolutional Neural Network试试。&lt;/p&gt;
&lt;h1&gt;卷积神经网络&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;概述&lt;/h2&gt;
&lt;p&gt;卷积神经网络是一种特殊的深层的神经网络模型，它的特殊性体现在两个方面，一方面它的神经元间的连接是&lt;strong&gt;非全连接&lt;/strong&gt;的，另一方面同一层中某些神经元之间的连接的&lt;strong&gt;权重是共享的&lt;/strong&gt;（即相同的）。它的非全连接和权值共享的网络结构使之更类似于生物神经网络，降低了网络模型的复杂度（对于很难学习的深层结构来说，这是非常重要的），减少了权值的数量。&lt;/p&gt;
&lt;p&gt;卷积网络最初是受视觉神经机制的启发而设计的，是为识别二维形状而设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者其他形式的变形具有高度不变性。1962年Hubel和Wiesel通过对猫视觉皮层细胞的研究，提出了&lt;strong&gt;感受野(receptive field)&lt;/strong&gt;的概念，1984年日本学者Fukushima基于感受野概念提出的神经认知机(neocognitron)模型，它可以看作是卷积神经网络的第一个实现网络，也是感受野概念在人工神经网络领域的首次应用。&lt;/p&gt;
&lt;p&gt;神经认知机将一个视觉模式分解成许多子模式(特征)，然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有位移或轻微变形的时候，也能完成识别。神经认知机能够利用位移恒定能力从激励模式中学习，并且可识别这些模式的变化形。在其后的应用研究中，Fukushima将神经认知机主要用于手写数字的识别。随后，国内外的研究人员提出多种卷积神经网络形式，在邮政编码识别（Y. LeCun etc）、车牌识别和人脸识别等方面得到了广泛的应用。&lt;/p&gt;
&lt;h2&gt;CNN的结构&lt;/h2&gt;
&lt;p&gt;卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。 这些良好的性能是网络在有监督方式下学会的，网络的结构主要有稀疏连接和权值共享两个特点，包括如下形式的约束：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特征提取。每一个神经元从上一层的局部接受域得到突触输人，因而迫使它提取局部特征。一旦一个特征被提取出来，只要它相对于其他特征的位置被近似地保留下来，它的精确位置就变得没有那么重要了。&lt;/li&gt;
&lt;li&gt;特征映射。网络的每一个计算层都是由多个特征映射组成的，每个特征映射都是平面形式的。平面中单独的神经元在约束下共享相同的突触权值集，这种结构形式具有如下的有益效果：a.平移不变性。b.自由参数数量的缩减(通过权值共享实现)。&lt;/li&gt;
&lt;li&gt;子抽样。每个卷积层跟着一个实现局部平均和子抽样的计算层，由此特征映射的分辨率降低。这种操作具有使特征映射的输出对平移和其他形式的变形的敏感度下降的作用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;稀疏连接(Sparse Connectivity)&lt;/h3&gt;
&lt;p&gt;卷积网络通过在相邻两层之间强制使用局部连接模式来利用图像的空间局部特性，在第$m$层的隐层单元只与第$m-1$层的输入单元的局部区域有连接，第$m-1$层的这些局部区域被称为空间连续的接受域。我们可以将这种结构描述如下：设第$m-1$层为视网膜输入层，第$m$层的接受域的宽度为3，也就是说该层的每个单元与且仅与输入层的3个相邻的神经元相连，第$m$层与第$m+1$层具有类似的链接规则，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="CNN_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cnn_1_zpsa4a2fbd1.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;可以看到$m+1$层的神经元相对于第$m$层的接受域的宽度也为3，但相对于输入层的接受域为5，这种结构将学习到的过滤器（对应于输入信号中被最大激活的单元)限制在局部空间模式（因为每个单元对它接受域外的variation不做反应）。从上图也可以看出，多个这样的层堆叠起来后，会使得过滤器（不再是线性的）逐渐成为全局的（也就是覆盖到了更大的视觉区域）。例如上图中第$m+1$层的神经元可以对宽度为5的输入进行一个非线性的特征编码。&lt;/p&gt;
&lt;h3&gt;权值共享(Shared Weights)&lt;/h3&gt;
&lt;p&gt;在卷积网络中，每个稀疏过滤器$h_i$通过共享权值都会覆盖整个可视域，这些共享权值的单元构成一个特征映射，如下图所示。
&lt;img alt="CNN_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cnn_2_zps12bba757.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;在图中，有3个隐层单元，他们属于同一个特征映射。同种颜色的链接的权值是相同的，我们仍然可以使用梯度下降的方法来学习这些权值，只需要对原始算法做一些小的改动，这里共享权值的梯度是所有共享参数的梯度的总和。我们不禁会问为什么要权重共享呢？一方面，重复单元能够对特征进行识别，而不考虑它在可视域中的位置。另一方面，权值共享使得我们能更有效的进行特征抽取，因为它极大的减少了需要学习的自由变量的个数。通过控制模型的规模，卷积网络对视觉问题可以具有很好的泛化能力。&lt;/p&gt;
&lt;h3&gt;The Full Model&lt;/h3&gt;
&lt;p&gt;卷积神经网络是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。网络中包含一些简单元和复杂元，分别记为S-元和C-元。S-元聚合在一起组成S-面，S-面聚合在一起组成S-层，用Us表示。C-元、C-面和C-层(Us)之间存在类似的关系。网络的任一中间级由S-层与C-层串接而成，而输入级只含一层，它直接接受二维视觉模式，样本特征提取步骤已嵌入到卷积神经网络模型的互联结构中。&lt;/p&gt;
&lt;p&gt;一般地，$U_s$为特征提取层，每个神经元的输入与前一层的局部感受野相连，并提取该局部的特征，一旦该局部特征被提取后，它与其他特征间的位置关系 也随之确定下来；$U_c$是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性(这一句表示没看懂，那位如果看懂了，请给我讲解一下)。此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。卷积神经网络中的每一个特征提取层(S-层)都紧跟着一个用来求局部平均与二次提取的计算层(C-层)，这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。&lt;/p&gt;
&lt;p&gt;下图是一个卷积网络的实例:&lt;/p&gt;
&lt;p&gt;&lt;img alt="CNN_3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cnn_3_zps8de51bda.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;图中的卷积网络工作流程如下，输入层由32×32个感知节点组成，接收原始图像。然后，计算流程在卷积和子抽样之间交替进行，如下所 述：第一隐藏层进行卷积，它由8个特征映射组成，每个特征映射由28×28个神经元组成，每个神经元指定一个5×5的接受域；第二隐藏层实现子 抽样和局部平均，它同样由8个特征映射组成，但其每个特征映射由14×14个神经元组成。每个神经元具有一个 2×2 的接受域，一个可训练系数，一个可训练偏置和一个sigmoid激活函数。可训练系数和偏置控制神经元的操作点。第三隐藏层进行第二次卷积，它由20个特征映射组成每个特征映射由10×10个神经元组成。该隐藏层中的每个神经元可能具有和下一个隐藏层几个特征映射相连的突触连接，它以与第一个卷积层相似的方式操作。第四个隐藏层进行第二次子抽样和局部平均汁算。它由20个特征映射组成，但每个特征映射由5×5个神经元组成，它以与第一次抽样相似的方式操作。第五个隐藏层实现卷积的最后阶段，它由 120 个神经元组成，每个神经元指定一个5×5的接受域。最后是个全连接层，得到输出向量。相继的计算层在卷积和抽样之间的连续交替，我们得到一个“双尖塔”的效果，也就是在每个卷积或抽样层，随着空间分辨率下降，与相应的前一层相比特征映射的数量增加。卷积之后进行子抽样的思想是受到动物视觉系统中的“简单的”细胞后面跟着“复杂的”细胞的想法的启发而产生的。图中所示的多层感知器包含近似100000个突触连接，但只有大约2600个自由参数。自由参数在数量上显著地减少是通过权值共享获得 的，学习机器的能力（以VC维的形式度量）因而下降，这又提高它的泛化能力。而且它对自由参数的调整通过反向传播学习的随机形式来实 现。另一个显著的特点是使用权值共享使得以并行形式实现卷积网络变得可能。这是卷积网络对全连接的多层感知器而言的另一个优点。&lt;/p&gt;
&lt;p&gt;以上我们简要介绍了一下卷积神经网络的基本概念和基本架构,如何对每个部分进行简单的代码实现可参考&lt;a href="http://www.deeplearning.net/tutorial/lenet.html"&gt;Convolutional Neural Networks (LeNet)&lt;/a&gt;,里面对如何对卷积以及Pooling进行代码实现做了粗略的介绍。实际上,我构建的用于识别手写数字的程序也是在以上代码的基础上理解之后稍加修改后形成的。以下给出具体代码(以下代码中&lt;a href="http://deeplearning.net/tutorial/code/mlp.py"&gt;mlp&lt;/a&gt;以及&lt;a href="http://deeplearning.net/tutorial/code/logistic_sgd.py"&gt;logistic_sgd&lt;/a&gt;模块可从文字上给出的链接处下载):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;cPickle&lt;/span&gt;
&lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;gzip&lt;/span&gt;
&lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;os&lt;/span&gt;
&lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;sys&lt;/span&gt;
&lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;time&lt;/span&gt;

&lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;numpy&lt;/span&gt;

&lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;
&lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;tensor&lt;/span&gt; &lt;span class="nx"&gt;as&lt;/span&gt; &lt;span class="nx"&gt;T&lt;/span&gt;
&lt;span class="nx"&gt;from&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;signal&lt;/span&gt; &lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;downsample&lt;/span&gt;
&lt;span class="nx"&gt;from&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;nnet&lt;/span&gt; &lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;conv&lt;/span&gt;

&lt;span class="nx"&gt;from&lt;/span&gt; &lt;span class="nx"&gt;logistic_sgd&lt;/span&gt; &lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;load_data&lt;/span&gt;
&lt;span class="nx"&gt;from&lt;/span&gt; &lt;span class="nx"&gt;mlp&lt;/span&gt; &lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;HiddenLayer&lt;/span&gt;
&lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;pandas&lt;/span&gt; &lt;span class="nx"&gt;as&lt;/span&gt; &lt;span class="nx"&gt;pd&lt;/span&gt;


&lt;span class="kr"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;LeNetConvPoolLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;object&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;Pool Layer of a convolutional network &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;rng&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;image_shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;poolsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s2"&gt;        Allocate a LeNetConvPoolLayer with shared variable internal parameters.&lt;/span&gt;

&lt;span class="s2"&gt;        :type rng: numpy.random.RandomState&lt;/span&gt;
&lt;span class="s2"&gt;        :param rng: a random number generator used to initialize weights&lt;/span&gt;

&lt;span class="s2"&gt;        :type input: theano.tensor.dtensor4&lt;/span&gt;
&lt;span class="s2"&gt;        :param input: symbolic image tensor, of shape image_shape&lt;/span&gt;

&lt;span class="s2"&gt;        :type filter_shape: tuple or list of length 4&lt;/span&gt;
&lt;span class="s2"&gt;        :param filter_shape: (number of filters, num input feature maps,&lt;/span&gt;
&lt;span class="s2"&gt;                              filter height,filter width)&lt;/span&gt;

&lt;span class="s2"&gt;        :type image_shape: tuple or list of length 4&lt;/span&gt;
&lt;span class="s2"&gt;        :param image_shape: (batch size, num input feature maps,&lt;/span&gt;
&lt;span class="s2"&gt;                             image height, image width)&lt;/span&gt;

&lt;span class="s2"&gt;        :type poolsize: tuple or list of length 2&lt;/span&gt;
&lt;span class="s2"&gt;        :param poolsize: the downsampling (pooling) factor (#rows,#cols)&lt;/span&gt;
&lt;span class="s2"&gt;        &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

        &lt;span class="nx"&gt;assert&lt;/span&gt; &lt;span class="nx"&gt;image_shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;input&lt;/span&gt;

        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;there&lt;/span&gt; &lt;span class="nx"&gt;are&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;num input feature maps * filter height * filter width&amp;quot;&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;inputs&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt; &lt;span class="nx"&gt;each&lt;/span&gt; &lt;span class="nx"&gt;hidden&lt;/span&gt; &lt;span class="nx"&gt;unit&lt;/span&gt;
        &lt;span class="nx"&gt;fan_in&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;each&lt;/span&gt; &lt;span class="nx"&gt;unit&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;lower&lt;/span&gt; &lt;span class="nx"&gt;layer&lt;/span&gt; &lt;span class="nx"&gt;receives&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="nx"&gt;gradient&lt;/span&gt; &lt;span class="nx"&gt;from&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;num output feature maps * filter height * filter width&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt;   &lt;span class="nx"&gt;pooling&lt;/span&gt; &lt;span class="nx"&gt;size&lt;/span&gt;
        &lt;span class="nx"&gt;fan_out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;
                   &lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;prod&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;poolsize&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;initialize&lt;/span&gt; &lt;span class="nx"&gt;weights&lt;/span&gt; &lt;span class="kd"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;random&lt;/span&gt; &lt;span class="nx"&gt;weights&lt;/span&gt;
        &lt;span class="nx"&gt;W_bound&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;fan_in&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;fan_out&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;shared&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="nx"&gt;rng&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;uniform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;low&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="nx"&gt;W_bound&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;high&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;W_bound&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;config&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;floatX&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                               &lt;span class="nx"&gt;borrow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;bias&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;D&lt;/span&gt; &lt;span class="nx"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt; &lt;span class="nx"&gt;one&lt;/span&gt; &lt;span class="nx"&gt;bias&lt;/span&gt; &lt;span class="nx"&gt;per&lt;/span&gt; &lt;span class="nx"&gt;output&lt;/span&gt; &lt;span class="nx"&gt;feature&lt;/span&gt; &lt;span class="nx"&gt;map&lt;/span&gt;
        &lt;span class="nx"&gt;b_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,),&lt;/span&gt; &lt;span class="nx"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;config&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;floatX&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;shared&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;value&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;b_values&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;borrow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;convolve&lt;/span&gt; &lt;span class="nx"&gt;input&lt;/span&gt; &lt;span class="nx"&gt;feature&lt;/span&gt; &lt;span class="nx"&gt;maps&lt;/span&gt; &lt;span class="kd"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;filters&lt;/span&gt;
        &lt;span class="nx"&gt;conv_out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;conv&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;conv2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;filters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;image_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;image_shape&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;downsample&lt;/span&gt; &lt;span class="nx"&gt;each&lt;/span&gt; &lt;span class="nx"&gt;feature&lt;/span&gt; &lt;span class="nx"&gt;map&lt;/span&gt; &lt;span class="nx"&gt;individually&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;using&lt;/span&gt; &lt;span class="nx"&gt;maxpooling&lt;/span&gt;
        &lt;span class="nx"&gt;pooled_out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;downsample&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;max_pool_2d&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;conv_out&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                            &lt;span class="nx"&gt;ds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;poolsize&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;ignore_border&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;add&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;bias&lt;/span&gt; &lt;span class="nx"&gt;term&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="nx"&gt;Since&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;bias&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="nx"&gt;vector&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;D&lt;/span&gt; &lt;span class="nx"&gt;array&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nx"&gt;we&lt;/span&gt; &lt;span class="nx"&gt;first&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;reshape&lt;/span&gt; &lt;span class="nx"&gt;it&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="nx"&gt;tensor&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;n_filters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt; &lt;span class="nx"&gt;Each&lt;/span&gt; &lt;span class="nx"&gt;bias&lt;/span&gt; &lt;span class="nx"&gt;will&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;thus&lt;/span&gt; &lt;span class="nx"&gt;be&lt;/span&gt; &lt;span class="nx"&gt;broadcasted&lt;/span&gt; &lt;span class="nx"&gt;across&lt;/span&gt; &lt;span class="nx"&gt;mini&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;batches&lt;/span&gt; &lt;span class="nx"&gt;and&lt;/span&gt; &lt;span class="nx"&gt;feature&lt;/span&gt; &lt;span class="nx"&gt;map&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;width&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="nx"&gt;height&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;pooled_out&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;b&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;dimshuffle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;store&lt;/span&gt; &lt;span class="nx"&gt;parameters&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="nx"&gt;layer&lt;/span&gt;
        &lt;span class="nx"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="bp"&gt;self.&lt;/span&gt;&lt;span class="nx"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self.&lt;/span&gt;&lt;span class="nx"&gt;b&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;


&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;evaluate_lenet5&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;learning_rate&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;n_epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="nx"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/home/qingyuanxingsi/data/digit_recognizer/mnist.pkl.gz&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="nx"&gt;nkerns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;50&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot; Demonstrates lenet on MNIST dataset&lt;/span&gt;

&lt;span class="s2"&gt;    :type learning_rate: float&lt;/span&gt;
&lt;span class="s2"&gt;    :param learning_rate: learning rate used (factor for the stochastic&lt;/span&gt;
&lt;span class="s2"&gt;                          gradient)&lt;/span&gt;

&lt;span class="s2"&gt;    :type n_epochs: int&lt;/span&gt;
&lt;span class="s2"&gt;    :param n_epochs: maximal number of epochs to run the optimizer&lt;/span&gt;

&lt;span class="s2"&gt;    :type dataset: string&lt;/span&gt;
&lt;span class="s2"&gt;    :param dataset: path to the dataset used for training /testing (MNIST here)&lt;/span&gt;

&lt;span class="s2"&gt;    :type nkerns: list of ints&lt;/span&gt;
&lt;span class="s2"&gt;    :param nkerns: number of kernels on each layer&lt;/span&gt;
&lt;span class="s2"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="nx"&gt;rng&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;RandomState&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;23455&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nx"&gt;datasets&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;load_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nx"&gt;train_set_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;train_set_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;datasets&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
    &lt;span class="nx"&gt;valid_set_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;valid_set_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;datasets&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
    &lt;span class="nx"&gt;test_set_x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;test_set_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;datasets&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="kr"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;my&lt;/span&gt; &lt;span class="nx"&gt;own&lt;/span&gt; &lt;span class="nx"&gt;dataset&lt;/span&gt;
    &lt;span class="nx"&gt;predict_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/home/qingyuanxingsi/data/digit_recognizer/test.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;as_matrix&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="nx"&gt;predict_set&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;predict_set&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mf"&gt;256.0&lt;/span&gt;
    &lt;span class="nx"&gt;predict_set_x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;shared&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predict_set&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                               &lt;span class="nx"&gt;dtype&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;config&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;floatX&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                 &lt;span class="nx"&gt;borrow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;predict_set_tag&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;predict_set.shape&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,])&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;compute&lt;/span&gt; &lt;span class="nx"&gt;number&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;minibatches&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;training&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;validation&lt;/span&gt; &lt;span class="nx"&gt;and&lt;/span&gt; &lt;span class="nx"&gt;testing&lt;/span&gt;
    &lt;span class="nx"&gt;n_train_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;train_set_x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_value&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;borrow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
    &lt;span class="nx"&gt;n_valid_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;valid_set_x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_value&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;borrow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
    &lt;span class="nx"&gt;n_test_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;test_set_x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_value&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;borrow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
    &lt;span class="nx"&gt;n_predict_batches&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;predict_set_x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;get_value&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;borrow&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;True&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;shape&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
    &lt;span class="nx"&gt;n_train_batches&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;
    &lt;span class="nx"&gt;n_valid_batches&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;
    &lt;span class="nx"&gt;n_test_batches&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;
    &lt;span class="nx"&gt;n_predict_batches&lt;/span&gt; &lt;span class="o"&gt;/=&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;allocate&lt;/span&gt; &lt;span class="nx"&gt;symbolic&lt;/span&gt; &lt;span class="nx"&gt;variables&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt;
    &lt;span class="nx"&gt;index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;lscalar&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;index&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;mini&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="nx"&gt;batch&lt;/span&gt;
    &lt;span class="nx"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;x&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;data&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="nx"&gt;presented&lt;/span&gt; &lt;span class="nx"&gt;as&lt;/span&gt; &lt;span class="nx"&gt;rasterized&lt;/span&gt; &lt;span class="nx"&gt;images&lt;/span&gt;
    &lt;span class="nx"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;ivector&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;y&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;labels&lt;/span&gt; &lt;span class="nx"&gt;are&lt;/span&gt; &lt;span class="nx"&gt;presented&lt;/span&gt; &lt;span class="nx"&gt;as&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="nx"&gt;D&lt;/span&gt; &lt;span class="nx"&gt;vector&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt;
                        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="nx"&gt;labels&lt;/span&gt;

    &lt;span class="nx"&gt;ishape&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;MNIST&lt;/span&gt; &lt;span class="nx"&gt;images&lt;/span&gt;

    &lt;span class="err"&gt;######################&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;BUILD&lt;/span&gt; &lt;span class="nx"&gt;ACTUAL&lt;/span&gt; &lt;span class="nx"&gt;MODEL&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;
    &lt;span class="err"&gt;######################&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;... building the model&amp;#39;&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;Reshape&lt;/span&gt; &lt;span class="nx"&gt;matrix&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;rasterized&lt;/span&gt; &lt;span class="nx"&gt;images&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nx"&gt;D&lt;/span&gt; &lt;span class="nx"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;compatible&lt;/span&gt; &lt;span class="kd"&gt;with&lt;/span&gt; &lt;span class="nx"&gt;our&lt;/span&gt; &lt;span class="nx"&gt;LeNetConvPoolLayer&lt;/span&gt;
    &lt;span class="nx"&gt;layer0_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;Construct&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;first&lt;/span&gt; &lt;span class="nx"&gt;convolutional&lt;/span&gt; &lt;span class="nx"&gt;pooling&lt;/span&gt; &lt;span class="nx"&gt;layer&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;filtering&lt;/span&gt; &lt;span class="nx"&gt;reduces&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;image&lt;/span&gt; &lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;maxpooling&lt;/span&gt; &lt;span class="nx"&gt;reduces&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="nx"&gt;further&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nx"&gt;D&lt;/span&gt; &lt;span class="nx"&gt;output&lt;/span&gt; &lt;span class="nx"&gt;tensor&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="nx"&gt;thus&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;nkerns&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;layer0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;LeNetConvPoolLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;rng&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;layer0_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nx"&gt;image_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nkerns&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nx"&gt;poolsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;Construct&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;second&lt;/span&gt; &lt;span class="nx"&gt;convolutional&lt;/span&gt; &lt;span class="nx"&gt;pooling&lt;/span&gt; &lt;span class="nx"&gt;layer&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;filtering&lt;/span&gt; &lt;span class="nx"&gt;reduces&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;image&lt;/span&gt; &lt;span class="nx"&gt;size&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;maxpooling&lt;/span&gt; &lt;span class="nx"&gt;reduces&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="nx"&gt;further&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="nx"&gt;D&lt;/span&gt; &lt;span class="nx"&gt;output&lt;/span&gt; &lt;span class="nx"&gt;tensor&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="nx"&gt;thus&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nkerns&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;nkerns&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;layer1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;LeNetConvPoolLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;rng&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;layer0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nx"&gt;image_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;nkerns&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;filter_shape&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nkerns&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;nkerns&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nx"&gt;poolsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;HiddenLayer&lt;/span&gt; &lt;span class="nx"&gt;being&lt;/span&gt; &lt;span class="nx"&gt;fully&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;connected&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;it&lt;/span&gt; &lt;span class="nx"&gt;operates&lt;/span&gt; &lt;span class="nx"&gt;on&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="nx"&gt;D&lt;/span&gt; &lt;span class="nx"&gt;matrices&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;num_pixels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;e&lt;/span&gt; &lt;span class="nx"&gt;matrix&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;rasterized&lt;/span&gt; &lt;span class="nx"&gt;images&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;This&lt;/span&gt; &lt;span class="nx"&gt;will&lt;/span&gt; &lt;span class="nx"&gt;generate&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="nx"&gt;matrix&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;shape&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;layer2_input&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;layer1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;output&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;construct&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="nx"&gt;fully&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;connected&lt;/span&gt; &lt;span class="nx"&gt;sigmoidal&lt;/span&gt; &lt;span class="nx"&gt;layer&lt;/span&gt;
    &lt;span class="nx"&gt;layer2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;HiddenLayer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;rng&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;layer2_input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;n_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;nkerns&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="nx"&gt;n_out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;activation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;tanh&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;classify&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;values&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;fully&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;connected&lt;/span&gt; &lt;span class="nx"&gt;sigmoidal&lt;/span&gt; &lt;span class="nx"&gt;layer&lt;/span&gt;
    &lt;span class="nx"&gt;layer3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;input&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;layer2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;n_in&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;500&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;n_out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;cost&lt;/span&gt; &lt;span class="nx"&gt;we&lt;/span&gt; &lt;span class="nx"&gt;minimize&lt;/span&gt; &lt;span class="nx"&gt;during&lt;/span&gt; &lt;span class="nx"&gt;training&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;NLL&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;model&lt;/span&gt;
    &lt;span class="nx"&gt;cost&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;layer3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;negative_log_likelihood&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;create&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt; &lt;span class="nx"&gt;compute&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;mistakes&lt;/span&gt; &lt;span class="nx"&gt;that&lt;/span&gt; &lt;span class="nx"&gt;are&lt;/span&gt; &lt;span class="nx"&gt;made&lt;/span&gt; &lt;span class="nx"&gt;by&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;model&lt;/span&gt;
    &lt;span class="nx"&gt;test_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;layer3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
             &lt;span class="nx"&gt;givens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;test_set_x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;test_set_y&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="nx"&gt;validate_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;layer3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;errors&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
            &lt;span class="nx"&gt;givens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;valid_set_x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;valid_set_y&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;create&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="nx"&gt;list&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;all&lt;/span&gt; &lt;span class="nx"&gt;model&lt;/span&gt; &lt;span class="nx"&gt;parameters&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt; &lt;span class="nx"&gt;be&lt;/span&gt; &lt;span class="nx"&gt;fit&lt;/span&gt; &lt;span class="nx"&gt;by&lt;/span&gt; &lt;span class="nx"&gt;gradient&lt;/span&gt; &lt;span class="nx"&gt;descent&lt;/span&gt;
    &lt;span class="nx"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;layer3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;layer2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;layer1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;layer0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;create&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="nx"&gt;list&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="nx"&gt;gradients&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;all&lt;/span&gt; &lt;span class="nx"&gt;model&lt;/span&gt; &lt;span class="nx"&gt;parameters&lt;/span&gt;
    &lt;span class="nx"&gt;grads&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;T&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;grad&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;train_model&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="kd"&gt;function&lt;/span&gt; &lt;span class="nx"&gt;that&lt;/span&gt; &lt;span class="nx"&gt;updates&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;model&lt;/span&gt; &lt;span class="nx"&gt;parameters&lt;/span&gt; &lt;span class="nx"&gt;by&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;SGD&lt;/span&gt; &lt;span class="nx"&gt;Since&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="nx"&gt;model&lt;/span&gt; &lt;span class="nx"&gt;has&lt;/span&gt; &lt;span class="nx"&gt;many&lt;/span&gt; &lt;span class="nx"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;it&lt;/span&gt; &lt;span class="nx"&gt;would&lt;/span&gt; &lt;span class="nx"&gt;be&lt;/span&gt; &lt;span class="nx"&gt;tedious&lt;/span&gt; &lt;span class="nx"&gt;to&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;manually&lt;/span&gt; &lt;span class="nx"&gt;create&lt;/span&gt; &lt;span class="nx"&gt;an&lt;/span&gt; &lt;span class="nx"&gt;update&lt;/span&gt; &lt;span class="nx"&gt;rule&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;each&lt;/span&gt; &lt;span class="nx"&gt;model&lt;/span&gt; &lt;span class="nx"&gt;parameter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt; &lt;span class="nx"&gt;We&lt;/span&gt; &lt;span class="nx"&gt;thus&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;create&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;updates&lt;/span&gt; &lt;span class="nx"&gt;list&lt;/span&gt; &lt;span class="nx"&gt;by&lt;/span&gt; &lt;span class="nx"&gt;automatically&lt;/span&gt; &lt;span class="nx"&gt;looping&lt;/span&gt; &lt;span class="nx"&gt;over&lt;/span&gt; &lt;span class="nx"&gt;all&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;grads&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;pairs&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
    &lt;span class="nx"&gt;updates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;param_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;grad_i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;grads&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;updates&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;append&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;param_i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;param_i&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;grad_i&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="nx"&gt;train_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;cost&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;updates&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;updates&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nx"&gt;givens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;train_set_x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nx"&gt;y&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;train_set_y&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;

    &lt;span class="err"&gt;###############&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;TRAIN&lt;/span&gt; &lt;span class="nx"&gt;MODEL&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;
    &lt;span class="err"&gt;###############&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;... training&amp;#39;&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;early&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;stopping&lt;/span&gt; &lt;span class="nx"&gt;parameters&lt;/span&gt;
    &lt;span class="nx"&gt;patience&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;look&lt;/span&gt; &lt;span class="nx"&gt;as&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="nx"&gt;many&lt;/span&gt; &lt;span class="nx"&gt;examples&lt;/span&gt; &lt;span class="nx"&gt;regardless&lt;/span&gt;
    &lt;span class="nx"&gt;patience_increase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;wait&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="nx"&gt;much&lt;/span&gt; &lt;span class="nx"&gt;longer&lt;/span&gt; &lt;span class="nx"&gt;when&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="nx"&gt;best&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt;
                           &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;found&lt;/span&gt;
    &lt;span class="nx"&gt;improvement_threshold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.995&lt;/span&gt;  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;a&lt;/span&gt; &lt;span class="nx"&gt;relative&lt;/span&gt; &lt;span class="nx"&gt;improvement&lt;/span&gt; &lt;span class="nx"&gt;of&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="nx"&gt;much&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt;
                                   &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;considered&lt;/span&gt; &lt;span class="nx"&gt;significant&lt;/span&gt;
    &lt;span class="nx"&gt;validation_frequency&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n_train_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;patience&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                                  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;go&lt;/span&gt; &lt;span class="nx"&gt;through&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="nx"&gt;many&lt;/span&gt;
                                  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;minibatche&lt;/span&gt; &lt;span class="nx"&gt;before&lt;/span&gt; &lt;span class="nx"&gt;checking&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;network&lt;/span&gt;
                                  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;validation&lt;/span&gt; &lt;span class="nx"&gt;set&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt; &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="nx"&gt;we&lt;/span&gt;
                                  &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;check&lt;/span&gt; &lt;span class="nx"&gt;every&lt;/span&gt; &lt;span class="nx"&gt;epoch&lt;/span&gt;

    &lt;span class="nx"&gt;best_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;None&lt;/span&gt;
    &lt;span class="nx"&gt;best_validation_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;inf&lt;/span&gt;
    &lt;span class="nx"&gt;best_iter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="nx"&gt;test_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
    &lt;span class="nx"&gt;start_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;time&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;clock&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="nx"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="nx"&gt;done_looping&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;False&lt;/span&gt;

    &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;and&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;not&lt;/span&gt; &lt;span class="nx"&gt;done_looping&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="nx"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="nx"&gt;minibatch_index&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n_train_batches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

            &lt;span class="nx"&gt;iter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;n_train_batches&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="nx"&gt;minibatch_index&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;iter&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;training @ iter = &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;iter&lt;/span&gt;
            &lt;span class="nx"&gt;cost_ij&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;train_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;minibatch_index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;iter&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nx"&gt;validation_frequency&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

                &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;compute&lt;/span&gt; &lt;span class="nx"&gt;zero&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nx"&gt;one&lt;/span&gt; &lt;span class="nx"&gt;loss&lt;/span&gt; &lt;span class="nx"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;validation&lt;/span&gt; &lt;span class="nx"&gt;set&lt;/span&gt;
                &lt;span class="nx"&gt;validation_losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;validate_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nb"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
                                     &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n_valid_batches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
                &lt;span class="nx"&gt;this_validation_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;validation_losses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;epoch %i, minibatch %i/%i, validation error %f %%&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="o"&gt;\&lt;/span&gt;
                      &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;minibatch_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;n_train_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;\&lt;/span&gt;
                       &lt;span class="nx"&gt;this_validation_loss&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;.))&lt;/span&gt;

                &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;we&lt;/span&gt; &lt;span class="nx"&gt;got&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;best&lt;/span&gt; &lt;span class="nx"&gt;validation&lt;/span&gt; &lt;span class="nx"&gt;score&lt;/span&gt; &lt;span class="nx"&gt;until&lt;/span&gt; &lt;span class="nx"&gt;now&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;this_validation_loss&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;best_validation_loss&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

                    &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="nx"&gt;improve&lt;/span&gt; &lt;span class="nx"&gt;patience&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;loss&lt;/span&gt; &lt;span class="nx"&gt;improvement&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="nx"&gt;good&lt;/span&gt; &lt;span class="nx"&gt;enough&lt;/span&gt;
                    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;this_validation_loss&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;best_validation_loss&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;  &lt;span class="o"&gt;\&lt;/span&gt;
                       &lt;span class="nx"&gt;improvement_threshold&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                        &lt;span class="nx"&gt;patience&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;patience&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;iter&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;patience_increase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

                    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;save&lt;/span&gt; &lt;span class="nx"&gt;best&lt;/span&gt; &lt;span class="nx"&gt;validation&lt;/span&gt; &lt;span class="nx"&gt;score&lt;/span&gt; &lt;span class="nx"&gt;and&lt;/span&gt; &lt;span class="nx"&gt;iteration&lt;/span&gt; &lt;span class="nx"&gt;number&lt;/span&gt;
                    &lt;span class="nx"&gt;best_validation_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;this_validation_loss&lt;/span&gt;
                    &lt;span class="nx"&gt;best_iter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;iter&lt;/span&gt;

                    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;test&lt;/span&gt; &lt;span class="nx"&gt;it&lt;/span&gt; &lt;span class="nx"&gt;on&lt;/span&gt; &lt;span class="nx"&gt;the&lt;/span&gt; &lt;span class="nx"&gt;test&lt;/span&gt; &lt;span class="nx"&gt;set&lt;/span&gt;
                    &lt;span class="nx"&gt;test_losses&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;test_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nb"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n_test_batches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
                    &lt;span class="nx"&gt;test_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;test_losses&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                    &lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;     epoch %i, minibatch %i/%i, test error of best &amp;#39;&lt;/span&gt;
                           &lt;span class="s1"&gt;&amp;#39;model %f %%&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;
                          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;minibatch_index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;n_train_batches&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                           &lt;span class="nx"&gt;test_score&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;.))&lt;/span&gt;

            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;patience&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="nx"&gt;iter&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="nx"&gt;done_looping&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;True&lt;/span&gt;
                &lt;span class="k"&gt;break&lt;/span&gt;

    &lt;span class="nx"&gt;end_time&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;time&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;clock&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Optimization complete.&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Best validation score of %f %% obtained at iteration %i,&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;\&lt;/span&gt;
          &lt;span class="s1"&gt;&amp;#39;with test performance %f %%&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt;
          &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;best_validation_loss&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;.,&lt;/span&gt; &lt;span class="nx"&gt;best_iter&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;test_score&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;.))&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;sys&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;stderr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;The code for file &amp;#39;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
                          &lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;path&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;__file__&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
                          &lt;span class="s1"&gt;&amp;#39; ran for %.2fm&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="nx"&gt;end_time&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nx"&gt;start_time&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;.))&lt;/span&gt;

    &lt;span class="err"&gt;###############&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="nx"&gt;PREDICT&lt;/span&gt; &lt;span class="nx"&gt;MODEL&lt;/span&gt; &lt;span class="err"&gt;#&lt;/span&gt;
    &lt;span class="err"&gt;###############&lt;/span&gt;
    &lt;span class="nx"&gt;print&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;... predicting&amp;#39;&lt;/span&gt;
    &lt;span class="nx"&gt;predict_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;theano&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="kd"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;layer3&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="nx"&gt;givens&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nx"&gt;x&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;predict_set_x&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;index&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nx"&gt;batch_size&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="nx"&gt;predict_set_y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;predict_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nb"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
                                     &lt;span class="k"&gt;in&lt;/span&gt; &lt;span class="nx"&gt;xrange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;n_predict_batches&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;
    &lt;span class="nx"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;predict_set_y&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="nx"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;/home/qingyuanxingsi/data/digit_recognizer/result_cnn.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nx"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;__main__&amp;#39;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;evaluate_lenet5&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;


&lt;span class="nx"&gt;def&lt;/span&gt; &lt;span class="nx"&gt;experiment&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;channel&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nx"&gt;evaluate_lenet5&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;state&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;dataset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;state&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;由于昨天晚上将n_epoches设置成200的时候,程序迭代到一定次数我的渣机就直接崩掉了,所以今天把它直接设置成60了,跑了大概4-5个小时后,最后终于拿到了结果,截图如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt="CNN_PERFORMANCE" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cnn_performance_zps05d5f2b6.png" /&gt;&lt;/p&gt;
&lt;p&gt;把预测结果提交给Kaggle之后,Score攀升至99.6%。由此可以看出,卷积神经网络相比随机森林而言还是强太多啊(好吧,另外一个因素是对于随机森林我根本没有做特征提取的工作)。此外,如果我的渣机强大一点的话,我们就能做更多次迭代,由此得到的准确率也可能更高吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果大家对于手写数字识别这个TASK有更好的模型或者有一些其他的想法,欢迎交流昂!&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ibillxia.github.io/blog/2013/04/06/Convolutional-Neural-Networks/"&gt;卷积神经网络（CNN）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;卷积神经网络基本概念可参考&lt;a href="http://blog.csdn.net/zouxy09/article/details/8781543"&gt;Deep Learning（深度学习）学习笔记整理系列之（七）&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.deeplearning.net/tutorial/lenet.html"&gt;Convolutional Neural Networks (LeNet)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="手写数字识别"></category></entry><entry><title>数据结构与算法学习札记(I):STL简介</title><link href="http://www.qingyuanxingsi.com/shu-ju-jie-gou-yu-suan-fa-xue-xi-zha-ji-istljian-jie.html" rel="alternate"></link><updated>2014-05-13T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-13:shu-ju-jie-gou-yu-suan-fa-xue-xi-zha-ji-istljian-jie.html</id><summary type="html">&lt;p&gt;今天想放松放松,所以看一下STL,算法以及C++的东西吧,以下简要记录一下看的过程中遇到的有意思的点,以及一些好玩的东东。&lt;/p&gt;
&lt;h1&gt;STL&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;入门篇&lt;sup id="sf-shu-ju-jie-gou-yu-suan-fa-xue-xi-zha-ji-istljian-jie-1-back"&gt;&lt;a class="simple-footnote" href="#sf-shu-ju-jie-gou-yu-suan-fa-xue-xi-zha-ji-istljian-jie-1" title="Power up C++ with the Standard Template Library: Part I[翻译]"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;也许你已经使用C++作为主要编程语言来解决Topcoder中的问题了，这就意味着你已经在简单的使用STL了，因为数组和字符串都是以STL对象的方式传入到你的函数中的。也许你已经注意到了，有许多程序员他们写代码要比你快并且代码也比你的简洁。&lt;/p&gt;
&lt;p&gt;也许你不是一个C++程序员，但是因为C++的强大功能以及它的库（或者因为你在Topcoder practice房间和比赛里面看到的简短的解决方案），你想成为一个这样的程序员。&lt;/p&gt;
&lt;p&gt;不管你来自哪里，这篇文章将会帮助你掌握它。我们将会看到STL中一些强大的特性--一个强大的工具有的时候可以为你在算法比赛中节省很多时间。&lt;/p&gt;
&lt;p&gt;最简单的方式去熟悉STL就是从它的容器入手了。&lt;/p&gt;
&lt;h3&gt;Containers&lt;/h3&gt;
&lt;p&gt;任何时候当你需要处理很多元素的时候你需要一些种类的容器。在C当中，这里只有一种这种类型的容器,那就是数组。现在的问题不是数组的功能有限（比如它不能够在运行时决定数组的大小，而是有许多问题需要一个具有更多功能的容器。&lt;/p&gt;
&lt;p&gt;比方说， 我们需要一下的一种或几种操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在容器中添加一些字符串&lt;/li&gt;
&lt;li&gt;从容器中移除一些字符串&lt;/li&gt;
&lt;li&gt;查看指定字符串是否在容器中&lt;/li&gt;
&lt;li&gt;返回容器中一些不同元素&lt;/li&gt;
&lt;li&gt;遍历整个容器并获得一定顺序的被添加的字符串的列表&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然，你可以在一个顺序的数组里面实现以上的功能。但是这些普通实现会变得非常低效。你也可以创建树型或者散列表结构来更得到一个更快的解决方案， 但是请思考一点：这个容器的实现是否依赖于它将存储的元素呢？例如当你需要存储一个平面上的点的时候，你是否需要重新实现这个模块来使得它工作呢？&lt;/p&gt;
&lt;p&gt;如果不想这样，我们可以为这样的容器创建一个接口，那样我们就可以在任何地方使用任何数据类型了。其实，那正是STL容器的概念。&lt;/p&gt;
&lt;h3&gt;Before we begin&lt;/h3&gt;
&lt;p&gt;当程序中使用STL的时候，需要包含相应的文件，对于大多数的容器，包含文件的名字与容器的名字相同，并且不需要后缀。如果你将要使用stack，只要在你的程序的开头加上下面的一句话就可以了：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#include &amp;lt;stack&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;容器类型（算法，函数对象和所有的STL内容）并不是定义在全局的名字空间中的，而是在一个特定的名叫“std”的名字空间里的。将下面的话添加在include之后和你的代码之前：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;using&lt;/span&gt; &lt;span class="n"&gt;namespace&lt;/span&gt; &lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;另外一个重要的事情你要记住的是容器的类型是模版参数。程序当中模版参数使用 '&amp;lt;' 和 '&amp;gt;'标注的。例如:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;N&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;当使用嵌套的构造时，确保括号不要紧随着另外一个--中间最好留个空格。 &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;CorrectDefinition&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;WrongDefinition&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// Wrong: compiler may be confused by 'operator &amp;gt;&amp;gt;'&lt;/span&gt;
&lt;span class="nx"&gt;Vector&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;NOTE:目前C++ 11版本两种写法都是支持的(笔者注）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最简单的STL容器是&lt;strong&gt;vector&lt;/strong&gt;。vector实际上是一个有着扩展功能的数组。此外，vector是仅有的一个向后兼容C的容器--这意味着vector实际上就是数组，但是有了一些附加的特性。 &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
&lt;span class="nb"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; = (i+1)*(i+1); 
} 
for(int i = 9; i &amp;gt; 0; i--) { 
    v&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; -= v&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;; 
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;事实上，当你输入 &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;空的vector被创建了。注意像一下创建的情形： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这里我们定义了一个有着10个vector&lt;int&gt;的数组，并且被初始化为空。在很多情形下，这并不是我们所想要的。这里应使用圆括号而不是方括号。&lt;/int&gt;&lt;/p&gt;
&lt;p&gt;最常使用的vector的特征是得到它的大小&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;elements_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;两个注意点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一，size()是无符号的，这个也许有的时候会产生一些问题。相应的，我通常定义一些宏，比如sz(C)，返回代表C的大小的带符号的数。&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;第二，将v.size()和0比较不是一个好的习惯，如果你想要知道这个容器是否为空。你最好使用empty()函数： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;is_nonempty_notgood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// Try to avoid this&lt;/span&gt;
&lt;span class="kt"&gt;bool&lt;/span&gt; &lt;span class="n"&gt;is_nonempty_ok&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;empty&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这个是因为并不是所有的容器可以在$O(1)$的复杂度内得到它的大小，并且你应当绝对的避免计算一个双端链表所有的元素     个数而仅仅是为了知道它是不是为空。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;另外一个经常使用的函数是push_back。&lt;strong&gt;push_back是在vector的尾部增加一个元素，使得它的大小增加1&lt;/strong&gt;。考虑下面的例子： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="nb"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;*=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="nx"&gt;v.push_back&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
&lt;span class="p"&gt;}&lt;/span&gt; 
&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;elements_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;v.size&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;不要担心内存分配--vector不会为一个元素每次都分配。相反的，vector在使用push_buck增加元素的时候会比它需要的分配的多。唯一你要担心的是内存使用，但是在TopCoder中这个没什么关系（后面将更多的关注vector的内存策略）&lt;/p&gt;
&lt;p&gt;当你需要重新规划vector的大小时，使用resize()函数： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
&lt;span class="nb"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; = i+1; 
} 
v.resize(25); 
for(int i = 20; i &lt;span class="nt"&gt;&amp;lt; 25&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt; &lt;span class="na"&gt;i&lt;/span&gt;&lt;span class="err"&gt;++)&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt; 
    &lt;span class="na"&gt;v&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="na"&gt; =&lt;/span&gt;&lt;span class="err"&gt; &lt;/span&gt;&lt;span class="s"&gt;i*2;&lt;/span&gt; 
&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;resize()函数使得vector包含需要数量的元素，如果你需要比vector已经有的元素少，最后部分的元素将被删除，如果你需要vector大小增长，它将会增加它的大小并且用用0进行初始化。&lt;/p&gt;
&lt;p&gt;注意如果你在resize()之后使用push_buck()，它将会在新增加的大小后继续增加元素，而不是填充。在上面的例子中，vector最后的大小是25。而当我们在第二个循环中使用push_buch，它的大小将变为30.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
&lt;span class="nb"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; = i+1; 
} 
v.resize(25); 
for(int i = 20; i &lt;span class="nt"&gt;&amp;lt; 25&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt; &lt;span class="na"&gt;i&lt;/span&gt;&lt;span class="err"&gt;++)&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt; 
    &lt;span class="na"&gt;v&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;push_back&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;i&lt;/span&gt;&lt;span class="err"&gt;*&lt;/span&gt;&lt;span class="na"&gt;2&lt;/span&gt;&lt;span class="err"&gt;);&lt;/span&gt; &lt;span class="err"&gt;//&lt;/span&gt; &lt;span class="na"&gt;Writes&lt;/span&gt; &lt;span class="na"&gt;to&lt;/span&gt; &lt;span class="na"&gt;elements&lt;/span&gt; &lt;span class="na"&gt;with&lt;/span&gt; &lt;span class="na"&gt;indices&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="nx"&gt;..30&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="nx"&gt;..25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!&lt;/span&gt; 
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;清除一个vector使用clear()成员函数。这个函数将使得vector包含0个元素。它并不是使这些元素为0。注意--它是完全清除这个容器。&lt;/p&gt;
&lt;p&gt;有许多方法初始化vector， 你也可以从另外一个vector创建一个vector&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="c1"&gt;//  &lt;/span&gt;
&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;v2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;v1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;v1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;v2和v3的初始化是基本一样的。&lt;/p&gt;
&lt;p&gt;如果你想要创建一个指定大小的vector，使用下面的构造函数 &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="kd"&gt;Data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;上面的例子中， Data在创建后将包含1000个0。记得用圆括号而不是方括号，如果你想使用指定的元素进行初始化，像如下方式构造：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;string&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="nx"&gt;Unknown&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;记住你可以创建任何类型的vector&lt;/p&gt;
&lt;p&gt;多维数组也很重要，最简单的方法来创建一个二维数组是通过声明一个元素为vector的vector。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;Matrix&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;下面很清楚的向你显示如何创建一个指定大小的二维数组： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="nb"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;N&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="c1"&gt;//  &lt;/span&gt;
&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;Matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;N&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;M&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这里我们创建了一个$N*M$的二维数组，并且初始化为-1.&lt;/p&gt;
&lt;p&gt;最简单的向一个vector中添加数据是使用push_back(),但是如果我们不想在尾部添加呢？这里我们利用insert()函数来达到这个目的。并且这里也有erase()函数来清除元素。但是首先我们需要谈谈迭代器。&lt;/p&gt;
&lt;p&gt;你需要记住一些非常重要的事情：当vector作为参数传递给一些函数的时候。一份拷贝也会被创建。创建一个这样的vector是非常耗时间和内存的，并且我们其实也不需要。事实上，发现一个需要vector的拷贝作为参数的情况是很少的，所以，你不应该写:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="bp"&gt;void&lt;/span&gt; &lt;span class="nx"&gt;some_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// Never do it unless you’re sure what you do! &lt;/span&gt;
    &lt;span class="c1"&gt;//  &lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;相反的，你应该写：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="bp"&gt;void&lt;/span&gt; &lt;span class="nx"&gt;some_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// OK &lt;/span&gt;
    &lt;span class="c1"&gt;//  &lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;如果你需要在函数中更改vector中的内容，忽略const修饰符就可以了。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="nx"&gt;modify_vector&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// Correct &lt;/span&gt;
    &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;++; 
}
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Pairs&lt;/h3&gt;
&lt;p&gt;在我们谈论迭代器之前，让我们先谈谈pairs，Pairs在STL中被广泛使用着，像TopCoder SRM 250中的500分的简单问题，通常需要一些有着一对元素的简单数据结构，而STL中的std::pair正好是一对元素。最简单的形式如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kr"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kr"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T2&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;pair&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="n"&gt;T1&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
    &lt;span class="n"&gt;T2&lt;/span&gt; &lt;span class="n"&gt;second&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;简单的有pair&lt;int, int=""&gt;是一对整数，复杂的pair&lt;string,pair&lt;int,int&gt;&amp;gt;是一对有着字符串和两个整数的pair。在第二种情况下，用法如下：&lt;/string,pair&lt;int,int&gt;&lt;/int,&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;string&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pair&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="n"&gt;string&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// extract string &lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;second&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// extract first int &lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;P&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;second&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;second&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// extract second int&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;pairs的最大好处在于他们内建了比较操作，pairs比较是从第一个比较到第二个。如果第一个元素不相等，那么结果仅仅基于第一个元素的比较；第二个元素比较被用到仅当第一个元素相等的情况。 数组（或者vector）能够轻易的被STL中内部函数排序。&lt;/p&gt;
&lt;p&gt;例如，如果你想对一个有着整数点的数组排序而使得它能够构成一个多边形，把他们放到vector&amp;lt; pair&lt;double,pair&lt;int, int=""&gt; &amp;gt;是一个好主意，这里的元素是{极坐标，{x, y}}。调用STL中的一个排序函数能够给你所需要的顺序。&lt;/double,pair&lt;int,&gt;&lt;/p&gt;
&lt;p&gt;此外，pairs也被广泛的用在联合容器中，我们稍后将会在这里谈到它。&lt;/p&gt;
&lt;h3&gt;Iterators&lt;/h3&gt;
&lt;p&gt;什么是迭代器呢？在STL中，迭代器是最常使用的访问容器中数据的方法。考虑这样一个简单的问题：将一个有着N个int类型元素的数组倒序。让我们先看C写的一个解决方法：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;void&lt;/span&gt; &lt;span class="nx"&gt;reverse_array_simple&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kr"&gt;int&lt;/span&gt; &lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
      &lt;span class="nx"&gt;Int&lt;/span&gt; &lt;span class="nx"&gt;first&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;last&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// First and last indices of elements to be swapped &lt;/span&gt;
      &lt;span class="nx"&gt;While&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;first&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="nx"&gt;last&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// Loop while there is something to swap &lt;/span&gt;
           &lt;span class="nx"&gt;swap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;A&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;first&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;A&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;last&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// swap(a,b) is the standard STL function &lt;/span&gt;
           &lt;span class="nx"&gt;first&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// Move first index forward &lt;/span&gt;
           &lt;span class="nx"&gt;last&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// Move last index back &lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt; 
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这段代码已经很清晰了。上面代码可以很容器的改成指针的形式:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;reverse_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
      &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;last&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;A&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;N&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
      &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
           &lt;span class="n"&gt;Swap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
           &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
           &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
      &lt;span class="p"&gt;}&lt;/span&gt; 
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;看这段代码的主循环部分，它在first和last指针上进行了4种不同的操作:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;比较指针(first &amp;lt; last)&lt;/li&gt;
&lt;li&gt;通过指针获取值 (&lt;em&gt;first, &lt;/em&gt;last)&lt;/li&gt;
&lt;li&gt;增加指针的值&lt;/li&gt;
&lt;li&gt;减少指针的值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;现在考虑处理第二个问题：将整个或者部分的双端链表进行倒序。第一段使用索引的代码将不在有效。至少，它在时间方面是没有优势的，因为在双端链表中不可能在O(1)的复杂度内通过索引获取到一个元素，只有在$O(N)$才行，所以整个算法将在$O(N^2)$内完成。厄。。。&lt;/p&gt;
&lt;p&gt;但是注意： 第二段代码可以在任何指针类型的对象中起作用。仅有的约束是对象只能够进行上述的操作：取值(unary *)，比较(&amp;lt;),和自增自减(++/--).与容器相关并且具有上述属性的对象被喻为迭代器。任何STL容器也许都可以通过迭代器的形式进行遍历。尽管对于vector来说不需要，但是对于其他类型的容器很重要。&lt;/p&gt;
&lt;p&gt;那么，我们拥有了一个什么了呢？一个语法和指针非常像的对象。下面的操作被定义为迭代器的操作：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;获取迭代器代表的值，int x = *it;&lt;/li&gt;
&lt;li&gt;增加和减少迭代器自身的值 it1++, it2--;&lt;/li&gt;
&lt;li&gt;迭代器间的比较, 通过 != 和 &amp;lt;&lt;/li&gt;
&lt;li&gt;迭代器后加上一个直接数 it += 20;等同于前移20个元素&lt;/li&gt;
&lt;li&gt;获取迭代器之间的距离, int n = it2 - it1;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是与指针不同的是，迭代器提供了更多的功能。它不仅可以作用在任何容器上，还可以进行比如，下标检查和描述容器用途等等。&lt;/p&gt;
&lt;p&gt;当然，迭代器最大的好处在于它很大程度上重用了代码，你自己的基于迭代器的算法，将会作用在很大范围的容器上(包括你自己的提供迭代器的容器)上，并且可以作为参数传递给很多标准函数。&lt;/p&gt;
&lt;p&gt;并不是所有类型的迭代器提供所有潜在的功能。事实上，迭代器中有“简单迭代器”和“随机访问迭代器”。简单迭代器可以使用'=='和'!='，并且他们能够自增和自减，但是他们不可以在减去或者加上一个值。一般来说，不可能为所有的容器在O(1)复杂度内完成上面描述的操作。倒置数组的函数应该像下面这样: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kr"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;reverse_array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
      &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
           &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
                &lt;span class="n"&gt;swap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
                &lt;span class="n"&gt;first&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
                &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
                     &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
                &lt;span class="p"&gt;}&lt;/span&gt; 
                &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
                &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;last&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
                     &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
                &lt;span class="p"&gt;}&lt;/span&gt; 
           &lt;span class="p"&gt;}&lt;/span&gt; 
      &lt;span class="p"&gt;}&lt;/span&gt; 
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这段代码和上面的不同之处在于我们没有在迭代器上使用'&amp;lt;'比较，而仅仅是使用'=='。另外，你不要对函数原型感到惊慌： 模版是声明函数的一个方法，它将使所有可以的参数类型得以工作。这个函数可以在参数是指向对象的指针或者所有简单迭代器时很好的工作。&lt;/p&gt;
&lt;p&gt;让我们回到STL。STL算法总是使用两种迭代器，叫做"begin"和"end".这个end迭代器并不是指向最后一个元素，而是指向第一个非法的元素，也就是最后一个元素后面的一个。通常使用它是非常方便的。&lt;/p&gt;
&lt;p&gt;每一个STL容器都有begin()和end()成员函数返回那个容器的begin和end迭代器。&lt;/p&gt;
&lt;p&gt;基于这些原理，我们可以得到，c.begin()==c.end()只有当c是空的情况下，并且c.end()-c.begin()总是等于c.size().(这个式子当迭代器可以进行减法操作时才是合法的。也就是说，begin()和end()返回了随机访问迭代器，而这并不是对所有的容器都适合。前面双端链表就是一个例子)&lt;/p&gt;
&lt;p&gt;服从STL规则的倒置函数应该像下面这么写:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;template&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kr"&gt;typename&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;reverse_array_stl_compliant&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;T&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
      &lt;span class="c1"&gt;// We should at first decrement 'end' &lt;/span&gt;
      &lt;span class="c1"&gt;// But only for non-empty range &lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
      &lt;span class="p"&gt;{&lt;/span&gt; 
           &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
           &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
                &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
                     &lt;span class="n"&gt;swap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
                     &lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
                     &lt;span class="n"&gt;If&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
                          &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
                     &lt;span class="p"&gt;}&lt;/span&gt; 
                     &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
                     &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
                          &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
                     &lt;span class="p"&gt;}&lt;/span&gt; 
                &lt;span class="p"&gt;}&lt;/span&gt; 
           &lt;span class="p"&gt;}&lt;/span&gt; 
      &lt;span class="p"&gt;}&lt;/span&gt; 
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;注意,这个函数和标准函数std::reverse(T begin, T end)所做的事情是一样的，后者可以在算法模块中找到(#include &lt;algorithm&gt;)&lt;/algorithm&gt;&lt;/p&gt;
&lt;p&gt;此外， 任何一个有足够功能的对象可以被当作一个迭代器传递给STL算法和函数。这就是模版的力量之所在！看下面的例子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="c1"&gt;//  &lt;/span&gt;
&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;v.begin&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nx"&gt;v.end&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt; &lt;span class="c1"&gt;// v3 equals to v2&lt;/span&gt;

&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="kd"&gt;data&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; = { 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31 }; 
vector&lt;span class="nt"&gt;&amp;lt;int&amp;gt;&lt;/span&gt; primes(data, data+(sizeof(data) / sizeof(data&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;)));
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;最后一句话展示了从一个C的数组构造vector的例子。没有加索引的data被当成指向数组开始的指针，'data+N'指向第N个元素。当N是这个数组的长度时，'data+N'指向第一个不在数组的元素，所以‘data+data的长度'可以被当成数组'data'的end迭代器。表达式 'sizeof(data)/sizeof(data[0])'返回data数组的大小，但是这样的语句只能用在一小部分情况下，所以不要在任何地方使用它除了像以上的情况。(C程序员会同意我的说法的！)&lt;/p&gt;
&lt;p&gt;更进一步的，我们可以甚至使用下面的构造: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="c1"&gt;// &lt;/span&gt;
&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;v.begin&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nx"&gt;v.begin&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;v.size&lt;/span&gt;&lt;span class="p"&gt;()/&lt;/span&gt;&lt;span class="nx"&gt;2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这样就创建的v2向量等同于v向量的前半部分。&lt;/p&gt;
&lt;p&gt;下面是使用reverse函数的一个例子 &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;11&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;13&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;17&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;19&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt; 
&lt;span class="n"&gt;reverse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// the range { 5, 7, 9, 11 } is now { 11, 9, 7, 5 };&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;每一个容器还拥有rbegin()和rend()函数，他们返回的是倒置过的迭代器。倒置的迭代器被用在倒序容器上的，比如：&lt;/p&gt;
&lt;p&gt;vector&lt;int&gt; v; 
vector&lt;int&gt; v2(v.rbegin()+(v.size()/2), v.rend()); &lt;/int&gt;&lt;/int&gt;&lt;/p&gt;
&lt;p&gt;这样创建的v2向量将等于v的前半部分，顺序是从后向前。&lt;/p&gt;
&lt;p&gt;要创建一个迭代器对象，我们通常要指定它的类型。迭代器的类型通常可以由该类型的容器加上"::iterator","::const_iterator","::reverse_iterator" 或者"const_reverse_iterator"来构造。因此vector可以使用下面的方式进行遍历： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="c1"&gt;//&lt;/span&gt;

&lt;span class="c1"&gt;// Traverse all container, from begin() to end() &lt;/span&gt;
&lt;span class="nb"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="nl"&gt;iterator&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;v.begin&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="nx"&gt;it&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="nx"&gt;v.end&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="nx"&gt;it&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;it&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// Increment the value iterator is pointing to &lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;建议你使用 '!=' 而不是 '&amp;lt;' 并且'empty()'而不是'size()!=0'--&lt;strong&gt;因为对于一些类型的容器而言，去判断一个迭代器是否在另外一个之前是非常低效的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;现在你知道STL的算法函数reverse()了。其实许多STL算法是以这么一种方式声明的：他们通常使用一对迭代器，开始和结束迭代器，并且返回一个迭代器。&lt;/p&gt;
&lt;p&gt;find()算法在一个区间内查找指定的元素，如果这个元素被找到了，那么指向这个元素第一次出现位置的迭代器将会被返回。反之，则返回这个区间的end迭代器。看下面的代码： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="nb"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="nx"&gt;v.push_back&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;v.begin&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="nx"&gt;v.end&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;49&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="nx"&gt;v.end&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="c1"&gt;//  &lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;想要得到被找到的元素的下标，需要用find()的结果减去开始迭代器：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="mi"&gt;49&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; 
&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;v&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="c1"&gt;//  &lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;记得在使用STL算法的时候，在源代码中加上#include&lt;algorithm&gt;&lt;/algorithm&gt;&lt;/p&gt;
&lt;p&gt;min_element和max_element返回一个指向单个元素的迭代器。要获得最小或者最大的元素，和find()一样，使用&lt;em&gt;min_element(...)或者 &lt;/em&gt;max_element(...)。要获得下标的话，减去容器或者一定范围的开始迭代器就可以了： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="kd"&gt;data&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; = { 1, 5, 2, 4, 3 }; 
vector&lt;span class="nt"&gt;&amp;lt;int&amp;gt;&lt;/span&gt; X(data, data+5); 
int v1 = *max_element(X.begin(), X.end()); // Returns value of max element in vector 
int i1 =  max_element(X.begin(), X.end()) – X.begin; // Returns index of max element in vector

int v2 = *min_element(data, data+5); // Returns value of min element in array 
int i3 = min_element(data, data+5) – data; // Returns index of min element in array
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;现在你可以看到有用的宏了:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;#define all(c) c.begin(), c.end()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;不要把右边的部分放到圆括号里面--那样是错的！&lt;/p&gt;
&lt;p&gt;另外一个好用的算法是sort(), 它使用起来非常简单。 看下面的一个例子：&lt;/p&gt;
&lt;p&gt;vector&lt;int&gt; X; &lt;/int&gt;&lt;/p&gt;
&lt;p&gt;//  &lt;/p&gt;
&lt;p&gt;sort(X.begin(), X.end()); // Sort array in ascending order 
sort(all(X));             // Sort array in ascending order, use our #define 
sort(X.rbegin(), X.rend()); // Sort array in descending order using with reverse iterators &lt;/p&gt;
&lt;h3&gt;Compiling STL Programs&lt;/h3&gt;
&lt;p&gt;一个需要值得指出的事情就是STL的错误消息。由于STL已经被广泛的使用在源代码中了，所以有必要要求编译器去创建高效的可执行文件，STL的一个习惯在于它的难读的错误消息.&lt;/p&gt;
&lt;p&gt;例如,如果你传递一个vector&lt;int&gt;的常引用给一些函数的时候：&lt;/int&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="bp"&gt;void&lt;/span&gt; &lt;span class="nb"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="nb"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; 
        &lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="nl"&gt;iterator&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;v.begin&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="c1"&gt;// hm where’s the error?.. &lt;/span&gt;
        &lt;span class="c1"&gt;//  &lt;/span&gt;
    &lt;span class="c1"&gt;//  &lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这里的错误是你正在从一个常量对象中创建一个非指向常量的迭代器(实际上找出错误比修改它要难).正确的代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="bp"&gt;void&lt;/span&gt; &lt;span class="nb"&gt;f&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
    &lt;span class="c1"&gt;// Traverse the vector using const_iterator &lt;/span&gt;
    &lt;span class="nb"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;::&lt;/span&gt;&lt;span class="nl"&gt;const_iterator&lt;/span&gt; &lt;span class="n"&gt;it&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;v.begin&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="nx"&gt;it&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="nx"&gt;v.end&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="nx"&gt;it&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
        &lt;span class="nb"&gt;r&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;it&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;it&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
    &lt;span class="p"&gt;}&lt;/span&gt; 
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;r&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;尽管如此,让我告诉你在GNU C++中成为"typeof"的一个重要特性。这个操作符会在编译时被替换为表达式的类型。考虑下面的例子： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;typeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这样将会创建一个和(a+b)表达式相同的类型变量x。请注意typeof(v.size())对于STL中的任何容器类型来说都是无符号的。但是在TopCoder中typeof最重要的应用在于遍历一个容器。考虑下面一些宏：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="cp"&gt;#define tr(container, iterator) \ &lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;typeof&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;container&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="n"&gt;iterator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;container&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="n"&gt;iterator&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;container&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt; &lt;span class="n"&gt;iterator&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;通过使用这些宏，我们可以遍历任何类型的容器，不仅仅是vector。这样将会为常量对象产生指向常量的迭代器并且为非常量对象产生普通迭代器，当然你也不会获得一个错误。&lt;/p&gt;
&lt;p&gt;void f(const vector&lt;int&gt;&amp;amp; v) { 
    int r = 0; 
    tr(v, it) { 
        r += (&lt;em&gt;it)&lt;/em&gt;(*it); 
    } 
    return r; 
} &lt;/int&gt;&lt;/p&gt;
&lt;p&gt;注意： 为了提高可读性，我没有在#define行上添加额外的圆括号。这篇文章的下面将会给出更加精确的#define语句，并且你可以直接将其拷贝到你的模版代码中。&lt;/p&gt;
&lt;p&gt;遍历宏对与vector来说并不是必需的，但是对于一些复杂的数据类型，由于它们没有索引访问而迭代器是唯一访问数据的方法，这是遍历宏将显得非常方便。我们将在这篇文章的后面继续讨论它。&lt;/p&gt;
&lt;h3&gt;Data manipulation in vector&lt;/h3&gt;
&lt;p&gt;我们可以使用insert()函数向vector中插入一个元素。 &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="c1"&gt;//  &lt;/span&gt;
&lt;span class="nx"&gt;v.insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// Insert value 42 after the first&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;所有从第二个元素（下标为1）到最后一个都会向右移动一个单位来腾出空位给新插入的元素。如果你打算加入很多元素的话，做许多这样的移动是不太好的，你最好只调用insert()一次。所以，&lt;strong&gt;insert()有插入一段区间的形式&lt;/strong&gt;： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="c1"&gt;// ..&lt;/span&gt;

&lt;span class="c1"&gt;// Shift all elements from second to last to the appropriate number of elements. &lt;/span&gt;
&lt;span class="c1"&gt;// Then copy the contents of v2 into v. &lt;/span&gt;
&lt;span class="nx"&gt;v.insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;vector也有一个叫做erase的函数，同样的，它也有两种形式，猜猜他们是什么样的： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;erase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;iterator&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
&lt;span class="n"&gt;erase&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;begin&lt;/span&gt; &lt;span class="n"&gt;iterator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt; &lt;span class="n"&gt;iterator&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;第一个例子中，vector中的单个元素将会被删除，在第二个例子中，由两个迭代器指定的区间将会从vector中删除。&lt;/p&gt;
&lt;p&gt;插入，删除的技巧很普遍，但对于STL的容器来说并不都是一样的。&lt;/p&gt;
&lt;h3&gt;String&lt;/h3&gt;
&lt;p&gt;这里有一个操纵字符串的特殊容器。这个字符串容器与vector&lt;char&gt;有一些不同之处。最大的不同之处在于字符串处理函数以及内存管理策略。唯一需要注意的事情是使用string::length()而不是vector::size(). string没有size()。&lt;/char&gt;&lt;/p&gt;
&lt;p&gt;string有一个不用迭代器而使用下标的取子串函数: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;string&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"hello"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="n"&gt;string&lt;/span&gt; 
    &lt;span class="n"&gt;s1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;substr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;// "hel" &lt;/span&gt;
    &lt;span class="n"&gt;s2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;substr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;// "ell" &lt;/span&gt;
    &lt;span class="n"&gt;s3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;substr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;"hell"&lt;/span&gt; 
    &lt;span class="n"&gt;s4&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;substr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// "ello"&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;注意当(s.length()-1)作用在空串上的情况，因为s.length()返回的是无符号的数，那么unsigned(0)-1得到的并不是你期望的！&lt;/p&gt;
&lt;h3&gt;Set&lt;/h3&gt;
&lt;p&gt;通常很难决定是先讲set容器还是map容器。我的意见是如果读者有一个基本的算法概念，那么从'set'讲起会容易理解一点。&lt;/p&gt;
&lt;p&gt;考虑我们需要一个有着下面特征的容器：&lt;/p&gt;
&lt;p&gt;增加一个元素，但是不允许它重复出现
删除元素
获取不同元素的数量
判断元素是否已经在集合当中&lt;/p&gt;
&lt;p&gt;这是一个经常被使用到的任务。STL为其提供了一个特殊的容器--set。set能够添加，移除和在O(logN)内检查是否有指定元素存在，这里的N是指在set中存在的对象数量。当像set中集合中添加元素时，重复的将会被丢弃。set中元素的数量N，可以在O(1)的时间复杂度内得到。我们将稍后谈到set和map的算法实现。现在，让我们先研究一下set的接口: &lt;/p&gt;
&lt;p&gt;set&lt;int&gt; S; &lt;/int&gt;&lt;/p&gt;
&lt;p&gt;for(int i = 1; i &amp;lt;= 100; i++) { 
      s.insert(i); // Insert 100 elements, [1..100] 
 } &lt;/p&gt;
&lt;p&gt;s.insert(42); // does nothing, 42 already exists in set &lt;/p&gt;
&lt;p&gt;for(int i = 2; i &amp;lt;= 100; i += 2) { 
      S.remove(i); // Remove even values 
 } &lt;/p&gt;
&lt;p&gt;int N = int(S.size()); // N will be 50 &lt;/p&gt;
&lt;p&gt;push_back()成员函数对set是不适用的。因为set中的元素没有什么顺序关系，因此push_back()在这里就不适用了。&lt;/p&gt;
&lt;p&gt;由于set并不是线性容器，所以使用索引去访问元素是不可能的。因此唯一访问set元素的方法是使用迭代器。&lt;/p&gt;
&lt;p&gt;// Calculate the sum of elements in set 
 set&lt;int&gt; S; 
 //&lt;br&gt;
 int r = 0; 
 for(set&lt;int&gt;::const_iterator it = S.begin(); it != S.end(); it++) { 
      r += *it; 
 } 
这里使用遍历宏会变得非常优雅，为什么呢？试想这里有一个集合 set&lt;pair&lt;string, pair&lt;int,vector&lt;int=""&gt; &amp;gt; &amp;gt;,
你怎样去访问它呢？写写迭代器的类型？哦，不。还是使用我们的遍历宏吧。&lt;/pair&lt;string,&gt;&lt;/int&gt;&lt;/int&gt;&lt;/p&gt;
&lt;p&gt;set&amp;lt; pair&lt;string, pair&lt;="" int,="" vector&lt;int=""&gt; &amp;gt; &amp;gt; SS; 
 int total = 0; 
 tr(SS, it) { 
      total += it-&amp;gt;second.first; 
 } 
注意 'it-&amp;gt;second.first'的语法。因为'it'是一个迭代器了，我们需要'it'中取出一个对象进行操作。
所以正确的语法是 '(&lt;em&gt;it).second.first'.尽管如此，写'something-&amp;gt;'要比'(&lt;/em&gt;something)'来的容易一点。
要完整的解释的话估计要写好长--只要记住，对于迭代器而言，两种语法都是允许的。&lt;/string,&gt;&lt;/p&gt;
&lt;p&gt;要判断set中是否已经存在一些元素，使用'find()'成员函数。不要感到迷惑，虽然：在STL中有许多'find()'.
有一个全局的算法'find()' 参数包括两个迭代器，元素并且在O(N)内完成。在set中使用它来查找元素是可能的，
但是为什么有已经存在的O(logN)的算法还要要使用O(N)的算法呢？当在set和map（multiset/multimap，
hash_map/hash_set,等等)进行搜索时，不要使用全局的find--相反的使用成员函数 'set::find()'。
作为顺序的查找，set::find将会返回一个迭代器，要么指向找到的元素，要么指向'end()'。所以检查元素是否
存在应该像下面这样： &lt;/p&gt;
&lt;p&gt;set&lt;int&gt; s; 
 //&lt;br&gt;
 if(s.find(42) != s.end()) { 
      // 42 presents in set 
 } 
 else { 
      // 42 not presents in set 
 } 
另外一个在O(logN)下工作的算法是计算函数。有些人认为&lt;/int&gt;&lt;/p&gt;
&lt;p&gt;set&lt;int&gt; s; 
 //&lt;br&gt;
 if(s.find(42) != s.end()) { 
      // 42 presents in set 
 } 
 else { 
      // 42 not presents in set 
 } 
甚至 &lt;/int&gt;&lt;/p&gt;
&lt;p&gt;if(s.count(42)) { 
      // … 
 } 
非常容易写。但是就个人而言，在set或者map中使用count()是没有意义的：元素要么存在要么不存在。
对我来说，我更加喜欢使用下面两个宏：&lt;/p&gt;
&lt;h1&gt;define present(container, element) (container.find(element) != container.end())&lt;/h1&gt;
&lt;h1&gt;define cpresent(container, element) (find(all(container),element) != container.end())&lt;/h1&gt;
&lt;p&gt;(记住 all(c) 代表 "c.begin(),c.end")&lt;/p&gt;
&lt;p&gt;这里，'present()'使用成员函数'find()'返回这个元素是否存在与容器中，而'cpresent'是用于vector的。&lt;/p&gt;
&lt;p&gt;想要从set中移除一个元素使用erase()函数。&lt;/p&gt;
&lt;p&gt;set&lt;int&gt; s; 
 // … 
 s.insert(54); 
 s.erase(29); 
erase也有区间的形式: &lt;/int&gt;&lt;/p&gt;
&lt;p&gt;set&lt;int&gt; s; 
 // .. &lt;/int&gt;&lt;/p&gt;
&lt;p&gt;set&lt;int&gt;::iterator it1, it2; 
 it1 = s.find(10); 
 it2 = s.find(100); 
 // Will work if it1 and it2 are valid iterators, i.e. values 10 and 100 present in set. 
 s.erase(it1, it2); // Note that 10 will be deleted, but 100 will remain in the container &lt;/int&gt;&lt;/p&gt;
&lt;p&gt;set有一个区间的构造函数： &lt;/p&gt;
&lt;p&gt;int data[5] = { 5, 1, 4, 2, 3 }; 
 set&lt;int&gt; S(data, data+5); 
它给我们提供一个消除vector中重复元素的办法，并且进行排序，如下： &lt;/int&gt;&lt;/p&gt;
&lt;p&gt;vector&lt;int&gt; v; 
 // … 
 set&lt;int&gt; s(all(v)); 
 vector&lt;int&gt; v2(all(s)); 
这里，'v2'将和 'v'包含一样的元素，但是进行了升序排序，并且不具有重复元素。&lt;/int&gt;&lt;/int&gt;&lt;/int&gt;&lt;/p&gt;
&lt;p&gt;任何可以进行比较的元素都可以在set中进行存储。这个将在后面进行讨论。&lt;/p&gt;
&lt;p&gt;Map&lt;/p&gt;
&lt;p&gt;有两种关于map的解释，简单的解释如下： &lt;/p&gt;
&lt;p&gt;map&lt;string, int=""&gt; M; 
 M["Top"] = 1; 
 M["Coder"] = 2; 
 M["SRM"] = 10; &lt;/string,&gt;&lt;/p&gt;
&lt;p&gt;int x = M["Top"] + M["Coder"]; &lt;/p&gt;
&lt;p&gt;if(M.find("SRM") != M.end()) { 
      M.erase(M.find("SRM")); 
 } &lt;/p&gt;
&lt;p&gt;非常简单，难道不是吗？&lt;/p&gt;
&lt;p&gt;事实上，map和set很像，除了map保存的不仅仅是值，而是一个队 &lt;key,value&gt;.
map确保一个队中最多只有一个指定的key存在。另外一个比较好的是map定义了
[]操作符。&lt;/key,value&gt;&lt;/p&gt;
&lt;p&gt;遍历map可以简单的使用'tr()'宏。注意迭代器将会使一个std::pair的key和value， 要获取值
可以使用it-&amp;gt;second.下面是个例子： &lt;/p&gt;
&lt;p&gt;map&lt;string, int=""&gt; M; 
 // … 
 int r = 0; 
 tr(M, it) { 
      r += it-&amp;gt;second; 
 } 
 不要使用迭代器去更改map中元素的key， 因为它可能会破坏map内部数据结构的集成性（看下面).&lt;/string,&gt;&lt;/p&gt;
&lt;p&gt;要记住的最重要的关于map的事情是，[]操作符将会创建一个元素,如果那个元素不存在的话。并且这个元素将会
被添加到map中。所以，如果你想使你的代码更加快，绝不要用[]操作符除非你能确保那个元素已经
存在与map中了。这就是为什么[]操作符不用在map作为常引用参数传递给一些函数的原因了:&lt;/p&gt;
&lt;p&gt;void f(const map&lt;string, int=""&gt;&amp;amp; M) { 
      if(M["the meaning"] == 42) { // Error! Cannot use [] on const map objects! 
      } 
      if(M.find("the meaning") != M.end() &amp;amp;&amp;amp; *M.find("the meaning") == 42) { // Correct 
           cout &amp;lt;&amp;lt; "Don't Panic!" &amp;lt;&amp;lt; endl; 
      } 
 } 
Notice on Map and Set&lt;/string,&gt;&lt;/p&gt;
&lt;p&gt;map和set内部使用红黑树实现的。因此，当访问这些容器的时候，map和set的元素总是以升序排列。
并且这就是为什么强烈建议不要在遍历map或者set时更改key值的原因了：如果你做了修改，那么将会
打乱顺序，这样至少导致容器算法的不正确。&lt;/p&gt;
&lt;p&gt;但是事实上map和set元素总是有序的这一点在解决TopCoder问题上面是很实用的。&lt;/p&gt;
&lt;p&gt;另外一个重要的事情是定义在map和set迭代器上的操作符++和--。因此，如果值42在set当中并且它不是第一个
也不是最后一个，那么下面这段代码是可以工作的：&lt;/p&gt;
&lt;p&gt;set&lt;int&gt; S; 
 //&lt;br&gt;
 set&lt;int&gt;::iterator it = S.find(42); 
 set&lt;int&gt;::iterator it1 = it, it2 = it; 
 it1--; 
 it2++; 
 int a = &lt;em&gt;it1, b = &lt;/em&gt;it2; &lt;/int&gt;&lt;/int&gt;&lt;/int&gt;&lt;/p&gt;
&lt;p&gt;这里a将包含42左边的第一个邻居，而b则包含右边的第一个邻居。&lt;/p&gt;
&lt;p&gt;More on algorithms&lt;/p&gt;
&lt;p&gt;是时候深入讨论一些算法了。大多数算法被定义在#include &lt;algorithm&gt;头文件中了。首先，STL提供了
3个非常简单的算法： min(a,b), max(a,b), swap(a,b)。 这里min(a,b)和max(a,b)返回两个元素中的
最小值和最大值，而swap(a,b)交换两个元素。&lt;/algorithm&gt;&lt;/p&gt;
&lt;p&gt;sort()算法也被广泛的使用着。调用sort(begin,end)将会将一定区间进行升序排列。注意sort()需要随机访问
迭代器,因此它并不是在任何容器上都适用。尽管如此，你也许不会在一个已经有序的set上调用sort()。&lt;/p&gt;
&lt;p&gt;你已经听说了find()算法了。呼叫find(begin,end,element)将返回'element'第一次出现的迭代器或者end如果这个元素
没有找到的话。与find(...)不同的是， count(begin,end,element)返回一个元素在整个容器或者部分中出现的数量。
记住set和map都有成员函数find()和count(),它们都是在O(logN)内工作的，而std::find()和std::count()在O(N)内工作。&lt;/p&gt;
&lt;p&gt;其他一些有用的算法有 next_permutation() 和 prev_permutation()。让我们先谈谈 next_permutation.
调用 next_permutation(begin, end)将会使区间[begin, end)保持同样元素的下一个排列或者当当前排列是最后一个排列时返回false。因此， next_permutation使得很多工作变得相当简单。如果你想要检查所有的排列，像下面这样写：&lt;/p&gt;
&lt;p&gt;vector&lt;int&gt; v; &lt;/int&gt;&lt;/p&gt;
&lt;p&gt;for(int i = 0; i &amp;lt; 10; i++) { 
      v.push_back(i); 
 } &lt;/p&gt;
&lt;p&gt;do { 
      Solve(, v); 
 } while(next_permutation(all(v)); &lt;/p&gt;
&lt;p&gt;不用忘记确保容器中的元素在你第一次调用 next_permutation(...)之前已经排序了，他们的初始
状态将决定第一个排列；否则的话，有一些排列将不会被检查到。&lt;/p&gt;
&lt;p&gt;String Streams&lt;/p&gt;
&lt;p&gt;你经常需要处理一些字符串的输入和输出。C++为其提供了两个有趣的对象： 'istringstream'和'ostringstream'&lt;/p&gt;
&lt;p&gt;他们都在 #include &lt;sstream&gt; 中定义&lt;/sstream&gt;&lt;/p&gt;
&lt;p&gt;istringstream 对象允许你像从标准输入&lt;/p&gt;
&lt;p&gt;void f(const string&amp;amp; s) { &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;  &lt;span class="c1"&gt;// Construct an object to parse strings &lt;/span&gt;
  &lt;span class="nx"&gt;istringstream&lt;/span&gt; &lt;span class="nx"&gt;is&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;s&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

  &lt;span class="c1"&gt;// Vector to store data &lt;/span&gt;
  &lt;span class="nx"&gt;vector&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

  &lt;span class="c1"&gt;// Read integer while possible and add it to the vector &lt;/span&gt;
  &lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="nx"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
  &lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;is&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
       &lt;span class="nx"&gt;v.push_back&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;tmp&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;} &lt;/p&gt;
&lt;p&gt;ostringstream是用来格式化输出的。下面是代码：&lt;/p&gt;
&lt;p&gt;string f(const vector&lt;int&gt;&amp;amp; v) { &lt;/int&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;  &lt;span class="c1"&gt;// Constucvt an object to do formatted output &lt;/span&gt;
  &lt;span class="nx"&gt;ostringstream&lt;/span&gt; &lt;span class="nx"&gt;os&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

  &lt;span class="c1"&gt;// Copy all elements from vector&amp;lt;int&amp;gt; to string stream as text &lt;/span&gt;
  &lt;span class="nx"&gt;tr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;v&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;it&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
       &lt;span class="nx"&gt;os&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="s1"&gt;' '&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="nx"&gt;it&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="c1"&gt;// Get string from string stream &lt;/span&gt;
  &lt;span class="kt"&gt;string&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;os.str&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

  &lt;span class="c1"&gt;// Remove first space character &lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nx"&gt;s.empty&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// Beware of empty string here &lt;/span&gt;
       &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;s.substr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; 
  &lt;span class="p"&gt;}&lt;/span&gt;

  &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;s&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;} &lt;/p&gt;
&lt;p&gt;Summary&lt;/p&gt;
&lt;p&gt;接着往下讲STL，我更想总结一下被用到的一些模版列表。这个将会简化阅读代码样例
并且我希望，提高你的TopCoder技巧。这些模版列表和宏如下： &lt;/p&gt;
&lt;p&gt;typedef vector&lt;int&gt; vi; 
 typedef vector&lt;vi&gt; vvi; 
 typedef pair&lt;int,int&gt; ii; 
 #define sz(a) int((a).size()) 
 #define pb push_back 
 #defile all(c) (c).begin(),(c).end() 
 #define tr(c,i) for(typeof((c).begin()) i = (c).begin(); i != (c).end(); i++) 
 #define present(c,x) ((c).find(x) != (c).end()) 
 #define cpresent(c,x) (find(all(c),x) != (c).end()) 
vector&lt;int&gt;容器被放在这是因为它真的很受欢迎。事实上，对于众多容器使用简短的假名
（尤其像 vector&lt;string&gt;, vector&lt;ii&gt;, vector&lt;pair&lt;double, ii=""&gt; &amp;gt;)是非常方便的。
但是这个列表仅仅包含需要理解接下来内容的宏。&lt;/pair&lt;double,&gt;&lt;/ii&gt;&lt;/string&gt;&lt;/int&gt;&lt;/int,int&gt;&lt;/vi&gt;&lt;/int&gt;&lt;/p&gt;
&lt;p&gt;另外一个需要记住的是： 当#define左边的序列出现在右边时，它应该加上圆括号来避免许多
不必要的问题。&lt;/p&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-shu-ju-jie-gou-yu-suan-fa-xue-xi-zha-ji-istljian-jie-1"&gt;&lt;a href="http://www.cnblogs.com/drizzlecrj/archive/2007/01/31/636116.html"&gt;Power up C++ with the Standard Template Library: Part I[翻译]&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-shu-ju-jie-gou-yu-suan-fa-xue-xi-zha-ji-istljian-jie-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="STL"></category></entry><entry><title>机器学习外传之Deep Learning(II):当Deep Learning遇上NLP</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-wai-chuan-zhi-deep-learningiidang-deep-learningyu-shang-nlp.html" rel="alternate"></link><updated>2014-05-10T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-10:ji-qi-xue-xi-wai-chuan-zhi-deep-learningiidang-deep-learningyu-shang-nlp.html</id><summary type="html">&lt;p&gt;&lt;strong&gt;Original Link&lt;/strong&gt;:&lt;a href="http://licstar.net/archives/328"&gt;Deep Learning in NLP （一）词向量和语言模型&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Deep Learning算法已经在图像和音频领域取得了惊人的成果，但是在NLP领域中尚未见到如此激动人心的结果。关于这个原因，引一条我比较赞同的微博。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;a href="http://weibo.com/u/1657470871"&gt;@王威廉&lt;/a&gt;：Steve Renals算了一下icassp录取文章题目中包含Deep learning的数量，发现有44篇，而naacl则有0篇。有一种说法是，语言（词、句子、篇章等）属于人类认知过程中产生的高层认知抽象实体，而语音和图像属于较为底层的原始输入信号，所以后两者更适合做deep learning来学习特征。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;第一句就先不用管了，毕竟今年的ACL已经被灌了好多Deep Learning的论文了。第二句我很认同，不过我也有信心以后一定有人能挖掘出语言这种高层次抽象中的本质。不论最后这种方法是不是Deep Learning，就目前而言，Deep Learning在NLP领域中的研究已经将高深莫测的人类语言撕开了一层神秘的面纱。&lt;/p&gt;
&lt;p&gt;我觉得其中最有趣也是最基本的，就是“词向量”了。&lt;/p&gt;
&lt;p&gt;将词用“词向量”的方式表示可谓是将Deep Learning算法引入NLP领域的一个核心技术。大多数宣称用了Deep Learning的论文，其中往往也用了词向量。&lt;/p&gt;
&lt;h1&gt;词向量是什么&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。NLP中最直观，也是到目前为止最常用的词表示方法是&lt;em&gt;One-hot Representation&lt;/em&gt;,这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为0，只有一个维度的值为 1，这个维度就代表了当前的词。&lt;/p&gt;
&lt;p&gt;举个栗子，“话筒”表示为[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...],“麦克”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ...]
每个词都是茫茫0海中的一个1。这种One-hot Representation如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字ID。比如刚才的例子中，话筒记为 3，麦克记为8（假设从0开始记）。如果要编程实现的话，用Hash表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了NLP领域的各种主流任务。&lt;/p&gt;
&lt;p&gt;当然这种表示方法也存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。&lt;/p&gt;
&lt;p&gt;Deep Learning中一般用到的词向量并不是刚才提到的用One-hot Representation表示的那种很长很长的词向量，而是用Distributed Representation（不知道这个应该怎么翻译，因为还存在一种叫“Distributional Representation”的表示方法，又是另一个不同的概念）表示的一种低维实数向量。这种向量一般长成这个样子：$[0.792, −0.177, −0.107, 0.109, −0.542,...]$。维度以50维和100维比较常见。这种向量的表示不是唯一的，后文会提到目前计算出这种向量的主流方法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(原文作者注)&lt;em&gt;Distributed representation&lt;/em&gt;最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用$cos$夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;词向量的来历&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Distributed representation最早是Hinton在1986年的论文《Learning distributed representations of concepts》中提出的。虽然这篇文章没有说要将词做Distributed representation，（甚至我很无厘头地猜想那篇文章是为了给他刚提出的 BP 网络打广告，）但至少这种先进的思想在那个时候就在人们的心中埋下了火种，到2000年之后开始逐渐被人重视。&lt;/p&gt;
&lt;p&gt;Distributed representation用来表示词，通常被称为“Word Representation”或“Word Embedding”，中文俗称“词向量”。真的只能叫“俗称”，算不上翻译。半年前我本想翻译的，但是硬是想不出Embedding应该怎么翻译的，后来就这么叫习惯了-_-|||如果有好的翻译欢迎提出。Embedding一词的意义可以参考维基百科的相应页面&lt;a href="https://en.wikipedia.org/wiki/Embedding"&gt;（链接）&lt;/a&gt;。&lt;strong&gt;后文提到的所有“词向量”都是指用Distributed Representation表示的词向量&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果用传统的稀疏表示法表示词，在解决某些任务的时候（比如构建语言模型）会造成维数灾难[Bengio 2003]。使用低维的词向量就没这样的问题。同时从实践上看，高维的特征如果要套用Deep Learning，其复杂度几乎是难以接受的，因此低维的词向量在这里也饱受追捧。&lt;/p&gt;
&lt;p&gt;同时如上一节提到的，相似词的词向量距离相近，这就让基于词向量设计的一些模型自带平滑功能，让模型看起来非常的漂亮。&lt;/p&gt;
&lt;h1&gt;词向量的训练&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;要介绍词向量是怎么训练得到的，就不得不提到语言模型。到目前为止我了解到的所有训练方法都是在训练语言模型的同时，顺便得到词向量的。&lt;/p&gt;
&lt;p&gt;这也比较容易理解，要从一段无标注的自然文本中学习出一些东西，无非就是统计出词频、词的共现、词的搭配之类的信息。而要从自然文本中统计并建立一个语言模型，无疑是要求最为精确的一个任务（也不排除以后有人创造出更好更有用的方法）。既然构建语言模型这一任务要求这么高，其中必然也需要对语言进行更精细的统计和分析，同时也会需要更好的模型，更大的数据来支撑。目前最好的词向量都来自于此，也就不难理解了。&lt;/p&gt;
&lt;p&gt;这里介绍的工作均为从大量未标注的普通文本数据中无监督地学习出词向量（语言模型本来就是基于这个想法而来的），可以猜测，如果用上了有标注的语料，训练词向量的方法肯定会更多。不过视目前的语料规模，还是使用未标注语料的方法靠谱一些。&lt;/p&gt;
&lt;p&gt;词向量的训练最经典的有3个工作，&lt;strong&gt;C&amp;amp;W 2008、M&amp;amp;H 2008、Mikolov 2010&lt;/strong&gt;。当然在说这些工作之前，不得不介绍一下这一系列中Bengio的经典之作。&lt;/p&gt;
&lt;h1&gt;语言模型简介&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;插段广告，简单介绍一下语言模型，知道的可以无视这节。&lt;/p&gt;
&lt;p&gt;语言模型其实就是看一句话是不是正常人说出来的。这玩意很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在NLP的其它任务里也都能用到。&lt;/p&gt;
&lt;p&gt;语言模型形式化的描述就是给定一个字符串，看它是自然语言的概率$P(w_1,w_2,…,w_t)$。$w_1$到$w_t$依次表示这句话中的各个词。有个很简单的推论是：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(w_1,w_2,…,w_t)=P(w_1)×P(w_2|w_1)×P(w_3|w_1,w_2)×…×P(w_t|w_1,w_2,…,w_{t−1})
\end{equation}&lt;/p&gt;
&lt;p&gt;常用的语言模型都是在近似地求$P(w_t|w_1,w_2,…,w_{t−1})$。比如&lt;strong&gt;n-gram&lt;/strong&gt;模型就是用$P(w_t|w_{t−n+1},…,w_{t−1})$近似表示前者。
　　
顺便提一句，由于后面要介绍的每篇论文使用的符号差异太大，本博文里尝试统一使用Bengio 2003的符号系统（略做简化），以便在各方法之间做对比和分析。&lt;/p&gt;
&lt;h2&gt;Bengio的经典之作&lt;/h2&gt;
&lt;p&gt;用神经网络训练语言模型的思想最早由百度IDL的徐伟于2000提出。（感谢&lt;a href="http://weibo.com/1862459915"&gt;@余凯_西二旗民工&lt;/a&gt;博士指出。）其论文《Can Artificial Neural Networks Learn Language Models?》提出一种用神经网络构建二元语言模型（即$P(w_t|w_{t−1})$的方法)。文中的基本思路与后续的语言模型的差别已经不大了。&lt;/p&gt;
&lt;p&gt;训练语言模型的最经典之作，要数Bengio等人在2001年发表在NIPS上的文章《A Neural Probabilistic Language Model》。当然现在看的话，肯定是要看他在2003年投到JMLR上的同名论文了。&lt;/p&gt;
&lt;p&gt;Bengio用了一个三层的神经网络来构建语言模型，同样也是n-gram模型。如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="NN_Language_Model" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/nn_language_model_zps54468eb4.png" /&gt;&lt;/p&gt;
&lt;p&gt;图中最下方的$w_{t−n+1},…,w_{t−2},w_{t−1}$就是前$n−1$个词。现在需要根据这已知的$n−1$个词预测下一个词$w_t$。$C(w)$表示词$w$所对应的词向量，整个模型中使用的是一套唯一的词向量，存在矩阵$C$（一个$|V|×m$的矩阵）中。其中$|V|$ 表示词表的大小（语料中的总词数），$m$表示词向量的维度。$w$到$C(w)$的转化就是从矩阵中取出一行。&lt;/p&gt;
&lt;p&gt;网络的第一层（输入层）是将$C(w_{t−n+1}),…,C(w_{t−2}),C(w_{t−1})$这$n−1$个向量首尾相接拼起来，形成一个$(n−1)m$ 维的向量，下面记为$x$。&lt;/p&gt;
&lt;p&gt;网络的第二层（隐藏层）就如同普通的神经网络，直接使用$d+Hx$计算得到。$d$是一个偏置项。在此之后，使用$tanh$作为激活函数。&lt;/p&gt;
&lt;p&gt;网络的第三层（输出层）一共有$|V|$个节点，每个节点$y_i$表示下一个词为$i$的未归一化log概率。最后使用softmax激活函数将输出值$y$归一化成概率。最终，$y$的计算公式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
y=b+Wx+Utanh(d+Hx)
\end{equation}&lt;/p&gt;
&lt;p&gt;式子中的$U$（一个$|V|×h$的矩阵)是隐藏层到输出层的参数，整个模型的多数计算集中在$U$和隐藏层的矩阵乘法中。后文的提到的3个工作，都有对这一环节的简化，提升计算的速度。&lt;/p&gt;
&lt;p&gt;式子中还有一个矩阵$W$（$|V|×(n−1)m$），这个矩阵包含了从输入层到输出层的直连边。直连边就是从输入层直接到输出层的一个线性变换，好像也是神经网络中的一种常用技巧（没有仔细考察过）。如果不需要直连边的话，将$W$置为$0$就可以了。在最后的实验中，Bengio发现直连边虽然不能提升模型效果，但是可以少一半的迭代次数。同时他也猜想如果没有直连边，可能可以生成更好的词向量。&lt;/p&gt;
&lt;p&gt;现在万事俱备，用随机梯度下降法把这个模型优化出来就可以了。需要注意的是，一般神经网络的输入层只是一个输入值，而在这里，输入层$x$也是参数（存在$C$中），也是需要优化的。优化结束之后，词向量有了，语言模型也有了。&lt;/p&gt;
&lt;p&gt;这样得到的语言模型自带平滑，无需传统n-gram模型中那些复杂的平滑算法。Bengio在APNews数据集上做的对比实验也表明他的模型效果比精心设计平滑算法的普通n-gram算法要好10%到20%。&lt;/p&gt;
&lt;p&gt;在结束介绍 Bengio大牛的经典作品之前再插一段八卦。在其JMLR论文中的未来工作一段，他提了一个能量函数，把输入向量和输出向量统一考虑，并以最小化能量函数为目标进行优化。后来M&amp;amp;H工作就是以此为基础展开的。&lt;/p&gt;
&lt;p&gt;他提到一词多义有待解决，9年之后Huang提出了一种解决方案。他还在论文中随口（不是在Future Work中写的）提到：可以使用一些方法降低参数个数，比如用循环神经网络。后来Mikolov就顺着这个方向发表了一大堆论文，直到博士毕业。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;大牛就是大牛&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;C&amp;amp;W的SENNA&lt;/h2&gt;
&lt;p&gt;Ronan Collobert和Jason Weston在2008年的ICML上发表的《A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning》里面首次介绍了他们提出的词向量的计算方法。和上一篇牛文类似，如果现在要看的话，应该去看他们在 2011 年投到JMLR上的论文《Natural Language Processing (Almost) from Scratch》。文中总结了他们的多项工作，非常有系统性。这篇JMLR的论文题目也很霸气啊：从头开始搞 NLP。他们还把论文所写的系统开源了，叫做&lt;a href="http://ml.nec-labs.com/senna/"&gt;SENNA&lt;/a&gt;，3500多行纯C代码也是写得非常清晰。我就是靠着这份代码才慢慢看懂这篇论文的。可惜的是，代码只有测试部分，没有训练部分。&lt;/p&gt;
&lt;p&gt;实际上C&amp;amp;W这篇论文主要目的并不是在于生成一份好的词向量，甚至不想训练语言模型，而是要用这份词向量去完成NLP里面的各种任务，比如词性标注、命名实体识别、短语识别、语义角色标注等等。&lt;/p&gt;
&lt;p&gt;由于目的的不同，C&amp;amp;W的词向量训练方法在我看来也是最特别的。他们没有去近似地求$P(w_t|w_1,w_2,…,w_{t−1})$，而是直接去尝试近似$P(w_1,w_2,…,w_t)$。在实际操作中，他们并没有去求一个字符串的概率，而是求窗口连续$n$个词的打分$f(w_{t−n+1},…,w_{t−1},w_t)$。打分$f$越高的说明这句话越是正常的话；打分低的说明这句话不是太合理；如果是随机把几个词堆积在一起，那肯定是负分（差评）。打分只有相对高低之分，并没有概率的特性。&lt;/p&gt;
&lt;p&gt;有了这个对$f$的假设，C&amp;amp;W就直接使用$pair-wise$的方法训练词向量。具体的来说，就是最小化下面的目标函数。&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum\limits_{x\in \mathfrak{X}} { \sum\limits_{w\in \mathfrak{D}} {\max {0 , 1-f(x)+f(x^{(w)})} } }
\end{equation}&lt;/p&gt;
&lt;p&gt;$\mathfrak{X}$为训练集中的所有连续的$n$元短语，$\mathfrak{D}$是整个字典。第一个求和枚举了训练语料中的所有的$n$元短语，作为正样本。第二个对字典的枚举是构建负样本。$x^{(w)}$是将短语$x$的最中间的那个词，替换成$w$。在大多数情况下，在一个正常短语的基础上随便找个词替换掉中间的词，最后得到的短语肯定不是正确的短语，所以这样构造的负样本是非常可用的（多数情况下确实是负样本，极少数情况下把正常短语当作负样本也不影响大局）。同时，&lt;strong&gt;由于负样本仅仅是修改了正样本中的一个词，也不会让分类面距离负样本太远而影响分类效果(?)&lt;/strong&gt;(?)。再回顾这个式子，$x$是正样本，$x^{(w)}$ 是负样本，$f(x)$是对正样本的打分，$f(x^{(w)})$是对负样本的打分。最后希望正样本的打分要比负样本的打分至少高1分。&lt;/p&gt;
&lt;p&gt;$f$函数的结构和Bengio 2003中提到的网络结构基本一致。同样是把窗口中的$n$个词对应的词向量串成一个长的向量，同样是经过一层网络（乘一个矩阵）得到隐藏层。不同之处在于C&amp;amp;W的输出层只有一个节点，表示得分，而不像Bengio那样的有$|V|$ 个节点。这么做可以大大降低计算复杂度，当然有这种简化还是因为C&amp;amp;W并不想做一个真正的语言模型，只是借用语言模型的思想辅助他完成NLP的其它任务。（其实C&amp;amp;W的方法与Bengio的方法还有一个区别，他们为了程序的效率用HardTanh代替tanh激活函数。）&lt;/p&gt;
&lt;p&gt;他们在实验中取窗口大小$n=11$，字典大小$|V|=130000$，在维基百科英文语料和路透社语料中一共训练了$7$周，终于得到了这份伟大的词向量。&lt;/p&gt;
&lt;p&gt;如前面所说C&amp;amp;W训练词向量的动机与其他人不同，因此他公布的词向量与其它词向量相比主要有两个区别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;他的词表中只有小写单词。也就是说他把大写开头的单词和小写单词当作同一个词处理。其它的词向量都是把他们当作不同的词处理的。&lt;/li&gt;
&lt;li&gt;他公布的词向量并不直接是上述公式的优化结果，而是在此基础上进一步跑了词性标注、命名实体识别等等一系列任务的 Multi-Task Learning之后，二次优化得到的。也可以理解为是半监督学习得到的，而非其他方法中纯无监督学习得到的。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;不过好在Turian在2010年对C&amp;amp;W和M&amp;amp;H向量做对比时，重新训练了一份词向量放到了网上，那份就没上面的两个“问题”（确切的说应该是差别），也可以用的更放心。后面会详细介绍Turian的工作。&lt;/p&gt;
&lt;p&gt;关于这篇论文其实还是有些东西可以吐槽的，不过训练词向量这一块没有，是论文其他部分的。把吐槽机会留给下一篇博文了。&lt;/p&gt;
&lt;h2&gt;M&amp;amp;H的HLBL&lt;/h2&gt;
&lt;p&gt;Andriy Mnih和Geoffrey Hinton在2007年和2008年各发表了一篇关于训练语言模型和词向量的文章。2007年发表在ICML上的《Three new graphical models for statistical language modelling》表明了Hinton将Deep Learning战场扩展到NLP领域的决心。2008年发表在 NIPS 上的《A scalable hierarchical distributed language model》则提出了一种层级的思想替换了Bengio 2003方法中最后隐藏层到输出层最花时间的矩阵乘法，在保证效果的基础上，同时也提升了速度。下面简单介绍一下这两篇文章。&lt;/p&gt;
&lt;p&gt;Hinton在2006年提出Deep Learning的概念之后，很快就来NLP最基础的任务上试了一把。果然，有效。M&amp;amp;H在ICML 2007上发表的这篇文章提出了“Log-Bilinear”语言模型。文章标题中可以看出他们其实一共提了3个模型。从最基本的RBM出发，一点点修改能量函数，最后得到了“Log-Bilinear”模型。&lt;/p&gt;
&lt;p&gt;模型如果用神经网络的形式写出来，是这个样子：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
h &amp;amp;= \sum\limits_{i=1}^{t-1}{H_i C(w_i)} \\
y_j &amp;amp;= C(w_j)^T h
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;这里的两个式子可以合写成一个$y_j = \sum\limits_{i=1}^{n-1}{C(w_j)^T H_iC(w_i)}$。$C(w)$是词$w$对应的词向量，形如$x^TMy$的模型叫做$Bilinear$模型，也就是M&amp;amp;H方法名字的来历了。&lt;/p&gt;
&lt;p&gt;为了更好地理解模型的含义，还是来看这两个拆解的式子。$h$在这里表示隐藏层，这里的隐藏层比前面的所有模型都更厉害，直接有语义信息。首先从第二个式子中隐藏层能和词向量直接做内积可以看出，隐藏层的维度和词向量的维度是一致的（都是 $m$维）。$H_i$就是一个$m×m$的矩阵，该矩阵可以理解为第$i$个词经过$H_i$这种变换之后，对第$t$个词产生的贡献。因此这里的隐藏层是对前$t−1$个词的总结，也就是说隐藏层$h$是对下一个词的一种预测。&lt;/p&gt;
&lt;p&gt;再看看第二个式子，预测下一个词为$w_j$的log概率是$y_j$，它直接就是$C(w_j)$和$h$的内积。内积基本上就可以反应相似度，如果各词向量的模基本一致的话，内积的大小能直接反应两个向量的cos夹角的大小。这里使用预测词向量$h$和各个已知词的词向量的相似度作为log概率，将词向量的作用发挥到了极致。这也是我觉得这次介绍的模型中最漂亮的一个。&lt;/p&gt;
&lt;p&gt;这种“Log-Bilinear”模型看起来每个词需要使用上文所有的词作为输入，于是语料中最长的句子有多长，就会有多少个$H$矩阵。这显然是过于理想化了。最后在实现模型时，还是迫于现实的压力，用了类似n-gram的近似，只考虑了上文的3到5个词作为输入来预测下一个词。&lt;/p&gt;
&lt;p&gt;M&amp;amp;H的思路如前面提到，是Bengio 2003提出的。经过大牛的实现，效果确实不错。虽然复杂度没有数量级上的降低，但是由于是纯线性模型，没有激活函数（当然在做语言模型的时候，最后还是对$y_j$跑了一个softmax），因此实际的训练和预测速度都会有很大的提升。同时隐藏层到输出层的变量直接用了词向量，这也就几乎少了一半的变量，使得模型更为简洁。最后论文中M&amp;amp;H用了和Bengio 2003完全一样的数据集做实验，效果有了一定的提升。&lt;/p&gt;
&lt;p&gt;2008年NIPS的这篇论文，介绍的是“hierarchical log-bilinear”模型，很多论文中都把它称作简称“HLBL”。和前作相比，该方法使用了一个层级的结构做最后的预测。可以简单地设想一下把网络的最后一层变成一颗平衡二叉树，二叉树的每个非叶节点用于给预测向量分类，最后到叶节点就可以确定下一个词是哪个了。这在复杂度上有显著的提升，以前是对$|V|$个词一一做比较，最后找出最相似的，现在只需要做$log_2(|V|)$ 次判断即可。&lt;/p&gt;
&lt;p&gt;这种层级的思想最初可见于Frederic Morin和Yoshua Bengio于2005年发表的论文《Hierarchical probabilistic neural network language model》中。但是这篇论文使用WordNet中的IS-A关系，转化为二叉树用于分类预测。实验结果发现速度提升了，效果变差了。&lt;/p&gt;
&lt;p&gt;有了前车之鉴，M&amp;amp;H 就希望能从语料中自动学习出一棵树，并能达到比人工构建更好的效果。M&amp;amp;H使用一种bootstrapping的方法来构建这棵树。从随机的树开始，根据分类结果不断调整和迭代。最后得到的是一棵平衡二叉树，并且同一个词的预测可能处于多个不同的叶节点。这种用多个叶节点表示一个词的方法，可以提升下一个词是多义词时候的效果。M&amp;amp;H做的还不够彻底，后面Huang的工作直接对每个词学习出多个词向量，能更好地处理多义词。&lt;/p&gt;
&lt;h2&gt;Mikolov的RNNLM&lt;/h2&gt;
&lt;p&gt;前文说到，Bengio 2003论文里提了一句，可以使用一些方法降低参数个数，比如用循环神经网络。Mikolov就抓住了这个坑，从此与循环神经网络结下了不解之缘。他最早用循环神经网络做语言模型是在INTERSPEECH 2010上发表的《Recurrent neural network based language model》里。Recurrent neural network是循环神经网络，简称 RNN，还有个Recursive neural networks是递归神经网络（Richard Socher借此发了一大堆论文），也简称RNN。看到的时候需要注意区分一下。不过到目前为止，RNNLM只表示循环神经网络做的语言模型，还没有歧义。&lt;/p&gt;
&lt;p&gt;在之后的几年中，Mikolov在一直在RNNLM上做各种改进，有速度上的，也有准确率上的。现在想了解RNNLM，看他的博士论文《Statistical Language Models based on Neural Networks》肯定是最好的选择。&lt;/p&gt;
&lt;p&gt;循环神经网络与前面各方法中用到的前馈网络在结构上有比较大的差别，但是原理还是一样的。网络结构大致如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="RNNLM" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/rnnlm_zpse53d7de5.png" /&gt;&lt;/p&gt;
&lt;p&gt;左边是网络的抽象结构，由于循环神经网络多用在时序序列上，因此里面的输入层、隐藏层和输出层都带上了“(t)”。$w(t)$是句子中第$t$个词的One-hot representation的向量，也就是说$w$是一个非常长的向量，里面只有一个元素是 1。而下面的 $s(t−1)$向量就是上一个隐藏层。最后隐藏层计算公式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
s(t)=sigmoid(Uw(t)+Ws(t−1))
\end{equation}&lt;/p&gt;
&lt;p&gt;从右图可以看出循环神经网络是如何展开的。每来一个新词，就和上一个隐藏层联合计算出下一个隐藏层，隐藏层反复利用，一直保留着最新的状态。各隐藏层通过一层传统的前馈网络得到输出值。&lt;/p&gt;
&lt;p&gt;$w(t)$是一个词的 One-hot representation，那么$Uw(t)$也就相当于从矩阵$U$中选出了一列，这一列就是该词对应的词向量。&lt;/p&gt;
&lt;p&gt;循环神经网络的最大优势在于，可以真正充分地利用所有上文信息来预测下一个词，而不像前面的其它工作那样，只能开一个 $n$个词的窗口，只用前$n$个词来预测下一个词。从形式上看，这是一个非常“终极”的模型，毕竟语言模型里能用到的信息，他全用上了。可惜的是，循环神经网络形式上非常好看，使用起来却非常难优化，如果优化的不好，长距离的信息就会丢失，甚至还无法达到开窗口看前若干个词的效果。Mikolov在RNNLM里面只使用了最朴素的BPTT优化算法，就已经比n-gram中的state of the art方法有更好的效果，这非常令人欣慰。如果用上了更强的优化算法，最后效果肯定还能提升很多。&lt;/p&gt;
&lt;p&gt;对于最后隐藏层到输出层的巨大计算量，Mikolov使用了一种分组的方法：根据词频将$|V|$个词分成$\sqrt{|V|}$组，先通过 $\sqrt{|V|}$次判断，看下一个词属于哪个组，再通过若干次判断，找出其属于组内的哪个元素。最后均摊复杂度约为 $O(\sqrt{|V|})$，略差于M&amp;amp;H的$O(log(|V|))$，但是其浅层结构某种程度上可以减少误差传递，也不失为一种良策。&lt;/p&gt;
&lt;p&gt;Mikolov的&lt;a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/"&gt;RNNLM&lt;/a&gt;也是开源的。非常算法风格的代码，几乎所有功能都在一个文件里，工程也很好编译。比较好的是，RNNLM可以完美支持中文，如果语料存成UTF-8格式，就可以直接用了。&lt;/p&gt;
&lt;p&gt;最后吐槽一句，我觉得他在隐藏层用sigmoid作为激活函数不够漂亮。&lt;strong&gt;因为隐藏层要和输入词联合计算得到下一个隐藏层，如果当前隐藏层的值全是正的，那么输入词对应的参数就会略微偏负(?)&lt;/strong&gt;，也就是说最后得到的词向量的均值不在0附近。总感觉不好看。当然，从实验效果看，是我太强迫症了。&lt;/p&gt;
&lt;h2&gt;Huang的语义强化&lt;/h2&gt;
&lt;p&gt;与前几位大牛的工作不同，Eric H. Huang的工作是在C&amp;amp;W的基础上改进而成的，并非自成一派从头做起。他这篇发表在ACL 2012上的《Improving Word Representations via Global Context and Multiple Word Prototypes》试图通过对模型的改进，使得词向量富含更丰富的语义信息。他在文中提出了两个主要创新来完成这一目标：（其实从论文标题就能看出来）第一个创新是使用全文信息辅助已有的局部信息，第二个创新是使用多个词向量来表示多义词。下面逐一介绍。&lt;/p&gt;
&lt;p&gt;Huang认为C&amp;amp;W的工作只利用了“局部上下文（Local Context）”。C&amp;amp;W 在训练词向量的时候，只使用了上下文各5个词，算上自己总共有11个词的信息，这些局部的信息还不能充分挖掘出中间词的语义信息。Huang直接使用C&amp;amp;W的网络结构计算出一个得分，作为“局部得分”。&lt;/p&gt;
&lt;p&gt;然后Huang提出了一个“全局信息”，这有点类似传统的词袋子模型。词袋子模型是把文章中所有词的One-hot Representation 加起来，形成一个向量（就像把词全都扔进一个袋子里），用来表示文章。Huang的全局模型是将文章中所有词的词向量求个加权平均（权重是词的$idf$），作为文章的语义。他把文章的语义向量和当前词的词向量拼接起来，形成一个两倍长度的向量作为输入，之后还是用C&amp;amp;W的网络结构算出一个打分。&lt;/p&gt;
&lt;p&gt;有了C&amp;amp;W方法的得到的“局部得分”，再加上在C&amp;amp;W方法基础上改造得到的“全局得分”，Huang直接把两个得分相加，作为最终得分。最终得分使用 C&amp;amp;W 提出的pair-wise目标函数来优化。&lt;/p&gt;
&lt;p&gt;加了这个全局信息有什么用处呢？Huang在实验中发现，他的模型能更好地捕捉词的语义信息。比如C&amp;amp;W的模型中，与markets最相近的词为firms、industries；而Huang的模型得到的结果是market、firms。很明显，C&amp;amp;W的方法由于只考虑了临近词的信息，最后的结果是词法特征最相近的词排在了前面（都是复数形式）。不过我觉得这个可能是英语才有的现象，中文没有词形变化，如果在中文中做同样的实验还不知道会有什么效果。&lt;/p&gt;
&lt;p&gt;Huang论文的第二个贡献是将多义词用多个词向量来表示。Bengio 2003在最后提过这是一个重要的问题，不过当时他还在想办法解决，现在Huang给出了一种思路。&lt;/p&gt;
&lt;p&gt;将每个词的上下文各$5$个词拿出来，对这$10$个词的词向量做加权平均（同样使用$idf$作为权重）。对所有得到的上下文向量做k-means聚类，根据聚类结果给每个词打上标签（不同类中的同一个词，当作不同的词处理），最后重新训练词向量。&lt;/p&gt;
&lt;p&gt;当然这个实验的效果也是很不错的，最后star的某一个表示最接近的词是movie、film；另一个表示最接近的词是galaxy、planet。
　　
这篇文章还做了一些对比实验，在下一章评价里细讲。&lt;/p&gt;
&lt;h1&gt;词向量的评价&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;词向量的评价大体上可以分成两种方式，第一种是把词向量融入现有系统中，看对系统性能的提升；第二种是直接从语言学的角度对词向量进行分析，如相似度、语义偏移等。&lt;/p&gt;
&lt;h2&gt;提升现有系统&lt;/h2&gt;
&lt;p&gt;词向量的用法最常见的有两种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接用于神经网络模型的输入层。如C&amp;amp;W的SENNA系统中，将训练好的词向量作为输入，用前馈网络和卷积网络完成了词性标注、语义角色标注等一系列任务。再如Socher将词向量作为输入，用递归神经网络完成了句法分析、情感分析等多项任务。&lt;/li&gt;
&lt;li&gt;作为辅助特征扩充现有模型。如Turian将词向量作为额外的特征加入到接近state of the art的方法中，进一步提高了命名实体识别和短语识别的效果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;具体的用法理论上会在下一篇博文中细讲。
　　
C&amp;amp;W的论文中有一些对比实验。实验的结果表明，使用词向量作为初始值替代随机初始值，其效果会有非常显著的提升（如：词性标注准确率从96.37%提升到97.20%；命名实体识别F值从81.47%提升到88.67%）。同时使用更大的语料来训练，效果也会有一些提升。&lt;/p&gt;
&lt;p&gt;Turian发表在ACL 2010上的实验对比了C&amp;amp;W向量与M&amp;amp;H向量用作辅助特征时的效果。在短语识别和命名实体识别两个任务中，C&amp;amp;W 向量的效果都有略微的优势。同时他也发现，如果将这两种向量融合起来，会有更好的效果。除了这两种词向量，Turian 还使用Brown Cluster作为辅助特征做了对比，效果最好的其实是Brown Cluster，不过这个已经超出本文的范围了。&lt;/p&gt;
&lt;h2&gt;语言学评价&lt;/h2&gt;
&lt;p&gt;Huang 2012的论文提出了一些创新，能提升词向量中的语义成分。他也做了一些实验对比了各种词向量的语义特性。实验方法大致就是将词向量的相似度与人工标注的相似度做比较。最后Huang的方法语义相似度最好，其次是C&amp;amp;W向量，再然后是Turian训练的HLBL向量与C&amp;amp;W向量。这里因为Turian训练词向量时使用的数据集（RCV1）与其他的对比实验（Wiki）并不相同，因此并不是非常有可比性。但从这里可以推测一下，可能更大更丰富的语料对于语义的挖掘是有帮助的。&lt;/p&gt;
&lt;p&gt;还有一个有意思的分析是Mikolov在2013年刚刚发表的一项发现。他发现两个词向量之间的关系，可以直接从这两个向量的差里体现出来。向量的差就是数学上的定义，直接逐位相减。比如$C(king)−C(queen) \approx C(man)−C(woman)$。更强大的是，与$C(king)−C(man)+C(woman)$最接近的向量就是$C(queen)$。&lt;/p&gt;
&lt;p&gt;为了分析词向量的这个特点， Mikolov使用类比&lt;strong&gt;（analogy）&lt;/strong&gt;的方式来评测。如已知$a$之于$b$犹如$c$之于$d$。现在给出 $a、b、c$，看$C(a)−C(b)+C(c)$最接近的词是否是$d$。&lt;/p&gt;
&lt;p&gt;在文章中Mikolov对比了词法关系（名词单复数good-better:rough-rougher、动词第三人称单数、形容词比较级最高级等）和语义关系（clothing-shirt:dish-bowl）。&lt;/p&gt;
&lt;p&gt;在词法关系上，RNN的效果最好，然后是Turian实现的HLBL，最后是Turian的C&amp;amp;W。（RNN-80:19%；RNN-1600:39.6%；HLBL-100:18.7%；C&amp;amp;W-100:5%；-100表示词向量为100维）&lt;/p&gt;
&lt;p&gt;在语义关系上，表现最好的还是$RNN$，然后是$Turian$的两个向量，差距没刚才的大。（RNN-80:0.211；C&amp;amp;W-100:0.154；HLBL-100:0.146）&lt;/p&gt;
&lt;p&gt;但是这个对比实验用的训练语料是不同的，也不能特别说明优劣。&lt;/p&gt;
&lt;p&gt;这些实验结果中最容易理解的是：&lt;strong&gt;语料越大，词向量就越好&lt;/strong&gt;。其它的实验由于缺乏严格控制条件进行对比，谈不上哪个更好哪个更差。不过这里的两个语言学分析都非常有意思，尤其是向量之间存在这种线性平移的关系，可能会是词向量发展的一个突破口。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Jauvin. &lt;strong&gt;A neural probabilistic language model&lt;/strong&gt;. Journal of Machine Learning Research (JMLR), 3:1137–1155, 2003. &lt;a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ronan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa. &lt;strong&gt;Natural Language Processing (Almost) from Scratch&lt;/strong&gt;. Journal of Machine Learning Research (JMLR), 12:2493-2537, 2011. &lt;a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Andriy Mnih &amp;amp; Geoffrey Hinton. &lt;strong&gt;Three new graphical models for statistical language modelling. International Conference on Machine Learning (ICML)&lt;/strong&gt;. 2007. &lt;a href="http://www.cs.utoronto.ca/~hinton/absps/threenew.pdf"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Andriy Mnih &amp;amp; Geoffrey Hinton.&lt;strong&gt;A scalable hierarchical distributed language model&lt;/strong&gt;. The Conference on Neural Information Processing Systems (NIPS) (pp. 1081–1088). 2008. &lt;a href="http://www.cs.utoronto.ca/~hinton/absps/andriytree.pdf"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mikolov Tomáš. &lt;strong&gt;Statistical Language Models based on Neural Networks&lt;/strong&gt;. PhD thesis, Brno University of Technology. 2012. &lt;a href="http://www.fit.vutbr.cz/~imikolov/rnnlm/thesis.pdf"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Turian Joseph, Lev Ratinov, and Yoshua Bengio. &lt;strong&gt;Word representations: a simple and general method for semi-supervised learning&lt;/strong&gt;. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL). 2010. &lt;a href="http://www.aclweb.org/anthology-new/P/P10/P10-1040.pdf"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Eric Huang, Richard Socher, Christopher Manning and Andrew Ng.&lt;strong&gt;Improving word representations via global context and multiple word prototypes&lt;/strong&gt;. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. 2012. &lt;a href="http://www-nlp.stanford.edu/pubs/HuangACL12.pdf"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mikolov, Tomas, Wen-tau Yih, and Geoffrey Zweig.&lt;strong&gt;Linguistic regularities in continuous space word representations&lt;/strong&gt;. Proceedings of NAACL-HLT. 2013. &lt;a href="https://www.aclweb.org/anthology/N/N13/N13-1090.pdf"&gt;PDF&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Deep Learning"></category><category term="NLP"></category></entry><entry><title>RSA是如何炼成的</title><link href="http://www.qingyuanxingsi.com/rsashi-ru-he-lian-cheng-de.html" rel="alternate"></link><updated>2014-05-09T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-09:rsashi-ru-he-lian-cheng-de.html</id><summary type="html">&lt;p&gt;最近为了研究某个极其无聊的问题，读了一些公钥加密的历史，意外地发现这段历史竟然非常有趣。尤其是&lt;strong&gt;RSA&lt;/strong&gt;算法的诞生过程，被很多书写得非常励志，看得人热血澎湃。果然比起算法本身，这些背后的故事更能吸引我的兴趣。&lt;/p&gt;
&lt;p&gt;RSA算法具体是怎么回事，我就不在这瞎说了。简介可以看&lt;a href="http://en.wikipedia.org/wiki/RSA_(cryptosystem)"&gt;Wikipedia&lt;/a&gt;，如果想形象一点理解算法本身，这儿有个&lt;a href="http://v.youku.com/v_show/id_XNDQ0NTE3MDA0.html"&gt;不错的视频&lt;/a&gt;，可以通过它了解RSA的基本思想。我就直接从RSA这三个人说起了。参考的书籍资料列在文末。&lt;/p&gt;
&lt;h1&gt;RSA背后的三个小伙&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;RSA是由三个提出者Ron Rivest、Adi Shamir和Leonard Adleman的姓氏首字母组成的。这三个人风格迥异，组成了一个技能互补的完美团队。&lt;/p&gt;
&lt;p&gt;Ron Rivest，RSA中的R。他在耶鲁读数学系，随后跑到斯坦福读计算机科学系研究人工智能。他所研究的课题——让机器人在没有人干预的情况下在停车场散步，在那个计算机科学系仅成立四年的年代明显太过乐观，但他依然乐此不疲。毕业后他接受MIT任教的工作机会。也许是因为多年积累的科研氛围，他对新技术非常兴奋，大量阅读前沿文献。与此同时，他认为迷人的理论应该与现实世界相结合，才能散发魅力，对世界有所改变，这也是他的理想。&lt;/p&gt;
&lt;p&gt;Adi Shamir，RSA中的S。他是以色列人，和Rivest一样，学数学后转计算机科学进一步深造，毕业后以访问学者的身份来到 MIT。他很聪明，学习能力超强。虽然他在数学上的造诣颇深，但起初他在算法方面的知识十分匮乏，当他接到Rivest关于算法高级课程讲授的邀请信时连连叫苦，教算法已经够呛了，还什么高级课程？给博士生讲的么？虽然如此，他还是硬着头皮前往 MIT，之后很快投入到学习中，整天泡在图书馆，读了一书架关于算法的书籍，最终仅用两周便掌握了所需的知识。&lt;/p&gt;
&lt;p&gt;Leonard Adleman，RSA中的A。自幼胸无大志，从未想过做什么数学家。读大学时受各方面影响，甚至包括电视节目的影响，在专业选择上犹豫不决，最终因为学数学会有大量时间做别的而选择就读数学系。毕业后在美国银行做程序员，之后想去学医，被录取了却又改变主意想研究物理，上了几堂课又觉得没意思。最后他怕挂科对找工作有影响，跑去图书馆借了本计算机科学的书，一直没还，学校就会因此扣留成绩单。辗转几次后，他最终选择读计算机科学 PhD。毕业后同样在 MIT 任教。&lt;/p&gt;
&lt;p&gt;这三人当时都非常年轻，二十多、三十出头的样子。他们的办公室距离很近，可以经常串门。于是故事就从一次Adleman 的串门开始了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="RSA" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/RSA_zpsa58c4c16.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;[注:自左至右：&lt;em&gt;Adi Shamir, Ron Rivest, Leonard Adleman&lt;/em&gt;]&lt;/p&gt;
&lt;h1&gt;42次的失败&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;1976年底的某天，Adleman无意推开Rivest的房门。热爱新技术的Rivest果不其然正拿着份楼上的Whitfield Diffie与Martin Hellman合作发表的新论文研究。自认为对前沿理念无所不晓的Rivest，没料到这篇文章提出了一个前所未有的思路，这让他兴奋不已。正琢磨着，Adleman推门进来了，Rivest便忘我地向Adleman讲解了这篇论文中所述的思想，在Adleman听起来大约是这样的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;公钥密码 blah blah blah…不对称密码 blah blah blah…单向函数 blah blah blah…但是符合条件的单向函数目前没找到。我有信心找到这样的单向函数，你看你要不要和我一起试试？&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这些显然不足以让早已下决心走纯理论路线的Adleman动心，对此他只是报以一个“礼貌的哈欠”。想当初选择读计算机科学，除了方便找工作，Adleman还深受马丁加德纳专栏的影响。专栏中写到的哥德尔定理让他感到惊艳，深深体会到数学之美，如今只有高斯之流方能入他的眼，眼下Rivest滔滔不绝的什么加密解密，在他看来既不高级也不好玩。&lt;/p&gt;
&lt;p&gt;好在Rivest拉到了另一个同盟，也就是隔壁的Shamir。Shamir一听说这篇文章就立刻意识到它的价值，二人一拍即合，开始了他们昼夜不休的单向函数寻找之旅。因为两人都头脑灵活，很快就想到了一些方案。尽管Adleman不情愿参与其中，他们还是会把结果拿给 Adleman，Adleman的角色就是逐个击破这些方案，找出各种漏洞，给那两个头脑发热的人泼点冷水，免得他们走弯路。&lt;/p&gt;
&lt;p&gt;三人走火入魔一般，吃饭聊、喝酒聊，甚至去滑雪度假也不忘讨论这件事。Shamir就在滑雪的时候想到了一个绝妙的点子，以至于把滑雪板都落在了身后。当他意识到这一点回头去取时，却又不幸忘记了那个一闪而过的点子。相对来说给他们启发的Diffie要幸运一些，他在下楼买可乐的时候同样让灵感溜走，好在上楼的过程中他又想起来了，这个差点溜走的灵感正是Rivest那天手中所拿论文的核心思想，为RSA算法奠定了基础。&lt;/p&gt;
&lt;p&gt;起初Rivest和Shamir构造出来的算法很快就能被Adleman破解，二人受到强烈的打击，以至于有一阶段他们走向了另一个极端，试图证明Diffie他们的想法根本就是不靠谱的。但慢慢的，破解变得没那么容易，特别是他们的第32号方案，Adleman用了一晚上才找出漏洞，这让他们感觉胜利就在眼前。&lt;/p&gt;
&lt;p&gt;就这样，Rivest和Shamir先后抛出了42个方案，虽然这42个全部被Adleman击破，不过他们的努力并不算白费，至少指出了42条错误的路线。&lt;/p&gt;
&lt;h1&gt;算法的诞生与命名&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;1977年4月，Rivest和其余两人参加了犹太逾越节的Party，喝了些酒。到家后Rivest睡不着，随手翻了翻数学书，随后一个灵感逐渐清晰起来。他大气不敢出一口，冷静下来连夜整理自己的思路，一气呵成写就了一篇论文。次日，Rivest把论文拿给Adleman，做好再一次徒劳的心理准备，但这一次Adleman认输了，认为这个方案应该是可行的。&lt;/p&gt;
&lt;p&gt;按照惯例，Rivest按姓氏字母序将三人的名字署在论文上，也就是Adleman、Rivest、Shamir，但Adleman总觉得自己贡献微乎其微，不过是泼泼冷水，不至于还要署个名，便要求Rivest拿掉自己的名字。在Rivest的坚持下，他最终要求至少把自己的名字放到最后。也正因为如此，RSA叫做RSA没有被叫做ARS。虽然Adleman一开始认为这注定是他诸多论文中最不起眼的一篇，RSA 走红后他还是调侃说，越来越觉得 ARS 更顺口了。&lt;/p&gt;
&lt;h1&gt;之后&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;之后的历史我们就非常熟悉了，他们的论文受到各方赞赏。Rivest还把论文发给马丁加德纳一份，加德纳非常感兴趣，把Rivest请到家里面谈，进一步了解RSA算法。当然此前，加德纳还不忘先表演一个扑克魔术。这次会面也促成后来马丁加德纳在他著名的专栏刊登RSA算法及破解奖金的故事。至于R、S、A这三人，依然保持着友谊，还成立了RSA公司，赚了一大笔钱。&lt;/p&gt;
&lt;p&gt;最后，既然RSA是根据提出者命名的，必然也逃不出&lt;a href="http://en.wikipedia.org/wiki/Stigler%27s_law_of_eponymy"&gt;Stigler’s law&lt;/a&gt;的魔爪。的确从时间上来说，RSA这三人并非最早提出这个算法的人。事实上早于这三人四年时间，RSA 算法的思想就被英国学者构造出来了。早在1969年，英国密码学家James Ellis 就想到了公钥密码的概念，但同样卡在寻找单向函数这个问题上。1973年9月，年仅26岁的数学家Clifford Cocks听说这个思想后，在完全不了解状况的心理状态下花不到半小时就找到了Rivest他们苦思冥想的方案。但是，他们效力于政府，这个绝妙的想法立刻被相关机构封锁，变为机密，谁也不能对外公开自己的发现，于是他们眼睁睁地看着 Diffie 及 RSA 等人重现了他们当时的研究并享有盛誉。直到1997年底，时隔二十余年，这件事情才被公之于众。遗憾的是那时候Ellis 已经过世一个月，直至逝世都是一个无名英雄。&lt;/p&gt;
&lt;h1&gt;参考资料&lt;/h1&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.amazon.cn/Crypto-How-the-Code-Rebels-Beat-the-Government-Saving-Privacy-in-the-Digital-Age-Levy-Steven/dp/0140244328/ref=sr_1_1?ie=UTF8&amp;amp;qid=1388319023"&gt;Crypto: How the Code Rebels Beat the Government–Saving Privacy in the Digital Age&lt;/a&gt;;Steven Levy所著，大部分内容都是从这本书了解到的。书里从Whitfield Diffie的八卦（是真的八卦，和他老婆的相遇之类）说起，一直说到非常现实的NSA涉入等情节，写得很详细很有趣。中译本译作《隐私的终结》，但翻译水平非常有限，比如把爱伦坡译成艾伦坡和阿兰坡两种不同译法，把cutting-edge翻译成边缘，或者干脆把算法水平有待提高翻译成精通算法什么的…&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.amazon.cn/The-Code-Book-The-Science-of-Secrecy-from-Ancient-Egypt-to-Quantum-Cryptography-Singh-Simon/dp/0385495323/ref=sr_1_1"&gt;The Code Book: The Science of Secrecy from Ancient Egypt to Quantum Cryptography&lt;/a&gt;;中译本《密码故事》，除了密码从古至今的演变，书里还单独对Ellis和Cocks等人的工作做了详细的讲述。我还没看完这本书，但感觉会很有意思。关于Adleman的更多八卦可以看&lt;a href="http://www.nytimes.com/1994/12/13/science/scientist-at-work-leonard-adleman-hitting-the-high-spots-of-computer-theory.html"&gt;这篇采访&lt;/a&gt;，总觉得他的气场和其余两人很不搭，各种变卦和无所谓。啊对了，Adleman也是将计算机病毒命名为Computer virus的人。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Original Link&lt;/strong&gt;:&lt;a href="http://localhost-8080.com/2013/12/history-of-rsa/"&gt;RSA 算法是如何诞生的&lt;/a&gt;.&lt;/p&gt;</summary><category term="RSA Algorithm"></category><category term="历史钩沉"></category></entry><entry><title>机器学习番外篇之在线学习(I):Online Learning与感知器</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-fan-wai-pian-zhi-zai-xian-xue-xi-ionline-learningyu-gan-zhi-qi.html" rel="alternate"></link><updated>2014-05-08T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-08:ji-qi-xue-xi-fan-wai-pian-zhi-zai-xian-xue-xi-ionline-learningyu-gan-zhi-qi.html</id><summary type="html">&lt;p&gt;我们以前讨论的都是批量学习的方法(batch learning),即给定一堆训练样本,我们一次对样本进行训练,然后得到对于模型参数的估计,用得到的模型预测未知样本。而&lt;strong&gt;在线学习&lt;/strong&gt;就是要根据新来的样例，边学习，边给出结果。&lt;/p&gt;
&lt;p&gt;假设样例按照到来的先后顺序依次定义为$((x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\cdots,(x^{(m)},y^{(m)}))$。$X$为样本特征，$y$为类别标签。我们的任务是到来一个样例$x$，给出其类别结果$y$的预测值，之后我们会看到$y$的真实值，然后根据真实值来重新调整模型参数，整个过程是重复迭代的过程，直到所有的样例完成。这么看来，我们也可以将原来用于批量学习的样例拿来作为在线学习的样例。在在线学习中我们主要关注在整个预测过程中预测错误的样例数。&lt;/p&gt;
&lt;p&gt;拿二值分类来讲，我们用$y=1$表示正例，$y=-1$表示负例。假设我们采用感知器算法&lt;strong&gt;（Perception algorithm）&lt;/strong&gt;进行学习。我们的假设函数为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
h_{\theta}(x) = g(\theta^T x)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$x$是$n+1$维特征向量,最后一维为常量$1$，$\theta$是$n+1$维参数权重,最后一维表示Bias。函数$g$用来将$\theta^Tx$计算结果映射到$-1$和$1$上。具体公式如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
g(z) = \left \lbrace
\begin{array}{cc}
1 &amp;amp; \text{if} \  z \geq 0 \\
-1 &amp;amp; \text{if} \ z &amp;lt; 0 
\end{array}
\right.
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;这个也是logistic回归中$g$的简化形式。&lt;/p&gt;
&lt;p&gt;现在我们提出一个在线学习算法如下：&lt;/p&gt;
&lt;p&gt;新来一个样例$(x,y)$，我们先用从之前样例学习到的$h_{\theta}(x)$来得到样例的预测值$y$，如果$h_{\theta}(x) = y$（即预测正确），那么不改变$\theta$，反之&lt;/p&gt;
&lt;p&gt;\begin{equation}
\theta := \theta + yx
\end{equation}&lt;/p&gt;
&lt;p&gt;也就是说，如果对于预测错误的样例，$\theta$进行调整时只需加上（实际上为正例）或者减去（实际负例）样本特征$x$值即可。$\theta$初始值为向量0。这里我们关心的是$\theta^Tx$的符号，而不是它的具体值。调整方法非常简单。然而这个简单的调整方法还是很有效的，它的错误率不仅是有上界的，而且这个上界不依赖于样例数和特征维度。&lt;/p&gt;
&lt;p&gt;下面定理阐述了错误率上界：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;定理（Block and Novikoff）&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;给定按照顺序到来的$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)},\cdots,(x^{(m)},y^{(m)}))$样例。假设对于所有的样例$||x^{(i)} \leq D$，也就是说特征向量长度有界为$D$。更进一步，假设存在一个单位长度向量$u$且$y^{(i)}(u^Tx^{(i)})\geq \gamma$。也就是说对于$y=1$的正例，$u^Tx^{(i)} \geq \gamma$，反例$u^Tx^{(i)} \leq -\gamma$，$u$能够有$\gamma$的间隔将正例和反例分开。那么感知算法的预测的错误样例数不超过$({D \over \gamma})^2$。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;根据对SVM的理解，这个定理就可以阐述为：如果训练样本线性可分，并且几何间距至少是$\gamma$，样例样本特征向量最长为$D$，那么感知算法错误数不会超过$({D \over \gamma})^2$。这个定理是62年提出的，63年Vapnik提出SVM，可见提出也不是偶然的，感知算法也许是当时的热门。&lt;/p&gt;
&lt;p&gt;下面主要讨论这个定理的证明：&lt;/p&gt;
&lt;p&gt;感知算法只在样例预测错误时进行更新，定义$\theta^{(k)}$是第$k$次预测错误时使用的样本特征权重，$\theta^{(1)} = \vec{0}$ 初始化为$\vec{0}$向量。假设第$k$次预测错误发生在样例$(x^{(i)},y^{(i)})$上，利用$\theta^{(k)}$计算$y^{(i)}$值时得到的结果不正确。也就是说下面的公式成立：&lt;/p&gt;
&lt;p&gt;\begin{equation}
(x^{(i)})^T\theta^{(k)}y^{(i)} \leq 0
\end{equation}&lt;/p&gt;
&lt;p&gt;根据感知算法的更新方法，我们有$\theta^{(k+1)} = \theta^{(k)} + y^{(i)}x^{(i)}$。这时候，两边都乘以$u$得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
(\theta^{(k+1)})^Tu &amp;amp;= (\theta^{(k)})^u + y^{(i)}(x^{(i)})^Tu  \\
&amp;amp;\geq (\theta^{(k)})^Tu + \gamma
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;这个式子是个递推公式，就像等差数列一样$f_{n+1}=f_n+d$。由此我们可得&lt;/p&gt;
&lt;p&gt;\begin{equation}
(\theta^{(k+1)})^Tu \geq k\gamma
\end{equation}&lt;/p&gt;
&lt;p&gt;因为初始$\theta$为$\vec{0}$。&lt;/p&gt;
&lt;p&gt;下面我们利用前面推导出的$(x^{(i)})^T\theta^{(k)}y^{(i)} \leq 0$和$||x^{(i)}|| \leq D$得到&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
||\theta^{(k+1)}||^2 &amp;amp;= ||\theta^{(k)} + y^{(i)}x^{(i)}||^2    \\
&amp;amp;= ||\theta^{k}||^2 + ||x^{(i)}||^2 + 2y^{(i)}(x^{(i)})^T\theta^{(i)} \\
&amp;amp;\leq ||\theta^{k}||^2 + ||x^{(i)}||^2 \\
&amp;amp;\leq ||\theta^{k}||^2 + D^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;也就是说$\theta^{(k+1)}$的长度平方不会超过$\theta^{(k)}$与$D$的平方和。&lt;/p&gt;
&lt;p&gt;又是一个等差不等式，得到：&lt;/p&gt;
&lt;p&gt;\begin{equation}
||\theta^{k+1}||^2 \leq kD^2
\end{equation}&lt;/p&gt;
&lt;p&gt;两边开根号得：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\sqrt{k}D &amp;amp; \geq ||\theta^{(k+1)}||   \\
&amp;amp; \geq (\theta^{(k+1)})^Tu  \\
&amp;amp; \geq k\gamma
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中第二步可能有点迷惑，我们细想$u$是单位向量的话:&lt;/p&gt;
&lt;p&gt;\begin{equation}
z^Tu = ||z||||u||cos \phi \leq ||z||||u||
\end{equation}&lt;/p&gt;
&lt;p&gt;因此上面的不等式成立，最后得到：&lt;/p&gt;
&lt;p&gt;\begin{equation}
k \leq (D/\gamma)^2
\end{equation}&lt;/p&gt;
&lt;p&gt;也就是预测错误的数目不会超过样本特征向量$x$的最长长度除以几何间隔的平方。实际上整个调整过程中$\theta$就是$x$的线性组合。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Original Link&lt;/strong&gt;:&lt;a href="http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020173.html"&gt;在线学习（Online Learning）&lt;/a&gt;&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="Perception"></category><category term="Online Learning"></category></entry><entry><title>小小收藏夹[持续更新中]</title><link href="http://www.qingyuanxingsi.com/xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html" rel="alternate"></link><updated>2014-05-07T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-07:xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html</id><summary type="html">&lt;h1&gt;NLP&lt;sup id="sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1-back"&gt;&lt;a class="simple-footnote" href="#sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1" title="Natural Language Processing"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;Relation Extration&lt;ul&gt;
&lt;li&gt;Hand-written approach more suitable for structured data,such as a telephone book,Facebook or eBay;&lt;/li&gt;
&lt;li&gt;Supervised Method;得到所有的命名实体组,使用一个分类器(&lt;em&gt;features&lt;/em&gt;)判断它们是否是关联的,如果是,则使用第二个分类器判断它们之间的关联关系具体是什么; &lt;/li&gt;
&lt;li&gt;Semi-Supervised(Relation Bootstrapping/Distant Supervised Learning) and unsupervised methods(Open Information Extraction);Strapping方法感觉很巧妙,个人很喜欢;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;SVM&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;对于SVM这么高端大气上档次的东西,当然要单独列出来。今天其他东西实在看不下去了,所以把Pluskid之前写的一系列讲SVM的文章再挖出来看看。以下是目录以及对每篇的简单说明:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=632"&gt;支持向量机: Maximum Margin Classifier&lt;/a&gt;;文中主要介绍了两个距离,&lt;em&gt;Functional Margin&lt;/em&gt; $\hat{\gamma}$和&lt;em&gt;Geometrical Margin&lt;/em&gt; $\tilde{\gamma}$.它们之间满足$\hat{\gamma} = ||w||\tilde{\gamma}$.我们固定$\hat{\gamma} = 1$,通过最大化$\frac{1}{||w||}$来得到&lt;strong&gt;Maximum Margin Classifier&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=682"&gt;支持向量机: Support Vector&lt;/a&gt;;简要介绍了Support Vector是指什么,另外对线性可分的情况利用Duality进行了推导并得出了两个比较重要的结论:&lt;ul&gt;
&lt;li&gt;对新点的预测只需要计算与训练点之间的内积即可;&lt;/li&gt;
&lt;li&gt;非支持向量不参与模型的计算过程之中。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=685"&gt;支持向量机: Kernel&lt;/a&gt;;&lt;strong&gt;Kernel&lt;/strong&gt;的基本思想。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=692"&gt;支持向量机：Outliers&lt;/a&gt;;通过引入松弛变量处理Outliers,而实际上最后的优化形式只是加上$\alpha_i \leq C$的限制。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=696"&gt;支持向量机：Numerical Optimization&lt;/a&gt;;以非常通俗易懂的方式介绍了一下&lt;strong&gt;SMO(Sequential Minimal Optimization)&lt;/strong&gt;,赞一个。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Deep Learning&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;稀疏编码(&lt;em&gt;Sparse Coding&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8777094"&gt;Deep Learning（深度学习）学习笔记整理系列之（五)&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep Learning总结&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8782018"&gt;Deep Learning（深度学习）学习笔记整理系列之（八)&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;受限Boltzmann机&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://pan.baidu.com/s/1gdh3P1x"&gt;A Brief Introduction to Restricted Boltzmann Machine&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Pocket&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://v.youku.com/v_show/id_XNzAwMTI0MDgw.html"&gt;罗辑思维 2014：右派为什么这么横 10&lt;/a&gt;;视频主要介绍了保守主义的三个特征,同时分析了人们在面临选择的时候的不同思维方式,个人觉得这一点很有借鉴意义，建议一看!&lt;/li&gt;
&lt;li&gt;&lt;a href="http://v.youku.com/v_show/id_XNzAzMTkyNDky.html"&gt;罗辑思维 2014：迷茫时代的明白人 11&lt;/a&gt;;活在当下。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;算法&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;今天看了一下网上流传的传说中的高大上的所谓的&lt;code&gt;十大海量数据处理算法&lt;/code&gt;,看了一下,实际上没有什么东西,唯独&lt;strong&gt;Bloom Filter&lt;/strong&gt;看着还挺好玩的,所以以下给出一个通俗易懂的链接。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.cnblogs.com/heaad/archive/2011/01/02/1924195.html"&gt;那些优雅的数据结构(1) : BloomFilter——大规模数据处理利器&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在过去的若干年里,有一个心结一直萦绕在我的心头挥之不去,它存在于我的脑海里，我的梦里，我的歌声里,TA就是&lt;strong&gt;B树&lt;/strong&gt;(好吧,其实是因为没有机会好好地研究一下它啦)。以下给出两个链接,它们主要介绍了B树的基本概念,性质以及针对B树的插入、删除操作,两个PPT还是相当直观的,应该能够比较直观地了解B树这个数据结构!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://cecs.wright.edu/~tkprasad/courses/cs707/L04-X-B-Trees.ppt"&gt;B-Trees&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://zh.scribd.com/doc/18210/B-TREE-TUTORIAL-PPT"&gt;B TREE TUTORIAL PPT&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;今天又重新看了一下这个写的很不错的&lt;strong&gt;A*算法&lt;/strong&gt;,恩,这篇文章想来是极好的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.raywenderlich.com/zh-hans/21503/a%E6%98%9F%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D"&gt;A星寻路算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;复习一下之前做智能提示时用到的&lt;strong&gt;Trie Tree&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.cs.umd.edu/class/fall2005/cmsc132/lecs/lec29.ppt"&gt;Indexed Search Tree (Trie) - Computer Science Department&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;看了一下传说中的数据挖掘十大算法,好像就&lt;strong&gt;Apriori算法&lt;/strong&gt;不是特别熟吧,所以重新看了一遍;个人觉得如果我早出生若干年,这种程度的算法也是能想出来的吧(我指思想).好吧,我认为着重要理解的有如下两点:&lt;ul&gt;
&lt;li&gt;Support;其实也就是某种组合在所有Transaction中出现的频度。&lt;/li&gt;
&lt;li&gt;Confidence;当生成关联规则$A\to B$时,有$confidence = \frac{Count(A,B)}{Count(A)}$,背后的Intuition就是如果我买了$A$,大概会有多大的可能买$B$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.cs.sjsu.edu/faculty/lee/cs157b/Gaurang%20Negandhi--Apriori%20Algorithm%20Presentation.ppt"&gt;Apriori Algorithm Review for Finals&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最近需要对1G的文本数据进行处理,所以想了解一下现行的分布式计算框架的应用场景,从而选择合适的框架用于这个任务,期间看到以下两篇文章写的很不错,特此摘录。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://langyu.iteye.com/blog/1407194"&gt;对实时分析与离线分析的思考&lt;/a&gt;
&lt;a href="http://langyu.iteye.com/blog/1407194"&gt;对实时分析与离线分析的思考(二)&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最近脑子总是不断宕机,宕机了就什么也看不了了,刚看了一篇一个人讲自己怎么学习算法的博文,感觉还不错(只是看了作者的经历,他看过的那些书还没来得及看)。以下给出链接:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://zh.lucida.me/blog/on-learning-algorithms/"&gt;我的算法学习之路&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Machine Learning&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;看的有点累了,不想看&lt;em&gt;EM&lt;/em&gt;算法复杂的数学公式推导了,所以找到之前看过的一篇,回顾一下,等以后想看了再详细介绍&lt;em&gt;Mixture Models&lt;/em&gt;和&lt;em&gt;EM&lt;/em&gt;算法吧!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blog.pluskid.org/?p=39"&gt;漫谈 Clustering (3): Gaussian Mixture Model&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;近期为了理解卷积,于是到处找资料,无意中发现了这一篇神一般的理解。(&lt;strong&gt;墙裂推荐&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.guokr.com/post/342476/"&gt;关于卷积的一个血腥的讲解，看完给跪了&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PCA数据预处理&lt;em&gt;Whitening&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;在使用PCA前需要对数据进行预处理，首先是均值化，即对每个特征维，都减掉该维的平均值，然后就是将不同维的数据范围归一化到同一范围，方法一般都是&lt;strong&gt;除以最大值&lt;/strong&gt;。但是比较奇怪的是，在对自然图像进行均值处理时并不是不是减去该维的平均值，而是减去这张图片本身的平均值。因为PCA的预处理是按照不同应用场合来定的。&lt;/p&gt;
&lt;p&gt;自然图像指的是人眼经常看见的图像，其符合某些统计特征。一般实际过程中，只要是拿正常相机拍的，没有加入很多人工创作进去的图片都可以叫做是自然图片，因为很多算法对这些图片的输入类型还是比较鲁棒的。在对自然图像进行学习时，其实不需要太关注对图像做方差归一化，因为自然图像每一部分的统计特征都相似，只需做均值为0化就ok了。不过对其它的图片进行训练时，比如首先字识别等，就需要进行方差归一化了。&lt;/p&gt;
&lt;p&gt;有一个观点需要注意，那就是&lt;strong&gt;PCA并不能阻止过拟合现象&lt;/strong&gt;。表明上看PCA是降维了，因为在同样多的训练样本数据下，其特征数变少了，应该是更不容易产生过拟合现象。但是在实际操作过程中，这个方法阻止过拟合现象效果很小，主要还是通过&lt;strong&gt;规则项&lt;/strong&gt;来进行阻止过拟合的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whitening&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whitening&lt;/strong&gt;的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。数据的Whitening必须满足两个条件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不同特征间相关性最小，接近0；&lt;/li&gt;
&lt;li&gt;所有特征的方差相等（不一定为1）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的白化操作有PCA whitening和ZCA whitening。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PCA whitening&lt;/em&gt;是指将数据x经过PCA降维为z后，可以看出z中每一维是独立的，满足whitening白化的第一个条件，这是只需要将z中的每一维都除以标准差就得到了每一维的方差为1，也就是说方差相等。公式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_{PCAwhite,i} = \frac{x_{rot,i}}{\sqrt{\lambda_i}}
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ZCA whitening&lt;/em&gt;是指数据x先经过PCA变换为z，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足whtienning的第一个条件，特征间相互独立。然后同样进行方差为1的操作，最后将得到的矩阵左乘一个特征向量矩阵U即可。ZCA whitening公式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_{ZCAwhite} = Ux_{PCAwhite}
\end{equation}&lt;/p&gt;
&lt;p&gt;参考&lt;a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/21/2973231.html"&gt;Deep learning：十(PCA和whitening)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;最近一直在看&lt;strong&gt;高斯过程&lt;/strong&gt;,挺难理解的,好吧,咱们慢慢来,先给个链接。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.eurandom.tue.nl/events/workshops/2010/YESIV/Prog-Abstr_files/Ghahramani-lecture2.pdf"&gt;Introduction to Gaussian Process&lt;/a&gt;
* 今天看自然语言处理Standford公开课的时候看到最大熵模型(Maximum Entropy Models),视频讲的实在太罗嗦了,在网上找了找,下面这个PPT貌似还挺不错的。(原始PPT有部分错误,以下网盘共享文件是部分修正后版本,可能还会有错误,欢迎指出)&lt;/p&gt;
&lt;p&gt;&lt;a href="http://pan.baidu.com/s/1gdze7h5"&gt;Maximum Entropy Model&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;EM/pLSA/LDA的一些参考资料&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006.&lt;/li&gt;
&lt;li&gt;Gregor Heinrich. Parameter estimation for text analysis. Technical report, 2004.&lt;/li&gt;
&lt;li&gt;Wayne Xin Zhao, Note for pLSA and LDA, Technical report, 2011.&lt;/li&gt;
&lt;li&gt;CX Zhai, A note on the expectation-maximization (em) algorithm 2007&lt;/li&gt;
&lt;li&gt;Qiaozhu Mei, A Note on EM Algorithm for Probabilistic Latent Semantic Analysis 2008&lt;/li&gt;
&lt;li&gt;Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008.&lt;/li&gt;
&lt;li&gt;Freddy Chong Tat Chua. Dimensionality reduction and clustering of text documents.Technical report, 2009.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LDA算法伪代码实现参考&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.arbylon.net/projects/"&gt;Gregor Heinrich’s LDA-J&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml1-2008.html"&gt;Yee Whye Teh’s Gibbs LDA Matlab codes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm"&gt;Mark Steyvers and Tom Griffiths’s topic modeling matlab toolbox&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://gibbslda.sourceforge.net/"&gt;GibbsLDA++&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A &lt;strong&gt;Very&lt;/strong&gt; Brief Introduction about Semi-supervised Learning.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://pan.baidu.com/s/1pJqGSSN"&gt;SEMI_SUPERVISED LEARNING&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;PGM&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;以下给出讲解PGM比较深入浅出的一系列Lecture Slides。&lt;/p&gt;
&lt;h2&gt;PART I:Introduction to PGM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-1-Introduction-Combined.pdf"&gt;Introduction and Overview&lt;/a&gt;;主要介绍了PGM的背景以及Factor的基础知识。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Bayes-Nets.pdf"&gt;Bayesian Network Fundamentals&lt;/a&gt;;简要介绍了什么是Bayesian Network、Reasoning Patterns以及Influence Flow.最后简要介绍了一下Naive Bayes Classifier.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Naive Bayes Classifier" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/naive_bayes_model_zps09771da2.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Template-Models.pdf"&gt;Template Models&lt;/a&gt;;主要介绍了Template Models,包括Bayesian Network(HMM)以及Plate Models;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-CPDs.pdf"&gt;Structured CPDs&lt;/a&gt;;介绍了几种CPD表示的其他常见形式,包括:&lt;ul&gt;
&lt;li&gt;Deterministic CPDs&lt;/li&gt;
&lt;li&gt;Tree-structured CPDs&lt;/li&gt;
&lt;li&gt;Logistic CPDs &amp;amp; generalizations&lt;/li&gt;
&lt;li&gt;Noisy OR/AND&lt;/li&gt;
&lt;li&gt;Linear Gaussian &amp;amp; generalizations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Markov-Nets.pdf"&gt;Markov Network Fundamentals&lt;/a&gt;;本部分涵盖的内容有Markov Network,General Gibbs Distribution,CRF,Log-Linear Models.(&lt;strong&gt;Logistic Models is a simple CRF;CRF does not need to concern about the correlation between features!&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;PART II:PGM Inference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Variable-Elimination.pdf"&gt;Variable Elimination&lt;/a&gt;;简要介绍了如何在Bayesian Network以及Ｍarkov Network中执行VE算法;接着对其复杂度进行了分析;最后从图的视角重新审视了一下VE算法.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Variable Elimination" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/variable_elimination_zps6fdc76a6.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Belief-Propagation.pdf"&gt;Belief Propagation(I)&lt;/a&gt; &amp;amp; &lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Belief-Propagation.pdf"&gt;Belief Propagation(II)&lt;/a&gt;;其基本内容如下:&lt;ul&gt;
&lt;li&gt;Belief Propagation算法基本流程;&lt;/li&gt;
&lt;li&gt;Cluster Graph的基本性质(&lt;code&gt;BP does poorly when we have strong correlations!&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;BP算法的基本性质;&lt;/li&gt;
&lt;li&gt;Clique Tree Algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Belief Propagation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/belief_propogation_zps866416cc.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-MAP.pdf"&gt;MAP Estimation&lt;/a&gt;;关于MAP Inference的基础知识。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Sampling.pdf"&gt;Sampling Methods&lt;/a&gt;;Basic Sampling Methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;PART III:PGM Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Parameters-Part-1.pdf"&gt;Learning: Parameter Estimation, Part 1&lt;/a&gt; &amp;amp; &lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Parameters-Part-2.pdf"&gt;Learning: Parameter Estimation, Part 2&lt;/a&gt;;Parameter Estimation for BN and MN;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-BN-Structures.pdf"&gt;Structure Learning&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Incomplete-Data.pdf"&gt;Learning With Incomplete Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:该课程网址见&lt;a href="https://class.coursera.org/pgm-003"&gt;PGM&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ising Model&lt;/li&gt;
&lt;li&gt;Dual Decomposition&lt;/li&gt;
&lt;li&gt;Decision Making&lt;/li&gt;
&lt;li&gt;Bayesian Scores&lt;/li&gt;
&lt;li&gt;Learning With Incomplete Data&lt;/li&gt;
&lt;li&gt;Lassos&lt;/li&gt;
&lt;li&gt;凸QP&lt;/li&gt;
&lt;li&gt;Duality&lt;/li&gt;
&lt;li&gt;KKT条件&lt;/li&gt;
&lt;li&gt;支持向量机番外篇I:&lt;a href="http://blog.pluskid.org/?p=702"&gt;支持向量机：Duality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;支持向量机番外篇II:&lt;a href="http://blog.pluskid.org/?p=723"&gt;支持向量机：Kernel II&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Apriori算法细节&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;pLSA算法EM算法求解&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1"&gt;&lt;a href="https://class.coursera.org/nlp/lecture"&gt;Natural Language Processing&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="算法"></category><category term="Fun"></category><category term="Staff"></category><category term="收藏夹"></category><category term="Bloom Filter"></category><category term="B Trees"></category><category term="Data Structure"></category><category term="Algorithm"></category><category term="PGM"></category></entry><entry><title>[墙裂推荐]自然语言处理(序章):我爱自然语言处理(II)</title><link href="http://www.qingyuanxingsi.com/qiang-lie-tui-jian-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-ii.html" rel="alternate"></link><updated>2014-05-06T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-06:qiang-lie-tui-jian-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-ii.html</id><summary type="html">&lt;p&gt;本文紧接上一篇&lt;a href="http://www.qingyuanxingsi.com/zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i.html"&gt;自然语言处理(序章):我爱自然语言处理(I)&lt;/a&gt;,由于文章篇幅过长导致编辑器响应速度变慢,所以将其拆分为两篇,本文即为第二部分。(&lt;strong&gt;本博文引用内容版权属我爱自然语言博客作者及其引用文章作者,特此再次声明&lt;/strong&gt;)。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;小编推荐&lt;/strong&gt;:本部分是我爱自然语言博客里写的最好的几篇文章了,墙裂推荐阅读;通过阅读本部分的文章,我对Metropolis Hastings算法以及Gibbs Sampling有了更为深入的了解。BTW,数学史真的很好玩啊！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;MCMC 和 Gibbs Sampling&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;随机模拟&lt;/h2&gt;
&lt;p&gt;随机模拟(或者统计模拟)方法有一个很酷的别名是蒙特卡罗方法(Monte Carlo Simulation)。这个方法的发展始于20世纪40年代，和原子弹制造的曼哈顿计划密切相关，当时的几个大牛，包括乌拉姆、冯.诺依曼、费米、费曼、Nicholas Metropolis，在美国洛斯阿拉莫斯国家实验室研究裂变物质的中子连锁反应的时候，开始使用统计模拟的方法,并在最早的计算机上进行编程实现。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Simulation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/simulation_zpsfd333536.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;现代的统计模拟方法最早由数学家乌拉姆提出，被Metropolis命名为蒙特卡罗方法，蒙特卡罗是著名的赌场，赌博总是和统计密切关联的，所以这个命名风趣而贴切，很快被大家广泛接受。被不过据说费米之前就已经在实验中使用了，但是没有发表。说起蒙特卡罗方法的源头，可以追溯到18世纪，布丰当年用于计算π的著名的投针实验就是蒙特卡罗模拟实验。统计采样的方法其实数学家们很早就知道，但是在计算机出现以前，随机数生成的成本很高，所以该方法也没有实用价值。随着计算机技术在二十世纪后半叶的迅猛发展，随机模拟技术很快进入实用阶段。对那些用确定算法不可行或不可能解决的问题，蒙特卡罗方法常常为人们带来希望。&lt;/p&gt;
&lt;p&gt;&lt;img alt="monte-carlo-simulation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/monte-carlo-simulation_zpsd57f8e88.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;统计模拟中有一个重要的问题就是给定一个概率分布$p(x)$，我们如何在计算机中生成它的样本。一般而言均匀分布 $Uniform(0,1)$的样本是相对容易生成的。通过线性同余发生器可以生成伪随机数，我们用确定性算法生成$[0,1]$之间的伪随机数序列后，这些序列的各种统计指标和均匀分布$Uniform(0,1)$的理论计算结果非常接近。这样的伪随机序列就有比较好的统计性质，可以被当成真实的随机数使用。&lt;/p&gt;
&lt;p&gt;&lt;img alt="sampling" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sampling_zpsb8ae4169.png" /&gt;&lt;/p&gt;
&lt;p&gt;而我们常见的概率分布，无论是连续的还是离散的分布，都可以基于$Uniform(0,1)$的样本生成。例如正态分布可以通过著名的&lt;strong&gt;Box-Muller&lt;/strong&gt;变换得到&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;[Box-Muller变换]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果随机变量$U_1$,$U_2$独立且$U_1,U_2 \sim\ Uniform[0,1]$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
Z_0 &amp;amp; = \sqrt{-2\ln U_1} cos(2\pi U_2) \\ 
Z_1 &amp;amp; = \sqrt{-2\ln U_1} sin(2\pi U_2) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;则$Z_0,Z_1$独立且服从标准正态分布。&lt;/p&gt;
&lt;p&gt;其它几个著名的连续分布，包括指数分布、Gamma分布、t分布、F分布、Beta分布、Dirichlet分布等等,也都可以通过类似的数学变换得到；离散的分布通过均匀分布更加容易生成。更多的统计分布如何通过均匀分布的变换生成出来，大家可以参考统计计算的书，其中 Sheldon M. Ross 的&lt;strong&gt;《统计模拟》&lt;/strong&gt;是写得非常通俗易懂的一本。&lt;/p&gt;
&lt;p&gt;不过我们并不是总是这么幸运的，当$p(x)$的形式很复杂，或者$p(x)$是个高维的分布的时候，样本的生成就可能很困难了。 譬如有如下的情况:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(x) = \frac{\tilde{p}(x)}{\int \tilde{p}(x) dx}$,而$\tilde{p}(x)$我们是可以计算的，但是底下的积分式无法显式计算。&lt;/li&gt;
&lt;li&gt;$p(x,y)$是一个二维的分布函数，这个函数本身计算很困难，但是条件分布$p(x|y),p(y|x)$的计算相对简单;如果$p(x)$是高维的，这种情形就更加明显。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此时就需要使用一些更加复杂的随机模拟的方法来生成样本。而本节中将要重点介绍的 MCMC(Markov Chain Monte Carlo) 和 Gibbs Sampling算法就是最常用的一种，这两个方法在现代贝叶斯分析中被广泛使用。要了解这两个算法，我们首先要对马氏链的平稳分布的性质有基本的认识。&lt;/p&gt;
&lt;h2&gt;马氏链及其平稳分布&lt;/h2&gt;
&lt;p&gt;马氏链的数学定义很简单:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(X_{t+1}=x|X_t, X_{t-1}, \cdots) =P(X_{t+1}=x|X_t)
\end{equation}&lt;/p&gt;
&lt;p&gt;也就是状态转移的概率只依赖于前一个状态。&lt;/p&gt;
&lt;p&gt;我们先来看马氏链的一个具体的例子。社会学家经常把人按其经济状况分成3类：下层(lower-class)、中层(middle-class)、上层(upper-class)，我们用1,2,3分别代表这三个阶层。社会学家们发现决定一个人的收入阶层的最重要的因素就是其父母的收入阶层。如果一个人的收入属于下层类别，那么他的孩子属于下层收入的概率是 0.65, 属于中层收入的概率是 0.28, 属于上层收入的概率是 0.07。事实上，从父代到子代，收入阶层的变化的转移概率如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt="table-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/table-1_zps3d0d323d.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="markov-transition" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/markov-transition_zps8213ffd9.png" /&gt;&lt;/p&gt;
&lt;p&gt;使用矩阵的表示方式，转移概率矩阵记为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P=\left[
\begin{array}{cc}
0.65 &amp;amp; 0.28 &amp;amp; 0.07 \\ 
0.15 &amp;amp; 0.67 &amp;amp; 0.18 \\ 
0.12 &amp;amp; 0.36 &amp;amp; 0.52 \\ 
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;假设当前这一代人处在下层、中层、上层的人的比例是概率分布向量 $\pi_0=[\pi_0(1),\pi_0(2),\pi_0(3)]$，那么他们的子女的分布比例将是$\pi_1=\pi_0P$, 他们的孙子代的分布比例将是 $\pi_2=\pi_1P=\pi_0P^2$, ……, 第n代子孙的收入分布比例将是$\pi_n=\pi_{n−1}P=\pi_0P^n$。&lt;/p&gt;
&lt;p&gt;假设初始概率分布为$\pi_0=[0.21,0.68,0.11]$，则我们可以计算前$n$代人的分布状况如下&lt;/p&gt;
&lt;p&gt;&lt;img alt="table-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/table-2_zps47ffb526.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;我们发现从第7代人开始，这个分布就稳定不变了，这个是偶然的吗？我们换一个初始概率分布$\pi_0=[0.75,0.15,0.1]$.试试看，继续计算前$n$代人的分布状况如下&lt;/p&gt;
&lt;p&gt;&lt;img alt="table-3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/table-3_zps3b41fb58.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;我们发现，到第9代人的时候, 分布又收敛了。最为奇特的是，两次给定不同的初始概率分布，最终都收敛到概率分布 $\pi=[0.286,0.489,0.225]$，也就是说收敛的行为和初始概率分布$\pi_0$无关。这说明这个收敛行为主要是由概率转移矩阵$P$决定的。我们计算一下$P^n$.&lt;/p&gt;
&lt;p&gt;\begin{equation}
P^{20} = P^{21} = \cdots = P^{100} = \cdots = 
\begin{bmatrix} 
0.286 &amp;amp; 0.489 &amp;amp; 0.225 \\
0.286 &amp;amp; 0.489 &amp;amp; 0.225 \\ 
0.286 &amp;amp; 0.489 &amp;amp; 0.225 \\ 
\end{bmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们发现，当$n$足够大的时候，这个$P^n$矩阵的每一行都是稳定地收敛到$\pi=[0.286,0.489,0.225]$这个概率分布。自然的，这个收敛现象并非是我们这个马氏链独有的，而是绝大多数马氏链的共同行为，关于马氏链的收敛我们有如下漂亮的定理：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;马氏链定理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果一个非周期马氏链具有转移概率矩阵$P$,且它的任何两个状态是连通的，那么$\lim_{n\rightarrow\infty}P_{ij}^n$存在且与$i$无关，记$\lim_{n\rightarrow\infty}P_{ij}^n = \pi(j)$, 我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\lim_{n \rightarrow \infty} P^n =\begin{bmatrix} 
\pi(1) &amp;amp; \pi(2) &amp;amp; \cdots &amp;amp; \pi(j) &amp;amp; \cdots \\ 
\pi(1) &amp;amp; \pi(2) &amp;amp; \cdots &amp;amp; \pi(j) &amp;amp; \cdots \\ 
\cdots &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \cdots \\ 
\pi(1) &amp;amp; \pi(2) &amp;amp; \cdots &amp;amp; \pi(j) &amp;amp; \cdots \\ 
\cdots &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \cdots \\ 
\end{bmatrix}    \\
\pi(j) = \sum_{i=0}^{\infty}\pi(i)P_{ij} \\
\end{split}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$\pi$是方程$\pi P=\pi$的唯一非负解。其中,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\pi = [\pi(1), \pi(2), \cdots, \pi(j),\cdots ], \quad \sum_{i=0}^{\infty} \pi_i = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;$\pi$称为马氏链的平稳分布。&lt;/p&gt;
&lt;p&gt;这个马氏链的收敛定理非常重要，&lt;strong&gt;所有的 MCMC(Markov Chain Monte Carlo) 方法都是以这个定理作为理论基础的&lt;/strong&gt;。 定理的证明相对复杂，一般的随机过程课本中也不给证明，所以我们就不用纠结它的证明了，直接用这个定理的结论就好了。我们对这个定理的内容做一些解释说明：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;该定理中马氏链的状态不要求有限，可以是有无穷多个的；&lt;/li&gt;
&lt;li&gt;定理中的“非周期“这个概念我们不打算解释了，因为我们遇到的绝大多数马氏链都是非周期的；&lt;/li&gt;
&lt;li&gt;两个状态$i,j$是连通并非指$i$可以直接一步转移到$j$($P_{ij}&amp;gt;0)$,而是指$i$可以通过有限的$n$步转移到达$j$($P^n_{ij}&amp;gt;0$)。马氏链的任何两个状态是连通的含义是指存在一个$n$,使得矩阵$P^n$中的任何一个元素的数值都大于零。&lt;/li&gt;
&lt;li&gt;我们用$X_i$表示在马氏链上跳转第$i$步后所处的状态，如果$\lim_{n\rightarrow\infty}P_{ij}^n=\pi(j)$存在，很容易证明以上定理的第二个结论。由于&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
P(X_{n+1}=j) &amp;amp; = \sum_{i=0}^\infty P(X_n=i) P(X_{n+1}=j|X_n=i) \\
&amp;amp; = \sum_{i=0}^\infty P(X_n=i) P_{ij} 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;上式两边取极限就得到$\pi(j) = \sum_{i=0}^{\infty}\pi(i)P_{ij}$.&lt;/p&gt;
&lt;p&gt;从初始概率分布$\pi_0$出发，我们在马氏链上做状态转移，记$X_i$的概率分布为$\pi_i$, 则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
X_0 &amp;amp; \sim \pi_0(x) \ 
X_i &amp;amp; \sim \pi_i(x), \quad\quad \pi_i(x) = \pi_{i-1}(x)P = \pi_0(x)P^n 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;由马氏链收敛的定理, 概率分布$\pi_i(x)$将收敛到平稳分布$\pi_(x)$。假设到第$n$步的时候马氏链收敛，则有&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
X_0 &amp;amp; \sim \pi_0(x) \\ 
X_1 &amp;amp; \sim \pi_1(x) \\ 
&amp;amp; \cdots \\ 
X_n &amp;amp; \sim \pi_n(x)=\pi(x) \\ 
X_{n+1} &amp;amp; \sim \pi(x) \\ 
X_{n+2}&amp;amp; \sim \pi(x) \\ 
&amp;amp; \cdots 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;所以$X_n,X_{n+1},X_{n+2},\cdots \sim \pi(x)$都是同分布的随机变量，当然他们并不独立。如果我们从一个具体的初始状态$x_0$开始,沿着马氏链按照概率转移矩阵做跳转，那么我们得到一个转移序列$x_0,x_1,x_2,\cdots,x_n,x_{n+1},\cdots$, 由于马氏链的收敛行为，$x_n,x_{n+1},...$ 都将是平稳分布$\pi(x)$的样本。&lt;/p&gt;
&lt;h2&gt;Markov Chain Monte Carlo&lt;/h2&gt;
&lt;p&gt;对于给定的概率分布$p(x)$,我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布，于是一个很漂亮的想法是：如果我们能构造一个转移矩阵为$P$的马氏链，使得该马氏链的平稳分布恰好是$p(x)$,那么我们从任何一个初始状态$x_0$出发沿着马氏链转移, 得到一个转移序列$x_0,x_1,x_2,\cdots,x_n,x_{n+1}\cdots$，如果马氏链在第$n$步已经收敛了，于是我们就得到了$p(x)$的样本$x_n,x_{n+1},\cdots$。&lt;/p&gt;
&lt;p&gt;这个绝妙的想法在1953年被Metropolis想到了，为了研究粒子系统的平稳性质，Metropolis考虑了物理学中常见的波尔兹曼分布的采样问题，首次提出了基于马氏链的蒙特卡罗方法，即Metropolis算法，并在最早的计算机上编程实现。Metropolis算法是首个普适的采样方法，并启发了一系列MCMC方法，所以人们把它视为随机模拟技术腾飞的起点。Metropolis的这篇论文被收录在《统计学中的重大突破》中，Metropolis算法也被遴选为二十世纪的十个最重要的算法之一。&lt;/p&gt;
&lt;p&gt;我们接下来介绍的MCMC算法是Metropolis算法的一个改进变种，即常用的&lt;strong&gt;Metropolis-Hastings&lt;/strong&gt;算法。由上一节的例子和定理我们看到了，马氏链的收敛性质主要由转移矩阵$P$决定,所以基于马氏链做采样的关键问题是如何构造转移矩阵$P$,使得平稳分布恰好是我们要的分布$p(x)$。如何能做到这一点呢？我们主要使用如下的定理。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;定理：&lt;strong&gt;[细致平稳条件]&lt;/strong&gt;如果非周期马氏链的转移矩阵$P$和分布$\pi(x)$满足:对于任意$i$和$j$,有:&lt;/p&gt;
&lt;p&gt;\begin{equation} 
\pi(i)P_{ij} = \pi(j)P_{ji}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;则$\pi(x)$是马氏链的平稳分布，上式被称为&lt;strong&gt;细致平稳条件(Detailed balance condition)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;其实这个定理是显而易见的，因为细致平稳条件的物理含义就是对于任何两个状态$i$,$j$,从$i$转移出去到$j$而丢失的概率质量，恰好会被从$j$转移回$i$的概率质量补充回来，所以状态$i$上的概率质量$\pi(i)$是稳定的，从而$\pi(x)$是马氏链的平稳分布。数学上的证明也很简单，由细致平稳条件可得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
&amp;amp; \sum_{i=1}^\infty \pi(i)P_{ij} = \sum_{i=1}^\infty \pi(j)P_{ji} 
= \sum_{i=1}^\infty \pi(j)P_{ji} = \pi(j) \\ 
&amp;amp; \rightarrow \pi P = \pi 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;由于$\pi$是方程$\pi P =\pi$的解，所以$\pi$是平稳分布。&lt;/p&gt;
&lt;p&gt;假设我们已经有一个转移矩阵为$Q$的马氏链($q(i,j)$表示从状态$i$转移到状态$j$的概率，也可以写为$q(j|i)$或者$q(i \to j)$), 显然，通常情况下$p(i) q(i,j)\neq p(j)q(j,i)$也就是细致平稳条件不成立，所以$p(x)$不太可能是这个马氏链的平稳分布。我们可否对马氏链做一个改造，使得细致平稳条件成立呢？譬如，我们引入一个$\alpha(i,j)$, 我们希望:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(i)q(i,j)\alpha(i,j)=p(j)q(j,i)\alpha(j,i) \quad (∗)
\end{equation}&lt;/p&gt;
&lt;p&gt;取什么样的$\alpha(i,j)$以上等式能成立呢？最简单的，按照对称性，我们可以取:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\alpha(i,j)=p(j)q(j,i)，\alpha(j,i)=p(i)q(i,j)
\end{equation}&lt;/p&gt;
&lt;p&gt;于是(*)式就成立了。所以有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Construct_Proposal" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/construct_proposal_zpsa66cecb9.png" /&gt;&lt;/p&gt;
&lt;p&gt;于是我们把原来具有转移矩阵$Q$的一个很普通的马氏链，改造为了具有转移矩阵$Q\prime$的马氏链，而 $Q\prime$恰好满足细致平稳条件，由此马氏链$Q\prime$的平稳分布就是$p(x)$了!&lt;/p&gt;
&lt;p&gt;在改造$Q$的过程中引入的$\alpha(i,j)$称为接受率，物理意义可以理解为在原来的马氏链上，从状态$i$以$q(i,j)$的概率转跳转到状态$j$的时候，我们以$\alpha(i,j)$的概率接受这个转移，于是得到新的马氏链$Q\prime$的转移概率为$q(i,j)\alpha(i,j)$。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mcmc-transition1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mcmc-transition1_zpsf3e7c727.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;假设我们已经有一个转移矩阵$Q$(对应元素为$q(i,j)$), 把以上的过程整理一下，我们就得到了如下的用于采样概率分布$p(x)$的算法。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mcmc-algo-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mcmc-algo-1_zps4581580d.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;上述过程中$p(x),q(x|y)$说的都是离散的情形，事实上即便这两个分布是连续的，以上算法仍然是有效，于是就得到更一般的连续概率分布$p(x)$的采样算法，而$q(x|y)$就是任意一个连续二元概率分布对应的条件分布。&lt;/p&gt;
&lt;p&gt;以上的MCMC采样算法已经能很漂亮的工作了，不过它有一个小的问题：马氏链$Q$在转移的过程中的接受率 $\alpha(i,j)$可能偏小，这样采样过程中马氏链容易原地踏步，拒绝大量的跳转，这使得马氏链遍历所有的状态空间要花费太长的时间，收敛到平稳分布$p(x)的$速度太慢。有没有办法提升一些接受率呢?&lt;/p&gt;
&lt;p&gt;假设$\alpha(i,j)=0.1,\alpha(j,i)=0.2$, 此时满足细致平稳条件，于是&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(i)q(i,j)×0.1=p(j)q(j,i)×0.2
\end{equation}&lt;/p&gt;
&lt;p&gt;上式两边扩大5倍，我们改写为&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(i)q(i,j)×0.5=p(j)q(j,i)×1
\end{equation}&lt;/p&gt;
&lt;p&gt;看，我们提高了接受率，而细致平稳条件并没有打破！这启发我们可以把细致平稳条件(**) 式中的$\alpha(i,j),\alpha(j,i)$同比例放大，使得两数中最大的一个放大到1，这样我们就提高了采样中的跳转接受率。所以我们可以取:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Choose_Alpha" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/choose_alpha_zps820b59d0.png" /&gt;&lt;/p&gt;
&lt;p&gt;于是，经过对上述MCMC采样算法中接受率的微小改造，我们就得到了如下教科书中最常见的&lt;strong&gt;Metropolis-Hastings算法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mcmc-algo-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mcmc-algo-2_zps26a1c8bb.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;对于分布$p(x)$,我们构造转移矩阵$Q\prime$使其满足细致平稳条件:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x)Q\prime (x→y)=p(y)Q\prime(y→x)
\end{equation}&lt;/p&gt;
&lt;p&gt;此处$x$并不要求是一维的，对于高维空间的$p(\mathbf{x})$，如果满足细致平稳条件:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\mathbf{x}) Q’(\mathbf{x}\rightarrow \mathbf{y}) = p(\mathbf{y}) Q’(\mathbf{y}\rightarrow \mathbf{x})
\end{equation}&lt;/p&gt;
&lt;p&gt;那么以上的Metropolis-Hastings算法一样有效。&lt;/p&gt;
&lt;h2&gt;Gibbs Sampling&lt;/h2&gt;
&lt;p&gt;对于高维的情形，由于接受率$\alpha$的存在(通常$\alpha$&amp;lt;1), 以上Metropolis-Hastings算法的效率不够高。能否找到一个转移矩阵$Q$使得接受率$\alpha=1$呢？我们先看看二维的情形，假设有一个概率分布 $p(x,y)$, 考察$x$坐标相同的两个点$A(x_1,y_1),B(x_1,y_2)$,我们发现:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(x_1,y_1)p(y_2|x_1) &amp;amp;= p(x_1)p(y_1|x_1)p(y_2|x_1) \\
p(x_1,y_2)p(y_1|x_1) &amp;amp;= p(x_1)p(y_2|x_1)p(y_1|x_1) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;所以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation} 
p(x_1,y_1)p(y_2|x_1) = p(x_1,y_2)p(y_1|x_1)  \quad (***) 
\end{equation}&lt;/p&gt;
&lt;p&gt;即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(A)p(y_2|x_1) = p(B)p(y_1|x_1)
\end{equation}&lt;/p&gt;
&lt;p&gt;基于以上等式，我们发现，在$x=x_1$这条平行于$y$轴的直线上，如果使用条件分布$p(y|x_1)$做为任何两个点之间的转移概率，那么任何两个点之间的转移满足细致平稳条件。同样的，如果我们在$y=y_1$这条直线上任意取两个点$A(x_1,y_1),C(x_2,y_1)$,也有如下等式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(A)p(x_2|y_1)=p(C)p(x_1|y_1)
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;img alt="gibbs-transition" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs-transition_zpsd5d8548a.png" /&gt;&lt;/p&gt;
&lt;p&gt;于是我们可以如下构造平面上任意两点之间的转移概率矩阵$Q$:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
Q(A\rightarrow B) &amp;amp; = p(y_B|x_1) &amp;amp; \text{如果} \quad x_A=x_B=x_1 &amp;amp; \\ 
Q(A\rightarrow C) &amp;amp; = p(x_C|y_1) &amp;amp; \text{如果} \quad y_A=y_C=y_1 &amp;amp; \\ 
Q(A\rightarrow D) &amp;amp; = 0 &amp;amp; \text{其它} &amp;amp; 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;有了如上的转移矩阵$Q$, 我们很容易验证对平面上任意两点$X,Y$, 满足细致平稳条件:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(X)Q(X→Y)=p(Y)Q(Y→X)
\end{equation}&lt;/p&gt;
&lt;p&gt;于是这个二维空间上的马氏链将收敛到平稳分布$p(x,y)$。而这个算法就称为&lt;strong&gt;Gibbs Sampling算法&lt;/strong&gt;,是 Stuart Geman 和Donald Geman 这两兄弟于1984年提出来的，之所以叫做Gibbs Sampling 是因为他们研究了Gibbs random field, 这个算法在现代贝叶斯分析中占据重要位置。&lt;/p&gt;
&lt;p&gt;&lt;img alt="gibbs-algo-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs-algo-1_zps3efe14aa.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="two-stage-gibbs" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/two-stage-gibbs_zps30faeda1.png" /&gt;&lt;/p&gt;
&lt;p&gt;以上采样过程中，如上图所示，马氏链的转移只是轮换的沿着坐标轴$x$轴和$y$轴做转移，于是得到样本 $(x_0,y_0),(x_0,y_1),(x_1,y_1),(x_1,y_2),(x_2,y_2),\cdots$,马氏链收敛后，最终得到的样本就是$p(x,y)$的样本，而收敛之前的阶段称为 burn-in period。额外说明一下，我们看到教科书上的 Gibbs Sampling 算法大都是坐标轴轮换采样的，但是这其实是不强制要求的。最一般的情形可以是，在$t$时刻，可以在$x$轴和$y$轴之间随机的选一个坐标轴，然后按条件概率做转移，马氏链也是一样收敛的。轮换两个坐标轴只是一种方便的形式。&lt;/p&gt;
&lt;p&gt;以上的过程我们很容易推广到高维的情形，对于(***)式，如果$x_1$变为多维情形$x_1$,可以看出推导过程不变，所以细致平稳条件同样是成立的.&lt;/p&gt;
&lt;p&gt;\begin{equation} 
p(\mathbf{x_1},y_1)p(y_2|\mathbf{x_1}) = p(\mathbf{x_1},y_2)p(y_1|\mathbf{x_1}) 
\end{equation}&lt;/p&gt;
&lt;p&gt;此时转移矩阵$Q$由条件分布$p(y|x_1)$定义。上式只是说明了一根坐标轴的情形，和二维情形类似，很容易验证对所有坐标轴都有类似的结论。所以$n$维空间中对于概率分布$p(x_1,x_2,\cdots,x_n)$可以如下定义转移矩阵:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果当前状态为$(x_1,x_2,⋯,x_n)$，马氏链转移的过程中，只能沿着坐标轴做转移。沿着$x_i$这根坐标轴做转移的时候，转移概率由条件概率$p(x_i|x_1,⋯,x_{i−1},x_{i+1},⋯,x_n)$定义;&lt;/li&gt;
&lt;li&gt;其它无法沿着单根坐标轴进行的跳转，转移概率都设置为 0。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;于是我们可以把Gibbs Smapling算法从采样二维的$p(\mathbf{x},\mathbf{y})$推广到采样$n$维的 $p(x_1,x_2,⋯,x_n)$.&lt;/p&gt;
&lt;p&gt;&lt;img alt="gibbs-algo-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs-algo-2_zps69519b9b.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;以上算法收敛后，得到的就是概率分布$p(x_1,x_2,⋯,x_n)$的样本，当然这些样本并不独立，但是我们此处要求的是采样得到的样本符合给定的概率分布，并不要求独立。同样的，在以上算法中，坐标轴轮换采样不是必须的，可以在坐标轴轮换中引入随机性，这时候转移矩阵$Q$中任何两个点的转移概率中就会包含坐标轴选择的概率，而在通常的 Gibbs Sampling 算法中，坐标轴轮换是一个确定性的过程，也就是在给定刻$t$，在一根固定的坐标轴上转移的概率是1。&lt;/p&gt;
&lt;h1&gt;文本建模&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;文本建模&lt;/h2&gt;
&lt;p&gt;我们日常生活中总是产生大量的文本，如果每一个文本存储为一篇文档，那每篇文档从人的观察来说就是有序的词的序列$d=(w_1,w_2,\cdots,w_n)$。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Corpus" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/corpus_zpsd5c55aaa.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;包含$M$篇文档的语料库统计文本建模的目的就是追问这些观察到语料库中的的词序列是如何生成的。统计学被人们描述为猜测上帝的游戏，人类产生的所有的语料文本我们都可以看成是一个伟大的上帝在天堂中抛掷骰子生成的，我们观察到的只是上帝玩这个游戏的结果 —— 词序列构成的语料，而上帝玩这个游戏的过程对我们是个黑盒子。所以在统计文本建模中，我们希望猜测出上帝是如何玩这个游戏的，具体一点，最核心的两个问题是:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上帝都有什么样的骰子；&lt;/li&gt;
&lt;li&gt;上帝是如何抛掷这些骰子的；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一个问题就是表示模型中都有哪些参数，骰子的每一个面的概率都对应于模型中的参数；第二个问题就表示游戏规则是什么，上帝可能有各种不同类型的骰子，上帝可以按照一定的规则抛掷这些骰子从而产生词序列。 &lt;/p&gt;
&lt;h3&gt;Unigram Model&lt;/h3&gt;
&lt;p&gt;假设我们的词典中一共有$V$个词$v_1,v_2,⋯v_V$，那么最简单的Unigram Model就是认为上帝是按照如下的游戏规则产生文本的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="game_unigram_model" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/game-unigram-model_zpseddfb645.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;上帝的这个唯一的骰子各个面的概率记为$\vec{p} = (p_1, p_2, \cdots, p_V)$, 所以每次投掷骰子类似于一个抛钢镚时候的贝努利实验， 记为$w\sim Mult(w|\vec{p})$.&lt;/p&gt;
&lt;p&gt;&lt;img alt="unigram_model" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/unigram-model_zps19f87fe3.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;上帝投掷$V$个面的骰子对于一篇文档$d=\vec w=(w_1, w_2, \cdots, w_n)$, 该文档被生成的概率就是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\vec w) = p(w_1, w_2, \cdots, w_n) = p(w_1)p(w_2) \cdots p(w_n)
\end{equation}&lt;/p&gt;
&lt;p&gt;而文档和文档之间我们认为是独立的， 所以如果语料中有多篇文档$\mathcal{W}=(\vec{w_1}, \vec{w_2},…,\vec{w_m})$,则该语料的概率是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\mathcal{W})= p(\vec{w_1})p(\vec{w_2}) 
\cdots p(\vec{w_m})
\end{equation}&lt;/p&gt;
&lt;p&gt;在Unigram Model中， 我们假设了文档之间是独立可交换的，而文档中的词也是独立可交换的，所以一篇文档相当于一个袋子，里面装了一些词，而词的顺序信息就无关紧要了，这样的模型也称为词袋模型(&lt;strong&gt;Bag-of-words&lt;/strong&gt;)。&lt;/p&gt;
&lt;p&gt;假设语料中总的词数是$N$, 在所有的$N$个词中,如果我们关注每个词$v_i$的发生次数$n_i$，那么$ n=(n_1, n_2,\cdots, n_V)$正好是一个多项分布:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p( n) = Mult( n|\vec{p}, N) 
= \binom{N}{ n} \prod_{k=1}^V p_k^{n_k}
\end{equation}&lt;/p&gt;
&lt;p&gt;此时， 语料的概率是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\mathcal{W})= p(\vec{w_1})p(\vec{w_2}) \cdots p(\vec{w_m}) 
= \prod_{k=1}^V p_k^{n_k} 
\end{equation}&lt;/p&gt;
&lt;p&gt;当然，我们很重要的一个任务就是估计模型中的参数$\vec{p}$，也就是问上帝拥有的这个骰子的各个面的概率是多大，按照统计学家中频率派的观点，使用最大似然估计最大化$P(\mathcal{W})$，于是参数$p_i$的估计值就是$\hat{p_i}=\frac{n_i}{N}$.&lt;/p&gt;
&lt;p&gt;对于以上模型，贝叶斯统计学派的统计学家会有不同意见，他们会很挑剔的批评只假设上帝拥有唯一一个固定的骰子是不合理的。在贝叶斯学派看来，一切参数都是随机变量，以上模型中的骰子$\vec{p}$不是唯一固定的，它也是一个随机变量。所以按照贝叶斯学派的观点，上帝是按照以下的过程在玩游戏的:&lt;/p&gt;
&lt;p&gt;&lt;img alt="bayesian-unigram-model" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bayesian-unigram-model_zpsa4eeab8f.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;上帝的这个坛子里面，骰子可以是无穷多个，有些类型的骰子数量多，有些类型的骰子少，所以从概率分布的角度看，坛子里面的骰子$\vec{p}$服从一个概率分布$p(\vec{p})$，这个分布称为参数$\vec{p}$的先验分布。&lt;/p&gt;
&lt;p&gt;&lt;img alt="dirichlet-multinomial-unigram" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/dirichlet-multinomial-unigram_zps6e82a36d.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;以上贝叶斯学派的游戏规则的假设之下，语料$\mathcal{W}$产生的概率如何计算呢？由于我们并不知道上帝到底用了哪个骰子$\vec{p}$,所以每个骰子都是可能被使用的，只是使用的概率由先验分布$p(\vec{p})$来决定。对每一个具体的骰子$\vec{p}$,由该骰子产生数据的概率是$p(\mathcal{W}|\vec{p})$, 所以最终数据产生的概率就是对每一个骰子$\vec{p}$上产生的数据概率进行积分累加求和:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\mathcal{W}) = \int p(\mathcal{W}|\vec{p}) p(\vec{p})d\vec{p}
\end{equation}&lt;/p&gt;
&lt;p&gt;在贝叶斯分析的框架下，此处先验分布$p(\vec{p})$就可以有很多种选择了，注意到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p( n) = Mult( n|\vec{p}, N)
\end{equation}&lt;/p&gt;
&lt;p&gt;实际上是在计算一个多项分布的概率，所以对先验分布的一个比较好的选择就是多项分布对应的共轭分布,即 Dirichlet 分布:&lt;/p&gt;
&lt;p&gt;\begin{equation}
Dir(\vec{p}|\vec\alpha)= 
\frac{1}{\Delta(\vec\alpha)} \prod_{k=1}^V p_k^{\alpha_k -1}， 
\quad \vec\alpha=(\alpha_1, \cdots, \alpha_V)
\end{equation}&lt;/p&gt;
&lt;p&gt;此处，$\Delta(\vec\alpha)$就是归一化因子$Dir(\vec\alpha)$，即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta(\vec\alpha) = 
\int \prod_{k=1}^V p_k^{\alpha_k -1} d\vec{p} 
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;img alt="dirichlet-multinomial-unigram" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/dirichlet-multinomial-unigram_zps6e82a36d.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="graph-model-unigram" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/graph-model-unigram_zps3d4b6a8b.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;回顾前一个小节介绍的Drichlet分布的一些知识，其中很重要的一点就是:&lt;strong&gt;Dirichlet 先验 + 多项分布的数据 → 后验分布为 Dirichlet 分布&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;\begin{equation}
Dir(\vec{p}|\vec\alpha) + MultCount( n)= Dir(\vec{p}|\vec\alpha+ n)
\end{equation}&lt;/p&gt;
&lt;p&gt;于是，在给定了参数$\vec{p}$的先验分布$Dir(\vec{p}|\vec\alpha)$的时候，各个词出现频次的数据$ n \sim Mult( n|\vec{p},N)$为多项分布, 所以无需计算，我们就可以推出后验分布是:&lt;/p&gt;
&lt;p&gt;\begin{equation} 
p(\vec{p}|\mathcal{W},\vec\alpha) 
= Dir(\vec{p}| n+ \vec\alpha) 
= \frac{1}{\Delta( n+\vec\alpha)} 
\prod_{k=1}^V p_k^{n_k + \alpha_k -1} d\vec{p} 
\end{equation}&lt;/p&gt;
&lt;p&gt;在贝叶斯的框架下，参数$\vec{p}$如何估计呢？由于我们已经有了参数的后验分布，所以合理的方式是使用后验分布的极大值点，或者是参数在后验分布下的平均值。在该文档中，我们取平均值作为参数的估计值。使用上个小节中的结论，由于$\vec{p}$的后验分布为$Dir(\vec{p}| n + \vec\alpha)$，于是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(\vec{p}) = \Bigl(\frac{n_1 + \alpha_1}{\sum_{i=1}^V(n_i + \alpha_i)}, 
\frac{n_2 + \alpha_2}{\sum_{i=1}^V(n_i + \alpha_i)}, \cdots, 
\frac{n_V + \alpha_V}{\sum_{i=1}^V(n_i + \alpha_i)} \Bigr)
\end{equation}&lt;/p&gt;
&lt;p&gt;也就是说对每一个$p_i$, 我们用下式做参数估计:&lt;/p&gt;
&lt;p&gt;\begin{equation} 
\hat{p_i} = \frac{n_i + \alpha_i}{\sum_{i=1}^V(n_i + \alpha_i)} 
\end{equation}&lt;/p&gt;
&lt;p&gt;考虑到$\alpha_i$在Dirichlet 分布中的物理意义是事件的先验的伪计数，这个估计式子的含义是很直观的：每个参数的估计值是其对应事件的先验的伪计数和数据中的计数的和在整体计数中的比例。&lt;/p&gt;
&lt;p&gt;进一步，我们可以计算出文本语料的产生概率为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(\mathcal{W}|\vec\alpha) &amp;amp; = \int p(\mathcal{W}|\vec{p}) p(\vec{p}|\vec\alpha)d\vec{p} \notag \\ 
&amp;amp; = \int \prod_{k=1}^V p_k^{n_k} Dir(\vec{p}|\vec\alpha) d\vec{p} \notag \\ 
&amp;amp; = \int \prod_{k=1}^V p_k^{n_k} \frac{1}{\Delta(\vec\alpha)} 
\prod_{k=1}^V p_k^{\alpha_k -1} d\vec{p} \notag \\ 
&amp;amp; = \frac{1}{\Delta(\vec\alpha)} 
\int \prod_{k=1}^V p_k^{n_k + \alpha_k -1} d\vec{p} \notag \\ 
&amp;amp; = \frac{\Delta( n+\vec\alpha)}{\Delta(\vec\alpha)} 
\end{split}
\end{equation}&lt;/p&gt;
&lt;h3&gt;Topic Model 和 PLSA&lt;/h3&gt;
&lt;p&gt;以上 Unigram Model 是一个很简单的模型，模型中的假设看起来过于简单，和人类写文章产生每一个词的过程差距比较大，有没有更好的模型呢？&lt;/p&gt;
&lt;p&gt;我们可以看看日常生活中人是如何构思文章的。如果我们要写一篇文章，往往是先确定要写哪几个主题。譬如构思一篇自然语言处理相关的文章，可能$40\%$会谈论语言学,$30\%$谈论概率统计,$20\%$谈论计算机、还有$10\%$谈论其它的主题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;说到语言学，我们容易想到的词包括：语法、句子、乔姆斯基、句法分析、主语…；&lt;/li&gt;
&lt;li&gt;谈论概率统计，我们容易想到以下一些词: 概率、模型、均值、方差、证明、独立、马尔科夫链、…；&lt;/li&gt;
&lt;li&gt;谈论计算机，我们容易想到的词是： 内存、硬盘、编程、二进制、对象、算法、复杂度…；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们之所以能马上想到这些词，是因为这些词在对应的主题下出现的概率很高。我们可以很自然的看到，一篇文章通常是由多个主题构成的、而每一个主题大概可以用与该主题相关的频率最高的一些词来描述。&lt;/p&gt;
&lt;p&gt;以上这种直观的想法由Hoffman 于 1999 年给出的PLSA(Probabilistic Latent Semantic Analysis) 模型中首先进行了明确的数学化。Hoffman 认为一篇文档(Document) 可以由多个主题(Topic) 混合而成， 而每个Topic 都是词汇上的概率分布，文章中的每个词都是由一个固定的 Topic 生成的。下图是英语中几个Topic 的例子。&lt;/p&gt;
&lt;p&gt;&lt;img alt="topic_example" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/topic-examples_zps1f8f6d28.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;所有人类思考和写文章的行为都可以认为是上帝的行为，我们继续回到上帝的假设中，那么在 PLSA 模型中，Hoffman 认为上帝是按照如下的游戏规则来生成文本的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="game-plsa" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/game-plsa_zpsea8eb70a.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;以上PLSA 模型的文档生成的过程可以图形化的表示为:&lt;/p&gt;
&lt;p&gt;&lt;img alt="plsa-doc-topic-word" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/plsa-doc-topic-word_zps2dc5aea1.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;PLSA模型的文档生成过程我们可以发现在以上的游戏规则下，文档和文档之间是独立可交换的，同一个文档内的词也是独立可交换的，还是一个bag-of-words模型。游戏中的$K$个topic-word骰子，我们可以记为$\vec\varphi_1, \cdots, \vec\varphi_K$, 对于包含$M$篇文档的语料$C=(d_1, d_2, \cdots, d_M)$中的每篇文档$d_m$，都会有一个特定的doc-topic骰子$\vec{\theta}_m$，所有对应的骰子记为$\vec{\theta}_1, \cdots, \vec{\theta}_M$。为了方便，我们假设每个词$w$ 都是一个编号，对应到topic-word骰子的面。于是在 PLSA 这个模型中，第$m$篇文档$d_m$中的每个词的生成概率为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(w|d_m) = \sum_{z=1}^K p(w|z)p(z|d_m) = \sum_{z=1}^K \varphi_{zw} \theta_{mz}
\end{equation}&lt;/p&gt;
&lt;p&gt;所以整篇文档的生成概率为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\vec w|d_m) = \prod_{i=1}^n \sum_{z=1}^K p(w_i|z)p(z|d_m) = 
\prod_{i=1}^n \sum_{z=1}^K \varphi_{zw_i} \theta_{mz}
\end{equation}&lt;/p&gt;
&lt;p&gt;由于文档之间相互独立，我们也容易写出整个语料的生成概率。求解PLSA这个Topic Model的过程汇总，模型参数并容易求解，可以使用著名的EM算法进行求得局部最优解，由于该模型的求解并不是本文的介绍要点，有兴趣的同学参考Hoffman的原始论文，此处略去不讲。&lt;/p&gt;
&lt;h1&gt;LDA文本建模&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;游戏规则&lt;/h2&gt;
&lt;p&gt;对于上述的 PLSA 模型，贝叶斯学派显然是有意见的，doc-topic 骰子$\vec{\theta}_m$和topic-word骰子$\vec\varphi_k$都是模型中的参数，参数都是随机变量，怎么能没有先验分布呢？于是，类似于对Unigram Model的贝叶斯改造， 我们也可以如下在两个骰子参数前加上先验分布从而把PLSA对应的游戏过程改造为一个贝叶斯的游戏过程。由于$\vec\varphi_k$和$\vec{\theta}_m$都对应到多项分布，所以先验分布的一个好的选择就是Drichlet分布，于是我们就得到了&lt;strong&gt;LDA(Latent Dirichlet Allocation)&lt;/strong&gt;模型。&lt;/p&gt;
&lt;p&gt;&lt;img alt="lda_dice" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-dice_zps843a7bb2.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;在 LDA 模型中, 上帝是按照如下的规则玩文档生成的游戏的.&lt;/p&gt;
&lt;p&gt;&lt;img alt="game-lda-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/game-lda-1_zpsb9cf4135.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;假设语料库中有$M$ 篇文档，所有的的word和对应的 topic 如下表示:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
\vec{\mathbf{w}} &amp;amp; = (\vec w_1, \cdots, \vec w_M) \\ 
\vec{\mathbf{z}} &amp;amp; = (\vec z_1, \cdots, \vec z_M) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\vec w_m$表示第$m$篇文档中的词，$\vec z_m$表示这些词对应的topic编号。&lt;/p&gt;
&lt;p&gt;&lt;img alt="word_topic_example" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/word-topic-vector_zpsa89d3e3d.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;物理过程分解&lt;/h2&gt;
&lt;p&gt;使用概率图模型表示， LDA模型的游戏过程如图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="LDA概率图模型表示" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-graph-model_zps41d58402.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;这个概率图可以分解为两个主要的物理过程： &lt;/p&gt;
&lt;p&gt;&lt;img alt="Two_Process" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/two_process_zps04f4e66e.png" /&gt;&lt;/p&gt;
&lt;p&gt;理解LDA最重要的就是理解这两个物理过程。LDA模型在基于$K$个topic生成语料中的$M$篇文档的过程中,由于是bag-of-words模型，有一些物理过程是相互独立可交换的。&lt;strong&gt;由此,LDA生成模型中,$M$篇文档会对应于$M$个独立的Dirichlet-Multinomial共轭结构；$K$个topic会对应于$K$个独立的Dirichlet-Multinomial 共轭结构&lt;/strong&gt;.所以理解LDA所需要的所有数学就是理解Dirichlet-Multiomail共轭，其它就是理解物理过程。现在我们进入细节，来看看LDA模型是如何被分解为$M+K$个Dirichlet-Multinomial共轭结构的。&lt;/p&gt;
&lt;p&gt;由第一个物理过程，我们知道$\vec\alpha \to \vec\theta_m  \to \vec z_m$表示生成第$m$篇文档中的所有词对应的topics，显然$\vec\alpha \to \vec\theta_m$对应于Dirichlet分布，$\vec\theta_m \to \vec z_{m}$对应于Multinomial分布,所以整体是一个Dirichlet-Multinomial共轭结构；&lt;/p&gt;
&lt;p&gt;&lt;img alt="lda-dir-mult-conjugate-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-dir-mult-conjugate-1_zpsce0d98eb.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;前文介绍Bayesian Unigram Model的小节中我们对Dirichlet-Multinomial共轭结构做了一些计算。借助于该小节中的结论，我们可以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\vec z_m |\vec\alpha) = \frac{\Delta(\vec n_m+\vec\alpha)}{\Delta(\vec\alpha)}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\vec n_m = (n_{m}^{(1)}, \cdots, n_{m}^{(K)})$表示第$m$篇文档中第$k$个topic产生的词的个数。进一步，利用Dirichlet-Multiomial共轭结构，我们得到参数$\vec{\theta}_m$的后验分布恰好是$Dir(\vec{\theta}_m|  n_m + \vec\alpha)$.&lt;/p&gt;
&lt;p&gt;由于语料中$M$篇文档的 topics生成过程相互独立，所以我们得到$M$个相互独立的Dirichlet-Multinomial共轭结构，从而我们可以得到整个语料中topics生成概率:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(\vec{\mathbf{z}} |\vec\alpha) &amp;amp; = \prod_{m=1}^M p(\vec z_m |\vec\alpha) \notag \\ 
&amp;amp;= \prod_{m=1}^M \frac{\Delta( n_m+\vec\alpha)}{\Delta(\vec\alpha)} \quad\quad  (*) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;目前为止，我们由$M$篇文档得到了$M$个Dirichlet-Multinomial共轭结构，还有额外$K$个Dirichlet-Multinomial共轭结构在哪儿呢？在上帝按照之前的规则玩LDA游戏的时候，上帝是先完全处理完成一篇文档，再处理下一篇文档。文档中每个词的生成都要抛两次骰子，第一次抛一个doc-topic骰子得到topic, 第二次抛一个topic-word骰子得到word，每次生成每篇文档中的一个词的时候这两次抛骰子的动作是紧邻轮换进行的。如果语料中一共有$N$个词，则上帝一共要抛$2N$次骰子，轮换的抛doc-topic骰子和topic-word骰子。但实际上有一些抛骰子的顺序是可以交换的，我们可以等价的调整$2N$次抛骰子的次序：前$N$次只抛doc-topic骰子得到语料中所有词的topics,然后基于得到的每个词的topic编号，后$N$次只抛topic-word骰子生成$N$个word。于是上帝在玩 LDA 游戏的时候，可以等价的按照如下过程进行：&lt;/p&gt;
&lt;p&gt;&lt;img alt="game-lda-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/game-lda-2_zps25e3e933.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;以上游戏是先生成了语料中所有词的topic, 然后对每个词在给定topic的条件下生成 word.在语料中所有词的 topic已经生成的条件下，任何两个word的生成动作都是可交换的。于是我们把语料中的词进行交换，把具有相同topic的词放在一起:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
\vec{\mathbf{w}}’ &amp;amp;= (\vec w_{(1)}, \cdots, \vec w_{(K)}) \\ 
\vec{\mathbf{z}}’ &amp;amp;= (\vec z_{(1)}, \cdots, \vec z_{(K)}) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\vec w_{(k)}$表示这些词都是由第$k$个topic生成的，$\vec z_{(k)}$对应于这些词的topic编号，所以$\vec z_{(k)}$中的分量都是$k$。&lt;/p&gt;
&lt;p&gt;对应于概率图中的第二个物理过程$\vec\beta \rightarrow \vec\varphi_k \rightarrow w_{m,n} | k=z_{m,n}$，在$k=z_{m,n}$的限制下，语料中任何两个由 topic $k$生成的词都是可交换的，即便他们不再同一个文档中，所以我们此处不再考虑文档的概念，转而考虑由同一个topic生成的词。考虑如下过程 $\vec\beta \rightarrow \vec\varphi_k \rightarrow \vec w_{(k)}$，容易看出， 此时$\vec\beta \rightarrow \vec\varphi_k$对应于 Dirichlet分布， $\vec\varphi_k \rightarrow \vec w_{(k)}$对应于 Multinomial 分布， 所以整体也还是一个Dirichlet-Multinomial共轭结构；&lt;/p&gt;
&lt;p&gt;&lt;img alt="lda-dir-mult-conjugate-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-dir-mult-conjugate-2_zps564a3b53.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;同样的，我们可以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\vec w_{(k)} |\vec\beta) = \frac{\Delta( n_k+\vec\beta)}{\Delta(\vec\beta)}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$ n_k = (n_{k}^{(1)}, \cdots, n_{k}^{(V)})$， $n_{k}^{(t)}$表示第$k$个topic产生的词中 word $t$的个数。进一步，利用Dirichlet-Multiomial共轭结构，我们得到参数$\vec\varphi_k$的后验分布恰好是$Dir( \vec\varphi_k|  n_k + \vec\beta)$.&lt;/p&gt;
&lt;p&gt;而语料中$K$个topics生成words的过程相互独立，所以我们得到$K$个相互独立的Dirichlet-Multinomial共轭结构，从而我们可以得到整个语料中词生成概率:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(\vec{\mathbf{w}} |\vec{\mathbf{z}},\vec\beta) &amp;amp;= p(\vec{\mathbf{w}}’ |\vec{\mathbf{z}}’,\vec\beta) \notag \\ 
&amp;amp;= \prod_{k=1}^K p(\vec w_{(k)} | \vec z_{(k)}, \vec\beta) \notag \\ 
&amp;amp;= \prod_{k=1}^K \frac{\Delta( n_k+\vec\beta)}{\Delta(\vec\beta)}  \quad\quad (**) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;结合(*)和(**)于是我们得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(\vec{\mathbf{w}},\vec{\mathbf{z}} |\vec\alpha, \vec\beta) &amp;amp;= 
p(\vec{\mathbf{w}} |\vec{\mathbf{z}}, \vec\beta) p(\vec{\mathbf{z}} |\vec\alpha) \notag \\ 
&amp;amp;= \prod_{k=1}^K \frac{\Delta( n_k+\vec\beta)}{\Delta(\vec\beta)} 
\prod_{m=1}^M \frac{\Delta( n_m+\vec\alpha)}{\Delta(\vec\alpha)}  \quad\quad (***) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;此处的符号表示稍微不够严谨, 向量$ n_k$, $ n_m$都用$n$表示， 主要通过下标进行区分， $k$下标为topic编号, $m$下标为文档编号。&lt;/p&gt;
&lt;h2&gt;Gibbs Sampling&lt;/h2&gt;
&lt;p&gt;有了联合分布$p(\vec{\mathbf{w}},\vec{\mathbf{z}})$, 万能的MCMC算法就可以发挥作用了！于是我们可以考虑使用Gibbs Sampling算法对这个分布进行采样。当然由于$\vec{\mathbf{w}}$是观测到的已知数据，只有$\vec{\mathbf{z}}$是隐含的变量，所以我们真正需要采样的是分布$p(\vec{\mathbf{z}}|\vec{\mathbf{w}})$。在Gregor Heinrich 那篇很有名的LDA 模型科普文章Parameter estimation for text analysis中，是基于(***) 式推导Gibbs Sampling 公式的。此小节中我们使用不同的方式，主要是基于Dirichlet-Multinomial共轭来推导 Gibbs Sampling 公式，这样对于理解采样中的概率物理过程有帮助。&lt;/p&gt;
&lt;p&gt;语料库$\vec{\mathbf{z}}$中的第$i$个词我们记为$z_i$, 其中$i=(m,n)$是一个二维下标，对应于第$m$篇文档的第$n$个词，我们用$\neg i$表示去除下标为$i$的词。那么按照 Gibbs Sampling 算法的要求，我们要求得任一个坐标轴$i$对应的条件分布$p(z_i = k|\vec{\mathbf{z}}_{\neg i}, \vec{\mathbf{w}})$。假设已经观测到的词$w_i=t$, 则由贝叶斯法则，我们容易得到:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gibbs_sampling_bayes" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs_sampling_bayes_rule_zps2af0b023.png" /&gt;&lt;/p&gt;
&lt;p&gt;由于$z_i=k,w_i=t$只涉及到第$m$篇文档和第$k$个topic，所以上式的条件概率计算中, 实际上也只会涉及到如下两个Dirichlet-Multinomial 共轭结构:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\vec\alpha \rightarrow \vec\theta_m \rightarrow \vec z_{m}$;&lt;/li&gt;
&lt;li&gt;$\vec\beta \rightarrow \vec\varphi_k \rightarrow \vec w_{(k)}$;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其它的$M+K−2$个Dirichlet-Multinomial共轭结构和$z_i=k,w_i=t$是独立的。由于在语料去掉第$i$个词对应的 $(z_i,w_i)$，并不改变我们之前讨论的$M+K$个Dirichlet-Multinomial共轭结构，只是某些地方的计数会减少。所以$\vec{\theta}_m, \vec\varphi_k$的后验分布都是Dirichlet:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Posterior_Dirichlet" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/posterior_dirichlet_zpsba59a8e5.png" /&gt;&lt;/p&gt;
&lt;p&gt;使用上面两个式子，把以上想法综合一下，我们就得到了如下的Gibbs Sampling公式的推导:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gibbs_sampling_formula_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs_sampling_formula_1_zps3d14e122.png" /&gt;&lt;/p&gt;
&lt;p&gt;以上推导估计是整篇文章中最复杂的数学了，表面上看上去复杂，但是推导过程中的概率物理意义是简单明了的：$z_i=k,w_i=t$的概率只和两个Dirichlet-Multinomial共轭结构关联。而最终得到的$\hat{\theta}&lt;em kt="kt"&gt;{mk}, \hat{\varphi}&lt;/em&gt;$就是对应的两个Dirichlet后验分布在贝叶斯框架下的参数估计。借助于前面介绍的Dirichlet 参数估计的公式 ，我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
\hat\theta_{mk} &amp;amp;= \frac{n_{m,\neg i}^{(k)} + \alpha_k}{\sum_{k=1}^K (n_{m,\neg i}^{(k)} + \alpha_k)} \\ 
\hat\varphi_{kt} &amp;amp;= \frac{n_{k,\neg i}^{(t)} + \beta_t}{\sum_{t=1}^V (n_{k,\neg i}^{(t)} + \beta_t)} 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;于是，我们最终得到了LDA模型的Gibbs Sampling公式:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gibbs_sampling_formula_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs_sampling_formula_2_zps3cc27196.png" /&gt;&lt;/p&gt;
&lt;p&gt;这个公式是很漂亮的， 右边其实就是$p(topic|doc)⋅p(word|topic)$,这个概率其实是doc→topic→word的路径概率，由于topic 有$K$个，所以Gibbs Sampling 公式的物理意义其实就是在这$K$条路径中进行采样。&lt;/p&gt;
&lt;p&gt;&lt;img alt="doc-topic-word" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs-path-search_zpsb83099d8.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;Training and Inference&lt;/h2&gt;
&lt;p&gt;有了LDA模型，当然我们的目标有两个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;估计模型中的参数$\vec\varphi_1, \cdots, \vec\varphi_K$和$\vec{\theta}_1, \cdots, \vec{\theta}_M$；&lt;/li&gt;
&lt;li&gt;对于新来的一篇文档$doc_{new}$，我们能够计算这篇文档的topic分布$\vec{\theta}_{new}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有了Gibbs Sampling公式， 我们就可以基于语料训练LDA模型，并应用训练得到的模型对新的文档进行topic 语义分析。训练的过程就是获取语料中的$(z,w)$的样本，而模型中的所有的参数都可以基于最终采样得到的样本进行估计。训练的流程很简单:&lt;/p&gt;
&lt;p&gt;&lt;img alt="LDA Training" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-training_zpsa31be49e.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;对于Gibbs Sampling算法实现的细节，请参考Gregor Heinrich的 Parameter estimation for text analysis 中对算法的描述，以及PLDA(http://code.google.com/p/plda)的代码实现，此处不再赘述。&lt;/p&gt;
&lt;p&gt;由这个topic-word频率矩阵我们可以计算每一个$p(word|topic)$概率，从而算出模型参数$\vec\varphi_1, \cdots, \vec\varphi_K$, 这就是上帝用的$K$个topic-word骰子。当然，语料中的文档对应的骰子参数$\vec{\theta}_1, \cdots, \vec{\theta}_M$在以上训练过程中也是可以计算出来的，只要在Gibbs Sampling收敛之后，统计每篇文档中的topic的频率分布，我们就可以计算每一个$p(topic|doc)$概率，于是就可以计算出每一个$\vec{\theta}_m$。由于参数$\vec{\theta}_m$是和训练语料中的每篇文档相关的，对于我们理解新的文档并无用处，所以工程上最终存储LDA模型时候一般没有必要保留。通常，在LDA模型训练的过程中，我们是取Gibbs Sampling收敛之后的$n$个迭代的结果进行平均来做参数估计，这样模型质量更高。&lt;/p&gt;
&lt;p&gt;有了LDA的模型，对于新来的文档 $doc_{new}$, 我们如何做该文档的topic语义分布的计算呢？基本上inference的过程和training的过程完全类似。对于新的文档， 我们只要认为Gibbs Sampling公式中的$\hat\varphi_{kt}$部分是稳定不变的，是由训练语料得到的模型提供的，所以采样过程中我们只要估计该文档的topic分布$\vec{\theta}_{new}$就好了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="LDA Inference" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-inference_zpsaa5c9320.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;后记LDA&lt;/h2&gt;
&lt;p&gt;对于专业做机器学习的兄弟而言，只能算是一个简单的Topic Model。但是对于互联网中做数据挖掘、语义分析的工程师，LDA 的门槛并不低。 LDA 典型的属于这样一种机器学习模型：要想理解它，需要比较多的数学背景，要在工程上进行实现，却相对简单。 Gregor Heinrich 的LDA 模型科普文章 Parameter estimation for text analysis 写得非常的出色，这是学习 LDA 的必看文章。不过即便是这篇文章，对于工程师也是有门槛的。我写的这个科普最好对照 Gregor Heinrich 的这篇文章来看， 我用的数学符号也是尽可能和这篇文章保持一致。这份LDA 科普是基于给组内兄弟做报告的 ppt 整理而成的，说是科普其实也不简单，涉及到的数学还是太多。在工业界也混了几年，经常感觉到工程师对于学术界的玩的模型有很强的学习和尝试的欲望，只是学习成本往往太高。所以我写 LDA 的初衷就是写给工业界的工程师们看的，希望把学术界玩的一些模型用相对通俗的方式介绍给工程师；如果这个科普对于读研究生的一些兄弟姐妹也有所启发，只能说那是一个 side effect :-)。我个人很喜欢LDA ，它是在文本建模中一个非常优雅的模型，相比于很多其它的贝叶斯模型， LDA 在数学推导上简洁优美。学术界自 2003 年以来也输出了很多基于LDA 的 Topic Model 的变体，要想理解这些更加高级的 Topic Model, 首先需要很好的理解标准的 LDA 模型。在工业界， Topic Model 在 Google、Baidu 等大公司的产品的语义分析中都有着重要的应用；所以Topic Model 对于工程师而言，这是一个很有应用价值、值得学习的模型。我接触 Topic Model 的时间不长，主要是由于2年前和 PLDA 的作者 Wangyi 一起合作的过程中，从他身上学到了很多 Topic Model 方面的知识。关于 LDA 的相关知识，其实可以写的还有很多：如何提高 LDA Gibbs Sampling 的速度、如何优化超参数、如何做大规模并行化、LDA 的应用、LDA 的各种变体…… 不过我的主要目标还是科普如何理解标准的LDA模型。学习一个模型的时候我喜欢追根溯源，常常希望把模型中的每一个数学推导的细节搞明白，把公式的物理意义想清楚，不过数学推导本身并不是我想要的，把数学推导还原为物理过程才是我乐意做的事。最后引用一下物理学家费曼的名言结束 LDA 的数学科普：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What I cannot create, I do not understand. — Richard Feynman&lt;/p&gt;
&lt;/blockquote&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="NLP"></category></entry><entry><title>自然语言处理(序章):我爱自然语言处理(I)</title><link href="http://www.qingyuanxingsi.com/zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i.html" rel="alternate"></link><updated>2014-05-05T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-05:zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i.html</id><summary type="html">&lt;p&gt;昨天浏览了一下&lt;a href="http://www.52nlp.cn"&gt;我爱自然语言处理&lt;/a&gt;站点上的全部文章,然后基本过滤下来自己感兴趣的90篇左右的文章,这一阵子就先把这90篇文章认认真真看完吧,总结看的过程中自己感兴趣而且重要的点,遂成此文。&lt;strong&gt;本文中所有资料属我爱自然语言处理及博客原文引用作者所有,特此声明&lt;/strong&gt;。&lt;/p&gt;
&lt;h1&gt;齐夫定律(Zipf’s Law)&lt;/h1&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Zipf's Law&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;在任何一个自然语言里第$n$个最常用的单词的频率与$n$近似成反比(The frequency of use of the nth-most-frequently-used word in any natural language is approximately inversely proportional to n).更正式地,我们可以说:存在一个常量$k$,使得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
f \times r =k
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$f$表示单词出现的频度,$r$表示单词出现次数的排名(RANK).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Zipf" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/zipf_zpsae557119.png"&gt;&lt;/p&gt;
&lt;p&gt;北京大学姜望琪老师的《Zipf与省力原则》讲得很好，部分摘录如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;省力原则(the Principle of Least Effort)，又称经济原则(the Economy Principle)，可以概括为：以最小的代价换取最大的收益。这是指导人类行为的一条根本性原则。在现代学术界，第一个明确提出这条原则的是美国学者 George Kingsley Zipf。　　&lt;/li&gt;
&lt;li&gt;George Kingsley Zipf1902年1月出生于一个德裔家庭（其祖父十九世纪中叶移居美国)。1924年，他以优异成绩毕业于哈佛学院。1925年在德国波恩、柏林学习。1929年完成Relative Frequency as a Determinant of Phonetic Change，获得哈佛比较语文学博士学位。然后，他开始在哈佛教授德语。1931年与Joyce Waters Brown结婚。1932年出版Selected Studies of the Principle of Relative Frequency in Language。1935年出版The Psycho- Biology of Language：An Introduction to Dynamic Philology。1939年被聘为讲师。1949年出版Human Behavior and the Principle of Least Effort：An Introduction to Human Ecology。1950年9月因患癌症病逝。　　&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zipf在1949年的书里提出了一条指导人类行为的基本原则——省力原则。Zipf在序言里指出，如果我们把人类行为纯粹看作一种自然现象，如果我们像研究蜜蜂的社会行为、鸟类的筑巢习惯一样研究人类行为，那么，我们就有可能揭示其背后的基本原则。这是他提出“省力原则”的大背景。当Zipf在众多互不相干的现象里都发现类似Zipf定律的规律性以后，他就开始思考造成这种规律性的原因。这是导致他提出“省力原则”的直接因素。在开始正式论证以前，Zipf首先澄清了“省力原则”的字面意义。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一，这是一种平均量。一个人一生要经历很多事情，他在一件事情上的省力可能导致在另一件事情上的费力。反过来，在一件事情上的费力，又可能导致在另一件事情上的省力。&lt;/li&gt;
&lt;li&gt;第二，这是一种概率。一个人很难在事先百分之百地肯定某种方法一定能让他省力，他只能有一个大概的估计。因为用词研究是理解整个言语过程的关键，而后者又是理解整个人类生态学的关键，他的具体论证从用词经济开始。Zipf认为，用词经济可以从两个角度来讨论：说话人的角度和听话人的角度。从说话人的角度看，用一个词表达所有的意义是最经济的。这样，说话人不需要花费气力去掌握更多的词汇，也不需要考虑如何从一堆词汇中选择一个合适的词。这种“单一词词汇量”就像木工的一种多用工具，集锯刨钻锤于一身，可以满足多种用途。但是，从听话人角度看，这种“单一词词汇量”是最费力的。他要决定这个词在某个特定场合到底是什么意思，而这几乎是不可能的。相反，对听话人来说，最省力的是每个词都只有一个意义，词汇的形式和意义之间完全一一对应。这两种经济原则是互相冲突、互相矛盾的。Zipf把它们叫做一条言语流中的两股对立的力量：“单一化力量”（the Force of Unification）和“多样化力量”（the Force of Diversification）。他认为，这两股力量只有达成妥协，达成一种平衡，才能实现真正的省力。事实正像预计的那样。请看Zipf的论证：假如只有单一化力量，那么任何语篇的单词数量（number）都会是1，而它的出现次数（frequency）会是100%。另一方面，假如只有多样化力量，那么每个单词的出现次数都会接近1，而单词总数量则由语篇的长度决定。这就是说， &lt;em&gt;number&lt;/em&gt;和&lt;em&gt;frequency&lt;/em&gt;是衡量词汇平衡程度的两个参数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;中文分词&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;对于英文而言,由于词自然一般有非常自然的分隔符(空格或标点符号等),因此对于英文而言基本不涉及分词这个任务,而对于中文而言,因为中文没有非常明显的自然分隔符,而且很多自然语言处理任务很大程度上依赖于分词质量,因此中文分词是中文自然语言处理中非常基础且重要的一个任务,以下对中文分词中涉及的基本算法做一个简要的介绍:&lt;/p&gt;
&lt;h2&gt;最长正向匹配算法&lt;/h2&gt;
&lt;p&gt;最长正向匹配算法的基本流程如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="MAX_SEGMENTATION" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/max_segmentation_zpsadc70b2d.png"&gt;&lt;/p&gt;
&lt;p&gt;逆向匹配法思想与正向一样，只是从右向左切分，这里举一个例子：&lt;/p&gt;
&lt;p&gt;输入例句:S1=”计算语言学课程有意思”;&lt;/p&gt;
&lt;p&gt;定义:最大词长MaxLen = 5；S2="";分隔符="/"；&lt;/p&gt;
&lt;p&gt;假设存在词表:计算语言学,课程,意思,...；最大逆向匹配分词算法过程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;S2=””；S1不为空，从S1右边取出候选子串W=”课程有意思”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有意思”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”有意思”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”意思”&lt;/li&gt;
&lt;li&gt;查词表，“意思”在词表中，将W加入到S2中，S2=” 意思/”，并将W从S1中去掉，此时S1=”计算语言学课程有”；&lt;/li&gt;
&lt;li&gt;S1不为空，于是从S1左边取出候选子串W=”言学课程有”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程有”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程有”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”有”，这W是单字，将W加入到S2中，S2=“/有/意思”，并将W从S1中去掉，此时S1=”计算语言学课程”；&lt;/li&gt;
&lt;li&gt;S1不为空，于是从S1左边取出候选子串W=”语言学课程”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”言学课程”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程”；&lt;/li&gt;
&lt;li&gt;查词表，“意思”在词表中，将W加入到S2中，S2=“ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=”计算语言学”；&lt;/li&gt;
&lt;li&gt;S1不为空，于是从S1左边取出候选子串W=”计算语言学”；&lt;/li&gt;
&lt;li&gt;查词表，“计算语言学”在词表中，将W加入到S2中，S2=“计算语言学/ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=””；&lt;/li&gt;
&lt;li&gt;S1为空，输出S2作为分词结果，分词过程结束。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;至于怎么实现,&lt;a href="http://yangshangchuan.iteye.com/blog/2031813"&gt;中文分词算法之基于词典的正向最大匹配算法&lt;/a&gt;一文中对针对JAVA的实现有非常详尽的性能分析,其实吧,个人觉得算法无非是在时间和空间间的权衡,对Hash式存储结构而言,一般来讲,空间开销是很大的,而时间上可以做的很好;对于类似于Trie树的数据结构,在某种程度上能节省一定的空间,但肯定比Hash类数据结构慢点。这里我们就不纠结数据结构和性能的差异了,我们使用STL set&lt;sup id="sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-1-back"&gt;&lt;a class="simple-footnote" href="#sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-1" title="STL中set的简单学习"&gt;1&lt;/a&gt;&lt;/sup&gt;实现上述功能。&lt;/p&gt;
&lt;p&gt;以下给出逆向最长匹配算法C++源码(&lt;strong&gt;代码中词典的初始化只用了几个词,实际中可从词表文件中读取并构造一个词典,此处代码只是为了演示算法框架&lt;/strong&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="vi"&gt;#include&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;iostream&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="vi"&gt;#include&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;

&lt;span class="nx"&gt;using&lt;/span&gt; &lt;span class="nx"&gt;namespace&lt;/span&gt; &lt;span class="nx"&gt;std&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt; *A simple inverse match algorithm&lt;/span&gt;
&lt;span class="cm"&gt; *@author:qingyuanxingsi&lt;/span&gt;
&lt;span class="cm"&gt; *@date:2014-05-04&lt;/span&gt;
&lt;span class="cm"&gt; *@version:1.0&lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;
&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="nx"&gt;argc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;char&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="nx"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;//Max word length&lt;/span&gt;
    &lt;span class="nx"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;max_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nx"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;string&lt;/span&gt; &lt;span class="n"&gt;split_sequence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"/"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nx"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;string&lt;/span&gt; &lt;span class="n"&gt;to_split&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"计算语言学真有意思啊"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;string&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="c1"&gt;//Initialize the dict&lt;/span&gt;
    &lt;span class="nx"&gt;dict.insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"计算语言学"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="nx"&gt;dict.insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"意思"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="c1"&gt;//Split the Chinese Sequence&lt;/span&gt;
    &lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;tail&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;to_split.length&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="nb"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;max_len&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
      &lt;span class="kt"&gt;string&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;to_split.substr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="na"&gt;-i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="c1"&gt;//If single word&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;temp.length&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="nx"&gt;cout&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;split_sequence&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;temp&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="nx"&gt;tail&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;max_len&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;max_len&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;dict.find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="nx"&gt;dict.end&lt;/span&gt;&lt;span class="p"&gt;()){&lt;/span&gt;
        &lt;span class="nx"&gt;cout&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;split_sequence&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;temp&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="nx"&gt;tail&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;max_len&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;max_len&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;  
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;NOTE:这种机械的分词方法实际上是远远满足不了我们的需要的,对于某些特定的句子不管采用正向最长匹配还是逆向最长匹配都会产生错误切分。比如说&lt;strong&gt;"结婚的和尚未结婚的"&lt;/strong&gt;,采用正向最长匹配就得不到正确的分词结果,逆向最长匹配也类同。类似的分词方法还有&lt;strong&gt;最小词数法&lt;/strong&gt;等。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;基于以上简单的中文分词算法，很多学者进行了改进,我爱自然语言网站上介绍了一个叫MMSEG的系统,个人不是很感兴趣,有兴趣的同学可参考如下链接:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%B3%95%E6%89%A9%E5%B1%951"&gt;中文分词入门之最大匹配法扩展1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%B3%95%E6%89%A9%E5%B1%952"&gt;中文分词入门之最大匹配法扩展2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E7%AF%87%E5%A4%96"&gt;中文分词入门之篇外&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;基于字标注的中文分词&lt;sup id="sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-2-back"&gt;&lt;a class="simple-footnote" href="#sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-2" title="本部分更多细节请参考我爱自然语言处理博客!"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;以往的分词方法，无论是基于规则的还是基于统计的，一般都依赖于一个事先编制的词表(词典)。自动分词过程就是通过词表和相关信息来做出词语切分的决策。与此相反，基于字标注的分词方法实际上是构词方法。即把分词过程视为字在字串中的标注问题。由于每个字在构造一个特定的词语时都占据着一个确定的构词位置(即词位)，假如规定每个字最多只有四个构词位置：即B(词首)，M (词中)，E(词尾)和S(单独成词)，那么下面句子(甲)的分词结果就可以直接表示成如(乙)所示的逐字标注形式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(甲)分词结果：／上海／计划／N／本／世纪／末／实现／人均／国内／生产／总值／五千美元／。&lt;/p&gt;
&lt;p&gt;(乙)字标注形式：上/B 海／E 计／B 划／E N／S 本／s 世／B 纪／E 末／S 实／B 现／E 人／B 均／E 国／B 内／E生／B产／E总／B值／E 五／B千／M 美／M 元／E 。／S&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先需要说明，这里说到的“字”不只限于汉字。考虑到中文真实文本中不可避免地会包含一定数量的非汉字字符，本文所说的“字”，也包括外文字母、阿拉伯数字和标点符号等字符。所有这些字符都是构词的基本单元。当然，汉字依然是这个单元集合中数量最多的一类字符。&lt;/p&gt;
&lt;p&gt;把分词过程视为字的标注问题的一个重要优势在于，&lt;strong&gt;它能够平衡地看待词表词和未登录词的识别问题&lt;/strong&gt;。在这种分词技术中，文本中的词表词和未登录词都是用统一的字标注过程来实现的。在学习架构上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词(如人名、地名、机构名)识别模块。这使得分词系统的设计大大简化。在字标注过程中，所有的字根据预定义的特征进行词位特性的学习，获得一个概率模型。然后，在待分字串上，根据字与字之间的结合紧密程度，得到一个词位的标注结果。最后，根据词位定义直接获得最终的分词结果。总而言之，在这样一个分词过程中，分词成为字重组的简单过程。然而这一简单处理带来的分词结果却是令人满意的。&lt;/p&gt;
&lt;p&gt;在&lt;a href="http://www.52nlp.cn/two-innovative-ideas-in-natural-language-processing-area"&gt;《自然语言处理领域的两种创新观念》&lt;/a&gt;中，张俊林博士谈了两种创新模式：&lt;strong&gt;一种创新是研究模式的颠覆，另外一种创新是应用创新&lt;/strong&gt;，前者需要NLP领域出现爱因斯坦式的革新人物，后者则是强调用同样的核心技术做不一样的应用。&lt;/p&gt;
&lt;p&gt;在自然语言处理领域，多数创新都属于后者，譬如统计机器翻译，Brown就是学习和借鉴了贾里尼克将语音识别看成通信问题的思想，将信源信道模型应用到了机器翻译之中，从而开辟了SMT这一全新领域。而Nianwen Xue将词性标注的思想应用到中文分词领域，成就了字标注的中文分词方法（Chinese Word Segmentation as Character Tagging），同样取得了巨大的成功。&lt;/p&gt;
&lt;p&gt;既然基于字标注的中文分词方法是将中文分词当作词性标注的问题来对待，那么就必须有标注对象和标注集了。形象一点，从这个方法的命名上我们就可以推断出它的标注是基本的汉字（还包括一定数量的非汉字字符），而标注集则比较灵活，这些标注集都是依据汉字在汉语词中的位置设计的，最简单的是2-tag，譬如将词首标记设计为B，而将词的其他位置标记设计为I，那么“中国”就可以标记为“中/B 国/I”，“海南岛”则可以标记为“海/B 南/I 岛/I”，相应地，对于如下分好词的句子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;瓦西里斯&lt;/span&gt; &lt;span class="err"&gt;的&lt;/span&gt; &lt;span class="err"&gt;船只&lt;/span&gt; &lt;span class="err"&gt;中&lt;/span&gt; &lt;span class="err"&gt;有&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="err"&gt;驶&lt;/span&gt; &lt;span class="err"&gt;向&lt;/span&gt; &lt;span class="err"&gt;远东&lt;/span&gt; &lt;span class="err"&gt;，&lt;/span&gt; &lt;span class="err"&gt;每个&lt;/span&gt; &lt;span class="err"&gt;月&lt;/span&gt; &lt;span class="err"&gt;几乎&lt;/span&gt; &lt;span class="err"&gt;都&lt;/span&gt; &lt;span class="err"&gt;有&lt;/span&gt; &lt;span class="err"&gt;两三条&lt;/span&gt; &lt;span class="err"&gt;船&lt;/span&gt; &lt;span class="err"&gt;停靠&lt;/span&gt; &lt;span class="err"&gt;中国&lt;/span&gt; &lt;span class="err"&gt;港口&lt;/span&gt; &lt;span class="err"&gt;。&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;基于2-tag(B,I)的标注就是：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;瓦&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;西&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;里&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;斯&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;的&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;船&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;只&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;中&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;有&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;４&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;０&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;％&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;驶&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;向&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;远&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;东&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;，&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;每&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;个&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;月&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;几&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;乎&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;都&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;有&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;两&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;三&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;条&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;船&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;停&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;靠&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;中&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;国&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;港&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;口&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;。&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;除了2-tag，还有4-tag、6-tag等，都是依据字在词中的位置设计的，本文主要目的是从实践的角度介绍基于字标注的中文分词方法设计，以达到抛砖引玉的作用，因此我们仅选用2-tag（B，I）标注集进行实验说明。有了标注对象和标注集，那么又如何进行中文分词呢？因为字标注本质上是采用POS Tagging的思想,只不过要TAG的基本单元现在变成字了而已,因此我们可以这样做:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;获取已分词语料,将其转化为字的形式并采用某种标注集根据分词信息对其进行标注;&lt;/li&gt;
&lt;li&gt;将得到的语料作为训练集输入到最大熵模型或者HMM模型中进行训练(&lt;strong&gt;可以使用Citar&lt;/strong&gt;);&lt;/li&gt;
&lt;li&gt;利用训练后模型对未分词语料进行字标注,最后还原成分词结果即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:利用现有开源工具时,如果能够构建适用于中文字标注的特征集合,然后再进行训练,可能会取得更好的结果。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;鲁棒性NLP系统(观点)&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;一个 real life 自然语言处理系统，其质量和可用度除了传统的 data quality 的衡量指标查准度（precision）和查全度（recall）外，还有更为重要的三大指标：&lt;strong&gt;海量处理能力（scalability）, 深度（depth）和鲁棒性（robustness）&lt;/strong&gt;。本部分就简单谈一下鲁棒性。&lt;/p&gt;
&lt;p&gt;为了取得语言处理的鲁棒性（robustness），一个行之有效的方法是实现四个形容词的所指：&lt;strong&gt;词汇主义（lexicalist）; 自底而上（bottom-up）; 调适性（adaptive）；和数据制导（data-driven）&lt;/strong&gt;。这四条是相互关联的，但各自重点和视角不同。系统设计和开发上贯彻这四项基本原则， 是取得坚固性的良好保证。有了坚固性，系统对于不同领域的语言，甚至对极不规范的社会媒体中的语言现象，都可以应对。这是很多实用系统的必要条件。&lt;/p&gt;
&lt;p&gt;先说&lt;strong&gt;词汇主义策略&lt;/strong&gt;。词汇主义的语言处理策略是学界和业界公认的一个有效的方法。具体说来就是在系统中增加词汇制导的个性规则的总量。自然语言的现象是如此复杂，几乎所有的规则都有例外，词汇制导是必由之路。从坚固性而言，更是如此。基本的事实是，语言现象中的所谓子语言（sublanguage），譬如专业用语，网络用语，青少年用语，他们之间的最大区别是在词汇以及词汇的用法上。一般来说，颗粒度大的普遍语法规则在各子语言中依然有效。因此，采用词汇主义策略，可以有效地解决子语言的分析问题，从而提高系统的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;自底而上的分析方法&lt;/strong&gt;。这种方法对于自浅而深的管式系统最自然。系统从单词出发，一步一步形成越来越大的句法单位，同时解析句法成分之间的关系。其结果是自动识别（构建）出来的句法结构树。很多人都知道社会媒体的混乱性，这些语言充满了错别字和行话，语法错误也随处可见。错别字和行话由词汇主义策略去对付，语法错误则可以借助自底而上的分析方法。其中的道理就是，即便是充满了语法错误的社会媒体语言，其实并不是说这些不规范的语言完全不受语法规则的束缚，无章可循。事实绝不是如此，否则人也不可理解，达不到语言交流的目的。完全没有语法的“语言”可以想象成一个随机发生器，随机抽取字典或词典的条目发射出来，这样的字串与我们见到的最糟糕的社会媒体用语也是截然不同的。事实上，社会媒体类的不规范语言（degraded text）就好比一个躁动不安的逆反期青年嬉皮士，他们在多数时候是守法的，不过情绪不够稳定，不时会”突破”一下规章法律。具体到语句，其对应的情形就是，每句话里面的多数短语或从句是合法的，可是短语（或从句）之间常常会断了链子。这种情形对于自底而上的系统，并不构成大的威胁。因为系统会尽其所能，一步一步组合可以预测（解构）的短语和从句，直到断链的所在。这样一来，一个句子可能形成几个小的句法子树（sub-tree），子树之内的关系是明确的。朋友会问：既然有断链，既然子树没有形成一个完整的句法树来涵盖所分析的语句，就不能说系统真正鲁棒了，自然语言理解就有缺陷。抽象地说，这话不错。但是在实际使用中，问题远远不是想象的那样严重。其道理就是，语言分析并非目标，语言分析只是实现目标的一个手段和基础。对于多数应用型自然语言系统来说，目标是信息抽取（Information Extraction），是这些预先定义的抽取目标在支持应用（app）。抽取模块的屁股通常坐在分析的结构之上，典型的抽取规则 by nature 是基于子树匹配的，这是因为语句可以是繁复的，但是抽取的目标相对单纯，对于与目标不相关的结构，匹配规则无需cover。这样的子树匹配分两种情形，其一是抽取子树（subtree1）的规则完全匹配在语句分析的子树（subtree2）之内（i.e. subtree2 &amp;gt; subtree1），这种匹配不受断链的任何影响，因此最终抽取目标的质量不受损失。只有第二种情形，即抽取子树恰好坐落在分析语句的断链上，抽取不能完成，因而印象了抽取质量。值得强调的是，一般来说，情形2的出现概率远低于情形1，因此自底而上的分析基本保证了语言结构分析的鲁棒性，从而保障了最终目标信息抽取的达成。其实，对于 worst case scenario 的情形2，我们也不是没有办法补救。补救的办法就是在分析的后期把断链 patch 起来，虽然系统无法确知断链的句法关系的性质，但是patched过的断链形成了一个完整的句法树，为抽取模块的补救创造了条件。此话怎讲？具体说来就是，只要系统的设计和开发者坚持&lt;strong&gt;调适性开发&lt;/strong&gt;抽取模块（adaptive extraction）的原则，部分抽取子树的规则完全可以建立在被patched的断链之上，从而在不规范的语句中达成抽取。其中的奥妙就是某样榜戏中所说的墙内损失墙外补，用到这里就是结构不足词汇补。展开来说就是，任何子树匹配不外乎check两种条件约束，一是节点之间的关系句法关系的条件（主谓，动宾，等等），另外就是节点本身的词汇条件（产品，组织，人，动物，等等）。这些抽取条件可以相互补充，句法关系的条件限制紧了，节点词汇的条件就可以放宽；反之亦然。即便对于完全合法规范的语句，由于语言分析器不可避免的缺陷而可能导致的断链（世界上除了上帝以外不存在完美的系统），以及词汇语义的模糊性，开发者为了兼顾查准率和查全率，也会在抽取子树的规则上有意平衡节点词汇的条件和句法关系的条件。如果预知系统要用于不规范的语言现象上，那么我们完全可以特制一些规则，利用强化词汇节点的条件来放宽对于节点句法关系的条件约束。其结果就是适调了patched的断链，依然达成抽取。说了一箩筐，总而言之，言而总之，对于语法不规范的语言现象，自底而上的分析策略是非常有效的，加上调适性开发，可以保证最终的抽取目标基本不受影响。&lt;/p&gt;
&lt;p&gt;调适性上面已经提到，作为一个管式系统的开发原则，这一条很重要，它是克服错误放大（error propagation）的反制。理想化的系统，模块之间的接口是单纯明确的，铁路警察，各管一段，步步推进，天衣无缝。但是实际的系统，特别是自然语言系统，情况很不一样，良莠不齐，正误夹杂，后面的模块必须设计到有足够的容错能力，针对可能的偏差做调适才不至于一错再错，步步惊心。如果错误是consistent/predictable 的，后面的模块可以矫枉过正，以毒攻毒，错错为正。还有一点就是歧义的保存（keeping ambiguity untouched）策略。很多时候，前面的模块往往条件不成熟，这时候尽可能保持歧义，运用系统内部的调适性开发在后面的模块处理歧义，往往是有效的。&lt;/p&gt;
&lt;p&gt;最后，&lt;strong&gt;数据制导&lt;/strong&gt;的开发原则，怎样强调都不过分。语言海洋无边无涯，多数语言学家好像一个爱玩水的孩子，跳进海洋往往坐井观天，乐不思蜀。见树木不见森林，一条路走到黑，是很多语言学家的天生缺陷。如果由着他们的性子来，系统的overhead越来越大，效果可能越来越小。数据制导是迫使语言学家回到现实，开发真正有现实和统计意义的系统的一个保证。这样的保证应该制度化，这牵涉到开发语料库（dev corpus）的选取，baseline 的建立和维护，unit testing 和regression testing 等开发操作规范的制定以及 data quality QA 的配合。理想的数据制导还应该包括引入机器学习的方法，来筛选制约具有统计意义的语言现象反馈给语言学家。从稍微长远一点看，自动分类用户的数据反馈，实现某种程度的粗颗粒度的自学习，建立半自动人际交互式开发环境，这是手工开发和机器学习以长补短的很有意义的思路。以上所述，每一条都是经验的总结，背后有成百上千的实例可以详加解说。不过，网文也不是科普投稿，没时间去细细具体解说了。做过的自然有同感和呼应，没做过的也许不明白，等做几年就自然明白了，又不是高精尖的火箭技术。&lt;/p&gt;
&lt;h1&gt;无约束最优化&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;看了一下我爱自然语言处理博客上关于无约束优化的几篇文章,可能是自己水平很烂的原因,感觉怪怪的,好像有点不对劲,还是自己查相关资料吧。以下给出那几篇的链接.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/unconstrained-optimization-one"&gt;无约束最优化一&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/unconstrained-optimization-two"&gt;无约束最优化二&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/unconstrained-optimization-three"&gt;无约束最优化三&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/unconstrained-optimization-four"&gt;无约束最优化四&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/unconstrained-optimization-five"&gt;无约束最优化五&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;资源集锦&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://clair.eecs.umich.edu/aan/index.php"&gt;ACL Anthology Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ldc.upenn.edu/"&gt;LDC (Linguistic Data Consortium)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://web-ngram.research.microsoft.com/info/quickstart.htm"&gt;Microsoft N-gram Service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://start.csail.mit.edu/index.php"&gt;Start Question-Answering System&lt;/a&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Start_China" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/start_china_zps584efc72.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://wing.comp.nus.edu.sg:8080/SMSCorpus/"&gt;Collecting SMS Messages for a Public Research Corpus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.statmt.org/moses/"&gt;Moses|统计机器翻译&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TODO Board:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;TBL&lt;/em&gt;(&lt;strong&gt;参考《自然语言处理综论》第8章&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最大熵求解算法IIS等&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/%E5%88%9D%E5%AD%A6%E8%80%85%E6%8A%A5%E9%81%93%EF%BC%882%EF%BC%89%EF%BC%9A%E5%AE%9E%E7%8E%B0-1-gram%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95"&gt;1-Gram Python分词实现&lt;/a&gt;;之后自己实现一个3-gram Language Model吧!&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/"&gt;Topic modeling made just simple enough&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-1"&gt;&lt;a href="http://www.cppblog.com/shongbee2/archive/2009/04/05/79011.html"&gt;STL中set的简单学习&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-2"&gt;本部分更多细节请参考我爱自然语言处理博客! &lt;a class="simple-footnote-back" href="#sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="NLP"></category></entry><entry><title>机器学习拾遗(II):大杂烩</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-shi-yi-iida-za-hui.html" rel="alternate"></link><updated>2014-05-02T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-02:ji-qi-xue-xi-shi-yi-iida-za-hui.html</id><summary type="html">&lt;p&gt;本文主要对之前看过的&lt;em&gt;Machine Learning:A Probabilistic Perspective&lt;/em&gt;一书中有关离散数据生成模型、高斯模型以及线性模型的相关部分进行梳理。&lt;/p&gt;
&lt;h1&gt;Closed Form&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1" title="Closed-form expression"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;在阅读&lt;em&gt;Machine Learning:A Probabilistic Perspective&lt;/em&gt;这本书的时候经常看到&lt;strong&gt;Closed Form&lt;/strong&gt;这个词。以下给出一简单说明:&lt;/p&gt;
&lt;p&gt;如果一个表达式可以表示成&lt;strong&gt;有限的&lt;/strong&gt;某些特定的&lt;strong&gt;Ｗell-known&lt;/strong&gt;函数的形式，那么我们说这个表达式是一个&lt;strong&gt;Closed Form&lt;/strong&gt;表达式。一般而言,这些&lt;strong&gt;Well-known&lt;/strong&gt;函数包括基本的函数，如常数、单个变量$x$、基本的算术操作、开方、指数对数函数、三角反三角函数等。如果一个问题可以得到一&lt;strong&gt;Closed Form&lt;/strong&gt;的解,我们通常就说这个问题是可解的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closed–form&lt;/strong&gt;表达式是解析表达式的一个非常重要的子类,解析表达式可包含有限或无限个&lt;strong&gt;Well-known&lt;/strong&gt;函数。和更为广泛的解析表达式不同,&lt;strong&gt;Closed–form&lt;/strong&gt;表达式并不包含无限级数和递归分数(&lt;em&gt;Continued Fractions&lt;/em&gt;)形式，也不能包含积分和极限。&lt;/p&gt;
&lt;p&gt;一个方程或方程组当且仅当它的一个解可写成&lt;strong&gt;Closed-Form&lt;/strong&gt;形式时,我们才说该方程或方程组具有一&lt;strong&gt;Closed-form solution&lt;/strong&gt;.类似地,一个方程或方程组当且仅当它的一个解可写成解析形式时,我们才说该方程或方程组具有一解析解。&lt;/p&gt;
&lt;h1&gt;Gaussian Models&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-2-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-2" title="Machine Learning:A Probabilistic Perspective Chapter 4"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;Diagonal LDA&lt;/h2&gt;
&lt;p&gt;在GLA中,如果我们限定不同类间共享$\Sigma$,并对每类使用一对角协方差矩阵，我们得到的模型则称为&lt;strong&gt;Diagonal LDA Model&lt;/strong&gt;.对应的判别函数如下所示:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\delta_c(x) = log\ p(x,y=c|\theta) = -\sum_{j=1}^D \frac{(x_j-\mu_{cj})^2}{2\sigma_j^2}+log\ \pi_c
\end{equation}&lt;/p&gt;
&lt;p&gt;一般我们令$\hat\mu_{cj} = \bar{x}_{cj}$且$\hat\sigma_j^2=s_j^2$,$s_j^2$称为&lt;em&gt;Pooled empirical variance&lt;/em&gt;,定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
s_j^2 = \frac{\sum_{c=1}^C \sum_{i:y_i=c} (x_{ij} - \bar{x}_{cj})^2}{N-C}
\end{equation}&lt;/p&gt;
&lt;p&gt;在高维情形下,该模型比LDA和RDA效果都要好。&lt;/p&gt;
&lt;h2&gt;方差MAP估计&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;ML&lt;/em&gt;(指Machine Learning:A Probabilistic Perspective,以后如果不作说明,也指该书)一书中关于$\Sigma$的估计写的很复杂,于是基本上没看那部分,关于$\Sigma$的MAP估计可参考以下链接:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/"&gt;Regularized Gaussian Covariance Estimation&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Linear Regression&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-3-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-3" title="Machine Learning:A Probabilistic Perspective Chapter 7"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;Bayesian Linear Regression&lt;/h2&gt;
&lt;p&gt;给定训练集$D={(x_i,y_i)|i=1,\cdots,n}$,我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Design_Matrix" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/design_matrix_zpse58dd2c8.png"&gt;&lt;/p&gt;
&lt;p&gt;对于Linear Regression,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
f(x) &amp;amp; = x^Tw \in R,x,w \in R^D   \\
y &amp;amp;= f(x) + \epsilon = x^Tw+\epsilon \in R \\
\epsilon &amp;amp;\sim\ N(0,\sigma^2) \in R
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们首先计算Likelihood:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
P(y|X,w) &amp;amp;= \prod_{i=1}^n P(y_i|x_i^Tw) \\
&amp;amp;= \prod_{i=1}^n N_{yi}(x_i^Tw,\sigma^2) \\
&amp;amp;= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}exp[\frac{-(y_i-x_i^Tw)^2}{2\sigma^2}] \\
&amp;amp;= \frac{1}{(2\pi\sigma^2)^{n/2}}exp[\frac{-1}{2\sigma^2}||y-X^Tw||^2] \\
&amp;amp;= N_y(X^Tw,\sigma^2I_n)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;另我们假定Prior服从$w \sim\ N_w(0,\Sigma_p)$&lt;/p&gt;
&lt;p&gt;则我们可得Posterior:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
P(w|X,y) &amp;amp;= \frac{P(y|X,w)P(w)}{P(y|X)}  \\
&amp;amp;= \frac{P(y|X,w)P(w)}{\int P(y|X,w)dw}  \\
&amp;amp;= \frac{N_y(X^Tw,\sigma^2I_n)N_w(0,\Sigma_p)}{\int N_y(X^Tw,\sigma^2I_n)N_w(0,\Sigma_p)dw} \\
&amp;amp;\sim\ N_y(X^Tw,\sigma^2I_n)N_w(0,\Sigma_p)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;更进一步地,我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="MAP_Estimation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/map_weight_zps49ddf4b7.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:Posterior协方差矩阵并不依赖观察值$y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们希望使用以上得到的Posterior计算$f$在点$x_{*}$处的值,我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Posterior Predicative" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/predicative_weight_zps31d23a10.png"&gt;&lt;/p&gt;
&lt;h1&gt;Logistic Regression&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-4-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-4" title="Machine Learning:A Probabilistic Perspective Chapter 8"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;ML&lt;/em&gt;一书中给出的形式并不便于计算Gradient以及Hessian,以下根据李航&lt;strong&gt;统计学习方法&lt;/strong&gt;中的相关内容给出以下较易计算的形式。&lt;/p&gt;
&lt;p&gt;对于给定的训练集$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$,其中,$x_i \in R^n,y_i \in {0,1}$,我们令:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(Y=1|x) = \mu(x),P(Y=0|x) = 1-\mu(x)
\end{equation}&lt;/p&gt;
&lt;p&gt;对数似然函数为:
\begin{equation}
\begin{split}
\ell(w) &amp;amp;= \sum_{i=1}^N [y_i log \mu(x_i)+(1-y_i)log(1-\mu(x_i))] \\
&amp;amp;= \sum_{i=1}^N [y_i log \frac{\mu(x_i)}{1-\mu(x_i)}+log(1-\mu(x_i))] \\
&amp;amp;= \sum_{i=1}^N [y_i(w^Tx_i)-log(1+exp(w^Tx_i)]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;为了使用最小值,在&lt;em&gt;ML&lt;/em&gt;一书中用到的是Negetive Log Likelihood(NLL),特此说明。&lt;/p&gt;
&lt;p&gt;由此我们得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
g &amp;amp;= \frac{df(w)}{dw} = \sum_i (\mu_i-y_i)x_i = X^T(\mu-y) \\
H &amp;amp;= \frac{dg(w)^T}{dw} = \sum_i (\bigtriangledown_w \mu_i)x_i^T \\
&amp;amp;= \sum_i \mu_i(1-\mu_i)x_ix_i^T \\
&amp;amp;= X^TSX
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$S \triangleq diag(\mu_i(1-\mu_i))$.得到梯度和Hessian矩阵之后,以下我们就能使用最速下降、牛顿法、BFGS等方法对Logistic Regression进行求解了。&lt;/p&gt;
&lt;h1&gt;EM&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;本部分与&lt;em&gt;ML&lt;/em&gt;一书第十一章无关,只是为了更清楚地了解&lt;strong&gt;EM算法&lt;/strong&gt;。以下给出一个挺不错的PPT:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://people.csail.mit.edu/fergus/research/tutorial_em.ppt"&gt;Tutorial EM&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Posterior Predicative for Multinomial-Dirichlet Models&lt;/em&gt;&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-5-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-5" title="Machine Learning:A Probabilistic Perspective Chapter 3"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Regularized LDA&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Nearest shrunken centroids classifier&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Numerically stable computation[7.5.2]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Connection with PCA[7.5.3]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Estimating $\sigma^2$[7.6.3]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Evidence Procedure[7.6.4]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Conjugate Gradient(CG)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;BFGS(LBFGS)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Multi-class Logistic Regression and Maximum Entropy Classfier&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Bayesian Logistic Regression[8.4]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Online Learning[8.5]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Generative vs Discriminative Classfiers[8.6]&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1"&gt;&lt;a href="http://en.wikipedia.org/wiki/Closed-form_expression"&gt;Closed-form expression&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-2"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 4&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-3"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 7&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-3-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-4"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 8&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-4-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-5"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 3&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-5-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="Machine Learning"></category><category term="Gaussian Models"></category><category term="Linear Models"></category><category term="Notes"></category></entry><entry><title>当最近邻遇到LSH</title><link href="http://www.qingyuanxingsi.com/dang-zui-jin-lin-yu-dao-lsh.html" rel="alternate"></link><updated>2014-05-01T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-01:dang-zui-jin-lin-yu-dao-lsh.html</id><summary type="html">&lt;p&gt;貌似感冒了,脑子昏昏沉沉的,啥都想不了,无意中发现这么一个高端大气上档次的算法---局部敏感哈希方法。于是Google了一下,发现这篇&lt;a href="http://www.strongczq.com/2012/04/locality-sensitive-hashinglsh%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%8A%95%E5%BD%B1%E6%B3%95.html"&gt;Locality Sensitive Hashing(LSH)之随机投影法&lt;/a&gt;关于局部敏感哈希算法的介绍还不错,于是摘录如下:&lt;/p&gt;
&lt;h1&gt;概述&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;LSH&lt;/strong&gt;是由文献[1]提出的一种用于高效求解最近邻搜索问题的Hash算法。LSH算法的基本思想是利用一个hash函数把集合中的元素映射成hash值，使得相似度越高的元素hash值相等的概率也越高。LSH算法使用的关键是针对某一种相似度计算方法，找到一个具有以上描述特性的hash函数。LSH所要求的hash函数的准确数学定义比较复杂，以下给出一种通俗的定义方式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于集合$S$，集合内元素间相似度的计算公式为$sim(a,b)$。如果存在一个hash函数$h()$满足以下条件：存在一个相似度$s$到概率$p$的单调递增映射关系，使得$S$中的任意两个满足$sim(a,b)\geq s$的元素$a$和$b$，$h(a)=h(b)$的概率大于等于$p$。那么$h()$就是该集合的一个LSH算法hash函数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一般来说在最近邻搜索中，元素间的关系可以用相似度或者距离来衡量。如果用距离来衡量，那么距离一般与相似度之间存在单调递减的关系。以上描述如果使用距离来替代相似度需要在单调关系上做适当修改。&lt;/p&gt;
&lt;p&gt;根据元素相似度计算方式的不同，LSH有许多不同的hash算法。两种比较常见的hash算法是&lt;strong&gt;随机投影法&lt;/strong&gt;和min-hash算法。本文即将介绍的随机投影法适用于集合元素可以表示成向量的形式，并且相似度计算是基于向量之间夹角的应用场景，如余弦相似度。min-hash法在参考文献[2]中有相关介绍。&lt;/p&gt;
&lt;h1&gt;随机投影法(Random projection)&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;假设集合$S$中的每个元素都是一个$n$维的向量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\vec{x} ={v_1,v_2,\cdots,v_n}
\end{equation}&lt;/p&gt;
&lt;p&gt;集合中两个元素$\vec{v}$和$\vec{u}$之间的相似度定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
sim(\vec{v},\vec{u})=\frac{\vec{v}*\vec{u}}{|\vec{v}||\vec{u}|}
\end{equation}&lt;/p&gt;
&lt;p&gt;对于以上元素集合$S$的随机投影法hash函数$h()$可以定义为如下：&lt;/p&gt;
&lt;p&gt;在$n$维空间中随机选取一个非零向量$\vec{x}={x_1, x_2, \ldots, x_n}$。考虑以该向量为法向量且经过坐标系原点的超平面，该超平面把整个$n$维空间分成了两部分，将法向量所在的空间称为正空间，另一空间为负空间。那么集合$S$中位于正空间的向量元素hash值为1，位于负空间的向量元素hash值为0。判断向量属于哪部分空间的一种简单办法是判断向量与法向量之间的夹角为锐角还是钝角，因此具体的定义公式可以写为&lt;/p&gt;
&lt;p&gt;&lt;img alt="Formula 1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/formula_2_zpsd4647f5a.png" /&gt;&lt;/p&gt;
&lt;p&gt;根据以上定义，假设向量$\vec{v}$和$\vec{u}$之间的夹角为$\theta$，由于法向量$\vec{x}$是随机选取的，那么这两个向量未被该超平面分割到两侧（即hash值相等）的概率应该为：$p(\theta)=1-\frac{\theta}{\pi}$。假设两个向量的相似度值为$s$，那么根据$\theta=arccos(s)$,有&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(s)=1-\frac{arccos(s)}{\pi}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此，存在相似度$s$到概率$p$的单调递增映射关系，使得对于任意相似度大于等于$s$的两个元素，它们hash值相等的概率大于等于$p(s)$。所以，以上定义的hash值计算方法符合LSH算法的要求。&lt;/p&gt;
&lt;p&gt;以上所描述的$h()$函数虽然符合LSH算法的要求，但是实用性不高。因为该hash函数只产生了两个hash值，没有达到hash函数将元素分散到多个分组的目的。为了增加不同hash值的个数，可以多次生成独立的函数$h()$，只有当两个元素的多个$h()$值都相等时才算拥有相同的hash值。根据该思路可以定义如下的hash函数$H()$：&lt;/p&gt;
&lt;p&gt;\begin{equation}
H(\vec{v})=(h_b(\vec{v})h_{b-1}(\vec{v})\ldots h_1(\vec{v}))_2
\end{equation}&lt;/p&gt;
&lt;p&gt;其中每个$h_i(\vec{v})$表示一个独立的$h()$函数，$H()$函数值的二进制表现形式中每一位都是一个$h()$函数的结果。
以$H()$为hash函数的话，两个相似度为$s$的元素具有相同hash值的概率公式为&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(s)=(1-\frac{arccos(s)}{\pi})^b
\end{equation}&lt;/p&gt;
&lt;p&gt;hash值的个数为$2^b$。很容易看出$H()$函数同样也是符合LSH算法要求的。一般随机按投影算法选用的hash函数就是$H()$。其中参数$b$的取值会在后面小节中讨论。&lt;/p&gt;
&lt;h1&gt;随机投影法在最近邻搜索中的应用&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;最近邻搜索&lt;/h2&gt;
&lt;p&gt;最近邻搜索可以简单的定义为：对于$m$个元素的集合$T$，为一个待查询元素$q$找到集合中相似度最高的$k$个元素。&lt;/p&gt;
&lt;p&gt;最近邻搜索最简单的实现方法为：计算$q$与集合$T$中每一个元素的相似度，使用一个具有$k$个元素的大顶堆（优先队列）保存相似度计算结果（相似度值为key）。这种实现方法每一次查询都要遍历整个集合$T$来计算相似度，当$m$很大并且查询的频率很高的时候这种暴力搜索的方法无法满足性能要求。&lt;/p&gt;
&lt;p&gt;当最近邻搜索的近邻要求并不是那么严格的时候，即允许top k近邻的召回率不一定为1（但是越高越好），那么可以考虑借助于LSH算法。&lt;/p&gt;
&lt;h2&gt;随机投影法提高执行速度&lt;/h2&gt;
&lt;p&gt;这里我们介绍当集合$T$的元素和查询元素$q$为同维度向量(维度为$n$)，并且元素相似度计算方法为余弦相似度时，使用随机投影法来提高最近邻搜索的执行速度。具体的实现方法为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;预处理阶段&lt;/strong&gt;:使用hash函数$H(*)$计算集合$T$中所有元素的hash值，将集合$T$分成一个个分组，每个分组内的元素hash值均相等。用合适的数据结构保存这些hash值到分组的映射关系（如HashMap）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;查询阶段&lt;/strong&gt;:计算查询元素$q$的hash值$H(q)$，取集合$T$中所有hash值为$H(q)$的分组，以该分组内的所有元素作为候选集合，在候选该集合内使用简单的最近邻搜索方法寻找最相似的$k$个元素。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该方法的执行效率取决于$H(*)$的hash值个数$2^b$，也就是分组的个数。理想情况下，如果集合$T$中的向量元素在空间中分布的足够均匀，那么每一个hash值对应的元素集合大小大致为$\frac{m} {2^b}$。当$m$远大于向量元素的维度时，每次查询的速度可以提高到$2^b$倍。&lt;/p&gt;
&lt;p&gt;根据以上分析$H()$中$b$的取值越大算法的执行速度的提升越多，并且是指数级别的提升。但是，在这种情况下$H()$函数下的概率公式$p(s)$，&lt;strong&gt;实际上表示与查询元素$q$的相似度为$s$的元素的召回率&lt;/strong&gt;。当$b$的取值越大时，top k元素的召回率必然会下降。因此算法执行速度的提升需要召回率的下降作为代价。例如：当$b$等于10时，如果要保证某个元素的召回率不小于0.9，那么该元素与查询元素$q$的相似度必须不小于0.9999998。&lt;/p&gt;
&lt;h2&gt;提高召回率改进&lt;/h2&gt;
&lt;p&gt;为了在保证召回率的前提下尽可能提高算法的执行效率，一般可以进行如下改进：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;预处理阶段&lt;/strong&gt;:生成$t$个独立的hash函数$H_i(∗)$，根据这$t$个不同的hash函数，对集合$T$进行$t$种不同的分组，每一种分组方式下，同一个分组的元素在对应hash函数下具有相同的hash值。用合适的数据结构保存这些映射关系（如使用$t$个HashMap来保存）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;查询阶段&lt;/strong&gt;:对于每一个hash函数$H_i(∗)$，计算查询元素$q$的hash值$H_i(q)$，将集合$T$中$H_i(∗)$所对应的分组方式下hash值为$H_i(q)$的分组添加到该次查询的候选集合中。然后，在该候选集合内使用简单的最近邻搜索方法寻找最相似的$k$个元素。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上改进使得集合中元素与查询元素$q$的$t$个hash值中，只要任意一个相等，那么该集合元素就会被加入到候选集中。那么，相似度为$s$的元素的召回率为&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(s)=1-(1-(1-\frac{arccos(s)}{\pi})^b)^t
\end{equation}&lt;/p&gt;
&lt;p&gt;在执行效率上，预处理阶段由于需要计算$t$个hash函数的值，所以执行时间上升为$t$倍。查询阶段，如果单纯考虑候选集合大小对执行效率的影响，在最坏的情况下，$t$个hash值获得的列表均不相同，候选集集合大小的期望值为$\frac{t∗m}{2^b}$，查询速度下降至$1 \over t$，与简单近邻搜索相比查询速度提升为$\frac{2^b}{t}$倍。&lt;/p&gt;
&lt;p&gt;下图是召回率公式$p(s)=1-(1-(1-\frac{arccos(s)}{\pi})^b)^t$在不同的$b$和$t$取值下的$s-p$曲线。我们通过这些曲线来分析这里引入参数$t$的意义。4条蓝色的线以及最右边红色的线表示当$t$取值为1（相当于没有引入$t$），而$b$的取值从1变化到5的过程，从图中可以看出随着$b$的增大，不同相似度下的召回率都下降的非常厉害，特别的，当相似度接近1时曲线的斜率很大，也就说在高相似度的区域，召回率对相似度的变化非常敏感。10条红色的线从右到左表示$b$的取值为5不变，$t$的取值从1到10的过程，从图中可以看出，随着$t$的增大，曲线的形状发生了变化，高相似度区域的召回率变得下降的非常平缓，而最陡峭的地方渐渐的被移动到相对较低的相似度区域。因此，从以上曲线的变化特点可以看出，引入适当的参数$t$使得高相似度区域在一段较大的范围内仍然能够保持很高的召回率从而满足实际应用的需求。&lt;/p&gt;
&lt;p&gt;&lt;img alt="SP Curve" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sp_curve_zpse1b5dbe6.png" /&gt;&lt;/p&gt;
&lt;h2&gt;参数选取&lt;/h2&gt;
&lt;p&gt;根据以上分析，$H(*)$函数的参数$b$越大查询效率越高，但是召回率越低；参数$t$越大查询效率越低但是召回率越高。因此选择适当参数$b$和$t$来折中查询效率与召回率之间的矛盾是应用好随机投影法的关键。下面提供一种在实际应用中选取$b$和$t$的参考方法。&lt;/p&gt;
&lt;p&gt;根据实际应用的需要确定一对$(s,p)$，表示相似度大于等于$s$的元素，召回率的最低要求为$p$。然后将召回率公式表示成$b-t$之间的函数关系$t=\log_{1-(1-\frac{acos(s)}{pi})^b}{(1-p)}$。根据$(s,p)$的取值，画出$b-t$的关系曲线。如$s=0.8,p=0.95$时的$b-t$曲线如下图所示。考虑具体应用中的实际情况，在该曲线上选取一组使得执行效率可以达到最优的$(b,t)$组合。&lt;/p&gt;
&lt;p&gt;&lt;img alt="BT_Curve" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bt_curve_zps5aba4948.png" /&gt;&lt;/p&gt;
&lt;h2&gt;关于最近邻文本搜索&lt;/h2&gt;
&lt;p&gt;在最近邻文本搜索中，一般待检索的文本或查询文本，都已被解析成一系列带有权重的关键词，然后通过余弦相似度公式计算两个文本之间的相似度。这种应用场景下的最近邻搜索与以上所提到的最近邻搜索问题相比存在以下两个特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果把每个文本的带权重关键词表都看作是一个向量元素的话，每个关键词都是向量的一个维度，关键词权重为该维度的值。理论上可能关键词的个数并不确定（所有单词的组合都可能是一个关键词），因此该向量元素的维数实际上是不确定的。&lt;/li&gt;
&lt;li&gt;由于关键词权重肯定是大于零的，所以向量元素的每一个维度的值都是非负的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于第一个特点，我们需要选取一个包含$n$个关键词的关键词集合，在进行文本相似度计算时只考虑属于该集合的关键词。也就是说，每一个文本都视为是一个$n$维度的向量，关键词权重体现为对应维度的值。该关键词集合可以有很多种生成办法，比如可以是网站上具有一定搜索频率的关键词集合，总的来说该关键词集合应当能够涵盖所有有意义并且具有一定使用频率的关键词。通常$n$的取值会比较大，如几十万到几百万，由于在使用随机投影算法时，每一个生成的随机向量维度都为$n$，这种情况下需要特别考虑利用这些高维随机向量对执行效率造成的影响，在确定$b、t$参数时需要考虑到这方面的影响。&lt;/p&gt;
&lt;p&gt;对于第二个特点，由于向量元素各维度值都非负，那么这些元素在高维空间中只会出现在特定的区域中。比如当$n$为3时，只会出现在第一象限中。一个直观的感觉是在生成随机向量的时候，会不会生成大量的无用切割平面（与第一个象限空间不相交，使得所有元素都位于切割平面的同侧）。这些切割平面对应的$H(*)$函数hash值中的二进制位恒定为1或者0，对于提高算法执行速度没有帮助。以下说明这种担心是没有必要的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;切割平面与第一象限空间不相交等价于其法向量的每一个维度值都有相同的符号（都为正或者负），否则总能在第一象限空间中找到两个向量与法向量的乘积符号不同，也就是在切割平面的两侧。那么，随机生成的n维向量所有维度值都同号的概率为$\frac{1}{2^{n−1}}$，当$n$的取值很大时，该概率可以忽略不计。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;[1] P. Indyk and R. Motwani. Approximate Nearest Neighbor:Towards Removing the Curse of Dimensionality. In Proc. of the 30th Annual ACM Symposium on Theory of Computing, 1998, pp. 604–613.&lt;/p&gt;
&lt;p&gt;[2] Google News Personalization: Scalable Online Collaborative Filtering.&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="算法"></category><category term="Algorithm"></category><category term="LSH"></category><category term="局部敏感哈希算法"></category></entry><entry><title>机器学习拾遗(I):概率图模型[待续]</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu.html" rel="alternate"></link><updated>2014-05-01T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-01:ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu.html</id><summary type="html">&lt;p&gt;今天突然发现&lt;em&gt;Machine Learning:A Probabilistic Perspective&lt;/em&gt;一书中看过的章节中还有一些东西没完全弄懂,或者有些东西干脆就漏掉了,所以这一阵子打算把看过的部分梳理一下,慢慢来,脚踏实地地把这本书吃透,至少达到让自己满意的理解水平吧。作为机器学习拾遗的第一篇,我们主要从这本书中涉及概率图模型的章节入手,以期对概率图模型有一个更为深入的了解,并把之前遗漏的部分知识点捡起来。&lt;/p&gt;
&lt;h1&gt;Undirected Graphical Models&lt;sup id="sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1" title="Machine Learning:A Probabilistic Perspective Chapter 10"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Family&lt;/strong&gt;;For a directed graph,the family of a node is the node and its parents,$fam(s) = {s} \cup pa(s)$.&lt;/li&gt;
&lt;li&gt;Influence(Decision Diagrams);人的行为是理智的,人们在决策时总希望自己所能得到的期望效用最大化(&lt;em&gt;Expected Utility&lt;/em&gt;),当我们对不同行动下的期望效用进行比较后就很容易做出对我们而言最好的&lt;em&gt;Action&lt;/em&gt;.然而,现实生活中信息对于决策具有非常重要的意义，当我们获得一些其他的有用的信息时，可能做出更为明智的决定。得知一定的决策信息后，一般而言，我们的最大期望效用会提高,而此时的最大期望效用值相对之前的提升量则称为&lt;strong&gt;Value of Perfect Information,VPI(完美信息价值)&lt;/strong&gt;。而实际上在现实生活中我们获取这些信息并不是不无代价的,因此 理智人能做出的理智选择是,当信息的VPI要大于获取信息的代价时,我们选择获取额外的信息，否则则不用获取该信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Decision Diagrams" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/decision_diagram_zps163a6ef8.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP(&lt;em&gt;Markov Decision Process&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 10&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="Machine Learning"></category><category term="Graphical Models"></category><category term="Notes"></category></entry><entry><title>机器学习系列(VII):Kernels</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-viikernels.html" rel="alternate"></link><updated>2014-04-29T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-29:ji-qi-xue-xi-xi-lie-viikernels.html</id><summary type="html">&lt;p&gt;这一篇作为机器学习系列的第七篇,主要介绍核方法,关于&lt;strong&gt;SVM&lt;/strong&gt;的部分Pluskid已经写的很通俗易懂了,具体请参考&lt;a href="http://www.qingyuanxingsi.com/category/pearls.html"&gt;小小收藏夹[持续更新中]&lt;/a&gt;中SVM部分的链接,本文主要补充关于核方法的一些其他知识吧。&lt;/p&gt;
&lt;p&gt;我们之前介绍的所有方法均假定Object可被恰当地表示为一定长的特征向量$x_i \in R^D$.然而,对于某些情况而言,我们也许并不知道如何将它们表示成定长的特征向量。如,我们如何表示可变长的文本文档或者蛋白质序列?如何表示具有复杂3D结构的分子结构?如何表示具有可变大小和形状的进化树?&lt;/p&gt;
&lt;p&gt;解决以上问题的方法之一是为数据定义一生成模型，并使用推断得到的隐含表示作为特征输入到标准的方法中,如&lt;em&gt;Deep Learning&lt;/em&gt;.另一方法则是我们假定我们有某种方法能够衡量Objects之间的相似度,因此我们并不需要将数据预处理为特征向量的形式。如当我们比较字符串时,我们可以计算它们之间的编辑距离。令$k(x,x\prime) \geq 0$作为衡量$x$和$x\prime$之间差异性的某种标准($k$被称为核函数&lt;strong&gt;Kernel Function&lt;/strong&gt;).以下我们介绍几种常见的核函数。&lt;/p&gt;
&lt;h1&gt;核函数&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;我们将核函数定义为:$k(x,x\prime) \in R$,即将两参数$x,x\prime \in X$($X$为某抽象空间)映射为一实数值。一般而言,它具有对称性,即$k(x,x\prime) = k(x\prime,x)$和非负性,即$k(x,x\prime) \geq 0$.以下我们给出几个例子:&lt;/p&gt;
&lt;h2&gt;RBF Kernels&lt;/h2&gt;
&lt;p&gt;Squared exponential kernel(SE kernel）或高斯核(Gaussian kernel)定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = exp(-{1 \over 2}(x-x\prime)^T\Sigma^{-1}(x-x\prime))
\end{equation}&lt;/p&gt;
&lt;p&gt;若$\Sigma$是对角阵,则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = exp(-{1 \over 2}\sum_{j=1}^D \frac{1}{\sigma_j^2}(x_j-x_j\prime)^2)
\end{equation}&lt;/p&gt;
&lt;p&gt;我们可将$\sigma_j$理解为对于维度$j$的&lt;em&gt;Characteristic length scale&lt;/em&gt;.若$\sigma_j = \infty$,则相应的维度可被忽略掉,此时则称其为&lt;em&gt;ARD kernel&lt;/em&gt;.若$\Sigma$呈球面分布,我们则得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = exp(-\frac{||x-x\prime||^2}{2\sigma^2})
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\sigma^2$被称为带宽(&lt;strong&gt;Bandwith&lt;/strong&gt;).上式为径向基核的一个例子(它只是$||x-x\prime||$的函数)。&lt;/p&gt;
&lt;h2&gt;文档核&lt;/h2&gt;
&lt;p&gt;当我们想要进行文档分类时,如果有一种方法能够比较文档$x_i$和$x_i\prime$想必是极好的。如果我们采用bag-of-words表示且令$x_{ij}$表示词$j$在文档$i$中出现的次数,我们可以采用余弦距离,定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x_i,x_i\prime) = \frac{x_i^T x_i \prime}{||x_i||_2 ||x_i \prime||_2}
\end{equation}&lt;/p&gt;
&lt;p&gt;然而这种方法并不好,主要缺点有如下两点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文档中可能包含一些&lt;strong&gt;停用词&lt;/strong&gt;(无实际意义的词,如&lt;code&gt;the&lt;/code&gt;,&lt;code&gt;and&lt;/code&gt;等),它们本身不能用于判断两个文档的相似度,而在上述计算中被包含在内了;&lt;/li&gt;
&lt;li&gt;如果一个词能被用于判断两个文档之间的相似度,那么如果它在一个文档中出现多次，则我们人为地提高了两个文档的相似度.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;幸运的是,我们通过简单的预处理就能大幅提升性能---我们将词频统计向量替换为&lt;strong&gt;词频-倒排文档频率TF-IDF&lt;/strong&gt;.我们首先作如下定义:&lt;/p&gt;
&lt;p&gt;我们将词频定义为(减少由于一个词在文档中出现多次造成的影响):&lt;/p&gt;
&lt;p&gt;\begin{equation}
tf(x_{ij}) \triangleq log(1+x_{ij})
\end{equation}&lt;/p&gt;
&lt;p&gt;我们将倒排文档频率定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
idf(j) \triangleq log \frac{N}{1+\sum_{i=1}^N 1_{x_{ij} &amp;gt; 0}}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$N$表示文档总数,分母则计算包含$j$的文档总数。&lt;/p&gt;
&lt;p&gt;最后,我们定义:&lt;/p&gt;
&lt;p&gt;\begin{equation}
tf-idf(x_i) \triangleq [tf(x_{ij} \times idf(j)]_{j=1}^V
\end{equation}&lt;/p&gt;
&lt;p&gt;我们将上式带入余弦距离即可。于是我们得到Kernel:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x_i,x_i\prime) = \frac{\phi(x_i)^T\phi(x_i\prime)}{||\phi(x_i)||_2||\phi(x_i\prime)||_2}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\phi(x) = tf-idf(x)$.&lt;/p&gt;
&lt;h2&gt;Mercer(positive definite) kernels&lt;/h2&gt;
&lt;p&gt;某些方法要求&lt;em&gt;Gram matrix&lt;/em&gt;对于任意输入集合${x_i}_{i=1}^N$均是正定的,这样的核称为Mercer kercel或正定核。我们1可以证明以上提到的高斯核以及文档核均是Mercel kernel.&lt;em&gt;Gram matrix&lt;/em&gt;定义为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
K = \left(
\begin{array}{cc}
k(x_1,x_1) &amp;amp; \ldots &amp;amp; k(x_1,x_N) \\
           &amp;amp; \vdots &amp;amp;           \\
k(x_N,x_1) &amp;amp; \ldots &amp;amp; k(x_N,x_N)
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Mercer's Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;若&lt;em&gt;Gram&lt;/em&gt;矩阵是正定的，则我们可作如下特征值分解:&lt;/p&gt;
&lt;p&gt;\begin{equation}
K = U^T\Lambda U
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\Lambda$是特征值均大于0的对角阵。现我们考虑$K$中的一个元素,有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k_{ij} = (\Lambda^{1 \over 2}U_{:,i})^T(\Lambda^{1 \over 2}U_{:,j})
\end{equation}&lt;/p&gt;
&lt;p&gt;令$\phi(x_i) = \Lambda^{1 \over 2}U_{:,i}$,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k_{ij} = \phi(x_i)^T \phi(x_j)
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们可以看到核矩阵的每一项可通过对特征向量作内积得到,而特征向量可由矩阵分解得到的特征向量$U$定义。一般地,如果kernel是Mercer Kernel,那么就存在一个从$x \in X$到$R^D$的映射$\phi$,使得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = \phi(x)^T \phi(x\prime)
\end{equation}&lt;/p&gt;
&lt;p&gt;比如,我们考察多项式核$k(x,x\prime) = (\gamma x^Tx\prime+r)^M$,其中$r&amp;gt;0$.我们可以证明$\phi(x)$将包含到$M$的所有项。例如,若我们取$M=2,\gamma = r = 1$且$x,x\prime \in R^2$,我们则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
(1+x^Tx\prime)^2 &amp;amp;= (1+x_1x_1\prime+x_2x_2\prime)^2 \\
&amp;amp;= 1+2x_1x_1\prime+2x_2x_2\prime+(x_1x_1\prime)^2+(x_2x_2\prime)^2+2x_1x_1\prime x_2x_2\prime
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;此时我们有：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\phi(x) = [1,\sqrt{2}x_1,\sqrt{2}x_2,x_1^2,x_2^2,\sqrt{2}x_1x_2]^T
\end{equation}&lt;/p&gt;
&lt;p&gt;因此采用该kernel等同于在6维空间内进行计算。对于Gaussian核而言，映射后则是在无穷维空间内。在这种情形下，显式表示特征向量显然是不可行的。&lt;/p&gt;
&lt;h2&gt;Linear Kernels&lt;/h2&gt;
&lt;p&gt;根据Kernel推导出特征向量是非常困难的一件事(仅当Kernel为Ｍercer时才可行),然而有特征向量推导出Kernel则容易的多。&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = \phi(x)^T \phi(x\prime) = &amp;lt;\phi(x),\phi(x\prime)&amp;gt;
\end{equation}&lt;/p&gt;
&lt;p&gt;如果$\phi(x)=x$,我们得到线性核,定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = x^Tx\prime
\end{equation}&lt;/p&gt;
&lt;p&gt;其实也就是在原空间内进行计算。&lt;/p&gt;
&lt;h2&gt;Matern kernels&lt;/h2&gt;
&lt;p&gt;Matern Kernels通常被用于高斯过程回归,并被定位为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(r) = \frac{2^{1-\nu}}{\Gamma(\nu)}(\frac{\sqrt{2\nu}r}{\ell})^{\nu}K_{\nu}(\frac{\sqrt{2\nu}r}{\ell})
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$r=||x-x\prime||,\nu&amp;gt;0,\ell&amp;gt;0$且$K_{\nu}$为修改后的Bessel函数。当$\nu \to \infty$时,它趋近于高斯核。若$\nu ={1 \over 2}$,它简化为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(r) = exp(-r/\ell)
\end{equation}&lt;/p&gt;
&lt;p&gt;若$D=1$,我们可以使用该核定义一高斯过程,我们得到&lt;em&gt;Ornstein-Uhlenbeck process&lt;/em&gt;,它刻画了一个正在做布朗运动的例子的速度.&lt;/p&gt;
&lt;h2&gt;字符串核&lt;/h2&gt;
&lt;p&gt;当输入项为结构化数据时,核方法才能真正发挥其威力。以下我们举一个例子简要说明一下多项式核。考察两个字符串长度分别为$D$和$D\prime的定义在符号表${A,R,N,D,C,E,Q,G,H,I,L,K,M,F,P,S,T,W,Y,V}$上的字符串$$x$和$x\prime$.令$x$为一长度为110的字符串,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="String_Data_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/string_data_1_zps0cae832f.png" /&gt;&lt;/p&gt;
&lt;p&gt;$x\prime$为长度为153的字符串。&lt;/p&gt;
&lt;p&gt;&lt;img alt="String_Data_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/string_data_2_zps284c6529.png" /&gt;&lt;/p&gt;
&lt;p&gt;两个字符串均包含$LQE$,如上图琐事。我们可以将两个字符串的相似度定义为它们共同包含的字串的数量。正式地，若$x = usv$,则我们可以说$s$是$x$的字串.令$\phi_s(x)$表示字串$s$在字符串$x$中出现的次数。因此字符串核可定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = \sum_{s \in A*} w_s \phi_s(x) \phi_s(x\prime)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$w_s \geq 0$且$A*$表示由字符表$A$所能生成的所有字符串。它是Mercer Kernel且可在$O(|x|+|x\prime|)$时间内计算得到。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:Why $O(|x|+|x\prime|)$,求大神解答。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有了上式之后,我们就能"为所欲为"了。若当$|s|&amp;gt;1$时我们令$w_s = 0$,那么我们得到一bag-of-characters核,此时$\phi(x)$表示$A$中每个字符在$x$中出现的次数;如果我们采用空格分隔$s$,我们得到一bag-of-words核,其中$\phi(x)$表示每个词在$x$中出现的次数。&lt;/p&gt;
&lt;h2&gt;Kernels derived from probabilistic models&lt;/h2&gt;
&lt;p&gt;假定我们有一生成概率模型$p(x|\theta)$.那么我们有多种方法可以通过它构造核函数,以使模型更适用于判别类人物。以下我们简要介绍两种方法:&lt;/p&gt;
&lt;h3&gt;Probability product models&lt;/h3&gt;
&lt;p&gt;方法之一是定义如下Kernel:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x_i,x_j) = \int p(x|x_i)^{\rho}p(x|x_j)^{\rho} dx
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\rho&amp;gt;0$,且$p(x|x_i)$一般通过计算$p(x|\hat{\theta}(x_i))$来近似,其中$\hat{\theta}(x_i)$是通过计算单一数据向量得到的对于参数的估计值。上式被称为&lt;em&gt;Probability product kernel&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;尽管使用单一数据点估计参数显得很诡异，然后我们需要注意的是它仅被用于衡量两个Objects之间的相似度。特别地,如果我们采用$x_i$进行估计而得到的模型认为$x_j$是很有可能得到的,则证明$x_i$和$x_j$是相似的。如我们假定$p(x|\theta) = N(\mu,\sigma^2I)$,其中$\sigma^2$是定值。若$\rho = 1$,且我们令$\hat{\mu}(x_i) = x_i$和$\hat{\mu}(x_j) = x_j$,我们得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x_i,x_j) = \frac{1}{(4\pi\sigma^2)^{D/2}}exp(-\frac{1}{4\sigma^2}||x_i-x_j||^2)
\end{equation}&lt;/p&gt;
&lt;p&gt;即RBF Kernel.&lt;/p&gt;
&lt;h3&gt;Fisher kernels&lt;/h3&gt;
&lt;p&gt;另一更为有效的通过生成模型定义Kernel的方法是采用Fisher kernel.定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = g(x)^T F^{-1} g(x\prime)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$g$是对数似然函数梯度或score vector在极大似然估计处的取值。&lt;/p&gt;
&lt;p&gt;\begin{equation}
g(x)  = \bigtriangledown_{\theta} log p(x|\theta)|{\hat{\theta}}
\end{equation}&lt;/p&gt;
&lt;p&gt;$F$为Fisher information matrix,即Hessian.&lt;/p&gt;
&lt;p&gt;\begin{equation}
F = \bigtriangledown \bigtriangledown log p(x|\theta)|{\hat{\theta}}
\end{equation}&lt;/p&gt;
&lt;p&gt;注意$\hat{\theta}$为所有数据的函数,因此$x$和$x\prime$之间的相似度的计算也是将所有数据考虑在内的,所以我们只需要Fit一次Model.&lt;/p&gt;
&lt;p&gt;该模型背后的Intuition是如果它们的方向梯度是相似的话,则两个向量也是相似的。(&lt;code&gt;好吧,其实这个也不是弄得灰常清楚啦,TAT&lt;/code&gt;)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;后缀树Suffix Tree&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="SVM"></category><category term="Kernels"></category></entry><entry><title>机器学习系列(VI):Latent Linear Models</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-vilatent-linear-models.html" rel="alternate"></link><updated>2014-04-27T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-27:ji-qi-xue-xi-xi-lie-vilatent-linear-models.html</id><summary type="html">&lt;h1&gt;因子分析&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;问题&lt;/h2&gt;
&lt;p&gt;当样本个数$m$远远大于其特征个数$n$时，这样不管是进行回归、聚类等都没有太大的问题。然而当训练样例个数$m$太小，甚至$m \ll n$的时候，使用梯度下降法进行回归时，如果初值不同，得到的参数结果会有很大偏差（因为方程数小于参数个数）。另外，如果使用多元高斯分布(Multivariate Gaussian distribution)对数据进行拟合时，也会有问题。让我们来演算一下，看看会有什么问题：&lt;/p&gt;
&lt;p&gt;多元高斯分布的参数估计公式如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\mu &amp;amp;= {1 \over m}\sum_{i=1}^m x^{(i)} \\
\Sigma &amp;amp;= {1 \over m}\sum_{i=1}^m (x^{(i)}-\mu)(x^{(i)}-\mu)^T
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$x^{(i)}$表示样例，共有$m$个，每个样例$n$个特征，因此$\mu$是$n$维向量，$\Sigma$是$n*n$协方差矩阵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;当$m \ll n$时，我们会发现$\Sigma$是奇异阵($|\Sigma|=0$)&lt;/strong&gt;，也就是说$\Sigma^{-1}$不存在，没办法拟合出多元高斯分布了，确切的说是我们估计不出来$\Sigma$。&lt;/p&gt;
&lt;p&gt;如果我们仍然想用多元高斯分布来估计样本，那怎么办呢？&lt;/p&gt;
&lt;h3&gt;限制协方差矩阵&lt;/h3&gt;
&lt;p&gt;当没有足够的数据去估计$\Sigma$时，那么只能对模型参数进行一定假设，之前我们想估计出完全的$\Sigma$(矩阵中的全部元素），现在我们假设$\Sigma$就是对角阵（各特征间相互独立），那么我们只需要计算每个特征的方差即可，最后的$\Sigma$只有对角线上的元素不为0.&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Sigma_{jj} = {1 \over m}\sum_{i=1}^m (x_j^{(i)}-\mu_j)^2
\end{equation}&lt;/p&gt;
&lt;p&gt;回想我们之前在&lt;strong&gt;Gaussian Models&lt;/strong&gt;一文中讨论过的二维多元高斯分布的几何特性，在平面上的投影是个椭圆，中心点由$\mu$决定，椭圆的形状由$\Sigma$决定。$\Sigma$如果变成对角阵，就意味着椭圆的两个轴都和坐标轴平行了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cov Matrix" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cov_matrix_zps81794a47.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;如果我们想对$\Sigma$进一步限制的话，可以假设对角线上的元素都是等值的。&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Sigma = \sigma^2 I
\end{equation}&lt;/p&gt;
&lt;p&gt;其中:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sigma^2 = \frac{1}{mn}\sum_{j=1}^n \sum_{i=1}^m (x_j^{(i)}-\mu_j)^2
\end{equation}&lt;/p&gt;
&lt;p&gt;也就是上一步对角线上元素的均值，反映到二维高斯分布图上就是椭圆变成圆。&lt;/p&gt;
&lt;p&gt;当我们要估计出完整的$\Sigma$时，我们需要$m&amp;gt;=n+1$才能保证在最大似然估计下得出的$\Sigma$是非奇异的。然而在上面的任何一种假设限定条件下，只要$m&amp;gt;=2$都可以估计出限定的$\Sigma$。&lt;/p&gt;
&lt;p&gt;这样做的缺点也是显然易见的，我们认为特征间独立，这个假设太强。接下来，我们给出一种称为因子分析的方法，使用更多的参数来分析特征间的关系，并且不需要计算一个完整的$\Sigma$。&lt;/p&gt;
&lt;h2&gt;因子分析&lt;/h2&gt;
&lt;p&gt;下面通过一个简单例子，来引出因子分析背后的思想。&lt;/p&gt;
&lt;p&gt;因子分析的实质是认为$m$个$n$维特征的训练样例$x^{(i)}$的产生过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先在一个$k$维的空间中按照多元高斯分布生成$m$个$z^{(i)}$（$k$维向量),即&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
z^{(i)} \sim\ N(0,I)
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;然后存在一个变换矩阵$\Lambda \in R^{n*k}$，将$z^{(i)}$映射到$n$维空间中，即&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\Lambda z^{(i)}
\end{equation}&lt;/p&gt;
&lt;p&gt;因为$z^{(i)}$的均值是0，映射后仍然是0。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;然后将$\Lambda z^{(i)}$加上一个均值$\mu$（$n$维），即&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\mu + \Lambda z^{(i)}
\end{equation}&lt;/p&gt;
&lt;p&gt;对应的意义是将变换后的$\Lambda z^{(i)}$（$n$维向量）移动到样本$x^{(i)}$的中心点$\mu$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由于真实样例$x^{(i)}$与上述模型生成的有误差，因此我们继续加上误差$\epsilon$($n$维向量),而且$\epsilon$符合多元高斯分布，即&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\epsilon \sim\ N(0,\mathbf \Psi) \\
\mu + \Lambda z^{(i)} + \epsilon 
\end{split}
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最后的结果认为是真实的训练样例$x^{(i)}$的生成公式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
x^{(i)} = \mu + \Lambda z^{(i)} + \epsilon 
\end{equation}&lt;/p&gt;
&lt;p&gt;让我们使用一种直观方法来解释上述过程：&lt;/p&gt;
&lt;p&gt;假设我们有$m=5$个2维的样本点$x^{(i)}$（两个特征），如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Original Data" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/original_data_zps9626b246.png" /&gt;&lt;/p&gt;
&lt;p&gt;那么按照因子分析的理解，样本点的生成过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们首先认为在1维空间（这里$k=1$），存在着按正态分布生成的$m$个点$z^{(i)}$，如下:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Demo_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_1_zps3e189697.png" /&gt;&lt;/p&gt;
&lt;p&gt;均值为0，方差为1。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;然后使用某个$\Lambda=(a,b)^T$将一维的$z$映射到2维，图形表示如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Demo_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_2_zps166db3d8.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;之后加上$\mu\ (\mu_1,\mu_2)^T$，即将所有点的横坐标移动$\mu_1$，纵坐标移动$\mu_2$，将直线移到一个位置，使得直线过点$\mu$，原始左边轴的原点现在为$\mu$(红色点)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Demo_3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_3_zps178e1a6a.png" /&gt;&lt;/p&gt;
&lt;p&gt;然而，样本点不可能这么规则，在模型上会有一定偏差，因此我们需要将上步生成的点做一些扰动（误差），扰动$\epsilon \sim\ N(0,\mathbf \Psi)$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加入扰动后，我们得到黑色样本$x^{(i)}$如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Demo_4" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_4_zps0cc40309.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中由于$z$和$\epsilon$的均值都为0，因此$\mu$也是原始样本点（黑色点）的均值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由以上的直观分析，我们知道了&lt;strong&gt;因子分析其实就是认为高维样本点实际上是由低维样本点经过高斯分布、线性变换、误差扰动生成的，因此高维数据可以使用低维来表示&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;因子分析模型&lt;/h2&gt;
&lt;p&gt;上面的过程是从隐含随机变量$z$经过变换和误差扰动来得到观测到的样本点。其中$z$被称为因子，是低维的。&lt;/p&gt;
&lt;p&gt;我们将式子再列一遍如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
z &amp;amp;\sim\ N(0,I)   \\
\epsilon &amp;amp;\sim\ N(0,\mathbf \Psi) \\
x &amp;amp;= \mu + \Lambda z + \epsilon
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中误差$\epsilon$和$z$是独立的。&lt;/p&gt;
&lt;p&gt;下面使用的因子分析表示方法是矩阵表示法，在参考资料中给出了一些其他的表示方法，如果不明白矩阵表示法，可以参考其他资料。&lt;/p&gt;
&lt;p&gt;矩阵表示法认为$z$和$x$联合符合多元高斯分布，如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left[
\begin{array}{cc}
z \\
x
\end{array}
\right]\sim\ N(\mu_{zx},\Sigma)
\end{equation}&lt;/p&gt;
&lt;p&gt;求$\mu_{zx}$之前需要求$E[x]$&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E[x] &amp;amp;= E[\mu + \Lambda z + \epsilon] \\
&amp;amp;= \mu + \Lambda E[z] + E[\epsilon] \\
&amp;amp;= \mu
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们已知$E[z]=0$，因此:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mu_{zx} = \left[
\begin{array}{cc}
0 \\
\mu
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;下一步是计算$\Sigma$，其中$\Sigma_{zz} = Cov(z) = I$.接着求$\Sigma_{zx}$&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E[(z-E[z])(x-E[x])^T] &amp;amp;= E[z(\mu + \Lambda z + \epsilon - \mu)^T]   \\
&amp;amp;= E[zz^T]\Lambda^T + E[z\epsilon^T] \\
&amp;amp;= \Lambda^T
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;这个过程中利用了$z$和$\epsilon$独立假设($E[z\epsilon^T] = E[z]E[\epsilon^T]=0$)。并将$\Lambda$看作已知变量。&lt;/p&gt;
&lt;p&gt;接着求$\Sigma_{xx}$&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E[(x-E(x))(x-E[x])^T] &amp;amp;= E[(\mu+\Lambda z+\epsilon -\mu)(\mu+\Lambda z+\epsilon -\mu)^T]  \\
&amp;amp;= E[\Lambda zz^T \Lambda^T+ \epsilon z^T \Lambda^T+\Lambda z \epsilon^T+\epsilon \epsilon^T] \\
&amp;amp;= \Lambda E[zz^T] \Lambda^T + E[\epsilon \epsilon^T]  \\
&amp;amp;= \Lambda \Lambda^T + \mathbf \Psi
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;然后得出联合分布的最终形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left[
\begin{array}{cc}
z \\
x 
\end{array}
\right] \sim\
N\left(
\left[
\begin{array}{cc}
0 \\
\mu
\end{array}
\right],
\left[
\begin{array}{cc}
I &amp;amp; \Lambda^T \\
\Lambda &amp;amp; \Lambda \Lambda^T + \mathbf \Psi
\end{array}
\right]
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;从上式中可以看出$x$的边缘分布为$x \sim\ N(\mu,\Lambda \Lambda^T + \mathbf \Psi)$.&lt;/p&gt;
&lt;p&gt;那么对样本${x^{(i);i=1,...,m}}$进行最大似然估计&lt;/p&gt;
&lt;p&gt;\begin{equation}
\ell(\mu,\Lambda,\mathbf \Psi) = log \prod_{i=1}^m \frac{1}{(2\pi)^{n/2} |\Lambda \Lambda^T+\mathbf \Psi|}exp(-{1 \over 2}(x^{(i)}-\mu)^T(\Lambda \Lambda^T + \mathbf \Psi)^{-1} (x^{(i)}-\mu))
\end{equation}&lt;/p&gt;
&lt;p&gt;然后对各个参数求偏导数不就得到各个参数的值了么？可惜我们得不到closed-form。想想也是，如果能得到，还干嘛将$z$和$x$放在一起求联合分布呢。根据之前对参数估计的理解，在有隐含变量$z$时，我们可以考虑使用EM来进行估计。&lt;/p&gt;
&lt;h2&gt;因子分析的EM估计&lt;/h2&gt;
&lt;p&gt;我们先来明确一下各个参数,$z$为隐含变量,$\mu,\Lambda,\mathbf \Psi$为待估参数。&lt;/p&gt;
&lt;p&gt;回想EM算法两个步骤:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;循环重复直至收敛{ &lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;E Step&lt;/strong&gt;)对于每一个$i$,计算:
\begin{equation}
Q_i(z^{(i)}) := p(z^{(i)}|x^{(i)};\theta)
\end{equation}
(&lt;strong&gt;M Step&lt;/strong&gt;)计算:
\begin{equation}
\theta := arg max_{\theta} \sum_{i} \sum_{z^{(i)}} Q_i(z^{(i)}) \ log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
\end{equation}
}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们套用一下:&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;E Step&lt;/strong&gt;):&lt;/p&gt;
&lt;p&gt;\begin{equation}
Q_i(z^{(i)}) := p(z^{(i)}|\mu,\Lambda,\mathbf \Psi)
\end{equation}&lt;/p&gt;
&lt;p&gt;根据之前在&lt;em&gt;Gaussian Models&lt;/em&gt;中提到过的&lt;strong&gt;Posterior Conditional&lt;/strong&gt;的相关知识,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\mu_{z^{(i)}|x^{i}} &amp;amp;= \Lambda^T (\Lambda \Lambda^T+\mathbf \Psi)^{-1}(x^{(i)}-\mu)  \\
\Sigma_{z^{(i)}|x^{i}} &amp;amp;= I - \Lambda^T (\Lambda \Lambda^T+\mathbf \Psi)^{-1} \Lambda
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;那么根据多元高斯分布公式，得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
Q_i(z^{(i)}) = \frac{1}{(2\pi)^{k/2}|\Sigma_{z^{(i)}|x^{(i)}}|^{1/2}}exp(-{1 \over 2}(z^{(i)}-\mu_{z^{(i)}|x^{(i)}})^T \Sigma_{z^{(i)}|x^{(i)}}^{-1} (z^{(i)}-\mu_{z^{(i)}|x^{(i)}}))
\end{equation}&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;M Step&lt;/strong&gt;):&lt;/p&gt;
&lt;p&gt;直接写要最大化的目标是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{i=1}^m \int_{z^{(i)}} Q_i(z^{(i)}) \ log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} dz^{(i)}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中待估参数是$\mu,\Lambda,\mathbf \Psi$.下面我们重点求$\Lambda$的估计公式&lt;/p&gt;
&lt;p&gt;首先将上式简化为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\sum_{i=1}^m &amp;amp;\int_{z^{(i)}} Q_i(z^{(i)}) \ [log p(x^{(i)}|z^{(i)};\mu,\Lambda,\mathbf \Psi) +p(z^{(i)})-log\ Q_i(z^{(i)})]dz^{(i)} \\
&amp;amp;= \sum_{i=1}^m E_{z^{(i)} \sim\ Q_i} [log p(x^{(i)}|z^{(i)};\mu,\Lambda,\mathbf \Psi) +p(z^{(i)})-log\ Q_i(z^{(i)})]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;这里$z^{(i)} \sim\ Q_i$表示$z^{(i)}$服从$Q_i$分布。然后去掉与$\Lambda$不相关的项（后两项），得&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\sum_{i=1}^m &amp;amp; E[log p(x^{(i)}|z^{(I)};\mu,\Lambda,\Psi)] \\
&amp;amp;= \sum_{i=1}^m E[log \frac{1}{(2\pi)^{n/2}|\mathbf \Psi|^{1/2}}exp(-{1 \over 2}(x^{(i)}-\mu-\Lambda z^{(i)})^T |\mathbf \Psi|^{-1} (x^{(i)}-\mu-\Lambda z^{(i)})]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;去掉不相关的前两项后，对$\Lambda$求偏导得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\bigtriangledown_{\Lambda} &amp;amp; \sum_{i=1}^N -E[{1 \over 2}(x^{(i)}-\mu-\Lambda z^{(i)})^T |\mathbf \Psi|^{-1} (x^{(i)}-\mu-\Lambda z^{(i)})] \\
&amp;amp;= \sum_{i=1}^m \bigtriangledown_{\Lambda} E[- tr({1 \over 2}{z^{(i)}}^T\Lambda^T \Psi^{-1} z^{(i)}) + tr({z^{(i)}}^T\Lambda^T \Psi^{-1} (x^{(i)}-\mu))] \\
&amp;amp;= \sum_{i=1}^m \bigtriangledown_{\Lambda} E[- tr({1 \over 2}\Lambda^T \Psi^{-1} z^{(i)}{z^{(i)}}^T) + tr(\Lambda^T \Psi^{-1} (x^{(i)}-\mu){z^{(i)}}^T)]  \\
&amp;amp;= \sum_{i=1}^m E[-\Psi^{-1} z^{(i)}{z^{(i)}}^T) + \Psi^{-1} (x^{(i)}-\mu){z^{(i)}}^T]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;最后让其值为0，并且化简得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Lambda = (\sum_{i=1}^m (x^{(i)}-\mu) E_{z^{(i)} \sim\ Q_i}{z^{(i)}}^T)(\sum_{i=1}^m E_{z^{(i)} \sim\ Q_i}[{z^{(i)}z^{(i)}}^T])^{-1}
\end{equation}&lt;/p&gt;
&lt;p&gt;到这里我们发现，这个公式有点眼熟，与之前回归中的最小二乘法矩阵形式类似:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\theta^T = (y^TX)(X^TX)^{-1}
\end{equation}&lt;/p&gt;
&lt;p&gt;这里解释一下两者的相似性，我们这里的$x$是$z$的线性函数（包含了一定的噪声）。在E步得到$z$的估计后，我们找寻的$\Lambda$实际上是$x$和$z$的线性关系。而最小二乘法也是去找特征和结果直接的线性关系。&lt;/p&gt;
&lt;p&gt;到这还没完，我们需要求得括号里面的值.&lt;/p&gt;
&lt;p&gt;根据我们之前对$z|x$的定义，我们知道:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E_{z^{(i)} \sim\ Q_i}{z^{(i)}}^T &amp;amp;= \mu_{z^{(i)}|x^{(i)}}^T \\
E_{z^{(i)} \sim\ Q_i}[{z^{(i)}z^{(i)}}^T] &amp;amp;= \mu_{z^{(i)}|x^{(i)}} \mu_{z^{(i)}|x^{(i)}}^T + \Sigma_{z^{(i)}|x^{(i)}}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;将上式带入即可得到$\Lambda$的EM估计值。其他参数也可以通过类似的方法获得。&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;根据上面的EM的过程，要对样本$X$进行因子分析，只需知道要分解的因子数($z$的维度)即可。通过EM，我们能够得到转换矩阵$\Lambda$和误差协方差$\Psi$。&lt;/p&gt;
&lt;p&gt;因子分析实际上是降维，在得到各个参数后，可以求得$z$。但是$z$的各个参数含义需要自己去琢磨。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;因子分析(&lt;strong&gt;Factor Analysis&lt;/strong&gt;)是一种数据简化的技术。它通过研究众多变量之间的内部依赖关系，探求观测数据中的基本结构，并用少数几个假想变量来表示其基本的数据结构。这几个假想变量能够反映原来众多变量的主要信息。原始的变量是可观测的显在变量，而假想变量是不可观测的潜在变量，称为因子。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;PCA&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;PCA的思想是将$n$维特征映射到$k$维上($k&amp;lt;n$），这$k$维是全新的正交特征。这$k$维特征称为主元，是重新构造出来的$k$维特征，而不是简单地从$n$维特征中去除其余$n-k$维特征。&lt;/p&gt;
&lt;h2&gt;PCA计算过程&lt;/h2&gt;
&lt;p&gt;首先介绍PCA的计算过程,假设我们得到的2维数据如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt="PCA_DATA" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/pca_data_zps6051a959.png" /&gt;&lt;/p&gt;
&lt;p&gt;行代表了样例，列代表特征，这里有10个样例，每个样例两个特征。可以这样认为，有10篇文档，$x$是10篇文档中“learn”出现的TF-IDF，$y$是10篇文档中“study”出现的TF-IDF。也可以认为有10辆汽车，$x$是千米/小时的速度，$y$是英里/小时的速度，等等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一步&lt;/strong&gt;:分别求$x$和$y$的平均值，然后对于所有的样例，都减去对应的均值。这里$x$的均值是1.81，$y$的均值是1.91，那么第一个样例减去均值后即为（0.69,0.49），得到&lt;/p&gt;
&lt;p&gt;&lt;img alt="Data_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/data_1_zpsa7474565.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二步&lt;/strong&gt;，求特征协方差矩阵，如果数据是3维，那么协方差矩阵是&lt;/p&gt;
&lt;p&gt;\begin{equation}
C=\left(
\begin{array}{cc}
cov(x,x) &amp;amp; cov(x,y) &amp;amp; cov(x,z) \\
cov(y,x) &amp;amp; cov(y,y) &amp;amp; cov(y,z) \\
cov(z,x) &amp;amp; cov(z,y) &amp;amp; cov(z,z)
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;这里只有$x$和$y$，求解得&lt;/p&gt;
&lt;p&gt;\begin{equation}
cov=\left(
\begin{array}{cc}
.616555566 &amp;amp; .615444444 \\
.615444444 &amp;amp; .716555556
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;对角线上分别是$x$和$y$的方差，非对角线上是协方差。协方差大于0表示$x$和$y$若有一个增，另一个也增；小于0表示一个增，一个减；协方差为0时，两者独立。协方差绝对值越大，两者对彼此的影响越大，反之越小。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第三步&lt;/strong&gt;，求协方差的特征值和特征向量，得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
eigenvalues=\left(
\begin{array}{cc}
.0490833989 \\
1.28402771
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
eigenvectors=\left(
\begin{array}{cc}
-.735178656 &amp;amp; -.677873399 \\
.677873399 &amp;amp; -.735178656
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;上面是两个特征值，下面是对应的特征向量，特征值0.0490833989对应特征向量为$(-0.735178656,0.677873399)^T$，这里的特征向量都归一化为单位向量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第四步&lt;/strong&gt;，将特征值按照从大到小的顺序排序，选择其中最大的$k$个，然后将其对应的$k$个特征向量分别作为列向量组成特征向量矩阵。&lt;/p&gt;
&lt;p&gt;这里特征值只有两个，我们选择其中最大的那个，这里是1.28402771，对应的特征向量是$(-0.677873399,-0.735178656)^T$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第五步&lt;/strong&gt;，将样本点投影到选取的特征向量上。假设样例数为$m$，特征数为$n$，减去均值后的样本矩阵为$DataAdjust(m \times n)$，协方差矩阵是$n \times n$，选取的$k$个特征向量组成的矩阵为$EigenVectors(n \times k)$。那么投影后的数据FinalData为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
FinalData(m \times k) = DataAdjust(m \times n) EigenVectors(n \times k)
\end{equation}&lt;/p&gt;
&lt;p&gt;于是我们得到结果:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Data_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/data_2_zps92a790d2.png" /&gt;&lt;/p&gt;
&lt;p&gt;这样，就将原始样例的$n$维特征变成了$k$维，这$k$维就是原始特征在$k$维上的投影。上面的数据可以认为是learn和study特征融合为一个新的特征叫做LS特征，该特征基本上代表了这两个特征。这样PCA的过程基本结束。在第一步减均值之后，其实应该还有一步对特征做方差归一化。比如一个特征是汽车速度（0到100），一个是汽车的座位数（2到6），显然第二个的方差比第一个小。因此，如果样本特征中存在这种情况，那么在第一步之后，求每个特征的标准差$\sigma$，然后对每个样例在该特征下的数据除以$\sigma$。&lt;/p&gt;
&lt;p&gt;归纳一下，使用我们之前熟悉的表示方法，在求协方差之前的步骤是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let $\mu = {1 \over m}\sum_{i=1}^m x^{(i)}$;&lt;/li&gt;
&lt;li&gt;Replace each $x^{(i)}$ with $x^{(i)}-\mu$;&lt;/li&gt;
&lt;li&gt;Let $\sigma_j^2 = {1 \over m}\sum_i (x_j^{(i)})^2$;&lt;/li&gt;
&lt;li&gt;Replace each $x_j^{i}$ with $x_j^{i}/\sigma_j$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中$x^{(i)}$是样例，共$m$个，每个样例$n$个特征，也就是说$x^{(i)}$是$n$维向量。$x_j^{(i)}$是第$i$个样例的第$j$个特征。$\mu$是样例均值。$\sigma_j$是第$j$个特征的标准差。&lt;/p&gt;
&lt;p&gt;整个PCA过程貌似及其简单，就是求协方差的特征值和特征向量，然后做数据转换。但是有没有觉得很神奇，为什么求协方差的特征向量就是最理想的$k$维向量？其背后隐藏的意义是什么？整个PCA的意义是什么？&lt;/p&gt;
&lt;h2&gt;PCA理论基础&lt;/h2&gt;
&lt;p&gt;要解释为什么协方差矩阵的特征向量就是$k$维理想特征，我看到的有三个理论：分别是最大方差理论、最小错误理论和坐标轴相关度理论。这里简单探讨前两种，最后一种在讨论PCA意义时简单概述。&lt;/p&gt;
&lt;h3&gt;最大方差理论&lt;/h3&gt;
&lt;p&gt;在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。因此我们认为，最好的$k$维特征是将$n$维样本点转换为$k$维后，每一维上的样本方差都很大。&lt;/p&gt;
&lt;p&gt;比如下图有5个样本点：（已经做过预处理，均值为0，特征方差归一）&lt;/p&gt;
&lt;p&gt;&lt;img alt="Max_var_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/max_var_1_zpsb6611a20.png" /&gt;&lt;/p&gt;
&lt;p&gt;下面将样本投影到某一维上，这里用一条过原点的直线表示（预处理的过程实质是将原点移到样本点的中心点）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Max_var_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/max_var_2_zps05e7cd62.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;假设我们选择两条不同的直线做投影，那么左右两条中哪个好呢？根据我们之前的方差最大化理论，左边的好，因为投影后的样本点之间方差最大。&lt;/p&gt;
&lt;p&gt;这里先解释一下投影的概念：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Max_var_3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/max_var_3_zps46d5b364.png" /&gt;&lt;/p&gt;
&lt;p&gt;红色点表示样例$x^{(i)}$，蓝色点表示$x^{(i)}$在$u$上的投影，$u$是直线的斜率也是直线的方向向量，而且是单位向量。蓝色点是示$x^{(i)}$在$u$上的投影点，离原点的距离是${x^{(i)}}^Tu$或$u^Tx^{(i)}$.由于这些样本点（样例）的每一维特征均值都为0，因此投影到$u$上的样本点（只有一个到原点的距离值）的均值仍然是0。&lt;/p&gt;
&lt;p&gt;回到上面左右图中的左图，我们要求的是最佳的$u$，使得投影后的样本点方差最大。&lt;/p&gt;
&lt;p&gt;由于投影后均值为0，因此方差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
{1 \over m}\sum_{i=1}^m ({x^{(i)}}^Tu)^2 &amp;amp;= {1 \over m}\sum_{i=1}^m u^Tx^{(i)}{x^{(i)}}^Tu  \\
&amp;amp;= u^T ({1 \over m}\sum_{i=1}^m x^{(i)}{x^{(i)}}^T)u.
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;中间那部分很熟悉啊，不就是样本特征的协方差矩阵么（$x^{(i)}$的均值为0，一般协方差矩阵都除以$m-1$，这里用$m$）。&lt;/p&gt;
&lt;p&gt;用$\lambda$来表示${1 \over m}\sum_{i=1}^m ({x^{(i)}}^Tu)^2$，$\Sigma$表示${1 \over m}\sum_{i=1}^m x^{(i)}{x^{(i)}}^T$，那么上式可写作:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\lambda = u^T \Sigma u
\end{equation}&lt;/p&gt;
&lt;p&gt;由于$u$是单位向量，即$u^Tu=1$，上式两边都左乘$u$得，$u\lambda = \lambda u = uu^T \Sigma u$&lt;/p&gt;
&lt;p&gt;即$\Sigma u = \lambda u$&lt;/p&gt;
&lt;p&gt;We got it！$\lambda$就是$\Sigma$的特征值，$u$是特征向量。最佳的投影直线是特征值$\lambda$最大时对应的特征向量，其次是$\lambda$第二大对应的特征向量，依次类推。&lt;/p&gt;
&lt;p&gt;因此，我们只需要对协方差矩阵进行特征值分解，得到的前$k$大特征值对应的特征向量就是最佳的$k$维新特征，而且这$k$维新特征是正交的。得到前$k$个$u$以后，样例$x^{(i)}$通过以下变换可以得到新的样本。&lt;/p&gt;
&lt;p&gt;\begin{equation}
y^{(i)} = \left[
\begin{array}{cc}
u_1^Tx^{(i)} \\
u_2^Tx^{(i)} \\
...  \\
u_k^Tx^{(i)}
\end{array}
\right] \in R^k
\end{equation}&lt;/p&gt;
&lt;p&gt;其中的第$j$维就是$x^{(i)}$在$u_j$上的投影。通过选取最大的$k$个$u$，使得方差较小的特征（如噪声）被丢弃。&lt;/p&gt;
&lt;h3&gt;最小平方误差理论&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Least Squard Error" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lqe_zps9c291e8a.png" /&gt;&lt;/p&gt;
&lt;p&gt;假设有这样的二维样本点（红色点），回顾我们前面探讨的是求一条直线，使得样本点投影到直线上的点的方差最大。本质是求直线，那么度量直线求的好不好，不仅仅只有方差最大化的方法。再回想我们学过的线性回归等，目的也是求一个线性函数使得直线能够最佳拟合样本点，那么我们能不能认为最佳的直线就是回归后的直线呢？回归时我们的最小二乘法度量的是样本点到直线的坐标轴距离。比如这个问题中，特征是$x$，类标签是$y$。回归时最小二乘法度量的是距离$d$。如果使用回归方法来度量最佳直线，那么就是直接在原始样本上做回归了，跟特征选择就没什么关系了。&lt;/p&gt;
&lt;p&gt;因此，我们打算选用另外一种评价直线好坏的方法，使用点到直线的距离$d\prime$来度量。&lt;/p&gt;
&lt;p&gt;现在有$n$个样本点$(x_1,x_2,\dots,x_n)$，每个样本点为$m$维（这节内容中使用的符号与上面的不太一致，需要重新理解符号的意义）。将样本点$x_k$在直线上的投影记为$x_k \prime$，那么我们就是要最小化&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{k=1}^n ||x_k \prime - x_k||^2
\end{equation}&lt;/p&gt;
&lt;p&gt;这个公式称作最小平方误差（Least Squared Error）。&lt;/p&gt;
&lt;p&gt;而确定一条直线，一般只需要确定一个点，并且确定方向即可。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一步确定点&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;假设要在空间中找一点$x_0$来代表这$n$个样本点，“代表”这个词不是量化的，因此要量化的话，我们就是要找一个$m$维的点$x_0$，使得&lt;/p&gt;
&lt;p&gt;\begin{equation}
J_0(x_0) = \sum_{k=1}^n ||x_0-x_k||^2
\end{equation}&lt;/p&gt;
&lt;p&gt;最小。其中$J_0(x_0)$是平方错误评价函数（squared-error criterion function).对$x_0$求偏导易知:&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_0 = \sum_{k=1}^n x_k
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二步确定方向&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;我们从$x_0$拉出要求的直线，假设直线的方向是单位向量$e$。那么直线上任意一点，比如$x_k \prime$就可以用点$x_0$和$e$来表示&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_k \prime = x_0 + a_k e
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$a_k$是$x_k \prime$到点$x_0$的距离。&lt;/p&gt;
&lt;p&gt;我们重新定义最小平方误差：&lt;/p&gt;
&lt;p&gt;\begin{equation}
J_1(a_1,\dots,a_n,e) = \sum_{k=1}^n ||x_k \prime - x_k||^2 = \sum_{k=1}^n ||(x_0+a_ke)-x_k)||^2
\end{equation}&lt;/p&gt;
&lt;p&gt;这里的$k$只是相当于$i$。$J_1$就是最小平方误差函数，其中的未知参数是$a_1,\dots,a_n$和$e$。&lt;/p&gt;
&lt;p&gt;实际上是求$J_1$的最小值。首先将上式展开：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
J_1(a_1,\dots,a_n,e) &amp;amp;= \sum_{k=1}^n ||(x_0+a_ke)-x_k)||^2 = \sum_{k=1}^n ||a_ke-(x_k-x_0)||^2 \\
&amp;amp;= \sum_{k=1}^n a_k^2||e||^2 - 2\sum_{k=1}^n a_ke^T(x_k-x_0)+ \sum_{k=1}^n ||x_k-x_0||^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们首先固定$e$，将其看做是常量,$||e||^2 = 1$，然后对$a_k$进行求导，得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
a_k = e^T(x_k-x_0)
\end{equation}&lt;/p&gt;
&lt;p&gt;这个结果意思是说，如果知道了$e$，那么将$x_k-x_0$与$e$做内积，就可以知道了$x_k$在$e$上的投影离$x_0$的长度距离，不过这个结果不用求都知道。&lt;/p&gt;
&lt;p&gt;然后是固定$a_k$，对$e$求偏导数，我们先将上式代入$J_1$，得 &lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
J_1(e) &amp;amp;= \sum_{k=1}^n a_k^2||e||^2 - 2\sum_{k=1}^n a_k^2 + \sum_{k=1}^n ||x_k -x_0||^2 \\
&amp;amp;= -\sum_{k=1}^n [e^T(x_k-x_0)]^2 + \sum_{k=1}^n ||x_k - x_0 ||^2  \\
&amp;amp;= -\sum_{k=1}^n e^T (x_k-x_0)(x_k-x_0)^T e + \sum_{k=1}^n ||x_k - x_0 ||^2 \\
&amp;amp;= -e^TSe+\sum_{k=1}^n ||x_k-x_0||^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$S=\sum_{k=1}^n (x_k-x_0)(x_k-x_0)^T$与协方差矩阵类似，只是缺少个分母$n-1$，我们称之为散列矩阵（scatter matrix）。&lt;/p&gt;
&lt;p&gt;然后可以对$e$求偏导数，但是$e$需要首先满足$e^Te=1$，引入拉格朗日乘子$\lambda$，来使$e^TSe$最大（$J_1$最小），令:&lt;/p&gt;
&lt;p&gt;\begin{equation}
u = e^TSe - \lambda(e^Te-1)
\end{equation}&lt;/p&gt;
&lt;p&gt;求偏导:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial u}{\partial e} = 2Se - 2\lambda e = 0 
\end{equation}&lt;/p&gt;
&lt;p&gt;于是我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
Se = \lambda e
\end{equation}&lt;/p&gt;
&lt;p&gt;两边除以$n-1$就变成了，对协方差矩阵求特征值向量了。&lt;/p&gt;
&lt;p&gt;从不同的思路出发，最后得到同一个结果，对协方差矩阵求特征向量，求得后特征向量上就成为了新的坐标，如下图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="PCA_Transform" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/pca_transform_zps137a3ecb.png" /&gt;&lt;/p&gt;
&lt;p&gt;这时候点都聚集在新的坐标轴周围，因为我们使用的最小平方误差的意义就在此。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:&lt;/p&gt;
&lt;p&gt;PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;SVD分解&lt;/h2&gt;
&lt;p&gt;以上我们介绍了获得PCA solution的两种方法，现我们提供基于奇异值分解(&lt;strong&gt;Singular Value Decomposition&lt;/strong&gt;)另一种方法.任一$N \times D$矩阵$X$均可分解为如下形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
X = USV^T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$U$是一$N \times N$矩阵,其列向量均正交(即$U^TU = I_N$),$V$是一$D \times D$矩阵且行列向量均正交（即$V^TV = VV^T = I_D$,$S$为一$N \times D$矩阵，其主对角线上包含$r = min(N,D)$奇异值$\sigma_i \geq 0$(&lt;code&gt;Why?&lt;/code&gt;),其余则由0填充。如下图中(a)所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt="SVD" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/svd_zpsfec54ad5.png" /&gt;&lt;/p&gt;
&lt;p&gt;假定$N&amp;gt;D$,则最多有$D$个奇异值；$U$右边的$N-D$行由于与$0$相乘，所以是无关项，于是我们得到的&lt;em&gt;Economy sized SVD&lt;/em&gt;或者&lt;em&gt;Thin SVD&lt;/em&gt;避免了对于这些无关元素的计算，可被表示为如下形式：若$N&amp;gt;D$,我们有：&lt;/p&gt;
&lt;p&gt;\begin{equation}
X = \hat{U} \hat{S} \hat{V}^T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\hat{S}$为$D \times D$矩阵，$\hat{V}$为$D \times D$矩阵。&lt;/p&gt;
&lt;p&gt;若$N&amp;lt;D$,此时$\hat{U}$为$N \times N$矩阵，$\hat{S}$为$N \times N$矩阵，$\hat{V}$为$N \times D$矩阵。&lt;/p&gt;
&lt;p&gt;以下正式开始我们对PCA solution的推导：&lt;/p&gt;
&lt;p&gt;若$X = USV^T$,则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
X^TX = VS^TU^TUSV^T = V(S^TS)V^T = VDV^T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$D=S^2$是包含奇异值平方的对角阵。于是有：$(X^TX)V = VD$&lt;/p&gt;
&lt;p&gt;故$V$为$X^TX$的特征向量集合，$D$为$X^TX$的特征值集合(奇异值的平方)。对$XX^T$也能得到类似的结果。&lt;/p&gt;
&lt;p&gt;实际上,PCA的过程实际上是取$S$中最大的若干特征值(为了降维，我们并不是取所有的特征值）然后重新构造原矩阵的过程，即&lt;strong&gt;PCA只是对于原矩阵的低秩近似&lt;/strong&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Factor Analysis关于$\mathbf \Psi$的EM估计具体证明过程。&lt;/li&gt;
&lt;li&gt;Kernel-PCA&lt;/li&gt;
&lt;li&gt;Probablistic PCA&lt;/li&gt;
&lt;li&gt;如何确定隐含变量空间的维度&lt;/li&gt;
&lt;li&gt;ICA&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/jerrylead/archive/2011/05/11/2043317.html"&gt;因子分析（Factor Analysis)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"&gt;Expectation–maximization algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html"&gt;主成分分析（Principal components analysis)-最大方差解释&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020216.html"&gt;主成分分析（Principal components analysis）-最小平方误差解释&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Machine Learning:A Probablistic Perspective Chapter 12&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/jerrylead/archive/2011/04/19/2021071.html"&gt;独立成分分析（Independent Component Analysis)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="Latent Linear Models"></category><category term="Factor Analysis"></category><category term="EM"></category><category term="PCA"></category></entry><entry><title>机器学习系列(V): Generalized linear models and the exponential family</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-v-generalized-linear-models-and-the-exponential-family.html" rel="alternate"></link><updated>2014-04-23T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-23:ji-qi-xue-xi-xi-lie-v-generalized-linear-models-and-the-exponential-family.html</id><summary type="html">&lt;h1&gt;指数分布族&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;一件特别神奇的事是实际上我们学过或者用到的很多分布函数均可以写成一种一致的形式,事实上，属于这个家族的分布函数还是很多的，例如高斯分布、Bernoulli分布、二项分布、多项分布、指数分布、泊松分布、Dirichlet分布等,作为一个庞大家族的一员，这些分布函数具有一个它们引以为豪的共同的名字------&lt;strong&gt;指数分布族&lt;/strong&gt;。以下我们就介绍一下指数分布族的基础知识吧。&lt;/p&gt;
&lt;h2&gt;定义&lt;/h2&gt;
&lt;p&gt;传说中的这个神奇的家族中的分一个分布函数均可写成如下形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x) = h(x)e^{\theta^T T(x)-A(\theta)}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\theta$为参数向量,$T(x)$为"Sufficient statistics"向量,$A(\theta)$为cumulate generating function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;上式中我们不难注意到$\theta$和$x$仅在$\theta^T T(x)$一项中&lt;code&gt;耦合&lt;/code&gt;在一起。另,指数分布族函数之积仍是指数分布族函数,只是可能不再具有良好的参数形式。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;为了得到一个归一化的分布，我们有:对于任一$\theta$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\int p(x)dx = e^{-A(\theta)}\int h(x) e^{\theta^T T(x)} dx = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
e^{A(\theta)} = \int h(x) e^{\theta^T T(x)} dx
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;当$T(x)=x$时,$A(\theta)$是对于$h(x)$做&lt;strong&gt;拉普拉斯变换&lt;/strong&gt;之后的$log$值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;以下我们举几个实例以使我们对其有一个更为清晰的认识:&lt;/p&gt;
&lt;h3&gt;Bernoulli Distribution&lt;/h3&gt;
&lt;p&gt;对于Bernoulli分布,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(x) &amp;amp;= \alpha^x (1-\alpha)^{1-x}   \\
     &amp;amp;= exp[log(\alpha^x (1-\alpha)^{1-x})]  \\
     &amp;amp;= exp[xlog \alpha + (1-x) log(1-\alpha)] \\
     &amp;amp;= exp[xlog \frac{\alpha}{1-\alpha}+log(1-\alpha)]  \\
     &amp;amp;= exp[x\theta - log(1+e^{\theta})]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;于是我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
T(x) &amp;amp;= x \\
\theta &amp;amp;= log \frac{\alpha}{1-\alpha} \\
A(\theta) &amp;amp;= log(1+e^{\theta})
\end{split}
\end{equation}&lt;/p&gt;
&lt;h3&gt;Univariate Gaussian&lt;/h3&gt;
&lt;p&gt;对于单变量高斯分布,我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Univariate Gaussian" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/univariate_gaussian_zps8407f3a0.png" /&gt;&lt;/p&gt;
&lt;p&gt;其中:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Univariate Gaussian Params" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/univariate_gaussian_params_zps58c5d9d3.png" /&gt;&lt;/p&gt;
&lt;h3&gt;Multivariate Gaussian&lt;/h3&gt;
&lt;p&gt;对于形如下式的多变量高斯分布:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} e^{(x-\mu)^T \Sigma^{-1} (x-\mu)/2}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们有:(下式我并未真正推导)&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
h(x) &amp;amp;= (2\pi)^{-D/2} \\
T(x) &amp;amp;= \left(
\begin{array}{cc}
x \\
xx^T
\end{array}
\right) \\
\theta &amp;amp;= \left(
\begin{array}{cc}
\Sigma^{-1}\mu \\
-{1 \over 2}\Sigma^{-1}
\end{array}
\right)
\end{split}
\end{equation}&lt;/p&gt;
&lt;h2&gt;一阶导数&lt;/h2&gt;
&lt;p&gt;&lt;img alt="First Derivative" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/first_derivative_zps6152bd77.png" /&gt;&lt;/p&gt;
&lt;h2&gt;二阶导数&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Second Derivative" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/second_deriative_zps8d444606.png" /&gt;&lt;/p&gt;
&lt;p&gt;即$A(\theta)$是凸的($\succeq$表示正定positive definite)&lt;/p&gt;
&lt;h2&gt;Maximum Likelihood&lt;/h2&gt;
&lt;p&gt;根据$p(x)$的定义,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l(\theta) = \sum_{i=1}^{N} log p(x_i|\theta) = \sum_{i=1}^{N} [log h(x_i) + \theta^T T(x_i) - A(\theta)]
\end{equation}&lt;/p&gt;
&lt;p&gt;为了求得极大似然解,我们对$\theta$求偏导有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l \prime(\theta) = [\sum_{i=1}^N T(x_i)] - NA\prime(\theta) = 0
\end{equation}&lt;/p&gt;
&lt;p&gt;于是我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
A\prime(\hat\theta_{ML})=\frac{1}{N} \sum_{i=1}^N T(x_i)
\end{equation}&lt;/p&gt;
&lt;h2&gt;Conjugate Priors in Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;根据Bayes Rule,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{\int p(x|\theta)p(\theta) d\theta}
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:分母项只是一归一化项,其值与$\theta$无关。于是我们有$p(\theta|x) \propto p(x|\theta)p(\theta)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;正如我们之前提到的那样，为了简化计算,我们最好使得先验分布$p(\theta)$与Marginal Likelihood $p(x|\theta)$具有相似的形式(此时它们成为共轭分布)。如当先验分布是Dirichlet分布时,当我们取Marginal Likelihood为多项式分布时,后验分布为Dirichlet分布。由于Dirichlet分布与多项式分布具有相似的形式,在一定程度上可以简化我们的计算。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:关于Dirichlet与Multinomial之间的关系我们会在之后的&lt;em&gt;Dirichlet Process&lt;/em&gt;一篇中详细展开,敬请期待,此处不再赘述。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;常见的共轭分布如下表所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Conjugate Prior" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/conjugate_priors_zps0d694b87.png" /&gt;&lt;/p&gt;
&lt;h1&gt;广义线性模型&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;从软件工程的视角来看,如果我们把广义线性模型(Generalized Linear Models,GLM)看作一个类,线性回归与Logistic回归只不过是该类的两个实例而已,由此可见GLM是一个较为高大上的东东啊!以下对其进行一个较为简单的介绍:&lt;/p&gt;
&lt;h2&gt;基础知识&lt;/h2&gt;
&lt;p&gt;为了理解GLM,我们首先引入:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y_i|\theta,\sigma^2) = exp[\frac{y_i\theta-A(\theta)}{\sigma^2}+c(y_i,\sigma^2)]
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\sigma^2$为&lt;em&gt;dispersion parameter&lt;/em&gt;(一般设置为1),$\theta$为自然参数,$A$为partition function,$c$为归一化常数.如对于Logistic Regression而言,$\theta$为log-odds ratio,$\theta = log(\frac{\mu}{1-\mu})$,其中$\mu = E[y] = p(y=1)$为mean.为了将mean转化为自然参数,我们引入函数$\psi$,有:$\theta = \mathbf \Psi(\mu)$.实际上,如果存在逆向映射,我们则有:$\mu = \mathbf \Psi^{-1}(\theta)$.此外,根据我们以上推导的$A(\theta)$的一阶导数知:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mu = \Psi^{-1}(\theta) = A\prime(\theta)
\end{equation}&lt;/p&gt;
&lt;p&gt;若我们令:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\eta_i = w^T x_i
\end{equation}&lt;/p&gt;
&lt;p&gt;我们可以定义如下mean function,记为$g^{-1}$,使得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mu_i = g^{-1}(\eta_i) = g^{-1}(w^T x_i)
\end{equation}&lt;/p&gt;
&lt;p&gt;Mean function的逆函数,即$g()$,则称为&lt;em&gt;Link function&lt;/em&gt;.我们可以选取任意函数作为$g()$,只要它是可逆的,且使得逆函数具有合适的取值范围。如针对Logistic Regression而言,我们有:$\mu_i = g^{-1}(\eta_i) = sigm(\eta_i)$&lt;/p&gt;
&lt;p&gt;一种最为简单的Link function的函数是取$g=\psi$,称为Canonical Link Function.上面我们定义$\theta_i = \eta_i = w^Tx_i$,于是模型变为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y_i|x_i,w,\sigma^2) = exp[\frac{y_iw^Tx_i-A(w^Tx_i)}{\sigma^2}+c(y_i,\sigma^2)]
\end{equation}&lt;/p&gt;
&lt;p&gt;下表中,我们给出一些常见分布函数及其对应的Canonical Link Functions.我们可以看到对于Bernoulli和二项分布而言,Canonical Link Function为logit函数,$g(\mu)=log(\frac{\eta}{1-\eta})$,其逆函数为logistic函数,$\mu = sigm(\eta)$.&lt;/p&gt;
&lt;p&gt;根据我们第一部分对一阶导数以及二阶导数的推导我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E[y|x_i,w,\sigma^2] &amp;amp;= \mu_i = A\prime(\theta_i) \\
var[y|x_i,w,\sigma^2] &amp;amp;= \sigma_i^2 = A\prime\prime(\theta_i)\sigma^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;为了更便于理解,我们举几个具体的例子:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于线性回归而言,我们有:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
log p(y_i|x_i,w,\sigma^2) = \frac{y_i\mu_i-\frac{\mu_i^2}{2}}{\sigma^2} - {1 \over 2}(\frac{y_i^2}{\sigma_i^2}+log(2\pi \sigma^2))
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$y_i \in R$,且$\theta_i = \mu_i = w^T x_i$,这里我们取$A(\theta) = \theta^2/2$,因此$E[y_i] = \mu_i$,且$var[y_i]=\sigma^2$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于二项分布而言,我们有:(&lt;strong&gt;后面这两个未证明结论的正确性&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
log p(y_i|x_i,w) = y_i log(\frac{\pi_i}{1-\pi_i}) +N_ilog(1-\pi_i)+log 
\left(
\begin{array}{cc}
N_i \\
y_i
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$y_i \in {0,1,...,N_i}$,$\pi_i = sigm(w^Tx_i)$,$\theta_i = log(\pi_i/(1-\pi_i))=w^Tx_i$，且$\sigma^2=1$.这里我们取$A(\theta) = N_ilog(1+e^{\theta})$.于是我们有$E[y_i] = N_i \pi_i=\mu_i$,$var[y_i] = N_i \pi_i (1-\pi_i)$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于Poisson Regression而言,我们有:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
log p(y_i|x_i,w) = y_i log \mu_i - \mu_i - log(y_i!)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$y_i \in {0,1,2,...}$,$\mu_i = exp(w^Tx_i)$,$\theta_i = log(\mu_i) = w^Tx_i$且$\sigma^2=1$.这里我们取$A(\theta) = e^{\theta}$,于是我们有$E[y_i] = var[y_i] = \mu_i$.&lt;/p&gt;
&lt;h2&gt;MLE Estimation&lt;/h2&gt;
&lt;p&gt;其极大似然函数具有如下形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\ell(w) = log p(D|w) &amp;amp;= \frac{1}{\sigma^2}\sum_{i=1}^N \ell_i \\
\ell_i &amp;amp;\triangleq \theta_iy_i-A(\theta_i)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们可通过下式计算其梯度向量:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\frac{\ell_i}{w_j} &amp;amp;= \frac{\ell_i}{\theta_i}\frac{\theta_i}{\mu_i}\frac{\mu_i}{\eta_i}\frac{\eta_i}{w_j} \\
&amp;amp;= (y_i - A\prime(\theta_i))\frac{\theta_i}{\mu_i}\frac{\mu_i}{\eta_i}x_{ij} \\
&amp;amp;= (y_i-\mu_i)\frac{\theta_i}{\mu_i}\frac{\mu_i}{\eta_i}x_{ij}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;如果我们采用Canonical Link Function,$\theta_i = \eta_i$,上式可简化为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\bigtriangledown_w \ell(w) = \frac{1}{\sigma^2}[\sum_{i=1}^N (y_i-\mu_i)x_i]
\end{equation}&lt;/p&gt;
&lt;p&gt;利用该结果执行梯度下降即可得到ML估计值。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Probit Regression&lt;/li&gt;
&lt;li&gt;Multi-task Learning(&lt;em&gt;Transfer Learning&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Generalized Linear Mixture Models&lt;/li&gt;
&lt;li&gt;Learning to rank&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.cs.columbia.edu/~jebara/4771/tutorials/lecture12.pdf"&gt;Exponential Family&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="Exponential Family"></category><category term="Generalized Linear Models"></category></entry><entry><title>Play With Cardinality Estimation</title><link href="http://www.qingyuanxingsi.com/play-with-cardinality-estimation.html" rel="alternate"></link><updated>2014-04-14T00:00:00+08:00</updated><author><name>CodingLabs</name></author><id>tag:www.qingyuanxingsi.com,2014-04-14:play-with-cardinality-estimation.html</id><summary type="html">&lt;p&gt;记得之前某周例会的时候一个博士师兄抛出一个小问题:在大数据环境下,如何估计一个可能含有重复元素的集合中不同元素的数目,当时其实没有怎么在意。这两天因为看CNN的东西实在无法完全理解,所以到处逛了逛(&lt;code&gt;好吧,我每次逛了逛都能发现特别好玩的算法呀&lt;/code&gt;),于是不经意间发现了解决上述问题的一些现有算法,很是高兴呀。&lt;/p&gt;
&lt;p&gt;在开始今天的相关介绍之前,咱们扯点闲话吧,个人不是特别喜欢纯科研的科研,如果一个算法或者一个数据结构以至于一个理论不能应用到实际生活中去,不能解决实际生活中的某个问题的话,个人认为这种理论或者算法/数据结构的研究就是无意义的。个人还是比较倾向于好玩的科研吧,一方面研究的东西自己觉得有意思,另一方面又能应用到实际项目或生活实际中去,成为一个研究好玩问题的研究人员估计就是我毕生最大的志向了吧,呵呵。好吧,其实说这么多只是为了说明基数估计这个东西真的很好玩呀。(&lt;strong&gt;以后只要在Pearls目录下的博文均收集自他人博客,原始链接见脚注,版权属于原作者所有,无意侵犯,特此说明,以后不再说明&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;言归正传,开始我们正式的介绍。&lt;/p&gt;
&lt;h1&gt;基本概念&lt;sup id="sf-play-with-cardinality-estimation-1-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-1" title="解读Cardinality Estimation算法（第一部分：基本概念）"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;基数计数(&lt;strong&gt;Cardinality Counting&lt;/strong&gt;）是实际应用中一种常见的计算场景，在数据分析、网络监控及数据库优化等领域都有相关需求。精确的基数计数算法由于种种原因，在面对大数据场景时往往力不从心，因此如何在误差可控的情况下对基数进行估计就显得十分重要。目前常见的基数估计算法有Linear Counting、LogLog Counting、HyperLogLog Counting及Adaptive Counting等。这几种算法都是基于概率统计理论所设计的概率算法，它们克服了精确基数计数算法的诸多弊端（如内存需求过大或难以合并等），同时可以通过一定手段将误差控制在所要求的范围内。&lt;/p&gt;
&lt;p&gt;以下我们主要介绍一下基数估计(Cardinality Estimation)的基本概念。&lt;/p&gt;
&lt;h2&gt;基数的定义&lt;/h2&gt;
&lt;p&gt;简单来说，基数（Cardinality，也译作势），是指一个集合（这里的集合允许存在重复元素，与集合论对集合严格的定义略有不同，如不做特殊说明，本文中提到的集合均允许存在重复元素）中不同元素的个数。例如看下面的集合：&lt;/p&gt;
&lt;p&gt;${1,2,3,4,5,2,3,9,7}$&lt;/p&gt;
&lt;p&gt;这个集合有9个元素，但是2和3各出现了两次，因此不重复的元素为1,2,3,4,5,9,7，所以这个集合的基数是7。&lt;/p&gt;
&lt;p&gt;如果两个集合具有相同的基数，我们说这两个集合等势。基数和等势的概念在有限集范畴内比较直观，但是如果扩展到无限集则会比较复杂，一个无限集可能会与其真子集等势（例如整数集和偶数集是等势的）。不过在这个系列文章中，我们仅讨论有限集的情况，关于无限集合基数的讨论，有兴趣的同学可以参考实变分析相关内容。&lt;/p&gt;
&lt;p&gt;容易证明，如果一个集合是有限集，则其基数是一个自然数。&lt;/p&gt;
&lt;h2&gt;基数的应用实例&lt;/h2&gt;
&lt;p&gt;下面通过一个实例说明基数在电商数据分析中的应用。&lt;/p&gt;
&lt;p&gt;假设一个淘宝网店在其店铺首页放置了10个宝贝链接，分别从Item01到Item10为这十个链接编号。店主希望可以在一天中随时查看从今天零点开始到目前这十个宝贝链接分别被多少个独立访客点击过。所谓独立访客（&lt;code&gt;Unique Visitor，简称UV&lt;/code&gt;）是指有多少个自然人，例如，即使我今天点了五次Item01，我对Item01的UV贡献也是1，而不是5。&lt;/p&gt;
&lt;p&gt;用术语说这实际是一个实时数据流统计分析问题。&lt;/p&gt;
&lt;p&gt;要实现这个统计需求。需要做到如下三点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对独立访客做标识&lt;/li&gt;
&lt;li&gt;在访客点击链接时记录下链接编号及访客标记&lt;/li&gt;
&lt;li&gt;对每一个要统计的链接维护一个数据结构和一个当前UV值，当某个链接发生一次点击时，能迅速定位此用户在今天是否已经点过此链接，如果没有则此链接的UV增加1&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面分别介绍三个步骤的实现方案&lt;/p&gt;
&lt;h3&gt;对独立访客做标识&lt;/h3&gt;
&lt;p&gt;客观来说，目前还没有能在互联网上准确对一个自然人进行标识的方法，通常采用的是近似方案。例如通过登录用户+cookie跟踪的方式：当某个用户已经登录，则采用会员ID标识；对于未登录用户，则采用跟踪cookie的方式进行标识。为了简单起见，我们假设完全采用跟踪cookie的方式对独立访客进行标识。&lt;/p&gt;
&lt;h3&gt;记录链接编号及访客标记&lt;/h3&gt;
&lt;p&gt;这一步可以通过Javascript埋点及记录accesslog完成，具体原理和实现方案可以参考博文&lt;a href="http://blog.codinglabs.org/articles/how-web-analytics-data-collection-system-work.html"&gt;网站统计中的数据收集原理及实现&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;实时UV计算&lt;/h3&gt;
&lt;p&gt;可以看到，如果将每个链接被点击的日志中访客标识字段看成一个集合，那么此链接当前的UV也就是这个集合的基数，因此UV计算本质上就是一个基数计数问题。&lt;/p&gt;
&lt;p&gt;在实时计算流中，我们可以认为任何一次链接点击均触发如下逻辑（伪代码描述）：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;cand_counting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item_no&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_id&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;item_no&lt;/span&gt; &lt;span class="n"&gt;visitor&lt;/span&gt; &lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;add&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;item_no&lt;/span&gt; &lt;span class="n"&gt;visitor&lt;/span&gt; &lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;cand&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;item_no&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;逻辑非常简单，每当有一个点击事件发生，就去相应的链接被访集合中寻找此访客是否已经在里面，如果没有则将此用户标识加入集合，并将此链接的UV加1。&lt;/p&gt;
&lt;p&gt;虽然逻辑非常简单，但是在实际实现中尤其面临大数据场景时还是会遇到诸多困难，下面一节我会介绍两种目前被业界普遍使用的精确算法实现方案，并通过分析说明当数据量增大时它们面临的问题。&lt;/p&gt;
&lt;h2&gt;传统的基数计数实现&lt;/h2&gt;
&lt;p&gt;接着上面的例子，我们看一下目前常用的基数计数的实现方法。&lt;/p&gt;
&lt;h3&gt;基于B树的基数计数&lt;/h3&gt;
&lt;p&gt;对上面的伪代码做一个简单分析，会发现关键操作有两个：查找-迅速定位当前访客是否已经在集合中，插入-将新的访客标识插入到访客集合中。因此，需要为每一个需要统计UV的点（此处就是十个宝贝链接）维护一个查找效率较高的数据结构，又因为实时数据流的关系，这个数据结构需要尽量在内存中维护，因此这个数据结构在空间复杂度上也要比较适中。综合考虑一种传统的做法是在实时计算引擎采用了B树来组织这个集合。下图是一个示意图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="B Tree" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/B_tree_zpsda8ce41d.png"&gt;&lt;/p&gt;
&lt;p&gt;之所以选用B树是因为B树的查找和插入相关高效，同时空间复杂度也可以接受（关于B树具体的性能分析请参考&lt;a href="http://en.wikipedia.org/wiki/B-tree"&gt;B_Tree&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;这种实现方案为一个基数计数器维护一棵B树，由于B树在查找效率、插入效率和内存使用之间非常平衡，所以算是一种可以接受的解决方案。但是当数据量特别巨大时，例如要同时统计几万个链接的UV，如果要将几万个链接一天的访问记录全部维护在内存中，这个内存使用量也是相当可观的（假设每个B树占用1M内存，10万个B树就是100G！）。一种方案是在某个时间点将内存数据结构写入磁盘（双十一和双十二大促时一淘数据部的效果平台是每分钟将数据写入HBase）然后将内存中的计数器和数据结构清零，但是B树并不能高效的进行合并，这就使得内存数据落地成了非常大的难题。&lt;/p&gt;
&lt;p&gt;另一个需要数据结构合并的场景是查看并集的基数，例如在上面的例子中，如果我想查看Item1和Item2的总UV，是没有办法通过这种B树的结构快速得到的。当然可以为每一种可能的组合维护一棵B树。不过通过简单的分析就可以知道这个方案基本不可行。N个元素集合的非空幂集数量为2N−1，因此要为10个链接维护1023棵B树，而随着链接的增加这个数量会以幂指级别增长。&lt;/p&gt;
&lt;h3&gt;基于Bitmap的基数计数&lt;/h3&gt;
&lt;p&gt;为了克服B树不能高效合并的问题，一种替代方案是使用Bitmap表示集合。也就是使用一个很长的bit数组表示集合，将bit位顺序编号，bit为1表示此编号在集合中，为0表示不在集合中。例如“00100110”表示集合 {2，5，6}。bitmap中1的数量就是这个集合的基数。&lt;/p&gt;
&lt;p&gt;显然，与B树不同Bitmap可以高效的进行合并，只需进行按位或（or）运算就可以，而位运算在计算机中的运算效率是很高的。但是Bitmap方式也有自己的问题，就是内存使用问题。&lt;/p&gt;
&lt;p&gt;很容易发现，Bitmap的长度与集合中元素个数无关，而是与基数的上限有关。例如在上面的例子中，假如要计算上限为1亿的基数，则需要12.5M字节的Bitmap，十个链接就需要125M。关键在于，这个内存使用与集合元素数量无关，即使一个链接仅仅有一个1UV，也要为其分配12.5M字节。&lt;/p&gt;
&lt;p&gt;由此可见，虽然Bitmap方式易于合并，却由于内存使用问题而无法广泛用于大数据场景。&lt;/p&gt;
&lt;h1&gt;Linear Counting&lt;sup id="sf-play-with-cardinality-estimation-2-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-2" title="解读Cardinality Estimation算法（第二部分：Linear Counting）"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;通过上面的介绍我们知道传统的精确基数计数算法在数据量大时会存在一定瓶颈，瓶颈主要来自于&lt;strong&gt;数据结构合并和内存使用&lt;/strong&gt;两个方面。因此出现了很多基数估计的概率算法，这些算法虽然计算出的结果不是精确的，但误差可控，重要的是这些算法所使用的数据结构易于合并，同时比传统方法大大节省内存。&lt;/p&gt;
&lt;p&gt;作为本文的第二部分，我们讨论Linear Counting算法。&lt;/p&gt;
&lt;h2&gt;简介&lt;/h2&gt;
&lt;p&gt;Linear Counting（以下简称LC）在1990年的一篇论文“A linear-time probabilistic counting algorithm for database applications”中被提出。作为一个早期的基数估计算法，LC在空间复杂度方面并不算优秀，实际上LC的空间复杂度与上文中简单Bitmap方法是一样的（但是有个常数项级别的降低），都是$O(N_{max})$，因此目前很少单独使用LC。不过作为Adaptive Counting等算法的基础，研究一下LC还是比较有价值的。&lt;/p&gt;
&lt;h2&gt;基本算法&lt;/h2&gt;
&lt;h3&gt;思路&lt;/h3&gt;
&lt;p&gt;LC的基本思路是：设有一哈希函数$H$，其哈希结果空间有$m$个值（最小值$0$，最大值$m-1$），并且哈希结果服从均匀分布。使用一个长度为$m$的Bitmap，每个bit为一个桶，均初始化为0，设一个集合的基数为$n$，此集合所有元素通过$H$哈希到Bitmap中，如果某一个元素被哈希到第$k$个比特并且第$k$个比特为$0$，则将其置为$1$。当集合所有元素哈希完成后，设Bitmap中还有$u$个bit为$0$。则：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=−mlog_u m
\end{equation}&lt;/p&gt;
&lt;p&gt;为$n$的一个估计，且为最大似然估计（MLE）。&lt;/p&gt;
&lt;p&gt;示意图如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="LC Hash" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hash_lc_zpsad47853b.png"&gt;&lt;/p&gt;
&lt;h3&gt;推导及证明&lt;/h3&gt;
&lt;p&gt;由上文对$H$的定义已知$n$个不同元素的哈希值服从独立均匀分布。设$A_j$为事件“经过$n$个不同元素哈希后，第$j$个桶值为0”，则：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(A_j)=(1−{1 \over m})n
\end{equation}&lt;/p&gt;
&lt;p&gt;又每个桶是独立的，则$u$的期望为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(u)=\sum_{j=1}^mP(A_j)=m(1-\frac{1}{m})^n=m((1+\frac{1}{-m})^{-m})^{-n/m}
\end{equation}&lt;/p&gt;
&lt;p&gt;当$n$和$m$趋于无穷大时，其值约为$me^{-{n \over m}}$&lt;/p&gt;
&lt;p&gt;令：&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(u)=me^{-n/m}
\end{equation}&lt;/p&gt;
&lt;p&gt;得：&lt;/p&gt;
&lt;p&gt;\begin{equation}
n=−mlog \frac{E(u)}{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;显然每个桶的值服从参数相同0-1分布，因此$u$服从二项分布。由概率论知识可知，当$n$很大时，可以用正态分布逼近二项分布，因此可以认为当$n$和$m$趋于无穷大时$u$渐进服从正态分布。&lt;/p&gt;
&lt;p&gt;因此$u$的概率密度函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f(x) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}}
\end{equation}&lt;/p&gt;
&lt;p&gt;由于我们观察到的空桶数$u$是从正态分布中随机抽取的一个样本，因此它就是$\mu$的最大似然估计（正态分布的期望的最大似然估计是样本均值）。&lt;/p&gt;
&lt;p&gt;又由如下定理：&lt;/p&gt;
&lt;p&gt;设$f(x)$是可逆函数,$\hat{x}$是$x$的最大似然估计，则$f(\hat{x})$是$f(x)$的最大似然估计。
且$-mlog\frac{x}{m}$是可逆函数，则$\hat{n}=-mlog\frac{u}{m}$是$-mlog\frac{E(u)}{m}=n$的最大似然估计。&lt;/p&gt;
&lt;h2&gt;偏差分析&lt;/h2&gt;
&lt;p&gt;下面不加证明给出如下结论：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
Bias(\frac{\hat{n}}{n}) &amp;amp;=E(\frac{\hat{n}}{n})-1=\frac{e^t-t-1}{2n} \\
StdError(\frac{\hat{n}}{n}) &amp;amp;=\frac{\sqrt{m}(e^t-t-1)^{1/2}}{n}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$t=n/m$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:以上结论的推导在“A linear-time probabilistic counting algorithm for database applications”可以找到。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;算法应用&lt;/h2&gt;
&lt;p&gt;在应用LC算法时，主要需要考虑的是Bitmap长度$m$的选择。这个选择主要受两个因素的影响：基数$n$的量级以及容许的误差。这里假设估计基数$n$的量级大约为$N$，允许的误差为$\epsilon$，则$m$的选择需要遵循如下约束。&lt;/p&gt;
&lt;h3&gt;误差控制&lt;/h3&gt;
&lt;p&gt;这里以标准差作为误差。由上面标准差公式可以推出，当基数的量级为$N$，容许误差为$\epsilon$时，有如下限制：&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; \frac{e^t-t-1}{(\epsilon t)^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;将量级和容许误差带入上式，就可以得出$m$的最小值。&lt;/p&gt;
&lt;h3&gt;满桶控制&lt;/h3&gt;
&lt;p&gt;由LC的描述可以看到，如果$m$比$n$小太多，则很有可能所有桶都被哈希到了，此时$u$的值为0，LC的估计公式就不起作用了（变成无穷大）。因此$m$的选择除了要满足上面误差控制的需求外，还要保证满桶的概率非常小。&lt;/p&gt;
&lt;p&gt;上面已经说过，$u$满足二项分布，而当$n$非常大，$p$非常小时，可以用泊松分布近似逼近二项分布。因此这里我们可以认为$u$服从泊松分布（注意，上面我们说$u$也可以近似服从正态分布，这并不矛盾，实际上泊松分布和正态分布分别是二项分布的离散型和连续型概率逼近，且泊松分布以正态分布为极限）：&lt;/p&gt;
&lt;p&gt;当$n、m$趋于无穷大时：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Pr(u=k)=(\frac{\lambda^k}{k!})e^{-\lambda}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Pr(u=0)&amp;lt;e^{-5}=0.007
\end{equation}&lt;/p&gt;
&lt;p&gt;由于泊松分布的方差为$\lambda$，因此只要保证$u$的期望偏离$0$点$\sqrt{5}$的标准差就可以保证满桶的概率不大于$0.7%$。因此可得：&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; 5(e^t-t-1)
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:上式没看懂,望看懂的童鞋不吝赐教！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;综上所述，当基数量级为$N$，可接受误差为$\epsilon$，则$m$的选取应该遵从&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; \beta(e^t-t-1)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\beta = max(5, 1/(\epsilon t)^2)$&lt;/p&gt;
&lt;p&gt;下图是论文作者预先计算出的关于不同基数量级和误差情况下，$m$的选择表：&lt;/p&gt;
&lt;p&gt;&lt;img alt="m choice" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/m_choice_zpsb78fb0f8.png"&gt;&lt;/p&gt;
&lt;p&gt;可以看出精度要求越高，则Bitmap的长度越大。随着$m$和$n$的增大，$m$大约为$n$的十分之一。因此LC所需要的空间只有传统的Bitmap直接映射方法的1/10，但是从渐进复杂性的角度看，空间复杂度仍为$O(N_{max})$。&lt;/p&gt;
&lt;h3&gt;合并&lt;/h3&gt;
&lt;p&gt;LC非常方便于合并，合并方案与传统Bitmap映射方法无异，都是通过按位或的方式。&lt;/p&gt;
&lt;h1&gt;LogLog Counting&lt;sup id="sf-play-with-cardinality-estimation-3-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-3" title="解读Cardinality Estimation算法（第三部分：LogLog Counting）"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;上一部分介绍的Linear Counting算法相较于直接映射Bitmap的方法能大大节省内存（大约只需后者1/10的内存），但毕竟只是一个常系数级的降低，空间复杂度仍然为$O(N_max)$。而本文要介绍的LogLog Counting却只有$O(log_2(log_2(N_{max})))$。例如，假设基数的上限为1亿，原始Bitmap方法需要12.5M内存，而LogLog Counting只需不到1K内存（640字节）就可以在标准误差不超过4%的精度下对基数进行估计，效果可谓十分惊人。&lt;/p&gt;
&lt;p&gt;本部分将介绍LogLog Counting。&lt;/p&gt;
&lt;h2&gt;简介&lt;/h2&gt;
&lt;p&gt;LogLog Counting（以下简称LLC）出自论文“Loglog Counting of Large Cardinalities”。LLC的空间复杂度仅有$O(log_2(log_2(N_{max})))$，使得通过KB级内存估计数亿级别的基数成为可能，因此目前在处理大数据的基数计算问题时，所采用算法基本为LLC或其几个变种。下面来具体看一下这个算法。&lt;/p&gt;
&lt;h2&gt;基本算法&lt;/h2&gt;
&lt;h3&gt;均匀随机化&lt;/h3&gt;
&lt;p&gt;与LC一样，在使用LLC之前需要选取一个哈希函数$H$应用于所有元素，然后对哈希值进行基数估计。$H$必须满足如下条件（定性的）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H$的结果具有很好的均匀性，也就是说无论原始集合元素的值分布如何，其哈希结果的值几乎服从均匀分布（完全服从均匀分布是不可能的，D.Knuth已经证明不可能通过一个哈希函数将一组不服从均匀分布的数据映射为绝对均匀分布，但是很多哈希函数可以生成几乎服从均匀分布的结果，这里我们忽略这种理论上的差异，认为哈希结果就是服从均匀分布）。&lt;/li&gt;
&lt;li&gt;$H$的碰撞几乎可以忽略不计。也就是说我们认为对于不同的原始值，其哈希结果相同的概率非常小以至于可以忽略不计。&lt;/li&gt;
&lt;li&gt;$H$的哈希结果是固定长度的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上对哈希函数的要求是随机化和后续概率分析的基础。后面的分析均认为是针对哈希后的均匀分布数据进行。&lt;/p&gt;
&lt;h3&gt;思想来源&lt;/h3&gt;
&lt;p&gt;下面非正式的从直观角度描述LLC算法的思想来源。&lt;/p&gt;
&lt;p&gt;设$a$为待估集合（哈希后）中的一个元素，由上面对$H$的定义可知，$a$可以看做一个长度固定的比特串（也就是$a$的二进制表示），设$H$哈希后的结果长度为$L$比特，我们将这$L$个比特位从左到右分别编号为$1、2、…、L$：&lt;/p&gt;
&lt;p&gt;&lt;img alt="LLC Structure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/llc_structure_zpsefad7ee9.png"&gt;&lt;/p&gt;
&lt;p&gt;又因为$a$是从服从均与分布的样本空间中随机抽取的一个样本，因此$a$每个比特位服从如下分布且相互独立。&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(x=k)=\left \lbrace
\begin{array}{cc}
0.5 &amp;amp; (k=0) \\ 
0.5 &amp;amp; (k=1)
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;通俗说就是$a$的每个比特位为0和1的概率各为0.5，且相互之间是独立的。&lt;/p&gt;
&lt;p&gt;设$\rho(a)$为$a$的比特串中第一个“1”出现的位置，显然$1≤\rho(a)≤L$，这里我们忽略比特串全为0的情况（概率为$1/2^L$）。如果我们遍历集合中所有元素的比特串，取$\rho_{max}$为所有$\rho(a)$的最大值。&lt;/p&gt;
&lt;p&gt;此时我们可以将$2^{\rho_{max}}$作为基数的一个粗糙估计，即：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n} = 2^{\rho_{max}}
\end{equation}&lt;/p&gt;
&lt;p&gt;下面解释为什么可以这样估计。注意如下事实：&lt;/p&gt;
&lt;p&gt;由于比特串每个比特都独立且服从0-1分布，因此从左到右扫描上述某个比特串寻找第一个“1”的过程从统计学角度看是一个伯努利过程，例如，可以等价看作不断投掷一个硬币（每次投掷正反面概率皆为0.5），直到得到一个正面的过程。在一次这样的过程中，投掷一次就得到正面的概率为$1/2$，投掷两次得到正面的概率是$1/2^2$，…，投掷k次才得到第一个正面的概率为$1/2^k$。&lt;/p&gt;
&lt;p&gt;现在考虑如下两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;进行n次伯努利过程，所有投掷次数都不大于$k$的概率是多少？&lt;/li&gt;
&lt;li&gt;进行n次伯努利过程，至少有一次投掷次数等于$k$的概率是多少？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;首先看第一个问题，在一次伯努利过程中，投掷次数大于$k$的概率为$1/2^k$，即连续掷出$k$个反面的概率。因此，在一次过程中投掷次数不大于$k$的概率为$1−1/2^k$。因此，$n$次伯努利过程投掷次数均不大于$k$的概率为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X \leq k)=(1-1/2^k)^n
\end{equation}&lt;/p&gt;
&lt;p&gt;显然第二个问题的答案是：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X \neq k)=1-(1-1/2^{k-1})^n
\end{equation}&lt;/p&gt;
&lt;p&gt;从以上分析可以看出，当$n \ll 2^k$，$P_n(X \neq k)$的概率几乎为0，同时，当$n \gg k$时，$P_n(X \leq k)$的概率也几乎为0。用自然语言概括上述结论就是：当伯努利过程次数远远小于$2^k$时，至少有一次过程投掷次数等于$k$的概率几乎为0；当伯努利过程次数远远大于$2^k$时，没有一次过程投掷次数大于$k$的概率也几乎为0。&lt;/p&gt;
&lt;p&gt;如果将上面描述做一个对应：一次伯努利过程对应一个元素的比特串，反面对应0，正面对应1，投掷次数$k$对应第一个“1”出现的位置，我们就得到了下面结论：&lt;/p&gt;
&lt;p&gt;设一个集合的基数为$n$，$\rho_{max}$为所有元素中首个“1”的位置最大的那个元素的“1”的位置，如果$n$远远小于$2^{\rho_{max}}$，则我们得到$\rho_{max}$为当前值的概率几乎为0（它应该更小），同样的，如果$n$远远大于$2^{\rho_{max}}$，则我们得到$\rho_{max}$为当前值的概率也几乎为0（它应该更大），因此$2^{\rho_{max}}$可以作为基数$n$的一个粗糙估计。&lt;/p&gt;
&lt;h3&gt;分桶平均&lt;/h3&gt;
&lt;p&gt;上述分析给出了LLC的基本思想，不过如果直接使用上面的单一估计量进行基数估计会由于偶然性而存在较大误差。因此，LLC采用了分桶平均的思想来消减误差。具体来说，就是将哈希空间平均分成$m$份，每份称之为一个桶（bucket）。对于每一个元素，其哈希值的前$k$比特作为桶编号，其中$2^k=m$，而后$L-k$个比特作为真正用于基数估计的比特串。桶编号相同的元素被分配到同一个桶，在进行基数估计时，首先计算每个桶内元素最大的第一个“1”的位置，设为$M[i]$，然后对这$m$个值取平均后再进行估计，即：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=2^{\frac{1}{m}\sum{M[i]}}
\end{equation}&lt;/p&gt;
&lt;p&gt;这相当于物理试验中经常使用的多次试验取平均的做法，可以有效消减因偶然性带来的误差。&lt;/p&gt;
&lt;p&gt;下面举一个例子说明分桶平均怎么做。&lt;/p&gt;
&lt;p&gt;假设$H$的哈希长度为16bit，分桶数$m$定为32。设一个元素哈希值的比特串为“0001001010001010”，由于$m$为32，因此前5个bit为桶编号，所以这个元素应该归入“00010”即2号桶（桶编号从0开始，最大编号为$m-1$），而剩下部分是“01010001010”且显然ρ(01010001010)=2，所以桶编号为“00010”的元素最大的$\rho$即为$M[2]$的值。&lt;/p&gt;
&lt;h3&gt;偏差修正&lt;/h3&gt;
&lt;p&gt;上述经过分桶平均后的估计量看似已经很不错了，不过通过数学分析可以知道这并不是基数n的无偏估计。因此需要修正成无偏估计。这部分的具体数学分析在“Loglog Counting of Large Cardinalities”中，过程过于艰涩这里不再具体详述，有兴趣的朋友可以参考原论文。这里只简要提一下分析框架：&lt;/p&gt;
&lt;p&gt;首先上文已经得出：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X \leq k)=(1-1/2^k)^n
\end{equation}&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X = k)=(1-1/2^k)^n - (1-1/2^{k-1})^n
\end{equation}&lt;/p&gt;
&lt;p&gt;这是一个未知通项公式的递推数列，研究这种问题的常用方法是使用生成函数（generating function）。通过运用指数生成函数和poissonization得到上述估计量的Poisson期望和方差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\varepsilon _n&amp;amp;\sim [(\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^m+\epsilon _n]n \\
\nu _n&amp;amp;\sim [(\Gamma (-2/m)\frac{1-2^{2/m}}{log2})^m - (\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^{2m}+\eta _n]n^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$|\epsilon _n|$和$|\eta _n|$不超过$10^{−6}$。&lt;/p&gt;
&lt;p&gt;最后通过depoissonization得到一个渐进无偏估计量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=\alpha _m 2^{\frac{1}{m}\sum{M[i]}}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Equation_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/equation_one_zps76f405b2.png"&gt;&lt;/p&gt;
&lt;p&gt;其中$m$是分桶数。这就是LLC最终使用的估计量。&lt;/p&gt;
&lt;h3&gt;误差分析&lt;/h3&gt;
&lt;p&gt;不加证明给出如下结论：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E_n(\hat{n})/n &amp;amp;= 1 + \theta_{1,n} + o(1) \\
\sqrt{Var_n(E)}/n &amp;amp;= \beta_m / \sqrt{m} + \theta_{2,n} + o(1)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$|\theta_{1,n}|$和$|\theta_{2,n}|$不超过$10^{−6}$。&lt;/p&gt;
&lt;p&gt;当$m$不太小（不小于64）时，$\beta$大约为1.30。因此：&lt;/p&gt;
&lt;p&gt;\begin{equation}
StdError(\hat{n}/n) \approx \frac{1.30}{\sqrt{m}}
\end{equation}&lt;/p&gt;
&lt;h2&gt;算法应用&lt;/h2&gt;
&lt;h3&gt;误差控制&lt;/h3&gt;
&lt;p&gt;在应用LLC时，主要需要考虑的是分桶数$m$，而这个$m$主要取决于误差。根据上面的误差分析，如果要将误差控制在$\epsilon$之内，则：&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; (\frac{1.30}{\epsilon})^2
\end{equation}&lt;/p&gt;
&lt;h3&gt;内存使用分析&lt;/h3&gt;
&lt;p&gt;内存使用与$m$的大小及哈希值得长度（或说基数上限）有关。假设$H$的值为32bit，由于$\rho_{max} \leq 32$，因此每个桶需要5bit空间存储这个桶的$\rho_{max}$，$m$个桶就是$5 \times m/8$字节。例如基数上限为一亿（约227），当分桶数$m$为1024时，每个桶的基数上限约为227/210=217，而$log_2(log_2(217))=4.09$，因此每个桶需要5bit，需要字节数就是$5×1024/8=640$，误差为$1.30 / \sqrt{1024} = 0.040625$，也就是约为$4%$。&lt;/p&gt;
&lt;h3&gt;合并&lt;/h3&gt;
&lt;p&gt;与LC不同，LLC的合并是以桶为单位而不是bit为单位，由于LLC只需记录桶的$\rho_{max}$，因此合并时取相同桶编号数值最大者为合并后此桶的数值即可。&lt;/p&gt;
&lt;h1&gt;HyperLogLog Counting及Adaptive Counting&lt;sup id="sf-play-with-cardinality-estimation-4-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-4" title="解读Cardinality Estimation算法（第四部分：HyperLogLog Counting及Adaptive Counting）"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;在前一部分，我们了解了LogLog Counting。LLC算法的空间复杂度为$O(log_2(log_2(N_{max})))$，并且具有较高的精度，因此非常适合用于大数据场景的基数估计。不过LLC也有自己的问题，就是当基数不太大时，估计值的误差会比较大。这主要是因为当基数不太大时，可能存在一些空桶，这些空桶的$\rho_{max}$为0。由于LLC的估计值依赖于各桶$\rho_{max}$的几何平均数，而几何平均数对于特殊值（这里就是指0）非常敏感，因此当存在一些空桶时，LLC的估计效果就变得较差。&lt;/p&gt;
&lt;p&gt;本部分将要介绍的HyperLogLog Counting及Adaptive Counting算法均是对LLC算法的改进，可以有效克服LLC对于较小基数估计效果差的缺点。&lt;/p&gt;
&lt;h2&gt;评价基数估计算法的精度&lt;/h2&gt;
&lt;p&gt;首先我们来分析一下LLC的问题。一般来说LLC最大问题在于当基数不太大时，估计效果比较差。上文说过，LLC的渐近标准误差为$1.30/\sqrt{m}$，看起来貌似只和分桶数$m$有关，那么为什么基数的大小也会导致效果变差呢？这就需要重点研究一下如何评价基数估计算法的精度，以及“渐近标准误差”的意义是什么。&lt;/p&gt;
&lt;h3&gt;标准误差&lt;/h3&gt;
&lt;p&gt;首先需要明确标准误差的意义。例如标准误差为0.02，到底表示什么意义。&lt;/p&gt;
&lt;p&gt;标准误差是针对一个统计量（或估计量）而言。在分析基数估计算法的精度时，我们关心的统计量是$\hat{n}/n$。注意这个量分子分母均为一组抽样的统计量。下面正式描述一下这个问题。&lt;/p&gt;
&lt;p&gt;设$S$是我们要估计基数的可重复有限集合。$S$中每个元素都是来自值服从均匀分布的样本空间的一个独立随机抽样样本。这个集合共有$C$个元素，但其基数不一定是$C$，因为其中可能存在重复元素。设$f_n$为定义在$S$上的函数：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f_n(S)=CardinalityofS
\end{equation}&lt;/p&gt;
&lt;p&gt;同时定义$\hat{f_n}$也是定义在$S$上的函数：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f_\hat{n}(S)=LogLog\;estimate\;value\;of\;S
\end{equation}&lt;/p&gt;
&lt;p&gt;我们想得到的第一个函数值，但是由于第一个函数值不好计算，所以我们计算同样集合的第二个函数值来作为第一个函数值得估计。因此最理想的情况是对于任意一个集合两个函数值是相等的，如果这样估计就是100%准确了。不过显然没有这么好的事，因此我们退而求其次，只希望fn^(S)是一个无偏估计，即：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f_\hat{n}(S)
\end{equation}&lt;/p&gt;
&lt;p&gt;这个在上一篇文章中已经说明了。同时也可以看到，$\frac{f_\hat{n}(S)}{f_n(S)}$实际上是一个随机变量，并且服从正态分布。对于正态分布随机变量，一般可以通过标准差$\sigma$度量其稳定性，直观来看，标准差越小，则整体分布越趋近于均值，所以估计效果就越好。这是定性的，那么定量来看标准误差$\sigma$到底表达了什么意思呢。它的意义是这样的：&lt;/p&gt;
&lt;p&gt;对于无偏正态分布而言，随机变量的一次随机取值落在均值一个标准差范围内的概率是68.2%，而落在两个和三个标准差范围内的概率分别为95.4%和99.6%，如下图所示（图片来自维基百科）：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Norm" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/norm_zps0655fdfe.png"&gt;&lt;/p&gt;
&lt;p&gt;因此，假设标准误差是0.02（2%），它实际的意义是：假设真实基数为$n$，$n$与估计值之比落入(0.98, 1.02)的概率是68.2%，落入(0.96,1.04)的概率是95.4%，落入(0.94,1.06)的概率是99.6%。显然这个比值越大则估计值越不准，因此对于0.02的标准误差，这个比值大于1.06或小于0.94的概率不到0.004。&lt;/p&gt;
&lt;p&gt;再直观一点，假设真实基数为10000，则一次估计值有99.6%的可能不大于10600且不小于9400。&lt;/p&gt;
&lt;h3&gt;组合计数与渐近分析&lt;/h3&gt;
&lt;p&gt;如果LLC能够做到绝对服从$1.30/\sqrt{m}$，那么也算很好了，因为我们只要通过控制分桶数$m$就可以得到一个一致的标准误差。这里的一致是指标准误差与基数无关。不幸的是并不是这样，上面已经说过，这是一个“渐近”标注误差。下面解释一下什么叫渐近。&lt;/p&gt;
&lt;p&gt;在计算数学中，有一个非常有用的分支就是组合计数。组合计数简单来说就是分析自然数的组合函数随着自然数的增长而增长的量级。可能很多人已经意识到这个听起来很像算法复杂度分析。没错，算法复杂度分析就是组合计数在算法领域的应用。&lt;/p&gt;
&lt;p&gt;举个例子，设$A$是一个有$n$个元素的集合（这里$A$是严格的集合，不存在重复元素），则$A$的幂集（即由$A$的所有子集组成的集合）有$2^n$个元素。&lt;/p&gt;
&lt;p&gt;上述关于幂集的组合计数是一个非常整齐一致的组合计数，也就是不管$n$多大，A的幂集总有$2^n$个元素。&lt;/p&gt;
&lt;p&gt;可惜的是现实中一般的组合计数都不存在如此干净一致的解。LLC的偏差和标准差其实都是组合函数，但是论文中已经分析出，LLC的偏差和标准差都是渐近组合计数，也就是说，随着$n$趋向于无穷大，标准差趋向于$1.30/\sqrt{m}$，而不是说$n$多大时其值都一致为$1.30/\sqrt{m}$。另外，其无偏性也是渐近的，只有当$n$远远大于$m$时，其估计值才近似无偏。因此当$n$不太大时，LLC的效果并不好。&lt;/p&gt;
&lt;p&gt;庆幸的是，同样通过统计分析方法，我们可以得到$n$具体小到什么程度我们就不可忍受了，另外就是当$n$太小时可不可以用别的估计方法替代LLC来弥补LLC这个缺陷。HyperLogLog Counting及Adaptive Counting都是基于这个思想实现的。&lt;/p&gt;
&lt;h2&gt;Adaptive Counting&lt;/h2&gt;
&lt;p&gt;Adaptive Counting（简称AC）在“Fast and accurate traffic matrix measurement using adaptive cardinality counting”一文中被提出。其思想也非常简单直观：实际上AC只是简单将LC和LLC组合使用，根据基数量级决定是使用LC还是LLC。具体是通过分析两者的标准差，给出一个阈值，根据阈值选择使用哪种估计。&lt;/p&gt;
&lt;h3&gt;基本算法&lt;/h3&gt;
&lt;p&gt;如果分析一下LC和LLC的存储结构，可以发现两者是兼容的，区别仅仅在于LLC关心每个桶的$\rho_{max}$，而LC仅关心此桶是否为空。因此只要简单认为$\rho_{max}$值不为0的桶为非空，桶为空就可以使用LLC的数据结构做LC估计了。&lt;/p&gt;
&lt;p&gt;而我们已经知道，LC在基数不太大时效果好，基数太大时会失效；LLC恰好相反，因此两者有很好的互补性。&lt;/p&gt;
&lt;p&gt;回顾一下，LC的标准误差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
SE_{lc}(\hat{n}/n)=\sqrt{e^t-t-1}/(t\sqrt{m})
\end{equation}&lt;/p&gt;
&lt;p&gt;LLC的标准误差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
SE_{llc}(\hat{n}/n)=1.30/\sqrt{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;将两个公式联立：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sqrt{e^t-t-1}/(t\sqrt{m})=1.30/\sqrt{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;解得$t \approx 2.89$。注意$m$被消掉了，说明这个阈值与$m$无关。其中$t=n/m$。&lt;/p&gt;
&lt;p&gt;设$\beta$为空桶率，根据LC的估算公式，带入上式可得：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\beta = e^{-t} \approx 0.051
\end{equation}&lt;/p&gt;
&lt;p&gt;因此可以知道，当空桶率大于0.051时，LC的标准误差较小，而当小于0.051时，LLC的标准误差较小。
完整的AC算法如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=\left \lbrace 
\begin{array}{cc}
\alpha_m m2^{\frac{1}{m}\sum{M}} &amp;amp; if &amp;amp; 0 \leq \beta &amp;lt; 0.051 \\ 
-mlog(\beta) &amp;amp; if &amp;amp; 0.051 \leq \beta \leq 1 \end{array} 
\right.
\end{equation}&lt;/p&gt;
&lt;h3&gt;误差分析&lt;/h3&gt;
&lt;p&gt;因为AC只是LC和LLC的简单组合，所以误差分析可以依照LC和LLC进行。值得注意的是，当β&amp;lt;0.051时，LLC最大的偏差不超过0.17%，因此可以近似认为是无偏的。&lt;/p&gt;
&lt;h2&gt;HyperLogLog Counting&lt;/h2&gt;
&lt;p&gt;HyperLogLog Counting（以下简称HLLC）的基本思想也是在LLC的基础上做改进，不过相对于AC来说改进的比较多，所以相对也要复杂一些。本文不做具体细节分析，具体细节请参考“HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm”这篇论文。&lt;/p&gt;
&lt;h3&gt;基本算法&lt;/h3&gt;
&lt;p&gt;HLLC的第一个改进是使用调和平均数替代几何平均数。注意LLC是对各个桶取算数平均数，而算数平均数最终被应用到2的指数上，所以总体来看LLC取得是几何平均数。由于几何平均数对于离群值（例如这里的0）特别敏感，因此当存在离群值时，LLC的偏差就会很大，这也从另一个角度解释了为什么n不太大时LLC的效果不太好。这是因为n较小时，可能存在较多空桶，而这些特殊的离群值强烈干扰了几何平均数的稳定性。&lt;/p&gt;
&lt;p&gt;因此，HLLC使用调和平均数来代替几何平均数，调和平均数的定义如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
H = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + ... + \frac{1}{x_n}} = \frac{n}{\sum_{i=1}^n \frac{1}{x_i}}
\end{equation}&lt;/p&gt;
&lt;p&gt;调和平均数可以有效抵抗离群值的扰动。使用调和平均数代替几何平均数后，估计公式变为如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=\frac{\alpha_m m^2}{\sum{2^{-M}}}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\alpha_m=(m\int _0^\infty (log_2(\frac{2+u}{1+u}))^m du)^{-1}
\end{equation}&lt;/p&gt;
&lt;h3&gt;偏差分析&lt;/h3&gt;
&lt;p&gt;根据论文中的分析结论，与LLC一样HLLC是渐近无偏估计，且其渐近标准差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
SE_{hllc}(\hat{n}/n)=1.04/\sqrt{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此在存储空间相同的情况下，HLLC比LLC具有更高的精度。例如，对于分桶数$m$为$2^13$（8k字节）时，LLC的标准误差为1.4%，而HLLC为1.1%。&lt;/p&gt;
&lt;h3&gt;分段偏差修正&lt;/h3&gt;
&lt;p&gt;在HLLC的论文中，作者在实现建议部分还给出了在$n$相对于$m$较小或较大时的偏差修正方案。具体来说，设$E$为估计值：&lt;/p&gt;
&lt;p&gt;当$E≤{5 \over 2}m$时，使用LC进行估计。&lt;/p&gt;
&lt;p&gt;当$\frac{5}{2}m &amp;lt; E \leq \frac{1}{30}2^{32}$时，使用上面给出的HLLC公式进行估计。&lt;/p&gt;
&lt;p&gt;当$E&amp;gt;\frac{1}{30}2^{32}$时，估计公式则为$\hat{n}=-2^{32}log(1-E/2^{32})$。&lt;/p&gt;
&lt;p&gt;关于分段偏差修正效果分析也可以在原论文中找到。&lt;/p&gt;
&lt;h1&gt;写在后面&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;原博客中作者对这几种算法进行了实验比较,因为个人对实验不是很感兴趣,现只摘录作者的&lt;strong&gt;个人建议&lt;/strong&gt;(对实验结果有兴趣的同学请参考&lt;a href="http://blog.codinglabs.org/articles/cardinality-estimate-exper.html"&gt;五种常用基数估计算法效果实验及实践建议&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear Counting和LogLog Counting由于分别在基数较大和基数较小（阈值可解析分析，具体方法和公式请参考后文列出的相关论文）时存在严重的失效，因此不适合在实际中单独使用。一种例外是，如果对节省存储空间要求不强烈，不要求空间复杂度为常数（Linear Counting的空间复杂度为$O(n)$，其它算法均为$O(1)$），则在保证Bitmap全满概率很小的条件下，Linear Counting的效果要优于其它算法。&lt;/li&gt;
&lt;li&gt;总体来看，不论哪种算法，提高分桶数都可以降低偏差和方差，因此总体来看基数估计算法中分桶数的选择是最重要的一个权衡——在精度和存储空间间的权衡。&lt;/li&gt;
&lt;li&gt;实际中，Adaptive Counting或HyperLogLog Counting都是不错的选择，前者偏差较小，后者对离群点容忍性更好，方差较小。&lt;/li&gt;
&lt;li&gt;Google的HyperLogLog Counting++算法属于实验性改进，缺乏严格的数学分析基础，通用性存疑，不宜在实际中贸然使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后感谢CodingLabs撰写的精彩博文,这两周就写这3篇博文吧，两周后再见。尼玛,都2:16了,大家晚安，睡了。&lt;/p&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-play-with-cardinality-estimation-1"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-i.html"&gt;解读Cardinality Estimation算法（第一部分：基本概念）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-play-with-cardinality-estimation-2"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-ii.html"&gt;解读Cardinality Estimation算法（第二部分：Linear Counting）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-play-with-cardinality-estimation-3"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iii.html"&gt;解读Cardinality Estimation算法（第三部分：LogLog Counting）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-3-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-play-with-cardinality-estimation-4"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iv.html"&gt;解读Cardinality Estimation算法（第四部分：HyperLogLog Counting及Adaptive Counting）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-4-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="基数估计"></category><category term="Cardinality Estimation"></category><category term="Big Data"></category></entry><entry><title>机器学习外传之Deep Learning(I):Sparse Autoencoder</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-wai-chuan-zhi-deep-learningisparse-autoencoder.html" rel="alternate"></link><updated>2014-04-13T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-13:ji-qi-xue-xi-wai-chuan-zhi-deep-learningisparse-autoencoder.html</id><summary type="html">&lt;p&gt;其实本来没准备看Deep Learning的,之前这个高端大气上档次的内容一直都不再我的学习计划之内。基于如下几个原因吧,最后决定还是稍微看一下吧:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep Learning是Machine Learning:A Probabilistic Perspective的最后一章,反正终归是要看的。(之前觉得自己可能看不懂,但是好歹试试水吧);&lt;/li&gt;
&lt;li&gt;一个小伙伴毕业设计就在做Deep Learning;&lt;/li&gt;
&lt;li&gt;实验室老大好像对Deep Learning很感兴趣;&lt;/li&gt;
&lt;li&gt;好奇心害死人啊！！！！！！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;言归正传,作为Deep Learning系列的第一篇,我们首先还是说明一下两件事:Deep Learning是什么?Deep Learning是用来干什么的?&lt;/p&gt;
&lt;h1&gt;A Brief Introduction to Deep Learning&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;背景&lt;/h2&gt;
&lt;p&gt;机器学习（Machine Learning)是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。机器能否像人类一样能具有学习能力呢？1959年美国的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。这个程序向人们展示了机器学习的能力，提出了许多令人深思的社会问题与哲学问题（呵呵，人工智能正常的轨道没有很大的发展，这些什么哲学伦理啊倒发展的挺快。什么未来机器越来越像人，人越来越像机器啊。什么机器会反人类啊，ATM是开第一枪的啊等等。人类的思维无穷啊）。&lt;/p&gt;
&lt;p&gt;机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Problems to be solved" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/problems_to_be_solved_zps19ee52eb.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等。目前我们通过机器学习去解决这些问题的思路都是这样的（以视觉感知为例子）：&lt;/p&gt;
&lt;p&gt;&lt;img alt="ML Process" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/ml_process_zpsd8b105ec.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;从开始的通过传感器（例如CMOS）来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。最后一个部分，也就是机器学习的部分，绝大部分的工作是在这方面做的，也存在很多的paper和研究。&lt;/p&gt;
&lt;p&gt;而中间的三部分，概括起来就是特征表达。&lt;strong&gt;良好的特征表达，对最终算法的准确性起了非常关键的作用&lt;/strong&gt;，而且系统主要的计算和测试工作都耗在这一大部分。但，这块实际中一般都是人工完成的。靠人工提取特征。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Feature Representation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/feature_representation_zps744a0f4f.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;截止现在，也出现了不少NB的特征（好的特征应具有不变性（大小、尺度和旋转等）和可区分性）：例如Sift的出现，是局部图像特征描述子研究领域一项里程碑式的工作。由于SIFT对尺度、旋转以及一定视角和光照变化等图像变化都具有不变性，并且SIFT具有很强的可区分性，的确让很多问题的解决变为可能。但它也不是万能的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="SIFT" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sift_zpsb99f6fe8.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;然而，手工地选取特征是一件非常费力、启发式（需要专业知识）的方法，能不能选取好很大程度上靠经验和运气，而且它的调节需要大量的时间。既然手工选取特征不太好，那么能不能自动地学习一些特征呢？答案是能！Deep Learning就是用来干这个事情的，看它的一个别名&lt;code&gt;Unsupervised Feature Learning&lt;/code&gt;,就可以顾名思义了，Unsupervised的意思就是不要人参与特征的选取过程。&lt;/p&gt;
&lt;p&gt;那它是怎么学习的呢？怎么知道哪些特征好哪些不好呢？我们说机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为的学科。好，那我们人的视觉系统是怎么工作的呢？为什么在茫茫人海，芸芸众生，滚滚红尘中我们都可以找到另一个她（因为，你存在我深深的脑海里，我的梦里 我的心里 我的歌声里……）。人脑那么NB，我们能不能参考人脑，模拟人脑呢？（好像和人脑扯上点关系的特征啊，算法啊，都不错，但不知道是不是人为强加的，为了使自己的作品变得神圣和高雅。）&lt;/p&gt;
&lt;p&gt;近几十年以来，认知神经科学、生物学等等学科的发展，让我们对自己这个神秘的而又神奇的大脑不再那么的陌生。也给人工智能的发展推波助澜。&lt;/p&gt;
&lt;h2&gt;人脑视觉机理&lt;/h2&gt;
&lt;p&gt;1981 年的诺贝尔医学奖，颁发给了David Hubel（出生于加拿大的美国神经生物学家)和TorstenWiesel，以及Roger Sperry。前两位的主要贡献，是发现了视觉系统的信息处理过程是分级的：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Brain Activity" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/brain_activity_zps40cf5595.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;我们看看他们做了什么。1958 年，DavidHubel 和Torsten Wiesel在JohnHopkins University，研究瞳孔区域与大脑皮层神经元的对应关系。他们在猫的后脑头骨上，开了一个3毫米的小洞，向洞里插入电极，测量神经元的活跃程度。&lt;/p&gt;
&lt;p&gt;然后，他们在小猫的眼前，展现各种形状、各种亮度的物体。并且，在展现每一件物体时，还改变物体放置的位置和角度。他们期望通过这个办法，让小猫瞳孔感受不同类型、不同强弱的刺激。&lt;/p&gt;
&lt;p&gt;之所以做这个试验，目的是去证明一个猜测。位于后脑皮层的不同视觉神经元，与瞳孔所受刺激之间，存在某种对应关系。一旦瞳孔受到某一种刺激，后脑皮层的某一部分神经元就会活跃。经历了很多天反复的枯燥的试验，同时牺牲了若干只可怜的小猫，David Hubel 和Torsten Wiesel发现了一种被称为“方向选择性细胞（Orientation Selective Cell）”的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。&lt;/p&gt;
&lt;p&gt;这个发现激发了人们对于神经系统的进一步思考。神经-中枢-大脑的工作过程，或许是一个不断迭代、不断抽象的过程。&lt;/p&gt;
&lt;p&gt;这里的关键词有两个，一个是&lt;code&gt;抽象&lt;/code&gt;，一个是&lt;code&gt;迭代&lt;/code&gt;。从原始信号，做低级抽象，逐渐向高级抽象迭代。人类的逻辑思维，经常使用高度抽象的概念。&lt;/p&gt;
&lt;p&gt;例如，从原始信号摄入开始（瞳孔摄入像素Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Layers" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/layers_zps17c2e46a.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;这个生理学的发现，促成了计算机人工智能，在四十年后的突破性发展。&lt;/p&gt;
&lt;p&gt;总的来说，人的视觉系统的信息处理是分级的。从低级的V1区提取边缘特征，再到V2区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。也就是说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，就越利于分类。例如，单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。&lt;/p&gt;
&lt;p&gt;敏感的人注意到关键词了：&lt;strong&gt;分层&lt;/strong&gt;。而Deep learning的Deep是不是就表示存在多少层，也就是多深呢？没错。那Deep Learning是如何借鉴这个过程的呢？毕竟是归于计算机来处理，面对的一个问题就是怎么对这个过程建模？&lt;/p&gt;
&lt;p&gt;因为我们要学习的是特征的表达，那么关于特征，或者说关于这个层级特征，我们需要了解地更深入点。所以在说Deep Learning之前，我们有必要再啰嗦下特征。&lt;/p&gt;
&lt;h2&gt;特征&lt;/h2&gt;
&lt;p&gt;特征是机器学习系统的原材料，对最终模型的影响是毋庸置疑的。如果数据被很好的表达成了特征，通常线性模型就能达到满意的精度。那对于特征，我们需要考虑什么呢？&lt;/p&gt;
&lt;h3&gt;特征表示的粒度&lt;/h3&gt;
&lt;p&gt;学习算法在一个什么粒度上的特征表示，才有能发挥作用？就一个图片来说，像素级的特征根本没有价值。例如下面的摩托车，从像素级别，根本得不到任何信息，其无法进行摩托车和非摩托车的区分。而如果特征是一个具有结构性（或者说有含义）的时候，比如是否具有车把手（handle），是否具有车轮（wheel），就很容易把摩托车和非摩托车区分，学习算法才能发挥作用。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Feature Motor" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/feature_motor_zps687d7adf.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;初级(浅层)特征表示&lt;/h3&gt;
&lt;p&gt;既然像素级的特征表示方法没有作用，那怎样的表示才有用呢？&lt;/p&gt;
&lt;p&gt;1995 年前后，Bruno Olshausen和 David Field 两位学者任职Cornell University，他们试图同时用生理学和计算机的手段，双管齐下，研究视觉问题。&lt;/p&gt;
&lt;p&gt;他们收集了很多黑白风景照片，从这些照片中，提取出400个小碎片，每个照片碎片的尺寸均为16x16像素，不妨把这400个碎片标记为$S[i]$,$i= 0,.. 399$。接下来，再从这些黑白风景照片中，随机提取另一个碎片，尺寸也是 16x16 像素，不妨把这个碎片标记为$T$。&lt;/p&gt;
&lt;p&gt;他们提出的问题是，如何从这400个碎片中，选取一组碎片$S[k]$,通过叠加的办法，合成出一个新的碎片，而这个新的碎片，应当与随机选择的目标碎片$T$尽可能相似，同时$S[k]$的数量尽可能少。用数学的语言来描述，就是：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_k (a[k] * S[k]) \rightarrow T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$a[k]$是在叠加碎片$S[k]$时的权重系数。&lt;/p&gt;
&lt;p&gt;为解决这个问题，Bruno Olshausen和 David Field发明了一个算法：稀疏编码(&lt;strong&gt;Sparse Coding&lt;/strong&gt;)。&lt;/p&gt;
&lt;p&gt;稀疏编码是一个重复迭代的过程，每次迭代分两步:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择一组$S[k]$，然后调整$a[k]$，使得$\sum_k (a[k]*S[k])$最接近$T$;&lt;/li&gt;
&lt;li&gt;固定住$a[k]$,在400个碎片中,选择其它更合适的碎片$S\prime[k]$，替代原先的$S[k]$,使得$\sum_k (a[k]*S\prime[k])$最接近$T$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;经过几次迭代后，最佳的$S[k]$组合，被遴选出来了。令人惊奇的是，被选中的$S[k]$,基本上都是照片上不同物体的边缘线，这些线段形状相似，区别在于方向。&lt;/p&gt;
&lt;p&gt;Bruno Olshausen和 David Field的算法结果，与David Hubel 和Torsten Wiesel的生理发现，不谋而合！&lt;/p&gt;
&lt;p&gt;也就是说，复杂图形，往往由一些基本结构组成。比如下图：一个图可以通过用64种正交的edges（可以理解成正交的基本结构）来线性表示。比如样例$x$可以用1-64个edges中的三个按照0.8,0.3,0.5的权重调和而成。而其他基本edge没有贡献，因此均为0 。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sparse Coding" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sparse_coding_zps5aa4b3d3.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;另外，大牛们还发现，不仅图像存在这个规律，声音也存在。他们从未标注的声音中发现了20种基本的声音结构，其余的声音可以由这20种基本结构合成。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sound Sparse Structure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sound_sparse_coding_zpsaddf2fb2.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;结构性特征表示&lt;/h3&gt;
&lt;p&gt;小块的图形可以由基本edge构成，更结构化，更复杂的，具有概念性的图形如何表示呢？这就需要更高层次的特征表示，比如V2，V4。因此V1看像素级是像素级。V2看V1是像素级，这个是层次递进的，高层表达由底层表达的组合而成。专业点说就是基basis。V1提取出的basis是边缘，然后V2层是V1层这些basis的组合，这时候V2区得到的又是高一层的basis。即上一层的basis组合的结果，上上层又是上一层的组合basis(所以有大牛说Deep learning就是“搞基”，因为难听，所以美其名曰Deep learning或者Unsupervised Feature Learning）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="NN Structure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/nn_structure_zps5884685c.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;直观上说，就是找到make sense的小patch再将其进行combine，就得到了上一层的feature，递归地向上learning feature。&lt;/p&gt;
&lt;p&gt;在不同object上做training所得的edge basis是非常相似的，但object parts和models就会completely different了（那咱们分辨car或者face是不是容易多了):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Object Classification" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/object_classification_zps81877999.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;从文本来说，一个doc表示什么意思？我们描述一件事情，用什么来表示比较合适？用一个一个字嘛，我看不是，字就是像素级别了，起码应该是term，换句话说每个doc都由term构成，但这样表示概念的能力就够了嘛，可能也不够，需要再上一步，达到topic级，有了topic，再到doc就合理。但每个层次的数量差距很大，比如doc表示的概念-&amp;gt;topic（千-万量级)-&amp;gt;term（10万量级)-&amp;gt;word（百万量级）。&lt;/p&gt;
&lt;p&gt;一个人在看一个doc的时候，眼睛看到的是word，由这些word在大脑里自动切词形成term，在按照概念组织的方式，先验的学习，得到topic，然后再进行高层次的learning。&lt;/p&gt;
&lt;h3&gt;需要有多少个特征？&lt;/h3&gt;
&lt;p&gt;我们知道需要层次的特征构建，由浅入深，但每一层该有多少个特征呢？&lt;/p&gt;
&lt;p&gt;任何一种方法，特征越多，给出的参考信息就越多，准确性会得到提升。但特征多意味着计算复杂，探索的空间大，可以用来训练的数据在每个特征上就会稀疏，都会带来各种问题，并不一定特征越多越好。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Feature Number" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/feature_number_zps90f6b4a2.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;好了，到了这一步，终于可以聊到Deep Learning了。上面我们聊到为什么会有Deep Learning（让机器自动学习良好的特征，而免去人工选取过程。还有参考人的分层视觉处理系统），我们得到一个结论就是Deep Learning需要多层来获得更抽象的特征表达。那么多少层才合适呢？用什么架构来建模呢？怎么进行非监督训练呢？以下我们给出Deep Learning的基本思想。&lt;/p&gt;
&lt;h2&gt;Deep Learning的基本思想&lt;/h2&gt;
&lt;p&gt;假设我们有一个系统$S$，它有$n$层$(S_1,…S_n)$，它的输入是$I$，输出是$O$，形象地表示为:$I \rightarrow S_1 \rightarrow S_2 \rightarrow ….\rightarrow S_n \rightarrow O$，如果输出$O$等于输入$I$，即输入$I$经过这个系统变化之后没有任何的信息损失(呵呵，大牛说，这是不可能的。信息论中有个“信息逐层丢失”的说法（信息处理不等式):设处理$a$信息得到$b$，再对$b$处理得到$c$，那么可以证明：$a$和$c$的互信息不会超过$a$和$b$的互信息。这表明信息处理不会增加信息，大部分处理会丢失信息。当然了，如果丢掉的是没用的信息那多好啊），保持了不变，这意味着输入$I$经过每一层$S_i$都没有任何的信息损失，即在任何一层$S_i$，它都是原有信息（即输入$I$）的另外一种表示。现在回到我们的主题Deep Learning，我们需要自动地学习特征，假设我们有一堆输入$I$(如一堆图像或者文本),假设我们设计了一个系统$S$（有$n$层），我们通过调整系统中参数，使得它的输出仍然是输入$I$，那么我们就可以自动地获取得到输入$I$的一系列层次特征，即$S_1，…, S_n$。&lt;/p&gt;
&lt;p&gt;对于深度学习来说，其思想就是对堆叠多个层，也就是说这一层的输出作为下一层的输入。通过这种方式，就可以实现对输入信息进行分级表达了。&lt;/p&gt;
&lt;p&gt;另外，前面是假设输出严格地等于输入，这个限制太严格，我们可以略微地放松这个限制，例如我们只要使得输入与输出的差别尽可能地小即可，这个放松会导致另外一类不同的Deep Learning方法。上述就是Deep Learning的基本思想。&lt;/p&gt;
&lt;h1&gt;BP Neural Network&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;上述我们主要意在建立关于Deep Learning一点直观的印象,下面我们就开始介绍Deep Learning所涉及的模型了。首先第一个要介绍的就是Sparse AutoEncoder(稀疏自编码器)。为了更好的理解Sparse AutoEncoder,我们首先介绍一下Back Propagation Algorithm,而采用Back Propagation算法的一个典型的代表就是BP神经网络(&lt;code&gt;想了解神经网络基础知识的童鞋请自行Google之&lt;/code&gt;)。以下我们便主要介绍BP神经网络。&lt;/p&gt;
&lt;h2&gt;概述&lt;/h2&gt;
&lt;p&gt;BP(Back Propagation)神经网络是1986年由Rumelhart和McCelland为首的科研小组提出，参见他们发表在Nature上的论文&lt;a href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf"&gt;Learning representations by back-propagating errors&lt;/a&gt;.值得一提的是，该文的第三作者Geoffrey E. Hinton就是在深度学习邻域率先取得突破的神犇。&lt;/p&gt;
&lt;p&gt;BP神经网络是一种&lt;strong&gt;按误差逆向传播算法训练的多层前馈网络&lt;/strong&gt;，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的 输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断 调整网络的权值和阈值，使网络的误差平方和最小。&lt;/p&gt;
&lt;p&gt;其中,&lt;strong&gt;前馈&lt;/strong&gt;是指输入从输入层到隐含层到输出层前向传播,而误差则由输出层反向经隐含层传到输入层,而在误差反向传播的过程中动态更新神经网络连接权值,以使得网络的误差平方和最小。&lt;/p&gt;
&lt;h2&gt;BP网络模型&lt;/h2&gt;
&lt;p&gt;一个典型的BP神经网络模型如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="BP Neural Network" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bp_nn_zps5c17c098.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;BP神经网络与其他神经网络模型类似，不同的是，BP神经元的传输函数为非线性函数(而在感知机中为阶跃函数，在线性神经网络中为线性函数)，最常用的是log-sigmoid函数或tan-sigmoid函数。BP神经网络(BPNN)一般为多层神经网络，上图中所示的BP神经网络的隐层的传输函数即为非线性函数，隐层可以有多层，而输出层的传输函数为线性函数，当然也可以是非线性函数，只不过线性函数的输出结果取值范围较大，而非线性函数则限制在较小范围（如logsig函数输出 取值在(0,1)区间）。上图所示的神经网络的输入输出关系如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入层与隐层的关系:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{h} = \mathit{f_{1}} (\boldsymbol{W^{(1)}x}+\boldsymbol{b^{(1)}})
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$x$为$m$维特征向量(列向量)，$\boldsymbol{W^{(1)}}$为$n×m$维权值矩阵，$\boldsymbol{b^{(1)}}$为$n$维的偏置(bias)向量(列向量)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隐层与输出层的关系:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{y} = \mathit{f_{2}} (\boldsymbol{W^{(2)}h}+\boldsymbol{b^{(2)}})
\end{equation}&lt;/p&gt;
&lt;h2&gt;BP网络的学习方法&lt;/h2&gt;
&lt;p&gt;神经网络的关键之一是权值的确定，也即神经网络的学习，下面主要讨论一下BP神经网络的学习方法，它是一种监督学习的方法。&lt;/p&gt;
&lt;p&gt;假定我们有$q$个带label的样本(即输入)$p_1,p_2,…,p_q$，对应的label(即期望输出Target)为$T_1,T_2,…,T_q$，神经网络的实际输出为$a2_1,a2_2,…,a2_q$，隐层的输出为$a1[.]$.那么可以定义误差函数：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{E(W,B)} = \frac{1}{2}\sum_{k=1}^{n}(t_{k} - a2_{k})^{2}
\end{equation}&lt;/p&gt;
&lt;p&gt;BP算法的目标是使得实际输出approximate期望输出，即使得训练误差最小化。BP算法利用梯度下降(Gradient Descent)法来求权值的变化及误差的反向传播。对于上图中的BP神经网络，我们首先计算输出层的权值的变化量，从第$i$个输入到第$k$个输出的权值改变为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\Delta w2_{ki} &amp;amp;= - \eta \frac{\partial E}{\partial w2_{ki}} \\
&amp;amp;= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial w2_{ki}} \\
&amp;amp;= \eta (t_{k}-a2_{k})f_{2}’a1_{i} \\
&amp;amp;= \eta \delta_{ki}a1_{i}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\eta$为学习速率。同理可得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\Delta b2_{ki} &amp;amp;= - \eta \frac{\partial E}{\partial b2_{ki}} \\
&amp;amp;= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial b2_{ki}} \\
&amp;amp;= \eta (t_{k}-a2_{k})f_{2}’ \\
&amp;amp;= \eta \delta_{ki}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;而隐层的权值变化为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\Delta w1_{ij} &amp;amp;= - \eta \frac{\partial E}{\partial w1_{ij}} \\
&amp;amp;= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial a1_{i}} \frac{\partial a1_{i}}{\partial w1_{ij}} \\
&amp;amp;= \eta \sum_{k=1}^{n}(t_{k}-a2_{k})f_{2}’w2_{ki}f_{1}’p_{j} \\
&amp;amp;= \eta \delta_{ij}p_{j}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\delta_{ij} = e_{i}f_{1}’, e_{i} = \sum_{k=1}^{n}\delta_{ki}w2_{ki}$。同理可得，$\Delta b1_{i} = \eta \delta_{ij}$。&lt;/p&gt;
&lt;p&gt;这里我们注意到，输出层的误差为$e_j,j=1..n$，隐层的误差为$e_i,i=1..m$，其中$e_i$可以认为是$e_j$的加权组合，由于作用函数的 存在，$e_j$的等效作用为$\delta_{ji} = e_{j}f’()$。&lt;/p&gt;
&lt;h2&gt;BP网络的设计&lt;/h2&gt;
&lt;p&gt;在进行BP网络的设计是，一般应从网络的层数、每层中的神经元个数和激活函数、初始值以及学习速率等几个方面来进行考虑，下面是一些选取的原则。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网络的层数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;理论已经证明，具有偏差和至少一个S型隐层加上一个线性输出层的网络，能够逼近任何有理函数，增加层数可以进一步降低误差，提高精度，但同时也是网络 复杂化。另外不能用仅具有非线性激活函数的单层网络来解决问题，因为能用单层网络解决的问题，用自适应线性网络也一定能解决，而且自适应线性网络的 运算速度更快，而对于只能用非线性函数解决的问题，单层精度又不够高，也只有增加层数才能达到期望的结果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隐层神经元的个数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;网络训练精度的提高，可以通过采用一个隐含层，而增加其神经元个数的方法来获得，这在结构实现上要比增加网络层数简单得多。一般而言，我们用精度和 训练网络的时间来恒量一个神经网络设计的好坏：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;神经元数太少时，网络不能很好的学习，训练迭代的次数也比较多，训练精度也不高。&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;神经元数太多时，网络的功能越强大，精确度也更高，训练迭代的次数也大，可能会出现过拟合(over fitting)现象。
    由此，我们得到神经网络隐层神经元个数的选取原则是：在能够解决问题的前提下，再加上一两个神经元，以加快误差下降速度即可。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;初始权值的选取
一般初始权值是取值在$(−1,1)$之间的随机数。另外威得罗等人在分析了两层网络是如何对一个函数进行训练后，提出选择初始权值量级为$\sqrt[r]{s}$的策略， 其中$r$为输入个数，$s$为第一层神经元个数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学习速率
学习速率一般选取为0.01−0.8，大的学习速率可能导致系统的不稳定，但小的学习速率导致收敛太慢，需要较长的训练时间。对于较复杂的网络， 在误差曲面的不同位置可能需要不同的学习速率，为了减少寻找学习速率的训练次数及时间，比较合适的方法是采用变化的自适应学习速率，使网络在 不同的阶段设置不同大小的学习速率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;期望误差的选取
在设计网络的过程中，期望误差值也应当通过对比训练后确定一个合适的值，这个合适的值是相对于所需要的隐层节点数来确定的。一般情况下，可以同时对两个不同 的期望误差值的网络进行训练，最后通过综合因素来确定其中一个网络。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;BP网络的局限性&lt;/h2&gt;
&lt;p&gt;BP网络具有以下的几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;需要较长的训练时间：这主要是由于学习速率太小所造成的，可采用变化的或自适应的学习速率来加以改进。&lt;/li&gt;
&lt;li&gt;完全不能训练：这主要表现在网络的麻痹上，通常为了避免这种情况的产生，一是选取较小的初始权值，而是采用较小的学习速率。&lt;/li&gt;
&lt;li&gt;局部最小值：这里采用的梯度下降法可能收敛到局部最小值，采用多层网络或较多的神经元，有可能得到更好的结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;BP网络的改进&lt;/h2&gt;
&lt;p&gt;BP算法改进的主要目标是加快训练速度，避免陷入局部极小值等，常见的改进方法有带动量因子算法、自适应学习速率、变化的学习速率以及作用函数后缩法等。 动量因子法的基本思想是在反向传播的基础上，在每一个权值的变化上加上一项正比于前次权值变化的值，并根据反向传播法来产生新的权值变化。而自适应学习速率的方法则是针对一些特定的问题的。改变学习速率的方法的原则是，若连续几次迭代中，若目标函数对某个权倒数的符号相同，则这个权的学习速率增加，反之若符号相反则减小它的学习速率。而作用函数后缩法则是将作用函数进行平移，即加上一个常数。&lt;/p&gt;
&lt;h1&gt;Sparse AutoEncoder&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;有了以上基础知识,我们以下介绍一下Sparse AutoEncoder(稀疏自编码器)。&lt;/p&gt;
&lt;p&gt;目前为止，我们已经介绍了BP神经网络,而它采用的是有监督学习(使用带标签数据)。现在假设我们只有一个没有带类别标签的训练样本集合 ${x^{(1)}, x^{(2)}, x^{(3)}, \ldots}$，其中$x^{(i)} \in \Re^{n}$.自编码神经网络是一种无监督学习算法，它使用了反向传播算法，并让目标值等于输入值，比如 $y^{(i)} = x^{(i)}$ 。下图是一个自编码神经网络的示例。&lt;/p&gt;
&lt;p&gt;&lt;img alt="AutoEncoder NN" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/Autoencoder_zps061072cf.png" /&gt;&lt;/p&gt;
&lt;p&gt;自编码神经网络尝试学习一个$h_{W,b}(x) \approx x$的函数。换句话说，它尝试逼近一个恒等函数，从而使得输出$\hat{x}$接近于输入 $x$ 。恒等函数虽然看上去不太有学习的意义，但是当我们为自编码神经网络加入某些限制，比如限定隐藏神经元的数量，我们就可以从输入数据中发现一些有趣的结构。举例来说，假设某个自编码神经网络的输入$x$是一张$10\times10$图像（共100个像素）的像素灰度值，于是 $n=100$,其隐藏层$L_2$中有50个隐藏神经元。注意，输出也是100维的$y \in \Re^{100}$由于只有50个隐藏神经元，我们迫使自编码神经网络去学习输入数据的压缩表示，也就是说，它必须从50维的隐藏神经元激活度向量$a^{(2)} \in \Re^{50}$中重构出100维的像素灰度值输入$x$。如果网络的输入数据是完全随机的，比如每一个输入$x_i$都是一个跟其它特征完全无关的独立同分布高斯随机变量，那么这一压缩表示将会非常难学习。但是如果输入数据中隐含着一些特定的结构，比如某些输入特征是彼此相关的，那么这一算法就可以发现输入数据中的这些相关性。事实上，这一简单的自编码神经网络通常可以学习出一个跟主元分析（PCA）结果非常相似的输入数据的低维表示。&lt;/p&gt;
&lt;p&gt;我们刚才的论述是基于隐藏神经元数量较小的假设。但是即使隐藏神经元的数量较大（可能比输入像素的个数还要多），我们仍然通过给自编码神经网络施加一些其他的限制条件来发现输入数据中的结构。具体来说，如果我们给隐藏神经元加入稀疏性限制，那么自编码神经网络即使在隐藏神经元数量较多的情况下仍然可以发现输入数据中一些有趣的结构。&lt;/p&gt;
&lt;p&gt;稀疏性可以被简单地解释如下。如果当神经元的输出接近于1的时候我们认为它被激活，而输出接近于0的时候认为它被抑制，那么使得神经元大部分的时间都是被抑制的限制则被称作稀疏性限制。这里我们假设的神经元的激活函数是sigmoid函数。如果你使用tanh作为激活函数的话，当神经元输出为-1的时候，我们认为神经元是被抑制的。&lt;/p&gt;
&lt;p&gt;注意到$a^{(2)}_j$表示隐藏神经元$j$的激活度，但是这一表示方法中并未明确指出哪一个输入$x$带来了这一激活度。所以我们将使用 $a^{(2)}_j(x)$来表示在给定输入为$x$情况下，自编码神经网络隐藏神经元$j$的激活度。 进一步，让&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat\rho_j = \frac{1}{m} \sum_{i=1}^m \left[ a^{(2)}_j(x^{(i)}) \right]
\end{equation}&lt;/p&gt;
&lt;p&gt;表示隐藏神经元$j$的平均活跃度（在训练集上取平均）。我们可以近似的加入一条限制&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat\rho_j = \rho,
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\rho$是稀疏性参数，通常是一个接近于0的较小的值（比如$\rho=0.05$）。换句话说，我们想要让隐藏神经元$j$的平均活跃度接近0.05。为了满足这一条件，隐藏神经元的活跃度必须接近于0。&lt;/p&gt;
&lt;p&gt;为了实现这一限制，我们将会在我们的优化目标函数中加入一个额外的惩罚因子，而这一惩罚因子将惩罚那些$\hat\rho_j$和$\rho$有显著不同的情况从而使得隐藏神经元的平均活跃度保持在较小范围内。惩罚因子的具体形式有很多种合理的选择，我们将会选择以下这一种：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{j=1}^{s_2} \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j}.
\end{equation}&lt;/p&gt;
&lt;p&gt;这里，$s_2$是隐藏层中隐藏神经元的数量，而索引$j$依次代表隐藏层中的每一个神经元。如果你对相对熵(KL divergence）比较熟悉，这一惩罚因子实际上是基于它的。于是惩罚因子也可以被表示为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{j=1}^{s_2} {\rm KL}(\rho || \hat\rho_j),
\end{equation}&lt;/p&gt;
&lt;p&gt;其中 ${\rm KL}(\rho || \hat\rho_j)= \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j}$是一个以 $\rho$为均值和一个以$\hat\rho_j$为均值的两个伯努利随机变量之间的相对熵。相对熵是一种标准的用来测量两个分布之间差异的方法。&lt;/p&gt;
&lt;p&gt;这一惩罚因子有如下性质，当$\hat\rho_j = \rho$时,$\textstyle {\rm KL}(\rho || \hat\rho_j) = 0$，并且随着$\hat\rho_j$与$\rho$ 之间的差异增大而单调递增。举例来说，在下图中，我们设定$\rho = 0.2$并且画出了相对熵值${\rm KL}(\rho || \hat\rho_j)$随着 $\hat\rho_j$变化的变化。&lt;/p&gt;
&lt;p&gt;&lt;img alt="KL Penalty" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/KL_zpsb42db2ff.png" /&gt;&lt;/p&gt;
&lt;p&gt;我们可以看出，相对熵在$\hat\rho_j=\rho$时达到它的最小值0，而当$\hat\rho_j$靠近0或者1的时候，相对熵则变得非常大（其实是趋向于$\infty$）。所以，最小化这一惩罚因子具有使得$\hat\rho_j$靠近$\rho$的效果。现在，我们的总体代价函数可以表示为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
J_{\rm sparse}(W,b) = J(W,b) + \beta \sum_{j=1}^{s_2} {\rm KL}(\rho || \hat\rho_j),
\end{equation}
其中$J(W,b)$如之前所定义，而$\beta$控制稀疏性惩罚因子的权重。$\hat\rho_j$项则也（间接地）取决于$W,b$,因为它是隐藏神经元$j$的平均激活度，而隐藏层神经元的激活度取决于$W,b$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总而言之,通过控制隐含层神经元的数量或对它们施加稀疏性限制,我们就能得到关于原始数据的一种有效的特征表示&lt;/strong&gt;。具体而言,当我们采用我们上面提到的2D图像作为输入时,不同的隐藏单元学会了在图像的不同位置和方向进行边缘检测。显而易见，这些特征对物体识别等计算机视觉任务是十分有用的。若将其用于其他输入域（如音频），该算法也可学到对这些输入域有用的表示或特征。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8775360"&gt;Deep Learning（深度学习）学习笔记整理系列之（一)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8775488"&gt;Deep Learning（深度学习）学习笔记整理系列之（二)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8775518"&gt;Deep Learning（深度学习）学习笔记整理系列之（三)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ibillxia.github.io/blog/2013/03/30/back-propagation-neural-networks/"&gt;反向传播(BP)神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/wentingtu/archive/2012/06/05/2536425.html"&gt;BP神经网络模型与学习算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.codeproject.com/Articles/13582/Back-propagation-Neural-Net"&gt;BP神经网络C++实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://deeplearning.stanford.edu/wiki/index.php/%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7"&gt;自稀疏算法与稀疏性&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%AF%E8%A7%86%E5%8C%96%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C"&gt;可视化自编码器训练结果&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Deep Learning"></category><category term="Machine Learning"></category><category term="Back Propagation"></category><category term="BP Neural Network"></category><category term="Sparse Autoencoder"></category></entry><entry><title>机器学习系列(IV):聚类大观园</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-ivju-lei-da-guan-yuan.html" rel="alternate"></link><updated>2014-04-11T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-11:ji-qi-xue-xi-xi-lie-ivju-lei-da-guan-yuan.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Clustering&lt;/em&gt;和&lt;em&gt;Classification&lt;/em&gt;无疑是机器学习领域两个重量级的TASK,而且这两个概念作为初学者是比较容易混淆的。以下简要说明一下这两个概念之间的区别。Classification和Clustering都是要把一堆Objects分到不同的Group,但是两者还是有很明显的差异的。具体而言,Classification属于Supervised Learning,即训练样本必须是已标注样本，而聚类是Unsupervised Learning,我们要从未标注样本中进行学习，然后把Objects分到不同的Group中去。&lt;/p&gt;
&lt;p&gt;言归正传,Clustering,一言以蔽之,其核心思想就是"物以类聚,人以群分"。但是其核心前提在于&lt;strong&gt;物以何聚,人以何分&lt;/strong&gt;,即使用什么用来度量Objects之间的差异性。一种比较自然的想法就是使用Objects属性之间的差异用以度量Objects之间的差异性。&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta(x_i,x_{x\prime}） = \sum_{j=1}^D \Delta_j(x_{ij},x_{x\prime j})
\end{equation}&lt;/p&gt;
&lt;p&gt;常见的用来Capture属性之间差异性的函数则有如下几种:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高斯距离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大家最熟悉的无疑就是高斯距离了,当然，其只能应用于实数值。&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta_j (x_{ij},x_{x\prime j}) = (x_{ij}-x{i\prime j})^2
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;街区距离($l_1$距离)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;高斯距离由于采用二次形式,因此较大的差异容易被放大化,因此在很多场景下Gaussian距离对于Outliers很敏感。为了解决这个问题,人们引入了街区距离(city block distance),直观上理解就是在一个街区中我们从一点到另外一点要经过的街区数。定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta_j (x_{ij},x_{x\prime j}) = |x_{ij}-x_{i\prime j}|
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;海明距离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于类别类型的变量,如&lt;em&gt;{red,green,blue}&lt;/em&gt;，我们通常采用的办法则是当特征不同时,赋值为1,否则赋值为0;将所有的类别型变量相加有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta(x_i,x_{i\prime}) = \sum_{j=1}^D 1_{x_{ij} \neq x_{x\prime j}}
\end{equation}&lt;/p&gt;
&lt;p&gt;有了衡量Objects之间差异性的尺度,我们就能采用合适的算法将Objects分到不同的Group当中去。在正式介绍相关算法之间,我们还是简要说明一下如何衡量不同的聚类方法的好坏。&lt;/p&gt;
&lt;h1&gt;聚类方法评价&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;总体而言,聚类算法的核心目的即在于将相似的Objects分到同一类中去,并保证不同类之间的Objects之间的差异性。目前针对聚类算法广泛使用的评价指标有如下三种:&lt;/p&gt;
&lt;h2&gt;Purity&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Cluster Objects" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cluster_objects_zps871f5eb1.png" /&gt;&lt;/p&gt;
&lt;p&gt;如上图所示,每一个圆圈代表一个Cluster,每个圆圈内的物体均已被标注为&lt;em&gt;{A,B,C}&lt;/em&gt;中的一种。令$N_{ij}$表示Cluster i中label为j的物体的数目,$N_i$表示Cluster i中物体的总数。另一个cluster的纯度(purity)被定义为$p_i \triangleq max_j p_{ij}$,因此整个聚类结果的纯度为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
purity \triangleq \sum_{i} \frac{N_i}{N}p_i
\end{equation}&lt;/p&gt;
&lt;p&gt;根据上述定义,上图中聚类结果的纯度为:&lt;/p&gt;
&lt;p&gt;${6 \over 17}{5 \over 6}+{6 \over 17}{4 \over 6}+{5 \over 17}{3 \over 5} = 0.71$ &lt;/p&gt;
&lt;p&gt;不难看出,聚类的纯度越高，则表明该聚类算法越好。&lt;/p&gt;
&lt;h2&gt;Rand index&lt;/h2&gt;
&lt;p&gt;令$U={u_1,...,u_R}$和$V={v_1,...,v_C}$分别表示对于$N$个数据点的两种不同的划分方式。例如,$U$可能是我们估计的聚类结果，而$V$则是根据物体的Label得到的参考聚类结果。现我们定义如下四个值:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TP(True Positives):同时属于$U$和$V$中同一集合的物体对的数目,即不管对于$U$还是$V$而言,这些物体对均被分配到同一集合中;&lt;/li&gt;
&lt;li&gt;TN(True Negatives):对于$U$和$V$而言,被分配到不同集合的物体对的数目,即在参考聚类中,它们被分到不同的集合,在我们估计的聚类结果中,它们也被划分到不同的集合。&lt;/li&gt;
&lt;li&gt;FP(False Positives):在$V$中被分配到不同集合，而在$U$中被分配到相同集合的物体对的数目。&lt;/li&gt;
&lt;li&gt;FN(False Negatives):在$V$中被分配到相同集合，而在$U$中被分配到不同集合的物体对的数目。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而Rand index被定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
R \triangleq \frac{TP+TN}{TP+FP+FN+TN}
\end{equation}&lt;/p&gt;
&lt;p&gt;即我们估计的结果中被分配到正确的类中的物体对所占的比例。显然,$0 \leq R \leq 1$.&lt;/p&gt;
&lt;p&gt;还是举上图中的例子为例,说明其计算过程:&lt;/p&gt;
&lt;p&gt;上图中三个Cluster中点的个数分别是6,6,5.因此其中“positives”的数目为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
TP+FP = C_6^2+C_6^2+C_5^2 = 40
\end{equation}&lt;/p&gt;
&lt;p&gt;其中True Positives的数目为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
TP = C_5^2+C_4^2+C_2^2+C_3^2 = 20
\end{equation}&lt;/p&gt;
&lt;p&gt;同理我们可以得到$FN=24,TN=72$.因此我们有$R = \frac{20+72}{40+24+72} = 0.68$.&lt;/p&gt;
&lt;h2&gt;Mutual Information&lt;/h2&gt;
&lt;p&gt;另外一种衡量聚类质量的方法是计算$U$和$V$之间的互信息。关于互信息的内容请参考&lt;a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html"&gt;机器学习系列(II):Generative models for discrete data&lt;/a&gt;,此处不再赘述。&lt;/p&gt;
&lt;h1&gt;基本聚类算法&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;本部分会着重介绍三种聚类算法,包括K-means,Spectral Clustering以及Hierarchical Clustering,且容我一一道来。&lt;/p&gt;
&lt;h2&gt;K-Means &amp;amp; K-Medoids&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Cluster Points" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cluster_01_zps3c7151b4.png" /&gt;&lt;/p&gt;
&lt;p&gt;如上图所示,对于二维空间上的若干个点,我们要将它们分成若干类。我们直观上来看,上图中数据点大致可以分为3类,如果我们将每一类用不同的颜色标注，则可得到下图:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cluster Color" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cluster_color_zps7de900bf.png" /&gt;&lt;/p&gt;
&lt;p&gt;那么计算机要如何来完成这个任务呢？当然，计算机还没有高级到能够“通过形状大致看出来”，不过，对于这样的$N$维欧氏空间中的点进行聚类，有一个非常简单的经典算法，也就是本节我们要介绍的K-Means。在介绍K-Means的具体步骤之前，让我们先来看看它对于需要进行聚类的数据的一个基本假设吧：对于每一个cluster，我们可以选出一个中心点(center)，使得该cluster中的所有的点到该中心点的距离小于到其他cluster的中心的距离。虽然实际情况中得到的数据并不能保证总是满足这样的约束，但这通常已经是我们所能达到的最好的结果，而那些误差通常是固有存在的或者问题本身的不可分性造成的。例如下图所示的两个高斯分布，从两个分布中随机地抽取一些数据点出来，混杂到一起，现在要让你将这些混杂在一起的数据点按照它们被生成的那个分布分开来：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gaussian" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gaussian_zpsb3e5d8e5.png" /&gt;&lt;/p&gt;
&lt;p&gt;由于这两个分布本身有很大一部分重叠在一起了，例如，对于数据点2.5来说，它由两个分布产生的概率都是相等的，你所做的只能是一个猜测；稍微好一点的情况是2，通常我们会将它归类为左边的那个分布，因为概率大一些，然而此时它由右边的分布生成的概率仍然是比较大的，我们仍然有不小的几率会猜错。而整个阴影部分是我们所能达到的最小的猜错的概率，这来自于问题本身的不可分性，无法避免。因此，我们将K-Means所依赖的这个假设看作是合理的。&lt;/p&gt;
&lt;p&gt;基于这样一个假设,我们再来导出K-Means所要优化的目标函数:设我们一共有$N$个数据点需要分为$K$个cluster,K-Means要做的就是最小化:&lt;/p&gt;
&lt;p&gt;\begin{equation}
J = \sum_{n=1}^N\sum_{k=1}^K r_{nk} \|x_n-\mu_k\|^2
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$r_{nk}$在数据点$n$被归类到cluster k的时候为1,否则为0。直接寻找$r_{nk}$和$\mu_k$来最小化$J$并不容易，不过我们可以采取迭代的办法：先固定$\mu_k$,选择最优的$r_{nk}$，很容易看出，只要将数据点归类到离他最近的那个中心就能保证J最小。下一步则固定$r_{nk}$，再求最优的$\mu_k$。将$J$对$\mu_k$ 求导并令导数等于零，很容易得到$J$最小的时候$\mu_k$应该满足：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mu_k=\frac{\sum_n r_{nk}x_n}{\sum_n r_{nk}}
\end{equation}
亦即$\mu_k$的值应当是所有cluster k中的数据点的平均值。由于每一次迭代都是取到$J$的最小值，因此$J$只会不断地减小（或者不变），而不会增加，这保证了K-Means最终会到达一个极小值。虽然K-Means并不能保证总是能得到全局最优解，但是对于这样的问题，像K-Means这种复杂度的算法，这样的结果已经是很不错的了。&lt;/p&gt;
&lt;p&gt;下面我们来总结一下K-Means算法的具体步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选定$K$个中心$\mu_k$的初值。这个过程通常是针对具体的问题有一些启发式的选取方法，或者大多数情况下采用随机选取的办法。因为前面说过K-Means并不能保证全局最优，而是否能收敛到全局最优解其实和初值的选取有很大的关系，所以有时候我们会多次选取初值跑K-Means,并取其中最好的一次结果。&lt;/li&gt;
&lt;li&gt;将每个数据点归类到离它最近的那个中心点所代表的cluster中。&lt;/li&gt;
&lt;li&gt;用公式$\mu_k = \frac{1}{N_k}\sum_{j\in\text{cluster}_k}x_j$计算出每个cluster的新的中心点。&lt;/li&gt;
&lt;li&gt;重复第二步，一直到迭代了最大的步数或者前后的$J$的值相差小于一个阈值为止。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:&lt;strong&gt;K-Means&lt;/strong&gt;并不能保证全局最优,而且该算法得到的结果的好坏直接依赖于初始值的选取,当初始值选取不当时,最后得到的结果可能并不好,所以一般采用的方法是多次随机选取初始值,并选择结果最好的一次就行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;K-Means方法有一个很明显的局限就是它的距离衡量标准是高斯函数,所以只适用于特征是实数值的情形,而不能适用于当数据包含类型数据的情形。因此人们引入了K-Medoids算法。&lt;/p&gt;
&lt;p&gt;K-Medoids算法在K-Means算法的基础上做出了如下两个改变:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将原来的目标函数$J$中的欧氏距离改为一个任意的dissimilarity measure函数$\mathcal{V}$：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\tilde{J} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\mathcal{V}(x_n,\mu_k)
\end{equation}&lt;/p&gt;
&lt;p&gt;最常见的方式是构造一个dissimilarity matrix $\mathbf{D}$来代表$\mathcal{V}$，其中的元素$\mathbf{D}_{ij}$表示Object $i$和Object $j$之间的差异程度。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中心点的选取不再取同一Cluster数据的均值,&lt;strong&gt;而是从在已有的数据点里面选取的&lt;/strong&gt;,具体而言,选一个到该Cluster中其他点距离之和最小的点。这使得K-Medoids算法不容易受到那些由于误差之类的原因产生的Outlier的影响，更加robust一些。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Spectral Clustering&lt;/h2&gt;
&lt;p&gt;Spectral Clustering(谱聚类)是一种基于图论的聚类方法，它能够识别任意形状的样本空间且收敛于全局最优解，其基本思想是利用样本数据的相似矩阵进行特征分解后得到的特征向量进行聚类，可见，它与样本feature无关而只与样本个数有关。&lt;/p&gt;
&lt;h3&gt;图的划分&lt;/h3&gt;
&lt;p&gt;图划分的目的是将有权无向图划分为两个或以上子图，使得子图规模差不多而割边权重之和最小。图的划分可以看做是有约束的最优化问题，它的目的是看怎么把每个点划分到某个子图中，比较不幸的是当你选择各种目标函数后发现该优化问题往往是NP-hard的。&lt;/p&gt;
&lt;p&gt;怎么解决这个问题呢？松弛方法往往是一种利器(比如SVM中的松弛变量),对于图的划分可以认为能够将某个点的一部分划分在子图1中，另一部分划分在子图2中,而不是非此即彼,使用松弛方法的目的是将组合优化问题转化为数值优化问题，从而可以在多项式时间内解决之，最后在还原划分时可以通过阈值来还原，或者使用类似K-Means这样的方法，之后会有相关说明。&lt;/p&gt;
&lt;h3&gt;相关定义&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;用$G=(V,E)$表示无向图，其中$V$和$E$分别为其顶点集和边集；&lt;/li&gt;
&lt;li&gt;说某条边属于某个子图是指该边的两个顶点都包含在子图中；&lt;/li&gt;
&lt;li&gt;假设边$e$的两个不同端点为$i$和$j$，则该边的权重用$\omega_{i,j}$表示，对于无向无环图有$\omega_{i,j} = \omega_{j,i}$且$\omega_{i,i}=0$,为方便以下的“图”都指无向无环图；&lt;/li&gt;
&lt;li&gt;对于图的某种划分方案$Cut$的定义为：所有两端点不在同一子图中的边的权重之和，它可以被看成该划分方案的&lt;strong&gt;损失函数&lt;/strong&gt;,希望这种损失越小越好,本文以二分无向图为例，假设原无向图$G$被划分为$G_1$和$G_2$，那么有:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}
\end{equation}&lt;/p&gt;
&lt;h3&gt;Laplacian矩阵&lt;/h3&gt;
&lt;p&gt;假设无向图$G$被划分为$G_1$和$G_2$两个子图，该图的顶点数为:n=|V|，用$q$表示维指示向量，表明该划分方案，每个分量定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
c_1 &amp;amp; i \in G_1 \\
c_2 &amp;amp; i \in G_2 
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;于是有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \sum_{i \in G_1,j \in G_2} \omega_{i,j} = \frac{\sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i-q_j)^2}{2(c_1-c_2)^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;又因为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i-q_j)^2 &amp;amp;= \sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i^2-2q_iq_j+q_j^2) \\
&amp;amp;= \sum_{i=1}^n \sum_{j=1}^n -2\omega_{i,j}q_iq_j + \sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i^2+q_j^2) \\
&amp;amp;= \sum_{i=1}^n \sum_{j=1}^n -2\omega_{i,j}q_iq_j + \sum_{i=1}^n 2q_i^2(\sum_{j=1}^n \omega_{i,j}) \\
&amp;amp;=2q^T(D-W)q
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$D$为对角矩阵，对角线元素为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
D_{i,i} = \sum_{j=1}^n \omega_{i,j}
\end{equation}&lt;/p&gt;
&lt;p&gt;$W$为权重矩阵：$W_{i,j} = \omega_{i,j}$且$W_{i,i}=0$。&lt;/p&gt;
&lt;p&gt;重新定义一个对称矩阵$L$，它便是Laplacian矩阵：&lt;/p&gt;
&lt;p&gt;\begin{equation}
L=D-W
\end{equation}&lt;/p&gt;
&lt;p&gt;矩阵元素为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
L_{i,j} = 
\left \lbrace
\begin{array}{cc}
\sum_{j=1}^n \omega_{i,j} &amp;amp; i = j \\
-\omega_{i,j} &amp;amp; i \neq j
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;进一步观察：&lt;/p&gt;
&lt;p&gt;\begin{equation}
q^TLq = {1 \over 2} \sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i-q_j)^2
\end{equation}&lt;/p&gt;
&lt;p&gt;如果所有权重值都为非负，那么就有，这说明Laplacian矩阵是半正定矩阵；而当无向图为连通图时有特征值0且对应特征向量为$[1,1,1...1]^T$，这反映了，如果将无向图划分成两个子图，一个为其本身，另一个为空时，为0(当然，这种划分是没有意义的)。&lt;/p&gt;
&lt;p&gt;其实上面推导的目的在于得到下面的关系：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \frac{q^TLq}{(c_1-c_2)^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;这个等式的核心价值在于：将最小化划分的问题转变为最小化二次函数；从另一个角度看，实际上是把原来求离散值松弛为求连续实数值。&lt;/p&gt;
&lt;p&gt;观察下图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Graph" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/graph_zps39bd50fe.png" /&gt;&lt;/p&gt;
&lt;p&gt;根据上图我们可以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
W = 
\left[
\begin{array}{cc}
0.0  &amp;amp; 0.8 &amp;amp; 0.6 &amp;amp; 0.0 &amp;amp; 0.1 &amp;amp; 0.0 \\
0.8  &amp;amp; 0.0 &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.6  &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.2 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.2 &amp;amp; 0.0 &amp;amp; 0.8 &amp;amp; 0.7 \\
0.1  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.8 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.7 &amp;amp; 0.8 &amp;amp; 0.0
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
D = 
\left[
\begin{array}{cc}
1.5  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 1.6 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 1.6 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.7 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.7 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.5
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;Laplacian矩阵为：
\begin{equation}
L = 
\left[
\begin{array}{cc}
1.5  &amp;amp; -0.8 &amp;amp; -0.6 &amp;amp; 0.0 &amp;amp; -0.1 &amp;amp; 0.0 \\
-0.8  &amp;amp; 1.6 &amp;amp; -0.8 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
-0.6  &amp;amp; -0.8 &amp;amp; 1.6 &amp;amp; -0.2 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; -0.2 &amp;amp; 1.7 &amp;amp; -0.8 &amp;amp; -0.7 \\
-0.1  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; -0.8 &amp;amp; 1.7 &amp;amp; -0.8 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; -0.7 &amp;amp; -0.8 &amp;amp; 1.5
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;Laplacian矩阵是一种有效表示图的方式，任何一个Laplacian矩阵都对应一个权重非负地无向有权图,而满足以下条件的就是Laplacian矩阵:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为对称半正定矩阵，保证所有特征值都大于等于0;&lt;/li&gt;
&lt;li&gt;矩阵有唯一的0特征值，其对应的特征向量为$[1,1,1...1]^T$，它反映了图的一种划分方式：一个子图包含原图所有端点，另一个子图为空。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;划分方法&lt;/h3&gt;
&lt;h4&gt;Minimum Cut方法&lt;/h4&gt;
&lt;p&gt;考虑最简单情况，另$C_1=1=-C_2$,无向图划分指示向量定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
1 &amp;amp; i \in G_1 \\
-1 &amp;amp; i \in G_2 
\end{array}
\right.
\end{equation}                          &lt;/p&gt;
&lt;p&gt;要优化的目标为$Cut(G_1,G_2)$，由之前的推导可以将该问题松弛为以下问题：&lt;/p&gt;
&lt;p&gt;\begin{equation}
min \ q^TLq \\
s.t. [1,1,...1]^Tq=0 \\
q^Tq = n
\end{equation}&lt;/p&gt;
&lt;p&gt;从这个问题的形式可以联想到&lt;strong&gt;Rayleigh quotient&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\begin{equation}
R(L,q) = \frac{q^TLq}{q^Tq}
\end{equation}&lt;/p&gt;
&lt;p&gt;原问题的最优解就是该Rayleigh quotient的最优解，而由Rayleigh quotient的性质可知：它的最小值，第二小值，......，最大值分别对应矩阵$L$的最小特征值，第二小特征值，......，最大特征值，且极值$q$在相应的特征向量处取得，即需要求解下特征系统的特征值和特征向量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Lq = \lambda q
\end{equation}&lt;/p&gt;
&lt;p&gt;这里需要注意约束条件$[1,1,...1]^Tq=0$，显然的最小特征值为0,此时对应特征向量为[1,1,...1]^T,不满足这个约束条件(剔除了无意义划分)，于是最优解应该在第二小特征值对应的特征向量处取得。&lt;/p&gt;
&lt;p&gt;当然，求得特征向量后还要考虑如何恢复划分，比如可以这样：特征向量分量值为正所对应的端点划分为$G_1$，反之划分为$G_2$；也可以这样：将特征向量分量值从小到大排列，以中位数为界划分$G_1$和$G_2$；还可以用K-Means算法聚类。&lt;/p&gt;
&lt;h4&gt;Ratio Cut方法&lt;/h4&gt;
&lt;p&gt;实际当中，划分图时除了要考虑最小化$Cut$外还要考虑划分的平衡性，为缓解出现类似一个子图包含非常多端点而另一个只包含很少端点的情况，出现了Ratio Cut，它衡量子图大小的标准是子图包含的端点个数。&lt;/p&gt;
&lt;p&gt;定义$n_1$为子图1包含端点数，$n_2$为子图2包含端点数，$n_2=n-n_1$，则优化目标函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
obj = Cut(G_1,G_2)(\frac{1}{n_1}+\frac{1}{n_2})
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}
\end{equation}&lt;/p&gt;
&lt;p&gt;做一个简单变换：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
obj &amp;amp;= Cut(G_1,G_2)(\frac{1}{n_1}+\frac{1}{n_2}) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}(\frac{1}{n_1}+\frac{1}{n_2}) \\
&amp;amp;= \sum_{i \in G_1,j \in G_2} \omega_{i,j}(\frac{n_1+n_2}{n_1n_2})=\sum_{i \in G_1,j \in G_2} \omega_{i,j}(\frac{(n_1+n_2)^2}{n_1n_2n}) \\
&amp;amp;= \sum_{i \in G_1,j \in G_2} \omega_{i,j}(\sqrt{\frac{n_1}{n_2n}}+\sqrt{\frac{n_2}{n_1n}})^2 \\
&amp;amp;= \sum_{i \in G_1,j \in G_2} \omega_{i,j}(q_i-q_j)^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
\sqrt{\frac{n_1}{n_2n}} &amp;amp; i \in G_1 \\
\sqrt{\frac{n_2}{n_1n}} &amp;amp; i \in G_2
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;看吧，这形式多给力，原问题就松弛为下面这个问题了：&lt;/p&gt;
&lt;p&gt;\begin{equation}
min \ q^TLq \\
s.t. q^Te=0 \\
q^Tq = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;依然用&lt;strong&gt;Rayleigh quotient&lt;/strong&gt;求解其第二小特征值及其特征向量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:关于原问题为什么能松弛为上述形式,没看懂,望高人指点(Normalized Cut这部分也没完全懂)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Normalized Cut方法&lt;/h4&gt;
&lt;p&gt;与Ratio Cut类似，不同之处是，它衡量子图大小的标准是:子图各个端点的Degree之和。
定义:$d_1$为子图1Degree之和:&lt;/p&gt;
&lt;p&gt;\begin{equation}
d_1 = \sum_{i \in G_1} d_i
\end{equation}&lt;/p&gt;
&lt;p&gt;$d_2$为子图2Degree之和:&lt;/p&gt;
&lt;p&gt;\begin{equation}
d_2 = \sum_{i \in G_2} d_i
\end{equation}&lt;/p&gt;
&lt;p&gt;则优化目标函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
obj = Cut(G_1,G_2)(\frac{1}{d_1}+\frac{1}{d_2}) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}(q_i-q_j)^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
\sqrt{\frac{d_1}{d_2d}} &amp;amp; i \in G_1 \\
\sqrt{\frac{d_2}{d_1d}} &amp;amp; i \in G_2
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;原问题就松弛为下面这个问题了：&lt;/p&gt;
&lt;p&gt;\begin{equation}
min \ q^TLq \\
s.t. q^De=0 \\
q^Dq = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;用泛化的Rayleigh quotient表示为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
R(L,q) = \frac{q^TLq}{q^TDq}
\end{equation}&lt;/p&gt;
&lt;p&gt;那问题就变成求解下特征系统的特征值和特征向量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
Lq &amp;amp;= \lambda Dq \\
&amp;amp;\Leftrightarrow Lq = \lambda D^{1 \over 2}D^{1 \over 2}q \\
&amp;amp;\Leftrightarrow D^{-\frac{1}{2}}LD^{-\frac{1}{2}}D^{1 \over 2}q = \lambda D^{1 \over 2}q \\
&amp;amp;\Leftrightarrow L\prime q\prime = \lambda q\prime
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$L\prime = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$,$q\prime=D^{1 \over 2}q$
                                                                                                显然，上式中最上面式子和最下面式子有相同的特征值，但是对应特征值的特征向量关系为:$q\prime=D^{1 \over 2}q$，因此我们可以先求最下面式子的特征值及其特征向量，然后为每个特征向量乘以$D^{-\frac{1}{2}}$就得到最上面式子的特征向量。&lt;/p&gt;
&lt;p&gt;哦,对了,矩阵$L\prime = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$叫做Normalized Laplacian，因为它对角线元素值都为1。&lt;/p&gt;
&lt;h3&gt;Spectral Clustering&lt;/h3&gt;
&lt;p&gt;上边说了这么多，其实就是想将图的划分应用于聚类，而且这种聚类只需要样本的相似矩阵即可，把每个样本看成图中的一个顶点，样本之间的相似度看成由这两点组成的边的权重值，那么相似矩阵就是一幅有权无向图。对照图的划分方法，有下列两类Spectral Clustering算法，他们的区别在于Laplacian矩阵是否是规范化的:&lt;/p&gt;
&lt;p&gt;&lt;img alt="SC_Classify" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sc_classify_zpse5f47df7.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unnormalized Spectral Clustering算法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;算法输入：样本相似矩阵$S$和要聚类的类别数$K$。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据矩阵$S$建立权重矩阵$W$、三角矩阵$D$；&lt;/li&gt;
&lt;li&gt;建立Laplacian矩阵$L$；&lt;/li&gt;
&lt;li&gt;求矩阵$L$的前$K$小个特征值及其对应的特征向量，注意最小的那个特征值一定是0且对应的特征向量为[1,1,...1]^T；&lt;/li&gt;
&lt;li&gt;以这K组特征向量组成新的矩阵，其行数为样本数，列数为$K$，这里就是做了降维操作，从$N$维降到$K$维，(实际上除去那个全为1的向量维度降为了$K-1$)；&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用K-Means算法进行聚类，得到$K$个Cluster。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Normalized Spectral Clustering算法&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;算法输入：样本相似矩阵$S$和要聚类的类别数$K$。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据矩阵$S$建立权重矩阵$W$、三角矩阵$D$；&lt;/li&gt;
&lt;li&gt;建立Laplacian矩阵$L$以及$L\prime = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$；&lt;/li&gt;
&lt;li&gt;求矩阵$L\prime$的前$K$小个特征值及其对应的特征向量，注意最小的那个特征值一定是0且对应的特征向量为$[1,1...1]^T$；&lt;/li&gt;
&lt;li&gt;利用$q\prime=D^{1 \over 2}q$求得矩阵$L$前$K$小个特征向量；&lt;/li&gt;
&lt;li&gt;以这$K$组特征向量组成新的矩阵，其行数为样本数$N$，列数为$K$；&lt;/li&gt;
&lt;li&gt;使用K-Means算法进行聚类，得到$K$个Cluster。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;源码实现&lt;/h3&gt;
&lt;p&gt;以下我们给出Unnormalized Spectral Clustering的Python源码实现:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;

&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;matplotlib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pyplot&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;eig&lt;/span&gt;
&lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cluster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vq&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;kmeans2&lt;/span&gt;
&lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sparse&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;eigen&lt;/span&gt;
&lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;spatial&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kdtree&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KDTree&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;get_noise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpoints&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="n"&gt;gaussian&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpoints&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpoints&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column_stack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;get_circle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpoints&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="n"&gt;polar&lt;/span&gt; &lt;span class="n"&gt;coordinates&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;get&lt;/span&gt; &lt;span class="n"&gt;uniformly&lt;/span&gt; &lt;span class="n"&gt;distributed&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;
    &lt;span class="n"&gt;step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;numpoints&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column_stack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;radial_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;inner&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conj&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;inner&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;circle_samples&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Generate noisy circle points&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;circles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;radius&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;circles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;get_circle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;radius&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;get_noise&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;circles&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;mutual_knn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;radial_kernel&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;knn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;kt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KDTree&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;cannot&lt;/span&gt; &lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="n"&gt;euclidean&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt; &lt;span class="n"&gt;directly&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;neighbour&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;neighbour&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setdefault&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[]).&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;neighbour&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;neighbour&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;knn&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;get_distance_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nearest_neighbours&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;neighbour&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nearest_neighbours&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;neighbour&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;rename_clusters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;so&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="n"&gt;cluster&lt;/span&gt; &lt;span class="n"&gt;has&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;seen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;newidx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;seen&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;seen&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;
        &lt;span class="n"&gt;newidx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seen&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;newidx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;cluster_points&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;sparse&lt;/span&gt; &lt;span class="n"&gt;eigen&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;little&lt;/span&gt; &lt;span class="n"&gt;bit&lt;/span&gt; &lt;span class="n"&gt;faster&lt;/span&gt; &lt;span class="n"&gt;than&lt;/span&gt; &lt;span class="n"&gt;eig&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evcts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eigen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;which&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SM&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evcts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evcts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evcts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real&lt;/span&gt;
    &lt;span class="n"&gt;edict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evcts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="n"&gt;evals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;edict&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;second&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;third&lt;/span&gt; &lt;span class="n"&gt;smallest&lt;/span&gt; &lt;span class="n"&gt;eigenvalue&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;edict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]]).&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kmeans2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;minit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rename_clusters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_xticklabels&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_yticklabels&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;get_colormap&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;map&lt;/span&gt; &lt;span class="n"&gt;cluster&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;orange&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blue&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;green&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;matplotlib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;colors&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;
    &lt;span class="n"&gt;orange&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.918&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.545&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;blue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.169&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.651&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.914&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;green&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.58&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.365&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;orange&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blue&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;green&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;plot_circles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;facecolors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;x1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;x2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;plot_eigenvalues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.58&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.365&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Eigenvalue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axhline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ls&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;plot_eigenvectors&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;matplotlib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MaxNLocator&lt;/span&gt;
    &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;mpl_toolkits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axes_grid&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;make_axes_locatable&lt;/span&gt;
    &lt;span class="n"&gt;divider&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_axes_locatable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;divider&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_vertical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;100%&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;fig1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;fig1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_axes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Eigenvectors&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;facecolors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axhline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ls&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yaxis&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_major_locator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MaxNLocator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yaxis&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_major_locator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MaxNLocator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axhline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ls&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;facecolors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;index&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2nd Smallest&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;3nd Smallest&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_xticklabels&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_visible&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;plot_spec_clustering&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Spectral Clustering&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;facecolors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Second Smallest Eigenvector&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Third Smallest Eigenvector&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;plot_figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;colormap&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_colormap&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots_adjust&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plot_circles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plot_eigenvalues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plot_eigenvectors&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plot_spec_clustering&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;circle_samples&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;knn_points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mutual_knn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_distance_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;knn_points&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Wi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Wi&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;unnormalized&lt;/span&gt; &lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="n"&gt;Laplacian&lt;/span&gt;
    &lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;
    &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cluster_points&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;plot_figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;运行源码得到下图:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spectral Clustering" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/SpectralClustering_zpsb5f59a0e.png" /&gt;&lt;/p&gt;
&lt;h3&gt;总结&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;图划分问题中的关键点在于选择合适的指示向量并将其进行松弛化处理，从而将最小化划分的问题转变为最小化二次函数，进而转化为求Rayleigh quotient极值的问题;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spectral Clustering的各个阶段为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择合适的相似性函数计算相似度矩阵；&lt;/li&gt;
&lt;li&gt;计算矩阵L的特征值及其特征向量，比如可以用&lt;code&gt;Lanczos&lt;/code&gt;迭代算法；&lt;/li&gt;
&lt;li&gt;如何选择K，可以采用启发式方法，比如，发现第1到m的特征值都挺小的，到了m+1突然变成较大的数，那么就可以选择K=m；&lt;/li&gt;
&lt;li&gt;用K-Means算法聚类，当然它不是唯一选择；&lt;/li&gt;
&lt;li&gt;Normalized Spectral Clustering在让Cluster间相似度最小而Cluster内部相似度最大方面表现要更好，所以首选这类方法。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Hierarchical Clustering&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Hierarchical_Clustering" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hierarchical_clustering_zps8134ae3b.png" /&gt;&lt;/p&gt;
&lt;p&gt;Hierarchical Clustering正如它字面上的意思那样，是层次化的聚类，得出来的结构是一棵树，如上图所示。在前面我们介绍过不少聚类方法，但是都是“平坦”型的聚类，然而他们还有一个更大的共同点，或者说是弱点，就是难以确定类别数。我们这里要说的Hierarchical Clustering则从某种意义上来说也算是解决了这个问题，因为在做Clustering的时候并不需要知道类别数，而得到的结果是一棵树，事后可以在任意的地方横切一刀，得到指定数目的cluster,按需取即可。&lt;/p&gt;
&lt;p&gt;听上去很诱人，不过其实Hierarchical Clustering的想法很简单，主要分为两大类：Agglomerative(自底向上)和Divisive(自顶向下)。首先说前者，自底向上，一开始，每个数据点各自为一个类别，然后每一次迭代选取距离最近的两个类别，把他们合并，直到最后只剩下一个类别为止，至此一棵树构造完成。&lt;/p&gt;
&lt;p&gt;看起来很简单吧？其实确实也是比较简单的，不过还是有两个问题需要先说清除才行：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如何计算两个点的距离？这个通常是problem dependent的，一般情况下可以直接用一些比较通用的距离就可以了，比如欧氏距离等。&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如何计算两个类别之间的距离？一开始所有的类别都是一个点，计算距离只是计算两个点之间的距离，但是经过后续合并之后，一个类别里就不止一个点了，那距离又要怎样算呢？到这里又有三个变种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single Linkage：又叫做nearest-neighbor,就是取两个集合中距离最近的两个点的距离作为这两个集合的距离，容易造成一种叫做Chaining的效果，两个cluster明明从“大局”上离得比较远，但是由于其中个别的点距离比较近就被合并了,并且这样合并之后Chaining效应会进一步扩大,最后会得到比较松散的cluster 。&lt;/li&gt;
&lt;li&gt;Complete Linkage：这个则完全是Single Linkage的反面极端，取两个集合中距离最远的两个点的距离作为两个集合的距离。其效果也是刚好相反的，限制非常大，两个cluster即使已经很接近了，但是只要有不配合的点存在，就顽固到底，老死不相合并，也是不太好的办法。&lt;/li&gt;
&lt;li&gt;Group Average：这种方法看起来相对有道理一些，也就是把两个集合中的点两两的距离全部放在一起求一个平均值,相对也能得到合适一点的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的来说，一般都不太用Single Linkage或者Complete Linkage这两种过于极端的方法。整个agglomerative hierarchical clustering 的算法就是这个样子，描述起来还是相当简单的，不过计算起来复杂度还是比较高的，要找出距离最近的两个点，需要一个双重循环，而且 Group Average计算距离的时候也是一个双重循环。&lt;/p&gt;
&lt;p&gt;另外，需要提一下的是本文一开始的那个树状结构图，它有一个专门的称呼，叫做Dendrogram，其实就是一种二叉树，画的时候让子树的高度和它两个后代合并时相互之间的距离大小成比例，就可以得到一个相对直观的结构概览。&lt;/p&gt;
&lt;p&gt;Agglomerative clustering差不多就这样了，再来看Divisive clustering,也就是自顶向下的层次聚类，这种方法并没有Agglomerative clustering这样受关注，大概因为把一个节点分割为两个并不如把两个节点结合为一个那么简单吧，通常在需要做hierarchical clustering 但总体的cluster数目又不太多的时候可以考虑这种方法，这时可以分割到符合条件为止，而不必一直分割到每个数据点一个cluster 。&lt;/p&gt;
&lt;p&gt;总的来说，Divisive clustering 的每一次分割需要关注两个方面：一是选哪一个cluster来分割；二是如何分割。关于cluster的选取，通常采用一些衡量松散程度的度量值来比较，例如cluster中距离最远的两个数据点之间的距离，或者cluster中所有节点相互距离的平均值等，直接选取最“松散”的一个cluster来进行分割。而分割的方法也有多种，比如，直接采用普通的flat clustering算法（例如K-Means来进行二类聚类，不过这样的方法计算量变得很大，而且像K-Means 这样的和初值选取关系很大的算法，会导致结果不稳定。另一种比较常用的分割方法(&lt;code&gt;简而言之就是一个排除异己的过程&lt;/code&gt;)如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;待分割的cluster记为$G$,在$G$中取出一个到其他点的平均距离最远的点$x$,构成新cluster H；&lt;/li&gt;
&lt;li&gt;在 G 中选取这样的点$x\prime$,$x\prime$到$G$中其他点的平均距离减去$x\prime$到$H$中所有点的平均距离这个差值最大，将其归入$H$中；&lt;/li&gt;
&lt;li&gt;重复上一个步骤，直到差值为负。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;到此为止，关于Hierarchical clustering介绍就结束了。总的来说，Hierarchical clustering算法似乎都是描述起来很简单，计算起来很困难（计算量很大）。并且，不管是Agglomerative还是 Divisive 实际上都是贪心算法了，也并不能保证能得到全局最优的。而得到的结果，虽然说可以从直观上来得到一个比较形象的大局观，但是似乎实际用处并不如众多Flat clustering 算法那么广泛。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Adjusted_rand_index#Adjusted_Rand_index"&gt;Rand Index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=17"&gt;漫谈 Clustering (1): k-means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=40"&gt;漫谈 Clustering (2): k-medoids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/vivounicorn/archive/2012/02/10/2343377.html"&gt;Spectral Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Machine Learning:A Probabilistic Perspective(&lt;em&gt;Chapter 25&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Clustering"></category><category term="Unsupervised Learning"></category><category term="K-Means"></category><category term="Spectral Learning"></category><category term="Hierarchical Clustering"></category></entry><entry><title>也说2048:Minimax算法以及Alpha-Beta剪枝</title><link href="http://www.qingyuanxingsi.com/ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi.html" rel="alternate"></link><updated>2014-04-06T00:00:00+08:00</updated><author><name>CodingLabs</name></author><id>tag:www.qingyuanxingsi.com,2014-04-06:ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi.html</id><summary type="html">&lt;p&gt;今天看机器学习Logistic Regression,脑子实在转不过来了,于是到处游荡了一下,然后不经意间发现了这篇特别好玩的文章&lt;a href="http://blog.codinglabs.org/articles/2048-ai-analysis.html"&gt;2048-AI程序算法分析&lt;/a&gt;,于是摘录如下。(&lt;strong&gt;版权属原作者所有,特此声明&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;针对目前火爆的2048游戏，有人实现了一个AI程序，可以以较大概率（高于90%）赢得游戏，想一睹该AI程序的童鞋们请移步&lt;a href="http://ov3y.github.io/2048-AI/"&gt;2048AI实现&lt;/a&gt;,点击Auto-run按钮即可运行。此外作者在&lt;a href="http://stackoverflow.com/questions/22342854/what-is-the-optimal-algorithm-for-the-game-2048"&gt;stackoverflow上简要介绍了AI的算法框架和实现思路&lt;/a&gt;。但是这个回答主要集中在启发函数的选取上，对AI用到的核心算法并没有仔细说明。这篇文章将主要分为两个部分，第一部分介绍其中用到的基础算法，即Minimax和Alpha-beta剪枝；第二部分分析作者具体的实现。&lt;/p&gt;
&lt;p&gt;&lt;img alt="2048" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/2048_zps4f3b681e.png"&gt;&lt;/p&gt;
&lt;h1&gt;基础算法&lt;/h1&gt;
&lt;p&gt;2048本质上可以抽象成信息对称双人对弈模型（玩家向四个方向中的一个移动，然后计算机在某个空格中填入2或4）。这里“信息对称”是指在任一时刻对弈双方对格局的信息完全一致，移动策略仅依赖对接下来格局的推理。作者使用的核心算法为对弈模型中常用的带Alpha-beta剪枝的Minimax。这个算法也常被用于如国际象棋等信息对称对弈AI中。&lt;/p&gt;
&lt;h2&gt;Minimax&lt;/h2&gt;
&lt;p&gt;下面先介绍不带剪枝的Minimax。首先本文将通过一个简单的例子说明Minimax算法的思路和决策方式。&lt;/p&gt;
&lt;h3&gt;问题&lt;/h3&gt;
&lt;p&gt;现在考虑这样一个游戏：有三个盘子A、B和C，每个盘子分别放有三张纸币。A放的是1、20、50；B放的是5、10、100；C放的是1、5、20。单位均为“元”。有甲、乙两人，两人均对三个盘子和上面放置的纸币有可以任意查看。游戏分三步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;甲从三个盘子中选取一个。&lt;/li&gt;
&lt;li&gt;乙从甲选取的盘子中拿出两张纸币交给甲。&lt;/li&gt;
&lt;li&gt;甲从乙所给的两张纸币中选取一张，拿走。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中甲的目标是最后拿到的纸币面值尽量大，乙的目标是让甲最后拿到的纸币面值尽量小。&lt;/p&gt;
&lt;p&gt;下面用Minimax算法解决这个问题。&lt;/p&gt;
&lt;h3&gt;基本思路&lt;/h3&gt;
&lt;p&gt;一般解决博弈类问题的自然想法是将格局组织成一棵树，树的每一个节点表示一种格局，而父子关系表示由父格局经过一步可以到达子格局。Minimax也不例外，它通过对以当前格局为根的格局树搜索来确定下一步的选择。而一切格局树搜索算法的核心都是对每个格局价值的评价。Minimax算法基于以下朴素思想确定格局价值：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimax是一种悲观算法，即假设对手每一步都会将我方引入从当前看理论上价值最小的格局方向，即对手具有完美决策能力。因此我方的策略应该是选择那些对方所能达到的让我方最差情况中最好的，也就是让对方在完美决策下所对我造成的损失最小。&lt;/li&gt;
&lt;li&gt;Minimax不找理论最优解，因为理论最优解往往依赖于对手是否足够愚蠢，Minimax中我方完全掌握主动，如果对方每一步决策都是完美的，则我方可以达到预计的最小损失格局，如果对方没有走出完美决策，则我方可能达到比预计的最悲观情况更好的结局。总之我方就是要在最坏情况中选择最好的。
上面的表述有些抽象，下面看具体示例。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;解题&lt;/h3&gt;
&lt;p&gt;下图是上述示例问题的格局树：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Situation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/01_zps1cf40396.png"&gt;&lt;/p&gt;
&lt;p&gt;注意，由于示例问题格局数非常少，我们可以给出完整的格局树。这种情况下我可以找到Minimax算法的全局最优解。而真实情况中，格局树非常庞大，即使是计算机也不可能给出完整的树，因此我们往往只搜索一定深度，这时只能找到局部最优解。&lt;/p&gt;
&lt;p&gt;我们从甲的角度考虑。其中正方形节点表示轮到我方（甲），而三角形表示轮到对方（乙）。经过三轮对弈后（我方-对方-我方），将进入终局。黄色叶结点表示所有可能的结局。从甲方看，由于最终的收益可以通过纸币的面值评价，我们自然可以用结局中甲方拿到的纸币面值表示终格局的价值。&lt;/p&gt;
&lt;p&gt;下面考虑倒数第二层节点，在这些节点上，轮到我方选择，所以我们应该引入可选择的最大价值格局，因此每个节点的价值为其子节点的最大值：&lt;/p&gt;
&lt;p&gt;&lt;img alt="02" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/02_zps80320dc2.png"&gt;&lt;/p&gt;
&lt;p&gt;这些轮到我方的节点叫做max节点，max节点的值是其子节点最大值。&lt;/p&gt;
&lt;p&gt;倒数第三层轮到对方选择，假设对方会尽力将局势引入让我方价值最小的格局，因此这些节点的价值取决于子节点的最小值。这些轮到对方的节点叫做min节点。&lt;/p&gt;
&lt;p&gt;最后，根节点是max节点，因此价值取决于叶子节点的最大值。最终完整赋值的格局树如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="03" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/03_zps171fcc9a.png"&gt;&lt;/p&gt;
&lt;p&gt;总结一下Minimax算法的步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先确定最大搜索深度D，D可能达到终局，也可能是一个中间格局。&lt;/li&gt;
&lt;li&gt;在最大深度为D的格局树叶子节点上，使用预定义的价值评价函数对叶子节点价值进行评价。&lt;/li&gt;
&lt;li&gt;自底向上为非叶子节点赋值。其中max节点取子节点最大值，min节点取子节点最小值。&lt;/li&gt;
&lt;li&gt;每次轮到我方时（此时必处在格局树的某个max节点），选择价值等于此max节点价值的那个子节点路径。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在上面的例子中，根节点的价值为20，表示如果对方每一步都完美决策，则我方按照上述算法可最终拿到20元，这是我方在Minimax算法下最好的决策。格局转换路径如下图红色路径所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt="04" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/04_zpsaa4d7848.png"&gt;&lt;/p&gt;
&lt;p&gt;对于真实问题中的Minimax，再次强调几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;真实问题一般无法构造出完整的格局树，所以需要确定一个最大深度D，每次最多从当前格局向下计算D层。&lt;/li&gt;
&lt;li&gt;因为上述原因，Minimax一般是寻找一个局部最优解而不是全局最优解，搜索深度越大越可能找到更好的解，但计算耗时会呈指数级膨胀。&lt;/li&gt;
&lt;li&gt;也是因为无法一次构造出完整的格局树，所以真实问题中Minimax一般是边对弈边计算局部格局树，而不是只计算一次，但已计算的中间结果可以缓存。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Alpha-beta剪枝&lt;sup id="sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1" title="以下描述看不懂可参考Step by Step:Alpha-Beta Cutting Example"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;简单的Minimax算法有一个很大的问题就是计算复杂性。由于所需搜索的节点数随最大深度呈指数膨胀，而算法的效果往往和深度相关，因此这极大限制了算法的效果。&lt;/p&gt;
&lt;p&gt;Alpha-beta剪枝是对Minimax的补充和改进。采用Alpha-beta剪枝后，我们可不必构造和搜索最大深度D内的所有节点，在构造过程中，如果发现当前格局再往下不能找到更好的解，我们就停止在这个格局及以下的搜索，也就是剪枝。&lt;/p&gt;
&lt;p&gt;Alpha-beta基于这样一种朴素的思想：&lt;strong&gt;时时刻刻记得当前已经知道的最好选择，如果从当前格局搜索下去，不可能找到比已知最优解更好的解，则停止这个格局分支的搜索（剪枝），回溯到父节点继续搜索&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Alpha-beta算法可以看成变种的Minimax，基本方法是从根节点开始采用深度优先的方式构造格局树，在构造每个节点时，都会读取此节点的alpha和beta两个值，其中alpha表示搜索到当前节点时已知的最好选择的下界，而beta表示从这个节点往下搜索最坏结局的上界。由于我们假设对手会将局势引入最坏结局之一，因此当beta小于alpha时，表示从此处开始不论最终结局是哪一个，其上限价值也要低于已知的最优解，也就是说已经不可能此处向下找到更好的解，所以就会剪枝。&lt;/p&gt;
&lt;p&gt;下面同样以上述示例介绍Alpha-beta剪枝算法的工作原理。我们从根节点开始，详述使用Alpha-beta的每一个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根节点的alpha和beta分别被初始化为$-\infty$，和$+\infty$。&lt;/li&gt;
&lt;li&gt;深度优先搜索第一个孩子，不是叶子节点，所以alpha和beta继承自父节点，分别为$-\infty$，和$+\infty$&lt;/li&gt;
&lt;li&gt;搜索第三层的第一个孩子，同上。&lt;/li&gt;
&lt;li&gt;搜索第四层，到达叶子节点，采用评价函数得到此节点的评价值为1。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="05" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/05_zps5136a83f.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;此叶节点的父节点为max节点，因此更新其alpha值为1，表示此节点取值的下界为1。&lt;/li&gt;
&lt;li&gt;再看另外一个子节点，值为20，大于当前alpha值，因此将alpha值更新为20。&lt;/li&gt;
&lt;li&gt;此时第三层最左节点所有子树搜索完毕，作为max节点，更新其真实值为当前alpha值：20。&lt;/li&gt;
&lt;li&gt;由于其父节点（第二层最左节点）为min节点，因此更新其父节点beta值为20，表示这个节点取值最多为20。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="06" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/06_zps09a0fcab.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;搜索第二层最左节点的第二个孩子及其子树，按上述逻辑，得到值为50（&lt;strong&gt;注意第二层最左节点的beta值要传递给孩子&lt;/strong&gt;）。由于50大于20，不更新min节点的beta值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="07" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/07_zps5a2b5a81.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;搜索第二层最左节点的第三个孩子。当看完第一个叶子节点后，发现第三个孩子的alpha=beta，此时表示这个节点下不会再有更好解，于是剪枝。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="08" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/08_zpsbee7e438.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;继续搜索B分支，当搜索完B分支的第一个孩子后，发现此时B分支的alpha为20，beta为10。这表示B分支节点的最大取值不会超过10，而我们已经在A分支取到20，此时满足alpha大于等于beta的剪枝条件，因此将B剪枝。并将B分支的节点值设为10，注意，这个10不一定是这个节点的真实值，而只是上线，B节点的真实值可能是5，可能是1，可能是任何小于10的值。但是已经无所谓了，反正我们知道这个分支不会好过A分支，因此可以放弃了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="09" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/09_zpsf2b60883.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在C分支搜索时遇到了与B分支相同的情况。因此将C分支剪枝。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="10" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/10_zps1254e8ee.png"&gt;&lt;/p&gt;
&lt;p&gt;此时搜索全部完毕，而我们也得到了这一步的策略：应该走A分支。&lt;/p&gt;
&lt;p&gt;可以看到相比普通Minimax要搜索18个叶子节点相比，这里只搜索了9个。采用Alpha-beta剪枝，可以在相同时间内加大Minimax的搜索深度，因此可以获得更好的效果。并且Alpha-beta的解和普通Minimax的解是一致的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:在查找讲解Alpha-Beta剪枝算法的过程中看到了这样一种表述,我个人觉得这么理解可能更容易看懂这个算法:Alpha是对于Max节点而言从该节点到根节点路径上最好的Option,而Beta是对于Min节点而言从该节点到根节点路径上最好的Option;Max节点的目标是最大化所得收益,而Min节点目标则是最小化Max节点所得收益.(MORE SEE AT &lt;a href="http://www.youtube.com/watch?v=xBXHtz4Gbdo"&gt;Step by Step: Alpha Beta Pruning|GoAgent翻墙&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;针对2048游戏的实现&lt;/h1&gt;
&lt;p&gt;下面看一下ov3y同学针对2048实现的AI。原程序见于&lt;a href="https://github.com/ov3y/2048-AI"&gt;github&lt;/a&gt;，主要程序都在&lt;a href="https://github.com/ov3y/2048-AI/blob/master/js/ai.js"&gt;ai.js&lt;/a&gt;中。&lt;/p&gt;
&lt;h2&gt;建模&lt;/h2&gt;
&lt;p&gt;上面说过Minimax和Alpha-beta都是针对信息对称的轮流对弈问题，这里作者是这样抽象游戏的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我方：游戏玩家。每次可以选择上、下、左、右四个行棋策略中的一种（某些格局会少于四种，因为有些方向不可走）。行棋后方块按照既定逻辑移动及合并，格局转换完成。&lt;/li&gt;
&lt;li&gt;对方：计算机。在当前任意空格子里放置一个方块，方块的数值可以是2或4。放置新方块后，格局转换完成。&lt;/li&gt;
&lt;li&gt;胜利条件：出现某个方块的数值为“2048”。&lt;/li&gt;
&lt;li&gt;失败条件：格子全满，且无法向四个方向中任何一个方向移动（均不能触发合并）。如此2048游戏就被建模成一个信息对称的双人对弈问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;格局评价&lt;/h2&gt;
&lt;p&gt;作为算法的核心，如何评价当前格局的价值是重中之重。在2048中，除了终局外，中间格局并无非常明显的价值评价指标，因此需要用一些启发式的指标来评价格局。那些分数高的“好”格局是容易引向胜利的格局，而分低的“坏”格局是容易引向失败的格局。&lt;/p&gt;
&lt;p&gt;作者采用了如下几个启发式指标。&lt;/p&gt;
&lt;h3&gt;单调性&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;单调性&lt;/code&gt;指方块从左到右、从上到下均遵从递增或递减。一般来说，越单调的格局越好。下面是一个具有良好单调格局的例子：&lt;/p&gt;
&lt;p&gt;&lt;img alt="单调性" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/11_zps7ce37f15.png"&gt;&lt;/p&gt;
&lt;h3&gt;平滑性&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;平滑性&lt;/code&gt;是指每个方块与其直接相邻方块数值的差，其中差越小越平滑。例如2旁边是4就比2旁边是128平滑。一般认为越平滑的格局越好。下面是一个具有极端平滑性的例子：&lt;/p&gt;
&lt;p&gt;&lt;img alt="平滑性" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/12_zpsb53c5ef7.png"&gt;&lt;/p&gt;
&lt;h3&gt;空格数&lt;/h3&gt;
&lt;p&gt;这个很好理解，因为一般来说，空格子越少对玩家越不利。所以我们认为空格越多的格局越好。&lt;/p&gt;
&lt;h3&gt;孤立空格数&lt;/h3&gt;
&lt;p&gt;这个指标评价空格被分开的程度，空格越分散则格局越差。&lt;/p&gt;
&lt;p&gt;具体来说，2048-AI在评价格局时，对这些启发指标采用了加权策略。具体代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// static evaluation function&lt;/span&gt;
&lt;span class="no"&gt;AI&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prototype&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;emptyCells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;availableCells&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;smoothWeight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="c1"&gt;//monoWeight   = 0.0,&lt;/span&gt;
        &lt;span class="c1"&gt;//islandWeight = 0.0,&lt;/span&gt;
        &lt;span class="n"&gt;mono2Weight&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;emptyWeight&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;maxWeight&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;smoothness&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;smoothWeight&lt;/span&gt;
        &lt;span class="c1"&gt;//+ this.grid.monotonicity() * monoWeight&lt;/span&gt;
        &lt;span class="c1"&gt;//- this.grid.islands() * islandWeight&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;monotonicity2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mono2Weight&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emptyCells&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;emptyWeight&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maxValue&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;maxWeight&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;有兴趣的同学可以调整一下权重看看有什么效果。&lt;/p&gt;
&lt;h2&gt;对对方选择的剪枝&lt;/h2&gt;
&lt;p&gt;在这个程序中，除了采用Alpha-beta剪枝外，在min节点还采用了另一种剪枝，即只考虑对方走出让格局最差的那一步（而实际2048中计算机的选择是随机的），而不是搜索全部对方可能的走法。这是因为对方所有可能的选择为“空格数×2”，如果全部搜索的话会严重限制搜索深度。
相关剪枝代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// try a 2 and 4 in each cell and measure how annoying it is&lt;/span&gt;
&lt;span class="c1"&gt;// with metrics from eval&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[];&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;availableCells&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="mh"&gt;2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="mh"&gt;4&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;push&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="k"&gt;cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;tile&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Tile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parseInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;10&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insertTile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tile&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;smoothness&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;islands&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;removeTile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;cell&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;// now just pick out the most annoying moves&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;maxScore&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mh"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mh"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]));&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// 2 and 4&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;maxScore&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;push&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nl"&gt;position:&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nl"&gt;value:&lt;/span&gt; &lt;span class="n"&gt;parseInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;搜索深度&lt;/h2&gt;
&lt;p&gt;在2048-AI的实现中，并没有限制搜索的最大深度，而是限制每次“思考”的时间。这里设定了一个超时时间，默认为100ms，在这个时间内，会从1开始，搜索到所能达到的深度。相关代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// performs iterative deepening over the alpha-beta search&lt;/span&gt;
&lt;span class="no"&gt;AI&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prototype&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterativeDeep&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;()).&lt;/span&gt;&lt;span class="n"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;newBest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mh"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;newBest&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;move&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mh"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="c1"&gt;//console.log('BREAKING EARLY');&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;newBest&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;()).&lt;/span&gt;&lt;span class="n"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;minSearchTime&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="c1"&gt;//console.log('depth', --depth);&lt;/span&gt;
    &lt;span class="c1"&gt;//console.log(this.translate(best.move));&lt;/span&gt;
    &lt;span class="c1"&gt;//console.log(best);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;因此这个算法实现的效果实际上依赖于执行javascript引擎机器的性能。当然可以通过增加超时时间来达到更好的效果，但此时每一步行走速度会相应变慢。&lt;/p&gt;
&lt;h2&gt;算法的改进&lt;/h2&gt;
&lt;p&gt;目前这个实现作者声称成功合成2048的概率超过90%，但是合成4096甚至8192的概率并不高。作者在&lt;a href="https://github.com/ov3y/2048-AI/blob/master/README.md"&gt;github项目的REAMDE&lt;/a&gt;中同时给出了一些优化建议，这些建议包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缓存结果。目前这个实现并没有对已搜索的树做缓存，每一步都要重新开始搜索。&lt;/li&gt;
&lt;li&gt;多线程搜索。由于javascript引擎的单线程特性，这一点很难做到，但如果在其它平台上也许也可考虑并行技术。&lt;/li&gt;
&lt;li&gt;更好的启发函数。也许可以总结出一些更好的启发函数来评价格局价值。&lt;/li&gt;
&lt;/ul&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1"&gt;以下描述看不懂可参考&lt;a href="http://www.youtube.com/watch?v=xBXHtz4Gbdo"&gt;Step by Step:Alpha-Beta Cutting Example&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="AI"></category><category term="2048"></category><category term="Minimax Algorithm"></category><category term="Alpha-Beta Pruning"></category><category term="Algorithm"></category></entry><entry><title>分布式计算系列(I):Yarn基础库初探</title><link href="http://www.qingyuanxingsi.com/fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan.html" rel="alternate"></link><updated>2014-04-01T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-01:fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan.html</id><summary type="html">&lt;h1&gt;PREFACE&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;在上一篇&lt;a href="http://www.qingyuanxingsi.com/fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing.html"&gt;分布式计算与存储系列(序章):初入门径&lt;/a&gt;中,我们主要介绍了分布式存储的一些基础知识,在本系列中,我们会结合论文以及源码对分布式计算的基本理论以及一些应用系统进行研究，以期对分布式系统有一个更为深入的了解。本系列的前若干篇均以研究Yarn为主,主要参考&lt;a href="http://book.douban.com/subject/25774649/"&gt;Hadoop技术内幕:深入解析YARN架构设计与实现原理&lt;/a&gt;一书的整体架构,但是个人对这本书不是特别满意，讲述的还是有点浅,很多问题只是浅尝则止而已。(真正想&lt;code&gt;深入&lt;/code&gt;了解YARN的不建议购买此书，如若只是想粗略的了解一下YARN的工作流程的童鞋倒是可以入手一本滴。)因此，本系列仅会采用其大体框架，在其大体框架下，对YARN设计的其他知识和设计模式等也会有进一步更为深入的介绍。好吧，闲话就不多说了,我们开始正式讨论。&lt;/p&gt;
&lt;p&gt;本文的主要目的是介绍一下YARN中用到的基础库,它们是YARN其他模块得以建立的基石,其重要性自然不言而喻。我们先从其RPC库说起。&lt;/p&gt;
&lt;h1&gt;The Secret of RPC&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;当前存在非常多的开源 RPC 框架,比较有名的有 Thrift、Protocol Buffers 和 Avro。同Hadoop RPC一样,它们均由两部分组成:对象序列化和远程过程调用(Protocol Buflers官方仅提供了序列化实现,未提供远程调用相关实现,但三方 RPC 库非常多 )。相比于Hadoop RPC,它们有以下几个特点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;跨语言特性&lt;/strong&gt;。对于 Hadoop RPC而言,由于Hadoop采用 Java 语言编写,因而其RPC客户端和服务器端仅支持Java语言;但对于更通用的 RPC框架,如Thrift或者Protocol Buffers等,其客户端和服务器端可采用任何语言编写,如Java、C++、Python等,这给用户编程带来极大方便。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;引入IDL&lt;/strong&gt;。开源RPC框架均提供了一套接口描述语言(Interface Description Language,IDL),它提供一套通用的数据类型,并以这些数据类型来定义更为复杂的数据类型和对外服务接口。一旦用户按照IDL定义的语法编写完接口文件后,可根据实际应用需要生成特定编程语言(如 Java、C++、Python 等)的客户端和服务器端代码。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;协议兼容性&lt;/strong&gt;。开源RPC框架在设计上均考虑到了协议兼容性问题,即当协议格式发生改变时,比如某个类需要添加或者删除一个成员变量(字段)后,旧版本代码仍然能识别新格式的数据,也就是说,具有向后兼容性。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;随着Hadoop版本的不断演化,研发人员发现Hadoop RPC在跨语言支持和协议兼容性两个方面存在不足,具体表现为:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从长远发展看,Hadoop RPC应允许某些协议的客户端或者服务器端采用其他语言实现,比如用户希望直接使用C/C++语言读写HDFS中的文件,这就需要有C/C++语言的HDFS客户端。&lt;/li&gt;
&lt;li&gt;当前 Hadoop 版本较多,而不同版本之间不能通信,比如0.20.2版本的JobTracker不能与0.21.0版本中的TaskTracker通信,如果用户企图这样做,会抛出&lt;em&gt;VersionMismatch&lt;/em&gt;异常。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了解决以上几个问题,Hadoop YARN将RPC中的序列化部分剥离开,以便将现有的开源RPC框架集成进来。Hadoop目前集成了Protocol Buffer以及Apache Avro的序列化部分,而函数调用调用机制仍采用Hadoop自带的,其中RPC采用Protocol Buffer,而Apache Avro则用于日志系统。以下对这两种序列化机制进行一个简要的介绍:&lt;/p&gt;
&lt;h2&gt;持久化框架&lt;/h2&gt;
&lt;h3&gt;Protocol Buffer&lt;sup id="sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-1-back"&gt;&lt;a class="simple-footnote" href="#sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-1" title="https://developers.google.com/protocol-buffers/docs/javatutorial"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Protocol Buffers 是一种轻便高效的结构化数据存储格式,可以用于结构化数据序列化/反序列化。它很适合做数据存储或RPC的数据交换格式,常用作通信协议、数据存储等领域的与语言无关、平台无关、可扩展的序列化结构数据格式。目前支持C++、Java、Python三种语言。在
Google 内部,几乎所有的RPC协议和文件格式都是采用Protocol Buffers。&lt;/p&gt;
&lt;p&gt;相比于常见的XML格式,Protocol Buffers官方网站这样描述它的优点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平台无关、语言无关;&lt;/li&gt;
&lt;li&gt;高性能,解析速度是 XML 的 20 ~ 100 倍;&lt;/li&gt;
&lt;li&gt;体积小,文件大小仅是 XML 的 1/10 ~ 1/3;&lt;/li&gt;
&lt;li&gt;使用简单;&lt;/li&gt;
&lt;li&gt;兼容性好。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通常编写一个 Protocol Buffers 应用需要以下三步:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定义报文格式(.proto文件)&lt;/li&gt;
&lt;li&gt;使用Protocol Buffer Compiler编译生成JAVA类&lt;/li&gt;
&lt;li&gt;使用Protocol Buffer API读写报文&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;定义报文格式&lt;/h4&gt;
&lt;p&gt;我们首先定义消息格式文件addressbook.proto,以下定义了一个人的通讯录的基本信息:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;package&lt;/span&gt; &lt;span class="n"&gt;tutorial&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;option&lt;/span&gt; &lt;span class="n"&gt;java_package&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"com.qingyuanxingsi.tutorial"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;option&lt;/span&gt; &lt;span class="n"&gt;java_outer_classname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"AddressBookProtos"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;message&lt;/span&gt; &lt;span class="n"&gt;Person&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;required&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;required&lt;/span&gt; &lt;span class="n"&gt;int32&lt;/span&gt; &lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt; &lt;span class="n"&gt;email&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;enum&lt;/span&gt; &lt;span class="n"&gt;PhoneType&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;MOBILE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;HOME&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;WORK&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;message&lt;/span&gt; &lt;span class="n"&gt;PhoneNumber&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;required&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt; &lt;span class="n"&gt;number&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;PhoneType&lt;/span&gt; &lt;span class="n"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="k"&gt;default&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;HOME&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;repeated&lt;/span&gt; &lt;span class="n"&gt;PhoneNumber&lt;/span&gt; &lt;span class="n"&gt;phone&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;message&lt;/span&gt; &lt;span class="n"&gt;AddressBook&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;repeated&lt;/span&gt; &lt;span class="n"&gt;Person&lt;/span&gt; &lt;span class="n"&gt;person&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;.proto文件开头包含一个包声明,以避免不同Project之间的命名冲突。Java中，package名即被用作Java包名,除非通过&lt;code&gt;java_package&lt;/code&gt;另外显式指定表明。以上我们制定生成的包名为&lt;code&gt;com.qingyuanxingsi.tutorial&lt;/code&gt;.&lt;code&gt;java_outer_classname&lt;/code&gt;则指定了类名，我们生成的所有类均会被放在这个文件中。如果未显式制定，则会将文件名自动转化成Camel形式的类名。如,&lt;code&gt;my_proto.proto&lt;/code&gt;默认情况下会生成&lt;code&gt;MyProto&lt;/code&gt;作为其类名。&lt;/p&gt;
&lt;p&gt;接下来则是报文定义。一个报文即是一系列带有类型信息的Field的集合。很多简单数据类型可被用作Field Type,包括bool, int32, float, double, and string. 当然，你也可以自定义类型作为Field Type.在上述例子中,Person报文就包含PhoneNumber报文,AddressBook报文则包括Person报文。另外,报文可被嵌套定义,如PhoneNumber就定义在Person报文中。如果你想让你的某个Field具有一个或多个预定义的值,你可以使用枚举类型，如上述，我们想让电话号码类型取MOBILE, HOME, or WORK中的值。每个字段后的&lt;code&gt;=？&lt;/code&gt;标记为每个Field分配了唯一的TAG,以用于二进制编码。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;required&lt;/code&gt;关键字指定该Field必须被赋值，否则报文将会被视为&lt;code&gt;uninitialized&lt;/code&gt;.编译此类报文则会抛出&lt;em&gt;RuntimeException&lt;/em&gt;异常,除此之外,它与optional field基本相同。&lt;code&gt;optional&lt;/code&gt;关键字则表明该Field可被设置，也可不设置。如果未设置,则会使用默认值。对于简单数据类型，我们可以定义我们自己的默认值,否则则会使用系统默认值。对于嵌套报文,默认值则通常会是报文的默认实例或者原型,其中每一个Field均未被设置。repeated则表明该字段可以重复任何多次。&lt;/p&gt;
&lt;h4&gt;编译生成JAVA类&lt;/h4&gt;
&lt;p&gt;使用以下命令即可生成相应的JAVA类:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;protoc&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;SRC_DIR&lt;/span&gt; &lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="n"&gt;java_out&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;DST_DIR&lt;/span&gt; &lt;span class="err"&gt;$&lt;/span&gt;&lt;span class="n"&gt;SRC_DIR&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;addressbook&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;proto&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;生成的类的结构如下图所示(此处不再给出源码):&lt;/p&gt;
&lt;p&gt;&lt;img alt="AddressBook Struture" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/addressBook_zps34178bad.png"&gt;&lt;/p&gt;
&lt;h4&gt;使用Protocol Buffer API读写报文&lt;/h4&gt;
&lt;p&gt;如上图所示,我们可以看到一个&lt;code&gt;AddressBookProtos.java&lt;/code&gt;类,其中则嵌套了多个类,每个类均有.proto中定义的message生成。每个类都有对应的一个&lt;code&gt;Builder&lt;/code&gt;类,可以用于构造类实例。&lt;/p&gt;
&lt;p&gt;报文类以及Builder类对于报文的每个Field均提供了访问器。值得注意的是,报文类仅提供了getters,而Builder类既有getters,又有setters.以下给出Person类的一个实例:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// required string name = 1;&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;boolean&lt;/span&gt; &lt;span class="n"&gt;hasName&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;getName&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

&lt;span class="c1"&gt;// required int32 id = 2;&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;boolean&lt;/span&gt; &lt;span class="n"&gt;hasId&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;int&lt;/span&gt; &lt;span class="n"&gt;getId&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

&lt;span class="c1"&gt;// optional string email = 3;&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;boolean&lt;/span&gt; &lt;span class="n"&gt;hasEmail&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;getEmail&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

&lt;span class="c1"&gt;// repeated .tutorial.Person.PhoneNumber phone = 4;&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;PhoneNumber&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;getPhoneList&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;int&lt;/span&gt; &lt;span class="n"&gt;getPhoneCount&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;PhoneNumber&lt;/span&gt; &lt;span class="n"&gt;getPhone&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;int&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;而与其对应的Builder类则getters和setters都有:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// required string name = 1;&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;boolean&lt;/span&gt; &lt;span class="n"&gt;hasName&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;getName&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="n"&gt;setName&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="n"&gt;clearName&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

&lt;span class="c1"&gt;// required int32 id = 2;&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;boolean&lt;/span&gt; &lt;span class="n"&gt;hasId&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;int&lt;/span&gt; &lt;span class="n"&gt;getId&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="n"&gt;setId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;int&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="n"&gt;clearId&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

&lt;span class="c1"&gt;// optional string email = 3;&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;boolean&lt;/span&gt; &lt;span class="n"&gt;hasEmail&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;getEmail&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="n"&gt;setEmail&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="n"&gt;clearEmail&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;

&lt;span class="c1"&gt;// repeated .tutorial.Person.PhoneNumber phone = 4;&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;PhoneNumber&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;getPhoneList&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;int&lt;/span&gt; &lt;span class="n"&gt;getPhoneCount&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;PhoneNumber&lt;/span&gt; &lt;span class="n"&gt;getPhone&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;int&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="n"&gt;setPhone&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;int&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PhoneNumber&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="n"&gt;addPhone&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PhoneNumber&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="n"&gt;addAllPhone&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Iterable&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;PhoneNumber&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="n"&gt;clearPhone&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;由Protocol Buffer Compiler编译生成的message类均是不可变的。Message实例一旦生成，就不能更改。为了构造一个message，我们首先构造一个builder,将Field设置成你想要的值,然后调用builder的build()方法。以下代码用于构造一个Person实例:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Person&lt;/span&gt; &lt;span class="n"&gt;john&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;
    &lt;span class="n"&gt;Person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;newBuilder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1234&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setName&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"John Doe"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setEmail&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"jdoe@example.com"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addPhone&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;Person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PhoneNumber&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;newBuilder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setNumber&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"555-4321"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PhoneType&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HOME&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;最后,每个Protocol Buffer类军定义了读写报文的方法,如下所示:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;byte[] toByteArray();持久化message对象并返回包含一字节数组。&lt;/li&gt;
&lt;li&gt;static Person parseFrom(byte[] data);通过给定字节数组解析构造报文实例。&lt;/li&gt;
&lt;li&gt;void writeTo(OutputStream output);将报文持久化到OutputStream中.&lt;/li&gt;
&lt;li&gt;static Person parseFrom(InputStream input);解析InputStream并构造报文实例.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;至此,我们给出一个报文读写实例,它用于将报文持久化到文件中然后从文件中解析构造得到原始报文:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;package&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;protoc&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FileInputStream&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FileNotFoundException&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FileOutputStream&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;io&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tutorial&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AddressBookProtos&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AddressBook&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tutorial&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AddressBookProtos&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Person&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tutorial&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AddressBookProtos&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PhoneNumber&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;com&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tutorial&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AddressBookProtos&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PhoneType&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt; * A toy example demonstrates the writing and reading process to an address book&lt;/span&gt;
&lt;span class="cm"&gt; * proto.&lt;/span&gt;
&lt;span class="cm"&gt; * &lt;/span&gt;
&lt;span class="cm"&gt; * @author qingyuanxingsi&lt;/span&gt;
&lt;span class="cm"&gt; * &lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;class&lt;/span&gt; &lt;span class="n"&gt;AddressBookDemo&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;private&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;final&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;FILE_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"addressbook.dat"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// TODO Auto-generated method stub&lt;/span&gt;
        &lt;span class="n"&gt;addPerson&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;printData&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt;     * Print the data out&lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;printData&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// TODO Auto-generated method stub&lt;/span&gt;
        &lt;span class="n"&gt;try&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;AddressBook&lt;/span&gt; &lt;span class="n"&gt;addressBook&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AddressBook&lt;/span&gt;
                    &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parseFrom&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;FileInputStream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FILE_PATH&lt;/span&gt;&lt;span class="p"&gt;)));&lt;/span&gt;
            &lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;addressBook&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FileNotFoundException&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="c1"&gt;// TODO Auto-generated catch block&lt;/span&gt;
            &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;printStackTrace&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="c1"&gt;// TODO Auto-generated catch block&lt;/span&gt;
            &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;printStackTrace&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt;     * Print the whole all data&lt;/span&gt;
&lt;span class="cm"&gt;     * &lt;/span&gt;
&lt;span class="cm"&gt;     * @param addressBook&lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AddressBook&lt;/span&gt; &lt;span class="n"&gt;addressBook&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// TODO Auto-generated method stub&lt;/span&gt;
        &lt;span class="c1"&gt;//Iterate over the address book&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Person&lt;/span&gt; &lt;span class="n"&gt;person&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;addressBook&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getPersonList&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Person ID: "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getId&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
            &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Person Name: "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getName&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hasEmail&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"E-mail address: "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getEmail&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;

            &lt;span class="c1"&gt;//Get phone numbers&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PhoneNumber&lt;/span&gt; &lt;span class="n"&gt;phoneNumber&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getPhoneList&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="k"&gt;switch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phoneNumber&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getType&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="n"&gt;MOBILE&lt;/span&gt;:
                    &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Mobile phone #: "&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
                    &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
                &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="n"&gt;HOME&lt;/span&gt;:
                    &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Home phone #: "&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
                    &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
                &lt;span class="k"&gt;case&lt;/span&gt; &lt;span class="n"&gt;WORK&lt;/span&gt;:
                    &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Work phone #: "&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
                    &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
                &lt;span class="p"&gt;}&lt;/span&gt;
                &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phoneNumber&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getNumber&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt;     * Add a person to the address book file&lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;addPerson&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// TODO Auto-generated method stub&lt;/span&gt;
        &lt;span class="n"&gt;Person&lt;/span&gt; &lt;span class="n"&gt;person&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Person&lt;/span&gt;
                &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;newBuilder&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
                &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setName&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"qingyuanxingsi"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setEmail&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"demo@server.com"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addPhone&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                        &lt;span class="n"&gt;PhoneNumber&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;newBuilder&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;setNumber&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"13456723421"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                                &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PhoneType&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MOBILE&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;()).&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;AddressBook&lt;/span&gt; &lt;span class="n"&gt;addressBook&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AddressBook&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;newBuilder&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;addPerson&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="c1"&gt;// System.out.println(person);&lt;/span&gt;
        &lt;span class="n"&gt;FileOutputStream&lt;/span&gt; &lt;span class="n"&gt;outputStream&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;try&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;outputStream&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;FileOutputStream&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FILE_PATH&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
            &lt;span class="n"&gt;addressBook&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;writeTo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputStream&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
            &lt;span class="n"&gt;outputStream&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FileNotFoundException&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="c1"&gt;// TODO Auto-generated catch block&lt;/span&gt;
            &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;printStackTrace&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="c1"&gt;// TODO Auto-generated catch block&lt;/span&gt;
            &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;printStackTrace&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Apache Avro&lt;sup id="sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-2-back"&gt;&lt;a class="simple-footnote" href="#sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-2" title="http://avro.apache.org/docs/current/gettingstartedjava.html"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Apache Avro 是Hadoop 下的一个子项目。它本身既是一个序列化框架,同时也实现了RPC 的功能。Avro官网描述Avro的特性和功能如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;丰富的数据结构类型;&lt;/li&gt;
&lt;li&gt;快速可压缩的二进制数据形式;&lt;/li&gt;
&lt;li&gt;存储持久数据的文件容器;&lt;/li&gt;
&lt;li&gt;提供远程过程调用 RPC;&lt;/li&gt;
&lt;li&gt;简单的动态语言结合功能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相比于Apache Thrift和Google Protocol Buffers,Apache Avro具有以下特点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持动态模式 。Avro 不需要生成代码,这有利于搭建通用的数据处理系统,同时避免了代码入侵。&lt;/li&gt;
&lt;li&gt;数据无须加标签 。读取数据前,Avro能够获取模式定义,这使得Avro在数据编码时只需要保留更少的类型信息,有利于减少序列化后的数据大小。&lt;/li&gt;
&lt;li&gt;无须手工分配的域标识。Thrift 和 Protocol Buffers使用一个用户添加的整型域唯一性定义一个字段,而Avro则直接使用域名,该方法更加直观、更加易扩展。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;编写一个 Avro 应用也需如下三步:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;定义消息格式文件,通常以 avro 作为扩展名;&lt;/li&gt;
&lt;li&gt;使用Avro编译器生成特定语言的代码文件(可选);&lt;/li&gt;
&lt;li&gt;使用Avro库提供的 API 来编写应用程序。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;定义消息格式文件&lt;/h4&gt;
&lt;p&gt;Avro schema是用JSON定义的。它有基本数据类型(null, boolean, int, long, float, double, bytes, string)以及复合数据类型(record, enum, array, map, union, and fixed)组成。以下给出一个具体实例user.avsc:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"namespace"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"example.avro"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"record"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"User"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;"fields"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"string"&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"favorite_number"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"int"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"null"&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;"name"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;"favorite_color"&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"type"&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;"string"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;"null"&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;Compiling the schema(Optional)&lt;/h4&gt;
&lt;p&gt;同Protocol Buffer一样,Apache Avro可以根据Scheme定义自动生成JAVA类。我们可以通过avro-tools工具生成相应的类: &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nb"&gt;java&lt;/span&gt; &lt;span class="na"&gt;-jar&lt;/span&gt; &lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nb"&gt;path&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="k"&gt;to&lt;/span&gt;&lt;span class="p"&gt;/&lt;/span&gt;&lt;span class="nx"&gt;avro&lt;/span&gt;&lt;span class="na"&gt;-tools&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.7.6&lt;/span&gt;&lt;span class="bp"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;jar&lt;/span&gt; &lt;span class="nb"&gt;compile&lt;/span&gt; &lt;span class="nx"&gt;schema&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;schema&lt;/span&gt; &lt;span class="nb"&gt;file&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;destination&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;就我们的例子而言,可以通过如下命令生成对应的JAVA类:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;java&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;avro&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;1.7.6&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jar&lt;/span&gt; &lt;span class="n"&gt;compile&lt;/span&gt; &lt;span class="n"&gt;schema&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;avsc&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;生成的类结构如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="User Structure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/User_zps8e56f737.png"&gt;&lt;/p&gt;
&lt;h4&gt;序列化与反序列化&lt;/h4&gt;
&lt;p&gt;当我们直接使用生成的JAVA类时,可通过如下方法持久化一个User:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// Serialize user user1 to disk&lt;/span&gt;
&lt;span class="n"&gt;File&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"user.avro"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;DatumWriter&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;User&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;userDatumWriter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;SpecificDatumWriter&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;User&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;User&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;DataFileWriter&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;User&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;dataFileWriter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;DataFileWriter&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;User&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;userDatumWriter&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;dataFileWriter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getSchema&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"users.avro"&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="n"&gt;dataFileWriter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;dataFileWriter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;我们首先创建一个&lt;code&gt;DatumWriter&lt;/code&gt;, 它负责将JAVA对象转化成内存中的序列化格式;SpecificDatumWriter类则与具体的生成类相关并从指定类中抽取Schema定义信息。 然后我们创建一个&lt;code&gt;DataFileWriter&lt;/code&gt;,它将内存中的持久化对象以及Schema定义持久化到文件中。 最后,我们通过其append方法添加一个User实例。&lt;/p&gt;
&lt;p&gt;当我们要反序列化时,则可以使用如下方法:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// Deserialize Users from disk&lt;/span&gt;
&lt;span class="n"&gt;DatumReader&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;User&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;userDatumReader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;SpecificDatumReader&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;User&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;User&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;DataFileReader&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;User&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;dataFileReader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;DataFileReader&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;User&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;userDatumReader&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;User&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataFileReader&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hasNext&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="c1"&gt;// Reuse user object by passing it to next(). This saves us from&lt;/span&gt;
&lt;span class="c1"&gt;// allocating and garbage collecting many objects for files with&lt;/span&gt;
&lt;span class="c1"&gt;// many items.&lt;/span&gt;
&lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataFileReader&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;反序列化与序列化基本相同,我们首先创建一个SpecificDatumReader,它将内存中的对象反序列化为我们的User实例,我们将DatumReader以及之前创建的文件传给DataFileReader,它从文件中读取信息。接着我们使用DataFileReader遍历已经序列化的User然后将反序列化之后的信息打印到标准输出流。&lt;/p&gt;
&lt;p&gt;当然,正如我们上述介绍Avro的特性时提到的那样,其实Avro可以不必生成对应的JAVA类,也就是说我们可以仅依靠Scheme定义直接序列化、反序列化对象。&lt;/p&gt;
&lt;p&gt;我们可以通过如下方法创建User实例:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;//Read Scheme definition and create a Schema object&lt;/span&gt;
&lt;span class="n"&gt;Schema&lt;/span&gt; &lt;span class="n"&gt;schema&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Parser&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;parse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"user.avsc"&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;

&lt;span class="c1"&gt;//Create user using above Schema &lt;/span&gt;
&lt;span class="n"&gt;GenericRecord&lt;/span&gt; &lt;span class="n"&gt;user1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;GenericData&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Record&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;user1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"Alyssa"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;user1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"favorite_number"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;256&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="c1"&gt;// Leave favorite color null&lt;/span&gt;

&lt;span class="n"&gt;GenericRecord&lt;/span&gt; &lt;span class="n"&gt;user2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;GenericData&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Record&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;user2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"name"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"Ben"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;user2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"favorite_number"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;7&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;user2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;put&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"favorite_color"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"red"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;接着,我们可以以如下方式序列化User实例:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// Serialize user1 and user2 to disk&lt;/span&gt;
&lt;span class="n"&gt;File&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;File&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"users.avro"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;DatumWriter&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;datumWriter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;GenericDatumWriter&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;DataFileWriter&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;dataFileWriter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;DataFileWriter&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;datumWriter&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;dataFileWriter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;dataFileWriter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;dataFileWriter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user2&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;dataFileWriter&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;以上序列化过程基本和使用Code Generation时相同。&lt;/p&gt;
&lt;p&gt;最后,我们反序列化如上对象,当然,和之前Code Generation时也类似:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// Deserialize users from disk&lt;/span&gt;
&lt;span class="n"&gt;DatumReader&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;datumReader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;GenericDatumReader&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;DataFileReader&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;dataFileReader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;DataFileReader&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;GenericRecord&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;datumReader&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;GenericRecord&lt;/span&gt; &lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataFileReader&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hasNext&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="c1"&gt;// Reuse user object by passing it to next(). This saves us from&lt;/span&gt;
&lt;span class="c1"&gt;// allocating and garbage collecting many objects for files with&lt;/span&gt;
&lt;span class="c1"&gt;// many items.&lt;/span&gt;
&lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataFileReader&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;next&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;以下给出一完整实例:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;package&lt;/span&gt; &lt;span class="nx"&gt;org.qingyuanxingsi.avrodemo&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;java.io.File&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;java.io.IOException&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.avro.Schema&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.avro.Schema.Parser&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.avro.file.DataFileReader&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.avro.file.DataFileWriter&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.avro.generic.GenericData&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.avro.generic.GenericRecord&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.avro.io.DatumReader&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.avro.io.DatumWriter&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.avro.specific.SpecificDatumReader&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.avro.specific.SpecificDatumWriter&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;example.avro.User&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt; * A toy example demonstrates the use of Apache Avro&lt;/span&gt;
&lt;span class="cm"&gt; * &lt;/span&gt;
&lt;span class="cm"&gt; * @author qingyuanxingsi&lt;/span&gt;
&lt;span class="cm"&gt; * @version 1.0&lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;
&lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;AvroDemo&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;// File Path storing demo data&lt;/span&gt;
    &lt;span class="k"&gt;private&lt;/span&gt; &lt;span class="nf"&gt;static&lt;/span&gt; &lt;span class="nx"&gt;final&lt;/span&gt; &lt;span class="kt"&gt;String&lt;/span&gt; &lt;span class="n"&gt;FILE_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"user.dat"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt;     * Test Case&lt;/span&gt;
&lt;span class="cm"&gt;     * &lt;/span&gt;
&lt;span class="cm"&gt;     * @param args&lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;
    &lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;static&lt;/span&gt; &lt;span class="bp"&gt;void&lt;/span&gt; &lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;String&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; args) {
        //Avro with code generation
        avroWithCode();
        //Avro without code generation
        avroWithoutCode();

    }

    /**
     * Avro without code generation
     */
    private static void avroWithoutCode() {
        // TODO Auto-generated method stub
        System.out.println("-----------------------------");
        System.out.println("2.Avro without code generation!");
        System.out.println("-----------------------------");
        Parser parser = new Parser();
        try {
            Schema schema = parser.parse(new File("user.avsc"));
            GenericRecord user = new GenericData.Record(schema);
            user.put("name", "qingyuanxingsi");
            user.put("favorite_number", 6);
            user.put("favorite_color", "BLUE");
            // Serialize it
            File file = new File(FILE_PATH);
            DatumWriter&lt;span class="nt"&gt;&amp;lt;GenericRecord&amp;gt;&lt;/span&gt; userDatumWriter = new SpecificDatumWriter&lt;span class="nt"&gt;&amp;lt;GenericRecord&amp;gt;&lt;/span&gt;(
                    schema);
            DataFileWriter&lt;span class="nt"&gt;&amp;lt;GenericRecord&amp;gt;&lt;/span&gt; dataFileWriter = new DataFileWriter&lt;span class="nt"&gt;&amp;lt;GenericRecord&amp;gt;&lt;/span&gt;(
                    userDatumWriter);
            try {
                dataFileWriter.create(schema, file);
                dataFileWriter.append(user);
                dataFileWriter.close();
            } catch (IOException e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            }

            // Deserialize Users from disk
            DatumReader&lt;span class="nt"&gt;&amp;lt;GenericRecord&amp;gt;&lt;/span&gt; userDatumReader = new SpecificDatumReader&lt;span class="nt"&gt;&amp;lt;GenericRecord&amp;gt;&lt;/span&gt;(
                    schema);
            DataFileReader&lt;span class="nt"&gt;&amp;lt;GenericRecord&amp;gt;&lt;/span&gt; dataFileReader;
            try {
                dataFileReader = new DataFileReader&lt;span class="nt"&gt;&amp;lt;GenericRecord&amp;gt;&lt;/span&gt;(file, userDatumReader);
                GenericRecord tmp = null;
                while (dataFileReader.hasNext()) {
                    // Reuse user object by passing it to next(). This saves us from
                    // allocating and garbage collecting many objects for files with
                    // many items.
                    tmp = dataFileReader.next(tmp);
                    System.out.println(tmp);
                }
            } catch (IOException e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            }
        } catch (IOException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
    }

    /**
     * Avro with code generation
     */
    private static void avroWithCode() {
        // TODO Auto-generated method stub
        System.out.println("-----------------------------");
        System.out.println("1.Avro with code generation!");
        System.out.println("-----------------------------");
        User user = User.newBuilder().setName("qingyuanxingsi")
                .setFavoriteNumber(5).setFavoriteColor("PURPLE").build();
        // Serialize it
        File file = new File(FILE_PATH);
        DatumWriter&lt;span class="nt"&gt;&amp;lt;User&amp;gt;&lt;/span&gt; userDatumWriter = new SpecificDatumWriter&lt;span class="nt"&gt;&amp;lt;User&amp;gt;&lt;/span&gt;(
                User.class);
        DataFileWriter&lt;span class="nt"&gt;&amp;lt;User&amp;gt;&lt;/span&gt; dataFileWriter = new DataFileWriter&lt;span class="nt"&gt;&amp;lt;User&amp;gt;&lt;/span&gt;(
                userDatumWriter);
        try {
            dataFileWriter.create(user.getSchema(), file);
            dataFileWriter.append(user);
            dataFileWriter.close();
        } catch (IOException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }

        // Deserialize Users from disk
        DatumReader&lt;span class="nt"&gt;&amp;lt;User&amp;gt;&lt;/span&gt; userDatumReader = new SpecificDatumReader&lt;span class="nt"&gt;&amp;lt;User&amp;gt;&lt;/span&gt;(
                User.class);
        DataFileReader&lt;span class="nt"&gt;&amp;lt;User&amp;gt;&lt;/span&gt; dataFileReader;
        try {
            dataFileReader = new DataFileReader&lt;span class="nt"&gt;&amp;lt;User&amp;gt;&lt;/span&gt;(file, userDatumReader);
            User tmp = null;
            while (dataFileReader.hasNext()) {
                // Reuse user object by passing it to next(). This saves us from
                // allocating and garbage collecting many objects for files with
                // many items.
                tmp = dataFileReader.next(tmp);
                System.out.println(tmp);
            }
        } catch (IOException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;也说动态代理&lt;sup id="sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-3-back"&gt;&lt;a class="simple-footnote" href="#sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-3" title="参考java动态代理(JDK和cglib)"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;Hadoop RPC中设计的基本设计模式就是代理模式,因此想要看懂RPC部分源码，代理不可不知。&lt;/p&gt;
&lt;p&gt;代理模式是常用的java设计模式，他的特征是代理类与委托类有同样的接口，代理类主要负责为委托类预处理消息、过滤消息、把消息转发给委托类，以及事后处理消息等。代理类与委托类之间通常会存在关联关系，一个代理类的对象与一个委托类的对象关联，代理类的对象本身并不真正实现服务，而是通过调用委托类的对象的相关方法，来提供特定的服务。 &lt;/p&gt;
&lt;p&gt;按照代理的创建时期，代理类可以分为两种。 &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;静态代理：由程序员创建或特定工具自动生成源代码，再对其编译。在程序运行前，代理类的.class文件就已经存在了。 &lt;/li&gt;
&lt;li&gt;动态代理：在程序运行时，运用反射机制动态创建而成。 &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;静态代理的基本原理我们就不说了,简而言之就是代理类和委托类实现相同的借口,Client通过代理类调用实现类的方法(代理类持有委托类的实例)。然后这样做有一个很明显的弊端，那就是每一个代理类只能为一个接口服务，这样一来程序开发中必然会产生过多的代理，而且，所有的代理操作除了调用的方法不一样之外，其他的操作都一样，则此时肯定是重复代码。解决这一问题最好的做法是可以通过一个代理类完成全部的代理功能，那么此时就必须使用动态代理完成。动态代理类的字节码在程序运行时由Java反射机制动态生成，无需程序员手工编写它的源代码。动态代理类不仅简化了编程工作，而且提高了软件系统的可扩展性，因为Java反射机制可以生成任意类型的动态代理类。&lt;code&gt;java.lang.reflect&lt;/code&gt;包中的Proxy类和InvocationHandler 接口提供了生成动态代理类的能力。 &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;InvocationHandler&lt;/span&gt;&lt;span class="err"&gt;接口：&lt;/span&gt;

&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;interface&lt;/span&gt; &lt;span class="n"&gt;InvocationHandler&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; 
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Method&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;Object&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;Throwable&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; 
&lt;span class="p"&gt;}&lt;/span&gt; 
&lt;span class="err"&gt;参数说明：&lt;/span&gt; 
&lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="err"&gt;：指被代理的对象。&lt;/span&gt; 
&lt;span class="n"&gt;Method&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="err"&gt;：要调用的方法&lt;/span&gt; 
&lt;span class="n"&gt;Object&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="err"&gt;：方法调用时所需要的参数&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;可以将InvocationHandler接口的子类想象成一个代理的最终操作类，替换掉ProxySubject。 &lt;/p&gt;
&lt;p&gt;Proxy类是专门完成代理的操作类，可以通过此类为一个或多个接口动态地生成实现类，此类提供了如下的操作方法： &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="kr"&gt;public&lt;/span&gt; &lt;span class="kr"&gt;static&lt;/span&gt; &lt;span class="nb"&gt;Object&lt;/span&gt; &lt;span class="nx"&gt;newProxyInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;ClassLoader&lt;/span&gt; &lt;span class="nx"&gt;loader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;Class&lt;/span&gt;&lt;span class="cp"&gt;&amp;lt;?&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="nx"&gt;interfaces&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
&lt;span class="nx"&gt;InvocationHandler&lt;/span&gt; &lt;span class="nx"&gt;h&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;throws&lt;/span&gt; &lt;span class="nx"&gt;IllegalArgumentException&lt;/span&gt; 
&lt;span class="err"&gt;参数说明：&lt;/span&gt; 
&lt;span class="nx"&gt;ClassLoader&lt;/span&gt; &lt;span class="nx"&gt;loader&lt;/span&gt;&lt;span class="err"&gt;：类加载器&lt;/span&gt; 
&lt;span class="nx"&gt;Class&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;?&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="nx"&gt;interfaces&lt;/span&gt;&lt;span class="err"&gt;：得到全部的接口&lt;/span&gt; 
&lt;span class="nx"&gt;InvocationHandler&lt;/span&gt; &lt;span class="nx"&gt;h&lt;/span&gt;&lt;span class="err"&gt;：得到&lt;/span&gt;&lt;span class="nx"&gt;InvocationHandler&lt;/span&gt;&lt;span class="err"&gt;接口的子类实例&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;NOTE:类加载器 
在Proxy类中的newProxyInstance（）方法中需要一个ClassLoader类的实例，ClassLoader实际上对应的是类加载器，在Java中主要有以下三种类加载器; &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Booststrap ClassLoader：此加载器采用C++编写，一般开发中是看不到的； &lt;/li&gt;
&lt;li&gt;Extension ClassLoader：用来进行扩展类的加载，一般对应的是jre\lib\ext目录中的类; &lt;/li&gt;
&lt;li&gt;AppClassLoader：(默认)加载classpath指定的类，是最常使用的是一种加载器。 &lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;以下给出一具体实例:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;//BookFacade.java,define interfaces&lt;/span&gt;
&lt;span class="kn"&gt;package&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;proxypattern&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;interface&lt;/span&gt; &lt;span class="n"&gt;BookFacade&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;//Add book interface&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;void&lt;/span&gt; &lt;span class="n"&gt;addBook&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;  
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;//BookFacadeImpl.java,具体实现类&lt;/span&gt;
&lt;span class="kn"&gt;package&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;proxypattern&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;BookFacadeImpl&lt;/span&gt; &lt;span class="n"&gt;implements&lt;/span&gt; &lt;span class="n"&gt;BookFacade&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

    &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="n"&gt;Override&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;void&lt;/span&gt; &lt;span class="n"&gt;addBook&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// TODO Auto-generated method stub&lt;/span&gt;
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Adding books to the database!"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;//BookFacadeProxy.java,生成动态代理&lt;/span&gt;
&lt;span class="kn"&gt;package&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;proxypattern&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reflect&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InvocationHandler&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reflect&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Method&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reflect&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Proxy&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;BookFacadeProxy&lt;/span&gt; &lt;span class="n"&gt;implements&lt;/span&gt; &lt;span class="n"&gt;InvocationHandler&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;private&lt;/span&gt; &lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="k"&gt;bind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="c1"&gt;//Get the proxy object&lt;/span&gt;
        &lt;span class="c1"&gt;//Interfaces must be bound,this is one drawback(which can be fixed by cglib)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Proxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;newProxyInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getClass&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;getClassLoader&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getClass&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;getInterfaces&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="n"&gt;Override&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Method&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Object&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;Throwable&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// TODO Auto-generated method stub&lt;/span&gt;
        &lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Transaction begins!"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="c1"&gt;//Execute the method&lt;/span&gt;
        &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;invoke&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Transaction ends!"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;//TestProxy.java,test case&lt;/span&gt;
&lt;span class="kn"&gt;package&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;proxypattern&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;TestProxy&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;void&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// TODO Auto-generated method stub&lt;/span&gt;
        &lt;span class="n"&gt;BookFacadeProxy&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;BookFacadeProxy&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;BookFacade&lt;/span&gt; &lt;span class="n"&gt;bookProxy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BookFacade&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;bind&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;BookFacadeImpl&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
        &lt;span class="n"&gt;bookProxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addBook&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;但是，JDK的动态代理依靠接口实现，如果有些类并没有实现接口，则不能使用JDK代理，这就要使用cglib动态代理了。JDK的动态代理机制只能代理实现了接口的类，而不能实现接口的类就不能实现JDK的动态代理，cglib是针对类来实现代理的，他的原理是对指定的目标类生成一个子类，并覆盖其中方法实现增强，但因为采用的是继承，所以不能对final修饰的类进行代理。具体示例如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;//BookFacadeImpl1.java,没有实现接口的实现类&lt;/span&gt;
&lt;span class="kn"&gt;package&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;battier&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dao&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;impl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="cm"&gt;/** &lt;/span&gt;
&lt;span class="cm"&gt; * 这个是没有实现接口的实现类 &lt;/span&gt;
&lt;span class="cm"&gt; *  &lt;/span&gt;
&lt;span class="cm"&gt; * @author student &lt;/span&gt;
&lt;span class="cm"&gt; *  &lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;  
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;BookFacadeImpl1&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;  
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;void&lt;/span&gt; &lt;span class="n"&gt;addBook&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;  
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"增加图书的普通方法..."&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;  
    &lt;span class="p"&gt;}&lt;/span&gt;  
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;//BookfacadeCglib.java,使用cglib实现动态代理&lt;/span&gt;
&lt;span class="kn"&gt;package&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;battier&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;java&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lang&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reflect&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Method&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;net&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cglib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Enhancer&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;net&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cglib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MethodInterceptor&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;net&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cglib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MethodProxy&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="cm"&gt;/** &lt;/span&gt;
&lt;span class="cm"&gt; * 使用cglib动态代理 &lt;/span&gt;
&lt;span class="cm"&gt; *  &lt;/span&gt;
&lt;span class="cm"&gt; * @author student &lt;/span&gt;
&lt;span class="cm"&gt; *  &lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;  
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;BookFacadeCglib&lt;/span&gt; &lt;span class="n"&gt;implements&lt;/span&gt; &lt;span class="n"&gt;MethodInterceptor&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;  
    &lt;span class="n"&gt;private&lt;/span&gt; &lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="cm"&gt;/** &lt;/span&gt;
&lt;span class="cm"&gt;     * 创建代理对象 &lt;/span&gt;
&lt;span class="cm"&gt;     *  &lt;/span&gt;
&lt;span class="cm"&gt;     * @param target &lt;/span&gt;
&lt;span class="cm"&gt;     * @return &lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;  
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;getInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;  
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;  
        &lt;span class="n"&gt;Enhancer&lt;/span&gt; &lt;span class="n"&gt;enhancer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Enhancer&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;  
        &lt;span class="n"&gt;enhancer&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setSuperclass&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getClass&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;  
        &lt;span class="c1"&gt;// 回调方法  &lt;/span&gt;
        &lt;span class="n"&gt;enhancer&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setCallback&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;  
        &lt;span class="c1"&gt;// 创建代理对象  &lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;enhancer&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;  
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="n"&gt;Override&lt;/span&gt;  
    &lt;span class="c1"&gt;// 回调方法  &lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;intercept&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Object&lt;/span&gt; &lt;span class="n"&gt;obj&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Method&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Object&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  
            &lt;span class="n"&gt;MethodProxy&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;Throwable&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;  
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"事物开始"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;  
        &lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;invokeSuper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;obj&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;  
        &lt;span class="n"&gt;System&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"事物结束"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;  
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;


    &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;//TestCglib.java,a test case&lt;/span&gt;
&lt;span class="kn"&gt;package&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;battier&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;net&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;battier&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dao&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;impl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BookFacadeImpl1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;  
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;net&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;battier&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BookFacadeCglib&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;TestCglib&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="k"&gt;void&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;  
        &lt;span class="n"&gt;BookFacadeCglib&lt;/span&gt; &lt;span class="n"&gt;cglib&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;BookFacadeCglib&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;  
        &lt;span class="n"&gt;BookFacadeImpl1&lt;/span&gt; &lt;span class="n"&gt;bookCglib&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BookFacadeImpl1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;cglib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;BookFacadeImpl1&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;  
        &lt;span class="n"&gt;bookCglib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;addBook&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;  
    &lt;span class="p"&gt;}&lt;/span&gt;  
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Dive Into Hadoop RPC&lt;/h2&gt;
&lt;p&gt;好吧,具备了以上两个基础知识后,我们可以一睹Hadoop RPC的真面目了。&lt;/p&gt;
&lt;h3&gt;RPC通信模型&lt;/h3&gt;
&lt;p&gt;RPC 是一种通过网络从远程计算机上请求服务,但不需要了解底层网络技术的协议。RPC协议假定某些传输协议(如TCP或UDP等)已经存在,并通过这些传输协议为通信程序之间传递访问请求或者应答信息。在OSI网络通信模型中,RPC跨越了传输层和应用层。RPC使得开发分布式应用程序更加容易。RPC通常采用客户机 / 服务器模型。请求程序是一个客户机,而服务提供程序则是一个服务器。一个典型的RPC框架如下图所示,主要包括以下几个部分:&lt;/p&gt;
&lt;p&gt;&lt;img alt="RPC ARCHITECTURE" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/rpc_structure_zpsf7959ccf.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通信模块。两个相互协作的通信模块实现请求-应答协议,它们在客户和服务器之间传递请求和应答消息,一般不会对数据包进行任何处理。请求–应答协议的实现方式有同步方式和异步方式两种。如下图所示,同步模式下客户端程序一直阻塞到服务器端发送的应答请求到达本地;
而异步模式不同,客户端将请求发送到服务器端后,不必等待应答返回,可以做其他事情,待服务器端处理完请求后,主动通知客户端。在高并发应用场景中,一般采用异步模式以降低访问延迟和提高带宽利用率。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="通信模式" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/rpc_talk_zps10f24c93.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stub 程序。客户端和服务器端均包含Stub程序,可将之看做代理程序。它使得远程函数调用表现得跟本地调用一样,对用户程序完全透明。在客户端,它表现得就像一个本地程序,但不直接执行本地调用,而是将请求信息通过网络模块发送给服务器端。此外,当服务器发送应答后,它会解码对应结果。在服务器端,Stub程序依次进行解码请求消息中的参数、调用相应的服务过程和编码应答结果的返回值等处理。&lt;/li&gt;
&lt;li&gt;调度程序。调度程序接收来自通信模块的请求消息,并根据其中的标识选择一个Stub程序进行处理。通常客户端并发请求量比较大时,会采用线程池提高处理效率。&lt;/li&gt;
&lt;li&gt;客户程序/服务过程。请求的发出者和请求的处理者。如果是单机环境,客户程序可直接通过函数调用访问服务过程,但在分布式环境下,需要考虑网络通信,这不得增加通信模块和Stub程序(保证函数调用的透明性)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通常而言,一个 RPC 请求从发送到获取处理结果,所经历的步骤如下所示。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;客户程序以本地方式调用系统产生的Stub程序;&lt;/li&gt;
&lt;li&gt;该Stub程序将函数调用信息按照网络通信模块的要求封装成消息包,并交给通信模块发送到远程服务器端。&lt;/li&gt;
&lt;li&gt;远程服务器端接收此消息后,将此消息发送给相应的Stub程序;&lt;/li&gt;
&lt;li&gt;Stub程序拆封消息,形成被调过程要求的形式,并调用对应函数;&lt;/li&gt;
&lt;li&gt;被调用函数按照所获参数执行,并将结果返回给Stub程序;&lt;/li&gt;
&lt;li&gt;Stub程序将此结果封装成消息,通过网络通信模块逐级地传送给客户程序。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;RPC总体架构&lt;/h3&gt;
&lt;p&gt;同其他RPC框架一样,Hadoop RPC主要分为四个部分,分别是序列化层、函数调用层、网络传输层和服务器端处理框架,具体实现机制如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;序列化层。序列化主要作用是将结构化对象转为字节流以便于通过网络进行传输或写入持久存储,在RPC框架中,它主要用于将用户请求中的参数或者应答转化成字节流以便跨机器传输。前面介绍的Protocol Buffers和Apache Avro均可用在序列化层,Hadoop本身也提供了一套序列化框架,一个类只要实现Writable接口即可支持对象序列化与反序列化。&lt;/li&gt;
&lt;li&gt;函数调用层。函数调用层主要功能是定位要调用的函数并执行该函数,Hadoop RPC采用了Java反射机制与动态代理实现了函数调用。&lt;/li&gt;
&lt;li&gt;网络传输层。网络传输层描述了Client与Server之间消息传输的方式,Hadoop RPC采用了基于TCP/IP的Socket机制。&lt;/li&gt;
&lt;li&gt;服务器端处理框架。服务器端处理框架可被抽象为网络I/O模型,它描述了客户端与服务器端间信息交互方式,它的设计直接决定着服务器端的并发处理能力,常见的网络 I/O 模型有阻塞式 I/O、非阻塞式 I/O、事件驱动 I/O 等,而Hadoop RPC采用了基于&lt;strong&gt;Reactor设计模式&lt;/strong&gt;的事件驱动 I/O 模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hadoop RPC 总体架构如下图所示,自下而上可分为两层,第一层是一个基于Java NIO (New I/O)实现的客户机–服务器(C/S)通信模型。其中,客户端将用户的调用方法及其参数封装成请求包后发送到服务器端。服务器端收到请求包后,经解包、调用函数、打包结果等一系列操作后,将结果返回给客户端。为了增强Sever端的扩展性和并发处理能力,Hadoop RPC采用了基于事件驱动的Reactor设计模式,在具体实现时,用到了JDK提供的各种功能包,主要包括java.nio(NIO)、java.lang.reflect(反射机制和动态代理)、java.net(网络编程库)等。第二层是供更上层程序直接调用的 RPC 接口,这些接口底层即为C/S通信模型。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hadoop RPC总体架构" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/whole_rpc_zpsddc95cd6.png"&gt;&lt;/p&gt;
&lt;h3&gt;Hadoop RPC使用方法&lt;/h3&gt;
&lt;p&gt;Hadoop RPC 对外主要提供了两种接口(见类 org.apache.hadoop.ipc.RPC),分别是:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;//Construct a client proxy instance for sending RPC requests to server&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="no"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;ProtocolProxy&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="no"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;getProxy&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;waitForProxy&lt;/span&gt;&lt;span class="p"&gt;(...)&lt;/span&gt;

&lt;span class="c1"&gt;//Build a server instance for a specific protocol,handle requests&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;Server&lt;/span&gt; &lt;span class="no"&gt;RPC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Builder&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Configuration&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;通常而言,使用Hadoop RPC可分为以下4个步骤。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;定义 RPC 协议
RPC协议是客户端和服务器端之间的通信接口,它定义了服务器端对外提供的服务接口。如下所示,我们定义一个ClientProtocol通信接口,声明了echo()和add()两个方法。需要注意的是,Hadoop 中所有自定义 RPC 接口都需要继承VersionedProtocol接口,它描述了协议的版本信息。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;interface&lt;/span&gt; &lt;span class="n"&gt;ClientProtocol&lt;/span&gt; &lt;span class="n"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apache&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hadoop&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ipc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;VersionedProtocol&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;// 版本号,默认情况下,不同版本号的 RPC Client 和 Server 之间不能相互通信&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;final&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;versionID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1L&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;echo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;v1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;实现 RPC 协议
Hadoop RPC协议通常是一个Java 接口,用户需要实现该接口。对ClientProtocol接口进行简单的实现如下所示:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="n"&gt;class&lt;/span&gt; &lt;span class="n"&gt;ClientProtocolImpl&lt;/span&gt; &lt;span class="n"&gt;implements&lt;/span&gt; &lt;span class="n"&gt;ClientProtocol&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

    &lt;span class="c1"&gt;// 重载的方法,用于获取自定义的协议版本号,&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;getProtocolVersion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;clientVersion&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ClientProtocol&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;versionID&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="c1"&gt;// 重载的方法,用于获取协议签名&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;ProtocolSignature&lt;/span&gt; &lt;span class="n"&gt;getProtocolSignature&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;clientVersion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;inthashcode&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;ProtocolSignature&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ClientProtocol&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;versionID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;null&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;echo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;v1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;v1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;v2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;构造并启动 RPC Server
直接使用静态类Builder构造一个RPC Server,并调用函数start()启动该Server:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Server&lt;/span&gt; &lt;span class="n"&gt;server&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;RPC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Builder&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;setProtocol&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ClientProtocol&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setInstance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;ClientProtocolImpl&lt;/span&gt;&lt;span class="p"&gt;()).&lt;/span&gt;&lt;span class="n"&gt;setBindAddress&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ADDRESS&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;setPort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setNumHandlers&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;build&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;其中,BindAddress(由函数setBindAddress设置)和Port(由函数setPort设置,0表示由系统随机选择一个端口号)分别表示服务器的host和监听端口号,而 NnumHandlers(由函数setNumHandlers设置)表示服务器端处理请求的线程数目。到此为止,服务器处理监听状态,等待客户端请求到达。
4. 构造 RPC Client 并发送 RPC 请求
使用静态方法getProxy构造客户端代理对象,直接通过代理对象调用远程端的方法,具体如下所示:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;proxy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ClientProtocol&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;RPC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getProxy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ClientProtocol&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ClientProtocol&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;versionID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;addr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;echoResult&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;echo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"result"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;经过以上四步,我们便利用Hadoop RPC搭建了一个非常高效的客户机–服务器网络模型。接下来,我们将深入到Hadoop RPC内部,剖析它的设计原理及技巧。&lt;/p&gt;
&lt;h3&gt;Hadoop RPC类详解&lt;/h3&gt;
&lt;p&gt;Hadoop RPC主要由三个大类组成,即RPC、Client和Server,分别对应对外编程接口、客户端实现和服务器实现。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ipc.RPC类分析
RPC类实际上是对底层客户机–服务器网络模型的封装,以便为程序员提供一套更方便简洁的编程接口。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="RPC类图" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/rpc_class_zpsae0a03f1.png"&gt;&lt;/p&gt;
&lt;p&gt;如上图所示,RPC 类定义了一系列构建和销毁RPC客户端的方法,构建方法分为getProxy和waitForProxy两类,销毁方只有一个,即为stopProxy。RPC服务器的构建则由静态内部类RPC.Builder,该类提供了一些列setXxx 方法(Xxx 为某个参数名称)供用户设置一些基本的参数,比如RPC 协议、RPC协议实现对象、服务器绑定地址、端口号等,一旦设置完成这些参数后,可通过调用RPC.Builder.build()完成一个服务器对象的构建,之后直接调用 Server.start() 方法便可以启动该服务器。与Hadoop 1.x中的RPC仅支持基于Writable序列化方式不同,Hadoop2.x允许用户使用其他序列化框架,比如Protocol Buffers等,目前提供了 Writable(WritableRpcEngine)和Protocol Buffers(ProtobufRpcEngine)两种,默认实现是Writable方式,用户可通过调用RPC.setProtocolEngine(...)修改采用的序列化方式。&lt;/p&gt;
&lt;p&gt;下面以采用 Writable序列化为例(采用Protocol Buffers的过程类似),介绍Hadoop RPC的远程过程调用流程。Hadoop RPC使用了Java动态代理完成对远程方法的调用:用户只需实现java.lang.reflect.InvocationHandler接口,并按照自己需求实现invoke方法即可完成动态代理类对象上的方法调用。但对于HadoopRPC,函数调用由客户端发出,并在服务器端执行并返回,因此不能像单机程序那样直接在invoke方法中本地调用相关函数,它的做法是,在invoke方法中,将函数调用信息(函数名,函数参数列表等)打包成可序列化的WritableRpcEngine.Invocation 对象,并通过网络发送给服务器端,服务端收到该调用信息后,解析出和函数名,函数参数列表等信息,利用Java反射机制完成函数调用,期间涉及到的类关系如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Invocation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/invocation_zps5a294c7c.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ipc.Client
Client主要完成的功能是发送远程过程调用信息并接收执行结果。它涉及到的类关系如下图所示:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Client" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/client_zpsedc34497.png"&gt;&lt;/p&gt;
&lt;p&gt;Client类对外提供了一类执行远程调用的接口,这些接口的名称一样,仅仅是参数列表不同,比如其中一个的声明如下所示:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;Writable&lt;/span&gt; &lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Writable&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ConnectionIdremoteId&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;InterruptedException&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Client 内部有两个重要的内部类,分别是Call和Connection。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Call 类 :封装了一个RPC请求,它包含5个成员变量,分别是唯一标识id、函数调用信息param、函数执行返回值value、出错或者异常信息error和执行完成标识符done。由于Hadoop RPC Server采用异步方式处理客户端请求,这使远程过程调用的发生顺序与结果返回顺序无直接关系,而Client端正是通过id识别不同的函数调用的。当客户端向服务器端发送请求时,只需填充id和param两个变量,而剩下的3个变量(value、error和done)则由服务器端根据函数执行情况填充。&lt;/li&gt;
&lt;li&gt;Connection 类:Client与每个Server之间维护一个通信连接,与该连接相关的基本信息及操作被封装到Connection类中,基本信息主要包括通信连接唯一标识(remoteId)、与Server端通信的Socket(socket)、网络输入数据流(in)、网络输出数据流(out)、保存RPC请求的哈希表(calls)等。操作则包括:
❍ addCall—将一个Call对象添加到哈希表中;
❍ sendParam—向服务器端发送RPC请求;
❍ receiveResponse—从服务器端接收已经处理完成的RPC请求;
❍ run—Connection 是一个线程类,它的run方法调用了receiveResponse方法,会一直等待接收 RPC 返回结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当调用call函数执行某个远程方法时,Client端需要进行(如下图所示)以下4个步骤。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建一个Connection 对象,并将远程方法调用信息封装成Call对象,放到Connection对象中的哈希表中;&lt;/li&gt;
&lt;li&gt;调用 Connection 类中的sendRpcRequest()方法将当前Call对象发送给Server端;&lt;/li&gt;
&lt;li&gt;Server端处理完RPC请求后,将结果通过网络返回给Client端,Client端通过receiveRpcResponse()函数获取结果;&lt;/li&gt;
&lt;li&gt;Client检查结果处理状态(成功还是失败),并将对应 Call 对象从哈希表中删除。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Client Procedure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/client_procedure_zps4f94859e.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ipc.Server 类分析&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hadoop采用了Master/Slave 结构,其中Master是整个系统的单点,如NameNode或JobTracker ,这是制约系统性能和可扩展性的最关键因素之一 ;而Master通过ipc.Server接收并处理所有Slave发送的请求,这就要求ipc.Server将高并发和可扩展性作为设计目标为此,ipc.Server采用了很多提高并发处理能力的技术,主要包括线程池、事件驱动和Reactor设计模式等,这些技术均采用了JDK自带的库实现,这里重点分析它是如何利用Reactor设计模式提高整体性能的。HDFS的单点故障已经在Hadoop 2.0中得到了解决,MRv1中的JobTracker的单点故障在CDH4中也得到了解决。&lt;/p&gt;
&lt;p&gt;Reactor&lt;sup id="sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-4-back"&gt;&lt;a class="simple-footnote" href="#sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-4" title="如若时间允许,该设计模式会进行详尽的分析。"&gt;4&lt;/a&gt;&lt;/sup&gt;是并发编程中的一种基于事件驱动的设计模式,它具有以下两个特点:通过派发/分离I/O操作事件提高系统的并发性能;提供了粗粒度的并发控制,使用单线程实现,避免了复杂的同步处理。典型的Reactor实现原理如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Reactor Pattern" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/reactor_pattern_zps5fc183a4.png"&gt;&lt;/p&gt;
&lt;p&gt;典型的Reactor模式中主要包括以下几个角色。&lt;/p&gt;
&lt;p&gt;❑ Reactor:I/O事件的派发者。&lt;/p&gt;
&lt;p&gt;❑ Acceptor:接受来自Client的连接,建立与Client对应的Handler,并向Reactor注册此Handler。&lt;/p&gt;
&lt;p&gt;❑ Handler :与一个Client通信的实体,并按一定的过程实现业务的处理。Handler内部往往会有更进一步的层次划分,用来抽象诸如read、decode、compute、encode和send等过程。在Reactor模式中,业务逻辑被分散的I/O事件所打破,所以Handler需要有适当的机制在所需的信息还不全(读到一半)的时候保存上下文,并在下一次I/O事件到来的时候(另一半可读)能继续上次中断的处理。&lt;/p&gt;
&lt;p&gt;❑Reader/Sender:为了加速处理速度,Reactor模式往往构建一个存放数据处理线程的线程池,这样数据读出后,立即扔到线程池中等待后续处理即可。为此,Reactor模式一般分离Handler中的读和写两个过程,分别注册成单独的读事件和写事件,并由对应的Reader和Sender线程处理。&lt;/p&gt;
&lt;p&gt;ipc.Server实际上实现了一个典型的Reactor设计模式,其整体架构与上述完全一致。一旦读者了解典型 Reactor 架构便可很容易地学习 ipc.Server的设计思路及实现。接下来,我们分析ipc.Server的实现细节。&lt;/p&gt;
&lt;p&gt;前面提到,ipc.Server的主要功能是接收来自客户端的RPC 请求,经过调用相应的函数获取结果后,返回给对应的客户端。为此,ipc.Server 被划分成3个阶段:接收请求、处理请求和返回结果,如下图所示。各阶段实现细节如下。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Reactor Details" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/server_procedure_zpsb92c63d0.png"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;接收请求;该阶段主要任务是接收来自各个客户端的RPC请求,并将它们封装成固定的格式(Call类)放到一个共享队列(callQueue)中,以便进行后续处理。该阶段内部又分为建立连接和接收请求两个子阶段,分别由Listener和Reader两种线程完成。整个Server只有一个Listener线程,统一负责监听来自客户端的连接请求,一旦有新的请求到达,它会采用轮询的方式从线程池中选择一个Reader线程进行处理,而Reader线程可同时存在多个,它们分别负责接收一部分客户端连接的RPC请求,至于每个Reader线程负责哪些客户端连接,完全由Listener决定,当前Listener只是采用了简单的轮询分配机制。Listener和Reader线程内部各自包含一个Selector对象,分别用于监听SelectionKey.OP_ACCEPT和SelectionKey.OP_READ 事件。对于Listener线程,主循环的实现体是监听是否有新的连接请求到达,并采用轮询策略选择一个Reader线程处理新连接;对于Reader线程,主循环的实现体是监听(它负责的那部分)客户端连接中是否有新的RPC请求到达,并将新的RPC请求封装成Call对象,放到共享队列callQueue 中。&lt;/li&gt;
&lt;li&gt;处理请求;该阶段主要任务是从共享队列callQueue中获取Call对象,执行对应的函数调用,并将结果返回给客户端,这全部由Handler线程完成。Server 端可同时存在多个Handler线程,它们并行从共享队列中读取Call对象,经执行对应的函数调用后,将尝试着直接将结果返回给对应的客户端。但考虑到某些函数调用返回结果很大或者网络速度过慢,可能难以将结果一次性发送到客户端,此时Handler将尝试着将后续发送任务交给Responder线程。&lt;/li&gt;
&lt;li&gt;返回结果;前面提到,每个Handler线程执行完函数调用后,会尝试着将执行结果返回给客户端,但对于特殊情况,比如函数调用返回结果过大或者网络异常情况(网速过慢),会将发送任务交给Responder线程。Server端仅存在一个Responder线程,它的内部包含一个Selector对象,用于监听SelectionKey.OP_WRITE事件。当Handler没能将结果一次性发送到客户端时,会向该Selector对象注册SelectionKey.OP_WRITE事件,进而由Responder 线程采用异步方式继续发送未发送完成的结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Hadoop RPC流程剖析&lt;/h3&gt;
&lt;p&gt;以下我们对Hadoop Yarn的具体实现进行剖析。Hadoop YARN将RPC中的序列化部分剥离开,以便将现有的开源RPC框架集成进来。经过改进之后,Hadoop RPC的类关系如下图所示,RPC类变成了一个工厂,它将具体的RPC实现授权给RpcEngine实现类,而现有的开源RPC只要实现RpcEngine接口,便可以集成到Hadoop RPC中。在该图中,WritableRpcEngine是采用Hadoop自带的序列化框架实现的RPC,而AvroRpcEngine和ProtobufRpcEngine分别是开源RPC(或序列化)框架Apache Avro和Protocol Buffers对应的 RpcEngine 实现,用户可通过配置参数rpc.engine.{protocol}以指定协议 {protocol} 采用的序列化方式。需要注意的是,当前实现中,Hadoop RPC 只是采用了这些开源框架的序列化机制,底层的函数调用机制仍采用 Hadoop 自带的。YARN提供的对外类是YarnRPC,用户只需使用该类便可以构建一个基于Hadoop RPC且采用Protocol Buffers序列化框架的通信协议。YarnRPC相关实现类如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="RPC Factory" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/rpc_factory_zpsf645e84c.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="YarnRPC" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/yarn_rpc_zpse183e893.png"&gt;&lt;/p&gt;
&lt;p&gt;YarnRPC是一个抽象类,实际的实现由参数yarn.ipc.rpc.class指定,默认值是org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC。HadoopYarnProtoRPC 通过RPC工厂生成器(工厂设计模式)RpcFactoryProvider生成客户端工厂(由参数yarn.ipc.client.factory.class指定,默认值是 org.apache.hadoop.yarn.factories.impl.pb.RpcClientFactoryPBImpl)和服务器工厂(由参数yarn.ipc.server.factory.class指定,默认值 是org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl),以根据通信协议的Protocol Buffers定义生成客户端对象和服
务器对象。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RpcClientFactoryPBImpl : 根据通信协议接口(实际上就是一个Java interface)及Protocol Buffers定义构造RPC客户端句柄, 但它对通 信协议的存放位置和类名命有一定要求。假设通信协议接口Xxx所在Java包名为XxxPackage,则客户端实现代码必须位于Java包XxxPackage.impl.pb.client 中(在接口包名后面增加&lt;code&gt;.impl.pb.client&lt;/code&gt;), 且实现类名为PBClientImplXxx(在接口名前面增加前缀&lt;code&gt;PBClientImpl&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;RpcServerFactoryPBImpl :根据通信协议接口(实际上就是一个Java interface)及Protocol Buffers定义构造RPC服务器句柄(具体会调用前面节介绍的RPC.Server类),但它对通信协议的存放位置和类命名有一定要求。假设通信协议接口Xxx 所在Java 包名为 XxxPackage,则客户端实现代码必须位于Java包XxxPackage.impl.pb.server中(在接口包名后面增加&lt;code&gt;.impl.pb.server&lt;/code&gt;),且实现类名为PBServiceImplXxx(在接口名前面增加前缀&lt;code&gt;PBServiceImpl&lt;/code&gt;)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hadoop YARN 已将Protocol Buffers作为默认的序列化机制(而不是Hadoop自带的Writable),这带来的好处主要表现在以下几个方面:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;继承了 Protocol Buffers 的优势.Protocol Buffers已在实践中证明了其高效性、可扩展性、紧凑性和跨语言特性。首先,它允许在保持向后兼容性的前提下修改协议,比如为某个定义好的数据格式添加一个新的字段 ;其次,它支持多种语言,进而方便用户为某些服务(比如 HDFS 的 NameNode)编写非 Java 客户端 ;此外,实验
表明 Protocol Buffers 比 Hadoop 自带的 Writable 在性能方面有很大提升。&lt;/li&gt;
&lt;li&gt;支持升级回滚。Hadoop 2.0已经将 NameNode HA方案合并进来,在该方案中,Name-Node 分为Active和Standby两种角色,其中, Active NameNode 在当前对外提供服务,而Standby NameNode则是能够在Active NameNode出现故障时接替它。采用Protocol Buffers序列化机制后,管理员能够在不停止NameNode对外服务的前提下,通过主备NameNode之间的切换,依次对主备NameNode进行在线升级(不用考虑版本和协议兼容性等问题)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了进一步说明 YARN RPC 的使用方法,本小节给出一个具体的应用实例。
在 YARN 中,ResourceManager和NodeManager之间的通信协议是ResourceTracker,其中NodeManager是该协议的客户端,ResourceManager是服务端,NodeManager通过该协议中定义的两个RPC函数(registerNodeManager和nodeHeartbeat)向ResourceManager注册和周期性发送心跳信息。ResourceManager(服务器端)中的相关代码如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// ResourceTrackerService 实现了 ResourceTracker 通信接口,并启动 RPC Server&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="n"&gt;ResourceTrackerService&lt;/span&gt; &lt;span class="k"&gt;extends&lt;/span&gt; &lt;span class="n"&gt;AbstractService&lt;/span&gt; &lt;span class="n"&gt;implements&lt;/span&gt;
&lt;span class="n"&gt;ResourceTracker&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;private&lt;/span&gt; &lt;span class="n"&gt;Server&lt;/span&gt; &lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
    &lt;span class="k"&gt;protected&lt;/span&gt; &lt;span class="k"&gt;void&lt;/span&gt; &lt;span class="n"&gt;serviceStart&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;super&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;serviceStart&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;Configuration&lt;/span&gt; &lt;span class="n"&gt;conf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;getConfig&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;YarnRPC&lt;/span&gt; &lt;span class="n"&gt;rpc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;YarnRPC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// 使用 YarnRPC 类&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;server&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rpc&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getServer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ResourceTracker&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;resourceTrackerAddress&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;YarnConfiguration&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="no"&gt;RM_RESOURCE_TRACKER_CLIENT_THREAD_COUNT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;YarnConfiguration&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="no"&gt;DEFAULT_RM_RESOURCE_TRACKER_CLIENT_THREAD_COUNT&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;server&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
    &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="n"&gt;Override&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;RegisterNodeManagerResponse&lt;/span&gt; &lt;span class="n"&gt;registerNodeManager&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;RegisterNodeManagerRequest&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;YarnException&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// 具体实现&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="n"&gt;Override&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;NodeHeartbeatResponse&lt;/span&gt; &lt;span class="n"&gt;nodeHeartbeat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NodeHeartbeatRequest&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;YarnException&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="c1"&gt;// 具体实现&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;NodeManager(客户端)中的相关代码如下。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;//该函数是从YARN源代码中简单修改而来的&lt;/span&gt;
&lt;span class="k"&gt;protected&lt;/span&gt; &lt;span class="n"&gt;ResourceTracker&lt;/span&gt; &lt;span class="n"&gt;getRMClient&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;Configuration&lt;/span&gt; &lt;span class="n"&gt;conf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;getConfig&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;InetSocketAddress&lt;/span&gt; &lt;span class="n"&gt;rmAddress&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;getRMAddress&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;RetryPolicy&lt;/span&gt; &lt;span class="n"&gt;retryPolicy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;createRetryPolicy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;ResourceTracker&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RMProxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="no"&gt;T&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="n"&gt;getProxy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ResourceTracker&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rmAddress&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="no"&gt;LOG&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Connecting to ResourceManager at "&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;rmAddress&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ResourceTracker&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;RetryProxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;protocol&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;retryPolicy&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resourceTracker&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;getRMClient&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;RegisterNodeManagerResponse&lt;/span&gt; &lt;span class="n"&gt;regNMResponse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resourceTracker&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;registerNodeManager&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;resourceTracker&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nodeHeartbeat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;为了能够让以上代码正常工作,YARN 按照以下流程实现各种功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤1&lt;/strong&gt; 定义通信协议接口(Java Interface)。定义通信协议接口ResourceTracker,它包含registerNodeManager和nodeHeartbeat两个函数,且每个函数包含一个参数和一个返
回值,具体如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;interface&lt;/span&gt; &lt;span class="n"&gt;ResourceTracker&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;RegisterNodeManagerResponse&lt;/span&gt; &lt;span class="n"&gt;registerNodeManager&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;RegisterNodeManagerRequest&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;YarnException&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;NodeHeartbeatResponse&lt;/span&gt; &lt;span class="n"&gt;nodeHeartbeat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NodeHeartbeatRequest&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;YarnException&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;步骤2&lt;/strong&gt; 为通信协议ResourceTracker提供Protocol Buffers定义和Java实现。前面提到,Protocol Buffers仅提供了序列化框架,但未提供RPC实现,因此RPC部分需要由用户自己实现,而YARN 则让ResourceTrackerService 类实现了ResourceTracker协议,它的 Protocol Buffers 定义(具体见文件ResourceTracker.proto)如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;option&lt;/span&gt; &lt;span class="n"&gt;java_package&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"org.apache.hadoop.yarn.proto"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;option&lt;/span&gt; &lt;span class="n"&gt;java_outer_classname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"ResourceTracker"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;option&lt;/span&gt; &lt;span class="n"&gt;java_generic_services&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;option&lt;/span&gt; &lt;span class="n"&gt;java_generate_equals_and_hash&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="s"&gt;"yarn_server_common_service_protos.proto"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;service&lt;/span&gt; &lt;span class="n"&gt;ResourceTrackerService&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;rpc&lt;/span&gt; &lt;span class="n"&gt;registerNodeManager&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;RegisterNodeManagerRequestProto&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;returns&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;RegisterNode&lt;/span&gt;
    &lt;span class="n"&gt;ManagerResponseProto&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="n"&gt;rpc&lt;/span&gt; &lt;span class="nf"&gt;nodeHeartbeat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NodeHeartbeatRequestProto&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;returns&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NodeHeartbeatResponseProto&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;ResourceTracker的RPC函数实现是由ResourceManager中的ResourceTrackerService完成的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤3&lt;/strong&gt; 为RPC函数的参数和返回值提供Protocol Buffers定义。YARN需要保证每个RPC函数的参数和返回值是采用Protocol Buffers定义的,因此 ResourceTracker协议中RegisterNodeManagerRequest、RegisterNodeManagerResponse、NodeHeartbeatRequest 和
NodeHeartbeatResponse 四个参数或者返回值需要使用Protocol Buffers定义,具体如下(见yarn_server_common_service_protos.proto 文件):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="s"&gt;"yarn_protos.proto"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="s"&gt;"yarn_server_common_protos.proto"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;message&lt;/span&gt; &lt;span class="n"&gt;RegisterNodeManagerRequestProto&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;NodeIdProto&lt;/span&gt; &lt;span class="n"&gt;node_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;int32&lt;/span&gt; &lt;span class="n"&gt;http_port&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;ResourceProto&lt;/span&gt; &lt;span class="n"&gt;resource&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;message&lt;/span&gt; &lt;span class="n"&gt;RegisterNodeManagerResponseProto&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;MasterKeyProto&lt;/span&gt; &lt;span class="n"&gt;container_token_master_key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;MasterKeyProto&lt;/span&gt; &lt;span class="n"&gt;nm_token_master_key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;NodeActionProto&lt;/span&gt; &lt;span class="n"&gt;nodeAction&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;int64&lt;/span&gt; &lt;span class="n"&gt;rm_identifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;optional&lt;/span&gt; &lt;span class="n"&gt;string&lt;/span&gt; &lt;span class="n"&gt;diagnostics_message&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;// 其他几个参数和返回值的定义&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;步骤4&lt;/strong&gt; 为RPC函数的参数和返回值提供Java定义和封装。YARN采用了Protocol Buffers 作为参数和返回值的序列化框架,且以原生态 .proto文件的方式给出了定义,而具体的Java代码生成需在代码编写之后完成。基于以上考虑,为了更容易使用Protocol Buffers生成的(Java 语言)参数和返回值定义,YARN RPC为每个RPC函数的参数和返回值提供Java定义和封装,以参数RegisterNodeManagerRequest 为例进行说明。
Java接口定义如下(见Java包org.apache.hadoop.yarn.server.api.protocolrecords):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;interface&lt;/span&gt; &lt;span class="n"&gt;RegisterNodeManagerRequest&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;NodeId&lt;/span&gt; &lt;span class="n"&gt;getNodeId&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="nf"&gt;getHttpPort&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="n"&gt;Resource&lt;/span&gt; &lt;span class="nf"&gt;getResource&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setNodeId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;NodeId&lt;/span&gt; &lt;span class="n"&gt;nodeId&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setHttpPort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;port&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;setResource&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Resource&lt;/span&gt; &lt;span class="n"&gt;resource&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Java封装如下(见Java包org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;RegisterNodeManagerRequestPBImpl&lt;/span&gt; &lt;span class="nx"&gt;extends&lt;/span&gt;
&lt;span class="nx"&gt;ProtoBase&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;RegisterNodeManagerRequestProto&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;implements&lt;/span&gt; &lt;span class="nx"&gt;RegisterNodeManagerRequest&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nx"&gt;RegisterNodeManagerRequestProto&lt;/span&gt; &lt;span class="n"&gt;proto&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;RegisterNodeManagerRequestProto.&lt;/span&gt;
    &lt;span class="nx"&gt;getDefaultInstance&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="nx"&gt;RegisterNodeManagerRequestProto.Builder&lt;/span&gt; &lt;span class="n"&gt;builder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kt"&gt;null&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;private&lt;/span&gt; &lt;span class="nf"&gt;NodeId&lt;/span&gt; &lt;span class="n"&gt;nodeId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kt"&gt;null&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nx"&gt;...&lt;/span&gt;
    &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nx"&gt;Override&lt;/span&gt;
    &lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;NodeId&lt;/span&gt; &lt;span class="nx"&gt;getNodeId&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;RegisterNodeManagerRequestProtoOrBuilder&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;viaProto&lt;/span&gt; &lt;span class="o"&gt;?&lt;/span&gt; &lt;span class="nx"&gt;proto&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nx"&gt;builder&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;this.nodeId&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="kt"&gt;null&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;this.nodeId&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nx"&gt;p.hasNodeId&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="kt"&gt;null&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;this.nodeId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;convertFromProtoFormat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;p.getNodeId&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;this.nodeId&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nx"&gt;Override&lt;/span&gt;
    &lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;void&lt;/span&gt; &lt;span class="nx"&gt;setNodeId&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;NodeId&lt;/span&gt; &lt;span class="nx"&gt;nodeId&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;maybeInitBuilder&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;nodeId&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="kt"&gt;null&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="nx"&gt;builder.clearNodeId&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;this.nodeId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;nodeId&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="nx"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;步骤5&lt;/strong&gt; 为通信协议提供客户端和服务器端实现。客户端代码放在org.apache.hadoop.yarn.server.api.impl.pb.client 包中,且类名为 ResourceTrackerPBClientImpl,实现如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;class&lt;/span&gt; &lt;span class="n"&gt;ResourceTrackerPBClientImpl&lt;/span&gt; &lt;span class="n"&gt;implements&lt;/span&gt; &lt;span class="n"&gt;ResourceTracker&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Closeable&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;private&lt;/span&gt; &lt;span class="n"&gt;ResourceTrackerPB&lt;/span&gt; &lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;ResourceTrackerPBClientImpl&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;clientVersion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;InetSocketAddress&lt;/span&gt; &lt;span class="n"&gt;addr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;Configuration&lt;/span&gt; &lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;RPC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setProtocolEngine&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ResourceTrackerPB&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ProtobufRpcEngine&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;proxy&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ResourceTrackerPB&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;RPC&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getProxy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;ResourceTrackerPB&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clientVersion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;addr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="err"&gt;@&lt;/span&gt;&lt;span class="n"&gt;Override&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;RegisterNodeManagerResponse&lt;/span&gt; &lt;span class="n"&gt;registerNodeManager&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;RegisterNodeManagerRequest&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;YarnException&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;RegisterNodeManagerRequestProto&lt;/span&gt; &lt;span class="n"&gt;requestProto&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;RegisterNodeManagerRequestP&lt;/span&gt;
        &lt;span class="n"&gt;BImpl&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;getProto&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="n"&gt;try&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;RegisterNodeManagerResponsePBImpl&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;proxy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;registerNodeManager&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;requestProto&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ServiceException&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;RPCUtil&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unwrapAndThrowException&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;null&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;服务端代码放在org.apache.hadoop.yarn.server.api.impl.pb.server包中,且类名为ResourceTrackerPBServerImpl,实现如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;class&lt;/span&gt; &lt;span class="n"&gt;ResourceTrackerPBServiceImpl&lt;/span&gt; &lt;span class="n"&gt;implements&lt;/span&gt; &lt;span class="n"&gt;ResourceTrackerPB&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;private&lt;/span&gt; &lt;span class="n"&gt;ResourceTracker&lt;/span&gt; &lt;span class="n"&gt;real&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;ResourceTrackerPBServiceImpl&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ResourceTracker&lt;/span&gt; &lt;span class="n"&gt;impl&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;impl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="err"&gt;@&lt;/span&gt;&lt;span class="n"&gt;Override&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;RegisterNodeManagerResponseProto&lt;/span&gt; &lt;span class="n"&gt;registerNodeManager&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;RpcController&lt;/span&gt; &lt;span class="n"&gt;controller&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RegisterNodeManagerRequestProto&lt;/span&gt; &lt;span class="n"&gt;proto&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;throws&lt;/span&gt; &lt;span class="n"&gt;ServiceException&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;RegisterNodeManagerRequestPBImpl&lt;/span&gt; &lt;span class="n"&gt;request&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;RegisterNodeManagerRequestPBImpl&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;proto&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;try&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;RegisterNodeManagerResponse&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;real&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;registerNodeManager&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;request&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;RegisterNodeManagerResponsePBImpl&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;).&lt;/span&gt;&lt;span class="n"&gt;getProto&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;YarnException&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;throw&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;ServiceException&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;IOException&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;throw&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;ServiceException&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;服务库与事件库&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;服务库&lt;/h2&gt;
&lt;p&gt;对于生命周期较长的对象,YARN采用了基于服务的对象管理模型对其进行管理,该模型主要有以下几个特点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将每个被服务化的对象分为 4 个状态: NOTINITED(被创建)、 INITED(已初始化)、STARTED(已启动)、STOPPED(已停止)。&lt;/li&gt;
&lt;li&gt;任何服务状态变化都可以触发另外一些动作。&lt;/li&gt;
&lt;li&gt;可通过组合的方式对任意服务进行组合,以便进行统一管理。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;YARN中关于服务模型的类图(位于包org.apache.hadoop.service中)如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Yarn Service" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/service_zps0c2f0998.png"&gt;&lt;/p&gt;
&lt;p&gt;在这个图中,我们可以看到,所有的服务对象最终均实现了接口Service,它定义了最基本的服务初始化、启动、停止等操作,而AbstractService类提供了一个最基本的Service实现。YARN中所有对象,如果是非组合服务,直接继承AbstractService类即可,否则需继承CompositeService。比如,对于ResourceManager而言,它是一个组合服务,它组合了各种服务对象,包括ClientRMService、ApplicationMasterLauncher、ApplicationMasterService 等。在 YARN 中,ResourceManager和NodeManager属于组合服务,它们内部包含多个单一服务和组合服务,以实现对内部多种服务的统一管理。&lt;/p&gt;
&lt;h2&gt;事件库&lt;sup id="sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-5-back"&gt;&lt;a class="simple-footnote" href="#sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-5" title="其实其基本实现结构与Android Handler类似,有兴趣的朋友可以查阅一下相关资料。"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;YARN采用了基于事件驱动的并发模型,该模型能够大大增强并发性,从而提高系统整体性能。为了构建该模型,YARN将各种处理逻辑抽象成事件和对应事件调度器,并将每类事件的处理过程分割成多个步骤,用有限状态机表示。YARN中的事件处理模型可概括为下图。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Yarn Events" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/yarn_event_zpsbaeaa655.png"&gt;&lt;/p&gt;
&lt;p&gt;整个处理过程大致为:处理请求会作为事件进入系统,由中央异步调度器(Async-Dispatcher)负责传递给相应事件调度器(Event Handler)。该事件调度器可能将该事件转发给另外一个事件调度器,也可能交给一个带有有限状态机的事件处理器,其处理结果也以事
件的形式输出给中央异步调度器。而新的事件会再次被中央异步调度器转发给下一个事件调度器,直至处理完成(达到终止条件)。&lt;/p&gt;
&lt;p&gt;在YARN 中,所有核心服务实际上都是一个中央异步调度器,包括ResourceManager、NodeManager、MRAppMaster(MapReduce 应 用 程 序 的 ApplicationMaster)等, 它们维护了事先注册的事件与事件处理器,并根据接收的事件类型驱动服务的运行。&lt;/p&gt;
&lt;p&gt;YARN中事件与事件处理器类的关系(位于包org.apache.hadoop.yarn.event中)如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Async Dispatcher" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/async_dispatcher_zps52f954d2.png"&gt;&lt;/p&gt;
&lt;p&gt;当使用YARN事件库时,通常先要定义一个中央异步调度器AsyncDispatcher,负责事件的处理与转发, 然后根据实际业务需求定义一系列事件 Event与事件处理器EventHandler,并注册到中央异步调度器中以实现事件统一管理和调度(&lt;strong&gt;异步调度器维护了Event与EventHandler之间的映射关系&lt;/strong&gt;)。以MRAppMaster为例, 它内部包含一个中央异步调度器AsyncDispatcher,并注册了TaskAttemptEvent/TaskAttemptImpl、TaskEvent/TaskImpl、JobEvent/JobImpl 等一系列事件/事件处理器,由中央异步调度器统一管理和调度。&lt;/p&gt;
&lt;p&gt;服务化和事件驱动软件设计思想的引入,使得YARN具有低耦合、高内聚的特点,各个模块只需完成各自功能,而模块之间则采用事件联系起来,系统设计简单且维护方便。&lt;/p&gt;
&lt;p&gt;为了说明 YARN 服务库和事件库的使用方法,本小节介绍一个简单的实例,该实例可看做MapReduceApplicationMaster(MRAppMaster)的简化版。该例子涉及任务和作业两种对象的事件以及一个中央异步调度器。步骤如下。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤1&lt;/strong&gt; 定义Task事件。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;package&lt;/span&gt; &lt;span class="nx"&gt;org.qingyuanxingsi.test&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.yarn.event.AbstractEvent&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;


&lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;TaskEvent&lt;/span&gt; &lt;span class="nx"&gt;extends&lt;/span&gt; &lt;span class="nx"&gt;AbstractEvent&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;TaskEventType&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;//Task id&lt;/span&gt;
    &lt;span class="k"&gt;private&lt;/span&gt; &lt;span class="nf"&gt;String&lt;/span&gt; &lt;span class="nx"&gt;taskId&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;TaskEvent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="na"&gt;String&lt;/span&gt; &lt;span class="nx"&gt;taskId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="na"&gt;TaskEventType&lt;/span&gt; &lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="nx"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;this.taskId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;taskId&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;String&lt;/span&gt; &lt;span class="nx"&gt;getTaskId&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;taskId&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;其中,Task事件类型定义如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;package&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;enum&lt;/span&gt; &lt;span class="n"&gt;TaskEventType&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;T_KILL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;T_SCHEDULE&lt;/span&gt;  
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;步骤2&lt;/strong&gt; 定义Job事件。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;package&lt;/span&gt; &lt;span class="nx"&gt;org.qingyuanxingsi.test&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.yarn.event.AbstractEvent&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;JobEvent&lt;/span&gt; &lt;span class="nx"&gt;extends&lt;/span&gt; &lt;span class="nx"&gt;AbstractEvent&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;JobEventType&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;

    &lt;span class="k"&gt;private&lt;/span&gt; &lt;span class="nf"&gt;String&lt;/span&gt; &lt;span class="nx"&gt;jobId&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;JobEvent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="na"&gt;String&lt;/span&gt; &lt;span class="nx"&gt;jobId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="na"&gt;JobEventType&lt;/span&gt; &lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nx"&gt;super&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;type&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="c1"&gt;// TODO Auto-generated constructor stub&lt;/span&gt;
        &lt;span class="n"&gt;this.jobId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;jobId&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;String&lt;/span&gt; &lt;span class="nx"&gt;getJobId&lt;/span&gt;&lt;span class="p"&gt;(){&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nx"&gt;jobId&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;其中,Job事件类型定义如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;package&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;enum&lt;/span&gt; &lt;span class="n"&gt;JobEventType&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="n"&gt;JOB_KILL&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;JOB_INIT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;JOB_START&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;步骤3&lt;/strong&gt; 事件调度器。
接下来定义一个中央异步调度器,它接收Job和Task两种类型事件,并交给对应的事件处理器处理,代码如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="nx"&gt;package&lt;/span&gt; &lt;span class="nx"&gt;org.qingyuanxingsi.test&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.conf.Configuration&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.service.CompositeService&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.service.Service&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.yarn.event.AsyncDispatcher&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.yarn.event.Dispatcher&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="k"&gt;import&lt;/span&gt; &lt;span class="nx"&gt;org.apache.hadoop.yarn.event.EventHandler&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;


&lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt; * A simple central async dispatcher&lt;/span&gt;
&lt;span class="cm"&gt; * @author qingyuanxingsi&lt;/span&gt;
&lt;span class="cm"&gt; *&lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;
&lt;span class="k"&gt;public&lt;/span&gt; &lt;span class="nf"&gt;class&lt;/span&gt; &lt;span class="nx"&gt;SimpleMRAppMaster&lt;/span&gt; &lt;span class="nx"&gt;extends&lt;/span&gt; &lt;span class="nx"&gt;CompositeService&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;//Central Async Dispatcher&lt;/span&gt;
    &lt;span class="k"&gt;private&lt;/span&gt; &lt;span class="nf"&gt;Dispatcher&lt;/span&gt; &lt;span class="nx"&gt;dispatcher&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;@&lt;/span&gt;&lt;span class="nx"&gt;SuppressWarnings&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"unused"&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;private&lt;/span&gt; &lt;span class="nf"&gt;String&lt;/span&gt; &lt;span class="nx"&gt;jobId&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="c1"&gt;//The number of tasks contained in this job&lt;/span&gt;
    &lt;span class="k"&gt;private&lt;/span&gt; &lt;span class="nf"&gt;int&lt;/span&gt; &lt;span class="nx"&gt;taskNum&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="c1"&gt;//All tasks&lt;/span&gt;
    &lt;span class="k"&gt;private&lt;/span&gt; &lt;span class="nf"&gt;String&lt;/span&gt;&lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt; taskIdArray;

    public SimpleMRAppMaster(String name,String jobId,
            int taskNum) {
        super(name);
        this.jobId = jobId;
        this.taskNum = taskNum;
        this.taskIdArray = new String&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;taskNum&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;;
        for(int i=0; i&lt;span class="nt"&gt;&amp;lt;taskNum&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt; &lt;span class="na"&gt;i&lt;/span&gt;&lt;span class="err"&gt;++){&lt;/span&gt;
            &lt;span class="na"&gt;taskIdArray&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="na"&gt; =&lt;/span&gt;&lt;span class="err"&gt; &lt;/span&gt;&lt;span class="s"&gt;new&lt;/span&gt; &lt;span class="na"&gt;String&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;jobId&lt;/span&gt;&lt;span class="err"&gt;+"&lt;/span&gt;&lt;span class="na"&gt;_task_&lt;/span&gt;&lt;span class="err"&gt;"+&lt;/span&gt;&lt;span class="na"&gt;i&lt;/span&gt;&lt;span class="err"&gt;);&lt;/span&gt;      
        &lt;span class="err"&gt;}&lt;/span&gt;
    &lt;span class="err"&gt;}&lt;/span&gt;


    &lt;span class="err"&gt;@&lt;/span&gt;&lt;span class="na"&gt;Override&lt;/span&gt;
    &lt;span class="na"&gt;protected&lt;/span&gt; &lt;span class="na"&gt;void&lt;/span&gt; &lt;span class="na"&gt;serviceInit&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;Configuration&lt;/span&gt; &lt;span class="na"&gt;conf&lt;/span&gt;&lt;span class="err"&gt;)&lt;/span&gt; &lt;span class="na"&gt;throws&lt;/span&gt; &lt;span class="na"&gt;Exception&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
        &lt;span class="err"&gt;//&lt;/span&gt; &lt;span class="na"&gt;TODO&lt;/span&gt; &lt;span class="na"&gt;Auto-generated&lt;/span&gt; &lt;span class="na"&gt;method&lt;/span&gt; &lt;span class="na"&gt;stub&lt;/span&gt;
        &lt;span class="na"&gt;dispatcher =&lt;/span&gt;&lt;span class="err"&gt; &lt;/span&gt;&lt;span class="s"&gt;new&lt;/span&gt; &lt;span class="na"&gt;AsyncDispatcher&lt;/span&gt;&lt;span class="err"&gt;();&lt;/span&gt;
        &lt;span class="err"&gt;//&lt;/span&gt;&lt;span class="na"&gt;Register&lt;/span&gt; &lt;span class="na"&gt;the&lt;/span&gt; &lt;span class="na"&gt;event&lt;/span&gt; &lt;span class="na"&gt;handler&lt;/span&gt; &lt;span class="na"&gt;for&lt;/span&gt; &lt;span class="na"&gt;job&lt;/span&gt; &lt;span class="na"&gt;and&lt;/span&gt; &lt;span class="na"&gt;task&lt;/span&gt; &lt;span class="na"&gt;events&lt;/span&gt;
        &lt;span class="na"&gt;dispatcher&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;register&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;JobEventType&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt; &lt;span class="na"&gt;new&lt;/span&gt; &lt;span class="na"&gt;JobEventDispatcher&lt;/span&gt;&lt;span class="err"&gt;());&lt;/span&gt;
        &lt;span class="na"&gt;dispatcher&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;register&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;TaskEventType&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;class&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt; &lt;span class="na"&gt;new&lt;/span&gt; &lt;span class="na"&gt;TaskEventDispatcher&lt;/span&gt;&lt;span class="err"&gt;());&lt;/span&gt;
        &lt;span class="na"&gt;addService&lt;/span&gt;&lt;span class="err"&gt;((&lt;/span&gt;&lt;span class="na"&gt;Service&lt;/span&gt;&lt;span class="err"&gt;)&lt;/span&gt;&lt;span class="na"&gt;dispatcher&lt;/span&gt;&lt;span class="err"&gt;);&lt;/span&gt;
        &lt;span class="na"&gt;super&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;serviceInit&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;conf&lt;/span&gt;&lt;span class="err"&gt;);&lt;/span&gt;
    &lt;span class="err"&gt;}&lt;/span&gt;

    &lt;span class="na"&gt;public&lt;/span&gt; &lt;span class="na"&gt;Dispatcher&lt;/span&gt; &lt;span class="na"&gt;getDispatcher&lt;/span&gt;&lt;span class="err"&gt;(){&lt;/span&gt;
        &lt;span class="na"&gt;return&lt;/span&gt; &lt;span class="na"&gt;dispatcher&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt;
    &lt;span class="err"&gt;}&lt;/span&gt;

    &lt;span class="na"&gt;private&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt; &lt;span class="na"&gt;JobEventDispatcher&lt;/span&gt; &lt;span class="na"&gt;implements&lt;/span&gt; &lt;span class="na"&gt;EventHandler&lt;/span&gt;&lt;span class="err"&gt;&amp;lt;&lt;/span&gt;&lt;span class="na"&gt;JobEvent&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;{

        @SuppressWarnings("unchecked")
        @Override
        public void handle(JobEvent event) {
            // TODO Auto-generated method stub
            if(event.getType() == JobEventType.JOB_KILL){
                System.out.println("Receive JOB_KILL event,killing all the tasks");
                for(int i=0;i&lt;span class="nt"&gt;&amp;lt;taskNum&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt;&lt;span class="na"&gt;i&lt;/span&gt;&lt;span class="err"&gt;++){&lt;/span&gt;
                    &lt;span class="na"&gt;dispatcher&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getEventHandler&lt;/span&gt;&lt;span class="err"&gt;().&lt;/span&gt;&lt;span class="na"&gt;handle&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;new&lt;/span&gt; &lt;span class="na"&gt;TaskEvent&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;taskIdArray&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt;
                            &lt;span class="na"&gt;TaskEventType&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;T_KILL&lt;/span&gt;&lt;span class="err"&gt;));&lt;/span&gt;
                &lt;span class="err"&gt;}&lt;/span&gt;
            &lt;span class="err"&gt;}&lt;/span&gt;
            &lt;span class="na"&gt;else&lt;/span&gt; &lt;span class="na"&gt;if&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;event&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getType&lt;/span&gt;&lt;span class="err"&gt;()&lt;/span&gt; &lt;span class="err"&gt;==&lt;/span&gt; &lt;span class="na"&gt;JobEventType&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;JOB_INIT&lt;/span&gt;&lt;span class="err"&gt;){&lt;/span&gt;
                &lt;span class="na"&gt;System&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;out&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;println&lt;/span&gt;&lt;span class="err"&gt;("&lt;/span&gt;&lt;span class="na"&gt;Receive&lt;/span&gt; &lt;span class="na"&gt;JOB_Init&lt;/span&gt; &lt;span class="na"&gt;event&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt;&lt;span class="na"&gt;initializing&lt;/span&gt; &lt;span class="na"&gt;all&lt;/span&gt; &lt;span class="na"&gt;the&lt;/span&gt; &lt;span class="na"&gt;tasks&lt;/span&gt;&lt;span class="err"&gt;");&lt;/span&gt;
                &lt;span class="na"&gt;for&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;int&lt;/span&gt; &lt;span class="na"&gt;i=&lt;/span&gt;&lt;span class="s"&gt;0;i&amp;lt;taskNum;i++){&lt;/span&gt;
                    &lt;span class="na"&gt;dispatcher&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;getEventHandler&lt;/span&gt;&lt;span class="err"&gt;().&lt;/span&gt;&lt;span class="na"&gt;handle&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;new&lt;/span&gt; &lt;span class="na"&gt;TaskEvent&lt;/span&gt;&lt;span class="err"&gt;(&lt;/span&gt;&lt;span class="na"&gt;taskIdArray&lt;/span&gt;&lt;span class="cp"&gt;[&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="cp"&gt;]&lt;/span&gt;&lt;span class="err"&gt;,&lt;/span&gt;
                            &lt;span class="na"&gt;TaskEventType&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;T_SCHEDULE&lt;/span&gt;&lt;span class="err"&gt;));&lt;/span&gt;
                &lt;span class="err"&gt;}&lt;/span&gt;
            &lt;span class="err"&gt;}&lt;/span&gt;
        &lt;span class="err"&gt;}&lt;/span&gt;   
    &lt;span class="err"&gt;}&lt;/span&gt;



    &lt;span class="err"&gt;@&lt;/span&gt;&lt;span class="na"&gt;Override&lt;/span&gt;
    &lt;span class="na"&gt;protected&lt;/span&gt; &lt;span class="na"&gt;void&lt;/span&gt; &lt;span class="na"&gt;serviceStart&lt;/span&gt;&lt;span class="err"&gt;()&lt;/span&gt; &lt;span class="na"&gt;throws&lt;/span&gt; &lt;span class="na"&gt;Exception&lt;/span&gt; &lt;span class="err"&gt;{&lt;/span&gt;
        &lt;span class="err"&gt;//&lt;/span&gt; &lt;span class="na"&gt;TODO&lt;/span&gt; &lt;span class="na"&gt;Auto-generated&lt;/span&gt; &lt;span class="na"&gt;method&lt;/span&gt; &lt;span class="na"&gt;stub&lt;/span&gt;
        &lt;span class="na"&gt;super&lt;/span&gt;&lt;span class="err"&gt;.&lt;/span&gt;&lt;span class="na"&gt;serviceStart&lt;/span&gt;&lt;span class="err"&gt;();&lt;/span&gt;
    &lt;span class="err"&gt;}&lt;/span&gt;

    &lt;span class="na"&gt;private&lt;/span&gt; &lt;span class="na"&gt;class&lt;/span&gt; &lt;span class="na"&gt;TaskEventDispatcher&lt;/span&gt; &lt;span class="na"&gt;implements&lt;/span&gt; &lt;span class="na"&gt;EventHandler&lt;/span&gt;&lt;span class="err"&gt;&amp;lt;&lt;/span&gt;&lt;span class="na"&gt;TaskEvent&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;{

        @Override
        public void handle(TaskEvent event) {
            // TODO Auto-generated method stub
            if(event.getType() == TaskEventType.T_KILL)
            {
                System.out.println("Killing this very task with taskId:"+event.getTaskId());
            }
            else if(event.getType() == TaskEventType.T_SCHEDULE)
            {
                System.out.println("Scheduling this very task with taskId:"+event.getTaskId());
            }
        }
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;步骤4&lt;/strong&gt; 测试程序。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;package&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;qingyuanxingsi&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apache&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hadoop&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Configuration&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apache&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hadoop&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yarn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;YarnConfiguration&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;


&lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt; * Just a simple test case&lt;/span&gt;
&lt;span class="cm"&gt; * @author qingyuanxingsi&lt;/span&gt;
&lt;span class="cm"&gt; * @version 1.0&lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;
&lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="n"&gt;class&lt;/span&gt; &lt;span class="n"&gt;SimpleMRAppMasterTest&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="err"&gt;@&lt;/span&gt;&lt;span class="n"&gt;SuppressWarnings&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt; &lt;span class="s"&gt;"unchecked"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;"resource"&lt;/span&gt; &lt;span class="p"&gt;})&lt;/span&gt;
    &lt;span class="n"&gt;public&lt;/span&gt; &lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;String&lt;/span&gt;&lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="n"&gt;String&lt;/span&gt; &lt;span class="n"&gt;jobId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;"job_20140321_01"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;SimpleMRAppMaster&lt;/span&gt; &lt;span class="n"&gt;master&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;SimpleMRAppMaster&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;"Simple MRAppMaster"&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;jobId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;YarnConfiguration&lt;/span&gt; &lt;span class="n"&gt;conf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;YarnConfiguration&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Configuration&lt;/span&gt;&lt;span class="p"&gt;());&lt;/span&gt;
        &lt;span class="n"&gt;try&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;master&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;serviceInit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
            &lt;span class="n"&gt;master&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;serviceStart&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
            &lt;span class="n"&gt;master&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDispatcher&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;getEventHandler&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;JobEvent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jobId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;JobEventType&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;JOB_KILL&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
            &lt;span class="n"&gt;master&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getDispatcher&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;getEventHandler&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;handle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;JobEvent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;jobId&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="n"&gt;JobEventType&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;JOB_INIT&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;catch&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Exception&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="c1"&gt;// TODO Auto-generated catch block&lt;/span&gt;
            &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;printStackTrace&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-1"&gt;https://developers.google.com/protocol-buffers/docs/javatutorial &lt;a class="simple-footnote-back" href="#sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-2"&gt;http://avro.apache.org/docs/current/gettingstartedjava.html &lt;a class="simple-footnote-back" href="#sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-3"&gt;参考&lt;a href="http://www.cnblogs.com/jqyp/archive/2010/08/20/1805041.html"&gt;java动态代理(JDK和cglib)&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-3-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-4"&gt;如若时间允许,该设计模式会进行详尽的分析。 &lt;a class="simple-footnote-back" href="#sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-4-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-5"&gt;其实其基本实现结构与Android Handler类似,有兴趣的朋友可以查阅一下相关资料。 &lt;a class="simple-footnote-back" href="#sf-fen-bu-shi-ji-suan-xi-lie-iyarnji-chu-ku-chu-tan-5-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="RPC"></category><category term="YARN"></category><category term="Hadoop"></category><category term="Designing Patterns"></category><category term="Reactor Pattern"></category><category term="Proxy Pattern"></category><category term="Service"></category><category term="Event Driven Design"></category></entry><entry><title>机器学习系列(III):Gaussian Models</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iiigaussian-models.html" rel="alternate"></link><updated>2014-03-15T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-03-15:ji-qi-xue-xi-xi-lie-iiigaussian-models.html</id><summary type="html">&lt;p&gt;在&lt;a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html"&gt;上一篇&lt;/a&gt;中我们着重介绍了对于离散数据的生成模型，紧接上一篇，本篇我们介绍对于连续数据的生成模型。好吧,废话我们就不多说了,直接进入正文。&lt;/p&gt;
&lt;h1&gt;MLE for MVN&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;Basics about MVN&lt;/h2&gt;
&lt;p&gt;谈到连续分布,我们很自然地就会想到高斯分布,从小学到现在，印象中第一个走入我脑海中的看着比较高端大气上档次的就是Gaussian分布了。这次我们的重点也会完全集中在Gaussian分布之了,在正式讨论之前，我们先介绍一些关于Gaussian分布的基础知识。&lt;/p&gt;
&lt;p&gt;在$D$维空间中,MVN(Multivariate Normal)多变量正态分布的概率分布函数具有如下形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
N(x|\mu,\Sigma) \triangleq \frac{1}{(2\pi)^{D/2}det(\Sigma)^{1/2}} exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中的指数部分是$x$与$\mu$之间的&lt;a href="http://en.wikipedia.org/wiki/Mahalanobis_distance"&gt;Mahalanobis距离&lt;/a&gt;。为了更好地理解这个量,我们对$\Sigma$做特征值分解,即$\Sigma = U \Lambda U^T$,其中$U$是一正交阵,满足$U^TU=I$,而$\Lambda$是特征值矩阵。&lt;/p&gt;
&lt;p&gt;通过特征值分解,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Sigma^{-1} = U^{-T}\Lambda^{-1}U^{-1} = U\Lambda^{-1}U^T = \sum_{i=1}^{D}\frac{1}{\lambda_i}u_iu_i^T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$u_i$是$U$的第$i$列。因此Mahalanobis距离可被改写为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
(x-\mu)^T\Sigma^{-1}(x-\mu) &amp;amp;= (x-\mu)^T (\sum_{i=1}^{D}\frac{1}{\lambda_i}u_iu_i^T) (x-\mu)   \\
                            &amp;amp;= \sum_{i=1}^{D} \frac{1}{\lambda_i}(x-\mu)^T u_iu_i^T (x-\mu)    \\
                            &amp;amp;= \sum_{i=1}^{D} \frac{y_i^2}{x_i}                                \\
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$y_i \triangleq u_i^T(x-\mu)$。另2维空间中的椭圆方程为:&lt;/p&gt;
&lt;p&gt;$$\frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2} = 1$$&lt;/p&gt;
&lt;p&gt;因此我们可知Gaussian概率密度的等高线沿着椭圆分布,特征向量决定椭圆的朝向,而特征值则决定椭圆有多&lt;code&gt;“椭”&lt;/code&gt;。一般来说，如果我们将坐标系移动$\mu$,然后按$U$旋转，此时的欧拉距离即为Mahalanobis距离。&lt;/p&gt;
&lt;h2&gt;MLE for MVN&lt;/h2&gt;
&lt;p&gt;以下我们给出MVN参数的MLE(极大似然估计)的证明:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 3.1 若我们获取的$N$个独立同分布的样本$x_i \sim\ N(x|\mu,\Sigma)$,则关于$\mu$以及
$\Sigma$的极大似然分布如下:
&lt;img alt="MLE for Gaussian" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mle_zpscaea1f03.png"&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们不加证明地给出如下公式组:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;\begin{equation}
  \begin{split}
  &amp;amp;\frac{\partial(b^Ta)}{\partial a} = b  \\
  &amp;amp;\frac{\partial(a^TAa)}{\partial a} = (A+A^T)a \\
  &amp;amp;\frac{\partial}{\partial A} tr(BA) = B^T  \\
  &amp;amp;\frac{\partial}{\partial A} log |A| = A^{-T} \\
  &amp;amp;tr(ABC) = tr(CAB) = tr(BCA)
  \end{split}
  \end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最后一个等式称为迹的循环置换性质(cyclic permutation property)。利用这个性质,我们使用&lt;code&gt;trace trick&lt;/code&gt;可以得到下式:&lt;/p&gt;
&lt;p&gt;$$x^TAx = tr(x^TAx) = tr(xx^TA) = tr(Axx^T)$$&lt;/p&gt;
&lt;p&gt;证明:&lt;/p&gt;
&lt;p&gt;对数似然函数为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l(\mu,\Sigma) = log p(D|\mu,\Sigma) = \frac{N}{2} log |\Lambda| - \frac{1}{2} \sum_{i=1}^{N} (x_i-\mu)^T \Lambda (x_i-\mu)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\Lambda = \Sigma^{-1}$为精度矩阵。令$y_i=x_i-\mu$并利用链式法则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial}{\partial \mu}(x_i-\mu)^T \Sigma^{-1} (x_i-\mu) = \frac {\partial}{\partial y_i} y_i^T \Sigma^{-1} y_i \frac{\partial y_i}{\partial \mu}=-(\Sigma^{-T}+\Sigma^{-1})y_i
\end{equation}&lt;/p&gt;
&lt;p&gt;即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial}{\partial \mu} l(\mu,\Sigma) = -\frac{1}{2} \sum_{i=1}^{N} -2\Sigma^{-1}(x_i-\mu) = 0
\end{equation}&lt;/p&gt;
&lt;p&gt;故有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} x_i = \bar{x}
\end{equation}&lt;/p&gt;
&lt;p&gt;即最大似然均值即为经验均值。&lt;/p&gt;
&lt;p&gt;利用trace trick我们重写对数似然函数为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l(\Lambda) = \frac{N}{2} log |\Lambda| - \frac{1}{2} \sum_{i} tr[(x_i-\mu)(x_i-\mu)^T \Lambda]
           = \frac{N}{2} log |\Lambda| - \frac{1}{2} tr[S_u\Lambda]
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$S_u \triangleq \sum_{i=1}^{N} (x_i-\mu)(x_i-\mu)^T$,业界尊称其为分散度矩阵(&lt;code&gt;Scatter Matrix&lt;/code&gt;),以后我们聊LDA的时候会再次碰到。对$\Lambda$求偏导有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial l(\Lambda)}{\partial \Lambda} = \frac{N}{2}\Lambda^{-T} - \frac{1}{2} S_u^{T} = 0
\end{equation}&lt;/p&gt;
&lt;p&gt;得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{\Sigma} = \frac{1}{N} \sum_{i=1}^{N} (x_i-\mu)(x_i-\mu)^T
\end{equation}&lt;/p&gt;
&lt;p&gt;证毕。&lt;/p&gt;
&lt;h1&gt;Gaussian Discriminant Analysis&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;我们在上一篇中提到了Naive Bayes方法,其实质无非是估计在每一类下特定的样本出现的概率，进而我们可以把该特定样本分配给概率值最大的那个类。而对于连续数据而言，其实质其实也是一样的，每一个MVN(我们可以看做一类或者一个Component)都可能生成一些数据，我们估计在每一个Component下生成特定样本的概率，然后把该特定样本分配给概率值最大的那个Component即可。即我们可以定义如下的条件分布:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x|y=c,\theta) = N(x|\mu_c,\Sigma_c)
\end{equation}&lt;/p&gt;
&lt;p&gt;上述模型即为高斯判别分析(Gaussian Discriminant Analysis,GDA)(&lt;code&gt;注意,该模型为生成模型，而不是判别模型&lt;/code&gt;)。如果$\Sigma_c$是对角阵，即所有的特征都是独立的时，该模型等同于Naive Bayes.&lt;/p&gt;
&lt;h2&gt;QDA&lt;/h2&gt;
&lt;p&gt;在上式中带入高斯密度函数的定义，则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=c|x,\theta) = \frac{\pi_c |2\pi\Sigma_c|^{-1 \over 2}exp[-{1 \over 2}(x-\mu_c)^T\Sigma_C^{-1}(x-\mu_c)]}{\sum_{c\prime}\pi_{c\prime} |2\pi\Sigma_{c\prime}|^{-1 \over 2}exp[-{1 \over 2}(x-\mu_{c\prime})^T\Sigma_{c\prime}^{-1}(x-\mu_{c\prime})]}
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中$\pi$为各个Component的先验概率分布。根据上式得到的模型则称为Quadratic Discriminant Analysis(QDA).以下给出在2类以及3类情形下可能的决策边界形状,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Decision Boundary" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/decision_boundary_zps289df588.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;Linear Discriminant Analysis(LDA)&lt;/h2&gt;
&lt;p&gt;当各个Gaussian Component的协方差矩阵相同时，此时我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(y=c|x,\theta) &amp;amp;\propto \pi_c exp[\mu_c^T\Sigma^{-1}x-{1 \over 2}x^T\Sigma^{-1}x-{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c] \\
&amp;amp;= exp[\mu_c^T\Sigma^{-1}x-{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c]exp[-{1 \over 2}x^T\Sigma^{-1}x]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中$exp[-{1 \over 2}x^T\Sigma^{-1}x]$是独立于$c$的，分子分母相除抵消到此项。&lt;/p&gt;
&lt;p&gt;令:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\gamma_c &amp;amp;= -{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c \\
\beta_c &amp;amp;= \Sigma^{-1}\mu_c
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;于是有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=c|x,\theta) = \frac{e^{\beta_c^Tx+\gamma_c}}{\sum_{c\prime}e^{\beta_{c\prime}^Tx+\gamma_{c\prime}}}=S(\eta)_c
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\eta = [\beta_1^Tx+\gamma_1,...,\beta_C^Tx+\gamma_c]$,$S$为softmax函数(类似于max函数,故得此名),定义如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Softmax Function" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/softmax_zpscdabec6d.png"&gt;&lt;/p&gt;
&lt;p&gt;若将$\eta_c$除以一个常数(temperature),当$T\to 0$时，我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Softmax Function" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/boltzman_distribution_zpsa00cdbf0.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;换句话说，当温度很低时，分布集中在概率最大的那个状态上，而当温度高时，所有的状态呈现均匀分布。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:该术语来自于统计物理学，在统计物理学中，人们更倾向于使用波尔兹曼分布（Boltzmann distribution）一词。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;关于&lt;code&gt;式16&lt;/code&gt;有一个有趣的性质，即对该式取log,我们则会得到一个关于$x$的线性方程。因此对于任意两类之间的决策边界将会是一条直线，据此该模型也被称为线性判别分析(Linear Discriminant Analysis,LDA)。而且对于二分类问题，我们可以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=c|x,\theta) = p(y=c\prime|x,\theta)
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
\beta_c^Tx+\gamma_c = \beta_{c\prime}^Tx+\gamma_{c\prime}
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
x^T(\beta_{c\prime}-\beta_c) = \gamma_{c\prime}-\gamma_c
\end{equation}&lt;/p&gt;
&lt;h2&gt;Two-class LDA&lt;/h2&gt;
&lt;p&gt;为了加深对以上等式的理解，对于二分类的情况，我们做如下说明:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(y=1|x,\theta) &amp;amp;= \frac{e^{\beta_1^Tx+\gamma_1}}{e^{\beta_1^Tx+\gamma_1}+e^{\beta_0^Tx+\gamma_0}} \\
&amp;amp;= \frac{1}{1+e^{(\beta_0-\beta_1)^Tx+(\gamma_0-\gamma_1)}} \\
&amp;amp;=sigm((\beta_1-\beta_0)^Tx+(\gamma_1-\gamma_0))
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$sigm(\eta)$代表&lt;a href="http://en.wikipedia.org/wiki/Sigmoid_function"&gt;Sigmoid函数&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;现有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\gamma_1-\gamma_0 &amp;amp;= -{1 \over 2}\mu_1^T\Sigma^{-1}\mu_1+{1 \over 2}\mu_0^T\Sigma^{-1}\mu_0+log(\pi_1/\pi_0)  \\
&amp;amp;=-{1 \over 2}(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1+\mu_0)+log(\pi_1/\pi_0)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此若我们另:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\omega &amp;amp;= \beta_1-\beta_0 = \Sigma^{-1}(\mu_1-\mu_0) \\
x_0 &amp;amp;= {1 \over 2}(\mu_1+\mu_0)-(\mu_1-\mu_0)\frac{log(\pi_1/\pi_0)}{(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0)}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;则有$\omega^Tx_0 = -(\gamma_1-\gamma_0)$,即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=1|x,\theta) = sigm(\omega^T(x-x_0))
\end{equation}&lt;/p&gt;
&lt;p&gt;因此最后的决策规则很简单:将$x$平移$x_0$,然后投影到$\omega$上，通过结果是正还是负决定它到底属于哪一类。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Two class LDA" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/2_class_lda_zps98b80132.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;当$\Sigma = \sigma^2I$时，$\omega$与$\mu_1-\mu_0$同向。这时我们只需要判断投影点离$\mu_1$和$\mu_0$中的那个点近。当它们的先验概率$\pi_1 = \pi_0$时，投影点位于其中点；当$\pi_1&amp;gt;\pi_0$时，则$x_0$越趋近于$\mu_0$,直线的更大部分先验地属于类1;反之亦然。&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1" title="LDA算法可能导致overfitting,具体解决方法请参阅Machine Learning:a probabilistic perspective一书4.2.*部分"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h1&gt;Inference in joint Gaussian distributions&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;给定联合概率分布$p(x_1,x_2)$,如果我们能够计算边际概率分布$p(x1)$以及条件概率分布$p(x_1|x_2)$想必是极好的而且是及有用的。以下我们仅给出结论,下式表明&lt;strong&gt;如果两变量符合联合高斯分布，则它们的边际分布以及条件分布也都是高斯分布&lt;/strong&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 3.2&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2" title="具体证明请参考ML:APP一书4.3.4.3一节"&gt;2&lt;/a&gt;&lt;/sup&gt; 假定$x=(x_1,x_2)$服从联合高斯分布,且参数如下:
\begin{equation}
\mu = \left(
        \begin{array}{ccc}
        \mu_1 \\
        \mu_2
        \end{array}
      \right)
\end{equation}
\begin{equation}
\Sigma = \left(
        \begin{array}{ccc}
        \Sigma_{11} &amp;amp; \Sigma_{12} \\
        \Sigma_{21} &amp;amp; \Sigma_{22}
        \end{array}
      \right)
\end{equation}
则我们可以得到如下边际概率分布:
\begin{equation}
p(x_1) = N(x_1|\mu_1,\Sigma_{11})  \\
p(x_2) = N(x_2|\mu_2,\Sigma_{22})
\end{equation}
另其后验条件分布为:
\begin{equation}
p(x_1|x_2) = N(x_1|\mu_{1|2},\Sigma_{1|2})
\end{equation}
\begin{equation}
\begin{split}
\mu_{1|2} &amp;amp;= \mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2) \\
&amp;amp;=\mu_1-\Lambda_{11}^{-1}\Lambda_{12}(x_2-\mu_2) \\
&amp;amp;=\Sigma_{1|2}(\Lambda_{11}\mu_1-\Lambda_{12}(x_2-\mu_2)) \\
\end{split}
\end{equation}
\begin{equation}
\Sigma_{1|2} = \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}=\Lambda_{11}^{-1}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;Linear Gaussian Systems&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;给定两变量，$x$和$y$.令$x \in R^{D_x}$为一隐含变量,$y \in R^{D_y}$为关于$x$的包含噪声的观察值。此外，我们假定存在如下prior和likelihood:
\begin{equation}
\begin{split}
p(x) &amp;amp;= N(x|\mu_x,\Sigma_x) \\
p(y|x) &amp;amp;= N(y|Ax+b,\Sigma_y)
\end{split}
\end{equation}
上式即称为&lt;em&gt;Linear Gaussian System&lt;/em&gt;。此时我们有:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 3.3&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3" title="证明请参考ML:APP一书4.4.3节"&gt;3&lt;/a&gt;&lt;/sup&gt; 给定一Linear Gaussian System.其后验分布$p(x|y)$具有如下形式:
\begin{equation}
\begin{split}
p(x|y) &amp;amp;= N(x|\mu_{x|y},\Sigma_{x|y}) \\
\Sigma_{x|y}^{-1} &amp;amp;= \Sigma_x^{-1}+A^T\Sigma_y^{-1}A  \\
\mu_{x|y} &amp;amp;= \Sigma_{x|y}[A^T\Sigma_y^{-1}(y-b)+\Sigma_x^{-1}\mu_x] 
\end{split}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Inferring an unknown vector from noisy measurements&lt;/h2&gt;
&lt;p&gt;下面我们举一个简单的例子以进一步说明Linear Gaussian System:
现有$N$个观测向量,$y_i \sim\ N(x,\Sigma_y)$,prior服从高斯分布$x \sim\ N(\mu_0,\Sigma_0)$.令$A=I,b=0$,此外，我们采用$\bar{y}$作为我们的有效估计值,其精度为$N\Sigma_y^{-1},$我们有:
\begin{equation}
\begin{split}
p(x|y_1,...,y_N) &amp;amp;= N(x|\mu_N,\Sigma_N) \\
\Sigma_N^{-1} &amp;amp;= \Sigma_{0}^{-1}+N\Sigma_{y}^{-1} \\
\mu_N &amp;amp;= \Sigma_N(\Sigma_y^{-1}(N\bar{y})+\Sigma_0^{-1}\mu_0)
\end{split}
\end{equation}
为了更直观地解释以上模型,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Radar Blips" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/radar_blips_zpsfd1a04d1.png"&gt;&lt;/p&gt;
&lt;p&gt;我们可将x视为2维空间中一个物体的真实位置(但我们并不知道),例如一枚导弹或者一架飞机,$y_i$则是我们的观测值(含噪声),可以视为雷达上的一些点。当我们得到越来越多的点时，我们就能够更好地进行定位。&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4" title="另外一种方法为Kalman Filter Algorithm"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;现假定我们有多个测量设备，且我们想利用多个设备的观测值进行估计，这种方法称为&lt;code&gt;sensor fusion&lt;/code&gt;.如果我们有具有不同方差的多组观测值，那么posterior将会是它们的加权平均。如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sensor Fusion" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sensor_fusion_zps4ad8202f.png"&gt;&lt;/p&gt;
&lt;p&gt;我们采用不带任何信息的关于$x$的先验分布，即$p(x)=N(\mu_0,\Sigma_0)=N(0,10^10I_2)$,我们得到2个观测值,$y_1 \sim\ N(x,\Sigma_{y,1}$,$y_2 \sim\ N(x,\Sigma_{y,2})$,我们需要计算$p(x|y_1,y_2)$.&lt;/p&gt;
&lt;p&gt;如上图(a),我们设定$\Sigma_{y,1} = \Sigma_{y,2} = 0.01I_2$,因此两个传感器都相当可靠，posterior mean即位于两个观测值中间；如上图(b)，我们设定$\Sigma_{y,1}=0.05I_2$且$\Sigma_{y,2}=0.01I_2$,因此传感器2比传感器1可靠，此时posterior mean更靠近于$y_2$;如上图(c),我们有:
\begin{equation}
\Sigma_{y,1} = 0.01
\left(
\begin{array}{cc}
10 &amp;amp; 1 \\
1  &amp;amp; 1
\end{array}
\right),
\Sigma_{y,2} = 0.01
\left(
\begin{array}{cc}
1 &amp;amp; 1 \\
1  &amp;amp; 10
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;从上式我们不难看出，传感器1在第2个分量上更可靠，传感器2在第1个分量上也更可靠。此时，posterior mean采用传感器1的第二分量以及传感器1的第二分量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:当sensor测量精度未知时，我们则需要它们的测量精度也进行估计。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;The Wishart Distribution&lt;/h1&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;在多变量统计学中,Wishart分布是继高斯分布后最重要且最有用的模型。  ------Press&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;既然Press他老人家都说了Wishart分布很重要，而且我们下一部分会用到它，那么我们就必须得介绍介绍它了。(它主要被用来Model关于协方差矩阵的不确定性)&lt;/p&gt;
&lt;h2&gt;Wishart Distribution&lt;/h2&gt;
&lt;p&gt;Wishart概率密度函数具有如下形式:
\begin{equation}
Wi(\Lambda|S,\nu)=\frac{1}{Z_{Wi}}|\Lambda|^{(\nu-D-1)/2}exp(-{1 \over 2}tr(\Lambda S^{-1}))
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\nu$为自由度，$S$为缩放矩阵。其归一项具有如下形式:(&lt;code&gt;很恐怖，对吧!&lt;/code&gt;)
\begin{equation}
Z_{Wi}=2^{\nu D/2}\Gamma_D(\nu/2)|S|^{\nu/2}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\Gamma_D(a)$为多变量Gamma函数:
\begin{equation}
\Gamma_D(x) = \pi^{D(D-1)/4}\prod_{i=1}^{D} \Gamma(x+(1-i)/2)
\end{equation}&lt;/p&gt;
&lt;p&gt;于是有$\Gamma_1(a)=\Gamma(a)$且有:
\begin{equation}
\Gamma_D(\nu_0/2)=\prod_{i=1}^{D} \Gamma(\frac{\nu_0+1-i}{2})
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;仅当$\nu&amp;gt;D-1$时归一项存在&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;其实Wishart分布和Gaussian分布是有联系的。具体而言，令$x_i \sim\ N(0,\Sigma)$,则离散度矩阵$S=\sum_{i=1}^{N}x_ix_i^T$服从Wishart分布,且$S \sim\ Wi(\Sigma,1)$。于是有$E(S)=N\Sigma$.&lt;/p&gt;
&lt;p&gt;更一般地，我们可以证明$Wi(S,\nu)$的mean和mode具有如下形式:
\begin{equation}
mean=\nu S,mode=(\nu-D-1)S
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;仅当$\nu&amp;gt;D+1$时mode存在&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;当$D=1$时，Wishart分布退化为Gamma分布，且有:
\begin{equation}
Wi(\lambda|s^{-1},v) = Ga(\lambda|{\nu \over 2},{s \over 2})
\end{equation}&lt;/p&gt;
&lt;h2&gt;Inverse Wishart Distribution&lt;/h2&gt;
&lt;p&gt;若$\Sigma^{-1} \sim\ Wi(S,\nu)$,则$\Sigma \sim\ IW(S^{-1},\nu+D+1)$,其中$IW$为逆Wishart分布。当$\nu&amp;gt;D-1$且$S \succ 0$时,我们有:
\begin{equation}
\begin{split}
IW(\Sigma|S,\nu) &amp;amp;= \frac{1}{Z_{IW}}|\Sigma|^{-(\nu+D+1)/2}exp(-{1 \over 2}tr(S^{-1}\Sigma^{-1})) \\
Z_{IW} &amp;amp;= |S|^{-\nu/2}2^{\nu D/2}\Gamma_D(\nu/2)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;此外我们可以证明逆Wishart分布具有如下性质:
\begin{equation}
mean=\frac{S^{-1}}{\nu-D-1},mode=\frac{S^{-1}}{\nu+D+1}
\end{equation}&lt;/p&gt;
&lt;p&gt;当$D=1$时,它们退化为逆Gamma分布:
\begin{equation}
IW(\sigma^2|S^{-1},\nu)=IG(\sigma^2|\nu/2,S/2)
\end{equation}&lt;/p&gt;
&lt;h1&gt;Inferring the parameters of an MVN&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5" title="本部分部分参考Regularized Gaussian Covariance Estimation"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;到目前为止，我们已经讨论了在$\theta=(\mu,\Sigma)$已知的条件下如何inference，现我们讨论一下如何对参数本身进行估计。假定$x_i \sim\ N(\mu,\Sigma)$ for $i=1:N$.本节主要分为两个部分:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算$p(\mu|D,\Sigma)$;&lt;/li&gt;
&lt;li&gt;计算$p(\Sigma|D,\mu)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Posterior distribution of $\mu$&lt;/h2&gt;
&lt;p&gt;我们已经就如何计算$\mu$的极大似然估计值进行了讨论,现我们讨论如何计算其posterior.&lt;/p&gt;
&lt;p&gt;其likelihood具有如下形式:
\begin{equation}
p(D|\mu) = N(\bar{x}|\mu,{1 \over N}\Sigma)
\end{equation}&lt;/p&gt;
&lt;p&gt;为了简便起见，我们采用共轭先验分布，即高斯。特别地，若$p(\mu)=N(\mu|m_0,V_0)$.此时我们可以根据之前Linear Gaussian System的结论得到(和我们之前提到的雷达的例子雷同):
\begin{equation}
\begin{split}
p(\mu|D,\Sigma) &amp;amp;= N(\mu|m_N,V_N) \\
V_N^{-1} &amp;amp;= V_0^{-1}+N\Sigma^{-1} \\
m_N &amp;amp;= V_N(\Sigma^{-1}(N\bar{x})+V_0^{-1}m_0)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们可以通过设定$V_0 = \infty I$提供一个不带任何信息的先验，此时我们有$p(\mu|D,\Sigma)=N(\bar{x},{1 \over N}\Sigma)$,即和MLE得到的结果相同。&lt;/p&gt;
&lt;h2&gt;Posterior distribution of $\Sigma$&lt;/h2&gt;
&lt;p&gt;现我们讨论如何计算$p(\Sigma|D,\mu)$,其likelihood具有如下形式:&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6" title="参见MLE for Gaussian部分"&gt;6&lt;/a&gt;&lt;/sup&gt;
\begin{equation}
p(D|\mu,\Sigma) \propto |\Sigma|^{-{N \over 2}}exp(-{1 \over 2}tr(S_{\mu}\Sigma^{-1}))
\end{equation}&lt;/p&gt;
&lt;p&gt;之前我们提到过如果采用共轭先验能够减少计算的复杂度，而此likelihood的共轭先验就是我们之前提到的非常恐怖的逆Wishart分布，即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
IW(\Sigma|S_0^{-1},\nu_0) \propto |\Sigma|^{-(\nu_0+D+1)/2}exp(-{1 \over 2}tr(S_0\Sigma^{-1}))
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中$N_0=\nu+D+1$控制着先验的强度,和likelihood中的$N$的作用基本相同。&lt;/p&gt;
&lt;p&gt;将先验和likelihood相乘我们得到如下posterior:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(\Sigma|D,\mu) &amp;amp;\propto |\Sigma|^{-{N \over 2}}exp(-{1 \over 2}tr(S_{\mu}\Sigma^{-1}))|\Sigma|^{-(\nu_0+D+1)/2}exp(-{1 \over 2}tr(S_0\Sigma^{-1})) \\
&amp;amp;=|\Sigma|^{-\frac{N+(\nu_0+D+1)}{2}}exp(-{1 \over 2}tr(\Sigma^{-1}(S_{\mu}+S_0))) \\
&amp;amp;=IW(\Sigma|S_N,\nu_N) \\
\nu_N &amp;amp;= \nu_0+N    \\
S_N^{-1} &amp;amp;= S_0+S_{\mu}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;总而言之，从上式我们可以看到，posterior $v_N$的强度为$\nu_0$加$N$;posterior离散度矩阵是先验离散度矩阵$S_0$和数据离散度矩阵$S_{\mu}$之和。&lt;/p&gt;
&lt;h3&gt;MAP Estimation&lt;/h3&gt;
&lt;p&gt;根据我们之前得到的关于$\Sigma$的极大似然估计值,即:&lt;/p&gt;
&lt;p&gt;&lt;img alt="MLE for Covariance" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mle_covariance_zpsb7265e9d.png"&gt;&lt;/p&gt;
&lt;p&gt;从上式我们可以看出矩阵的rank为$min(N,D)$。若$N$小于$D$,该矩阵不是full rank的，因此不可逆。另尽管$N$可能大于$D$,$\hat{\Sigma}$也可能是ill-conditioned(接近奇异)。&lt;/p&gt;
&lt;p&gt;为了解决上述问题，我们可以采用posterior mean或mode.我们可以证明$\Sigma$的MAP估计值如下:(使用我们推导MLE时所用的技巧，其实并不难，亲证下式正确):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat\Sigma_{MAP} = \frac{S_N}{\nu_N+D+1} = \frac{S_0+S_{\mu}}{N_0+N_{\mu}}
\end{equation}&lt;/p&gt;
&lt;p&gt;当我们采用improper uniform prior，即$S_0=0,N_0=0$时，我们即得MLE估计值。&lt;/p&gt;
&lt;p&gt;当$D/N$较大时，选择一个包含信息的合适的prior就相当必要了。令$\mu=\bar{x}$,故有$S_{\mu}=S_{\bar{x}}$,此时MAP估计值可被重写为prior mode和MLE的convex combination.令$\Sigma_0 \triangleq  \frac{S_0}{N_0}$为prior mode,则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\hat\Sigma_{MAP} = \frac{S_0+S_{\bar{x}}}{N_0+N} &amp;amp;= \frac{N_0}{N_0+N}\frac{S_0}{N_0}+\frac{N}{N_0+N}\frac{S}{N} \\
&amp;amp;=\lambda\Sigma_0+(1-\lambda)\hat{\Sigma}_{mle}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\lambda=\frac{N_0}{N_0+N}$,控制着向prior &lt;code&gt;shrinkage&lt;/code&gt;的程度。对于$\lambda$而言，我们可以通过交叉验证设置其值。&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7" title="其他方法见ML:APP一书4.6.2.1节"&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;而对于先验的协方差矩阵$S_0$,一般采用如下prior:$S_0=diag(\hat{\Sigma}_{mle})$.因此，我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="S_0" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/S0_zps47da8ac0.png"&gt;&lt;/p&gt;
&lt;p&gt;由上式我们可以看出，对角线的元素和极大似然估计值相等，而非对角线则趋近于0.因此该技巧也被称为&lt;em&gt;shrinkage estimation,or regularized estimation&lt;/em&gt;.&lt;/p&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1"&gt;LDA算法可能导致overfitting,具体解决方法请参阅Machine Learning:a probabilistic perspective一书4.2.*部分 &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2"&gt;具体证明请参考ML:APP一书4.3.4.3一节 &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3"&gt;证明请参考ML:APP一书4.4.3节 &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4"&gt;另外一种方法为Kalman Filter Algorithm &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5"&gt;本部分部分参考&lt;a href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/"&gt;Regularized Gaussian Covariance Estimation&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6"&gt;参见MLE for Gaussian部分 &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7"&gt;其他方法见ML:APP一书4.6.2.1节 &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="Machine Learning"></category><category term="Gaussian Models"></category><category term="Generative Models"></category></entry><entry><title>分布式计算与存储系列(序章):初入门径</title><link href="http://www.qingyuanxingsi.com/fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing.html" rel="alternate"></link><updated>2014-03-11T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-03-11:fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing.html</id><summary type="html">&lt;p&gt;过年回来之后就发现电脑坏了，一大通鼓捣之后把上学期记录的文档全都丢掉了，包括上学期看过的这本书&lt;a href="http://book.douban.com/subject/25723658/"&gt;《大规模分布式存储系统：原理解析与架构实战》&lt;/a&gt;,由于文档丢失了，也反而有了一次重温一下这本书的机会;本文基本上以该书为蓝本，对书中某些内容会做进一步的挖掘，对目前我能获得的所有资料综合整理，遂成此文。&lt;/p&gt;
&lt;h1&gt;概述&lt;/h1&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;无论是云计算、大数据还是互联网公司的各种应用，其后台基础设施的主要目标都是构建&lt;strong&gt;低成本、高性能、可扩展、易用&lt;/strong&gt;的分布式存储系统。(阿里日照)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;分布式存储面临的数据需求比较复杂，大致可以分为三类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非结构化数据 ：包括所有格式的办公文档、文本、图片、图像、音频和视频信息等。&lt;/li&gt;
&lt;li&gt;结构化数据：一般存储在关系数据库中，可以用二维关系表结构来表示。结构化数据的模式（Schema，包括属性、数据类型以及数据之间的联系）和内容是分开的，数据的模式需要预先定义。&lt;/li&gt;
&lt;li&gt;半结构化数据：介于非结构化数据和结构化数据之间，HTML 文档就属于半结构化数据。它一般是自描述的，与结构化数据最大的区别在于，半结构化数据的模式结构和内容混在一起，没有明显的区分，也不需要预先定义数据的模式结构。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不同的分布式存储系统适合处理不同类型的数据，本书将分布式存储系统分为四类：分布式文件系统、分布式键值（Key-Value）系统、分布式表格系统和分布式数据库。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分布式文件系统&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;互联网应用需要存储大量的图片、照片、视频等非结构化数据对象，这类数据以对象的形式组织，对象之间没有关联，这样的数据一般称为&lt;strong&gt;Blob&lt;/strong&gt;（Binary Large Object，二进制大对象）数据。分布式文件系统用于存储Blob 对象，典型的系统有&lt;em&gt;Facebook Haystack&lt;/em&gt;。另外，分布式文件系统也常作为分布式表格系统以及分布式数据库的底层存储，如谷歌的GFS（&lt;em&gt;Google File System&lt;/em&gt;，存储大文件）可以作为分布式表格系统Google Bigtable 的底层存储，Amazon的EBS（&lt;em&gt;Elastic Block Store&lt;/em&gt;，弹性块存储）系统可以作为分布式数据库（&lt;em&gt;Amazon RDS&lt;/em&gt;）的底层存储。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分布式键值系统&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分布式键值系统用于存储关系简单的半结构化数据，它只提供基于主键的CRUD(Create/Read/Update/Delete)功能，即根据主键创建、读取、更新或者删除一条键值记录。典型的系统有&lt;em&gt;Amazon Dynamo&lt;/em&gt;。从数据结构的角度看，分布式键值系统与传统的哈希表比较类似，不同的是，分布式键值系统支持将数据分布到集群中的多个存储节点。分布式键值系统是分布式表格系统的一种简化实现，&lt;strong&gt;一般用作缓存&lt;/strong&gt;，比如&lt;em&gt;Memcache&lt;/em&gt;。一致性哈希是分布式键值系统中常用的数据分布技术，因其被&lt;em&gt;Amazon DynamoDB&lt;/em&gt; 系统使用而变得相当有名。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分布式表格系统&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用于存储关系较为复杂的半结构化数据，与分布式键值系统相比,分布式表格系统不仅仅支持简单的CRUD操作，而且&lt;strong&gt;支持扫描某个主键范围&lt;/strong&gt;。分布式表格系统以&lt;code&gt;表格&lt;/code&gt;为单位组织数据，每个表格包括很多行，通过主键标识一行，支持根据主键的CRUD 功能以及范围查找功能。典型的系统包括Google Bigtable 以及Megastore，Microsoft Azure Table Storage，Amazon DynamoDB 等。与分布式数据库相比，分布式表格系统主要支持针对单张表格的操作，不支持一些特别复杂的操作，比如多表关联，多表联接，嵌套子查询；另外，在分布式表格系统中，同一个表格的多个数据行也不要求包含相同类型的列，适合半结构化数据。分布式表格系统是一种很好的权衡，这类系统可以做到超大规模，而且支持较多的功能，但实现往往比较复杂，而且有一定的使用门槛。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分布式数据库&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分布式数据库一般是从单机关系数据库扩展而来，用于存储结构化数据。分布式数据库采用二维表格组织数据，提供SQL 关系查询语言，支持多表关联，嵌套子查询等复杂操作，并提供数据库事务以及并发控制。典型的系统包括MySQL数据库分片（MySQL Sharding）集群，Amazon RDS 以及Microsoft SQL Azure。分布式数据库支持的功能最为丰富，符合用户使用习惯，但可扩展性往往受到限制。当然，这一点并不是绝对的.Google Spanner系统是一个支持多数据中心的分布式数据库，它不仅支持丰富的关系数据库功能，还能扩展到多个数据中心的成千上万台机器。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:
传统关系数据库的事务以及二维关系模型很难高效地扩展到多个存储节点上，另外，关系数据库对于要求高并发的应用在性能上优化空间较大。为了解决关系数据库面临的可扩展性、高并发以及性能方面的问题，各种各样的非关系数据库风起云涌，这类系统成为NoSQL&lt;sup id="sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-1-back"&gt;&lt;a class="simple-footnote" href="#sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-1" title="关于NoSQL的详细资料参见NoSQL数据库笔谈"&gt;1&lt;/a&gt;&lt;/sup&gt;系统，可以理解为“Not Only SQL”系统。NoSQL系统多得让人眼花缭乱，每个系统都有自己的独到之处，适合解决某种特定的问题。(阿里日照)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;单机存储系统&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;单机存储引擎就是哈希表、B 树等数据结构在机械磁盘、SSD等持久化介质上的实现。单机存储系统是单机存储引擎的一种封装,对外提供文件、键值、表格或者关系模型。单机存储系统的理论来源于关系数据库。数据库将一个或多个操作组成一组,称作事务,事务必须满足原子性(Atomicity)、一致性(Consistency)、隔离性(Isolation)以及持久性(Durability),简称为 ACID 特性。多个事务并发执行时,数据库的并发控制管理器必须能够保证多个事务的执行结果不能破坏某种约定,如不能出现事务执行到一半的情况,不能读取到未提交的事务,等等。为了保证持久性,对于数据库的每一个变化都要在磁盘上记录日志,当数据库系统突然发生故障,重启后能够恢复到之前一致的状态。&lt;/p&gt;
&lt;h2&gt;硬件基础&lt;/h2&gt;
&lt;h3&gt;CPU架构&lt;/h3&gt;
&lt;p&gt;早期的 CPU 为单核芯片,工程师们很快意识到,仅仅提高单核的速度会产生过多的热量且无法带来相应的性能改善。因此,现代服务器基本为多核或多个 CPU。经典的多CPU 架构为对称多处理结构(Symmetric Multi-Processing,SMP),即在一个计算机上汇集了一组处理器,它们之间对称工作,无主次或从属关系,共享相同的物理内存及总线。SMP架构的主要特征是共享,系统中所有资源(CPU、内存、I/O 等)都是共享的,由于多CPU对前端总线的竞争,SMP的扩展能力非常有限。为了提高可扩展性,现在的主流服务器架构一般为 NUMA(Non-Uniform Memory Access,非一致存储访问)架构。它具有多个 NUMA 节点,每个 NUMA 节点是一个 SMP 结构,一般由多个 CPU(如 4 个)组成,并且具有独立的本地内存、IO槽口等。NUMA节点可以直接快速访问本地内存,也可以通过NUMA 互联互通模块访问其他 NUMA 节点的内存,&lt;strong&gt;访问本地内存的速度远远高于远程访问的速度&lt;/strong&gt;。由于这个特点,为了更好地发挥系统性能,开发应用程序时需要尽量减少不同 NUMA 节点之间的信息交互。&lt;/p&gt;
&lt;h3&gt;存储层次架构&lt;/h3&gt;
&lt;p&gt;从分布式系统的角度看,整个集群中所有服务器上的存储介质(内存、机械硬盘,SSD)构成一个整体,其他服务器上的存储介质与本机存储介质一样都是可访问的,区别仅仅在于需要额外的网络传输及网络协议栈等访问开销。&lt;/p&gt;
&lt;p&gt;存储系统的性能主要包括两个维度:&lt;strong&gt;吞吐量以及访问延时&lt;/strong&gt;,设计系统时要求能够在保证访问延时的基础上,通过最低的成本实现尽可能高的吞吐量。磁盘和 SSD的访问延时差别很大,但带宽差别不大,因此,磁盘适合大块顺序访问的存储系统,SSD适合随机访问较多或者对延时比较敏感的关键系统。二者也常常组合在一起进行混合存储,热数据(访问频繁)存储到SSD中,冷数据(访问不频繁)存储到磁盘中。&lt;/p&gt;
&lt;h2&gt;单机存储引擎&lt;/h2&gt;
&lt;p&gt;存储引擎是存储系统的发动机,直接决定了存储系统能够提供的性能和功能。存储系统的基本功能包括:增、删、读、改,其中,读取操作又分为随机读取和顺序扫描。哈希存储引擎是哈希表的持久化实现,支持增、删、改,以及随机读取操作,但不支持顺序扫描,对应的存储系统为键值(Key-Value)存储系统;&lt;strong&gt;B树&lt;/strong&gt;(B-Tree)存储引擎是B树的持久化实现,不仅支持单条记录的增、删、读、改操作,还支持顺序扫描,对应的存储系统是关系数据库。当然,键值系统也可以通过 B 树存储引擎实现;LSM树(Log-Structured Merge Tree)存储引擎和 B 树存储引擎一样,支持增、删、改、随机读取以及顺序扫描。它通过&lt;code&gt;批量转储技术规避磁盘随机写入&lt;/code&gt;问题,广泛应用于互联网的后台 存 储 系 统, 例 如 Google Bigtable、Google LevelDB 以 及 Facebook 开 源 的Cassandra系统。本节分别以 Bitcask、MySQL InnoDB 以及 Google LevelDB 系统为例介绍这三种存储引擎。&lt;/p&gt;
&lt;h3&gt;哈希存储引擎&lt;/h3&gt;
&lt;p&gt;Bitcask是一个基于哈希表结构的键值存储系统,它仅支持追加操作(Append-only),即所有的写操作只追加而不修改老的数据。在 Bitcask 系统中,每个文件有一定的大小限制,当文件增加到相应的大小时,就会产生一个新的文件,老的文件只读不写。在任意时刻,只有一个文件是可写的,用于数据追加,称为活跃数据文件(active data file)。而其他达到大小限制的文件，称为老数据文件(older data file)。&lt;/p&gt;
&lt;h4&gt;数据结构&lt;/h4&gt;
&lt;p&gt;如下图所示，Bitcask 数据文件中的数据是一条一条的写入操作，每一条记录的数据项分别为主键（key）、value 内容（value）、主键长度（key_sz）、value长度（value_sz）、时间戳（timestamp）以及crc 校验值。（数据删除操作也不会删除旧的条目，而是将value设定为一个特殊的值用作标识）。内存中采用基于哈希表的索引数据结构，哈希表的作用是通过主键快速地定位到value的位置。哈希表结构中的每一项包含了三个用于定位数据的信息，分别是文件编号（file id），value 在文件中的位置（value_pos），value 长度（value_sz），通过读取file_id 对应文件的value_pos 开始的value_sz 个字节，这就得到了最终的value值。写入时首先将Key-Value记录追加到活跃数据文件的末尾，接着更新内存哈希表，因此，每个写操作总共需要进行一次顺序的磁盘写入和一次内存操作。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Structure for Bitcask" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/DS1_1_zps07d562a0.png"&gt;&lt;/p&gt;
&lt;p&gt;Bitcask 在内存中存储了主键和value的索引信息，磁盘文件中存储了主键和value的实际内容。系统基于一个假设，value 的长度远大于主键的长度。假如value的平均长度为1KB，每条记录在内存中的索引信息为32 字节，那么，磁盘内存比为32 : 1。这样，32GB 内存索引的数据量为32GB×32 =1TB。&lt;/p&gt;
&lt;h4&gt;定期合并&lt;/h4&gt;
&lt;p&gt;Bitcask 系统中的记录删除或者更新后，原来的记录成为垃圾数据。如果这些数据一直保存下去，文件会无限膨胀下去，为了解决这个问题，Bitcask需要定期执行合并（Compaction）操作以实现垃圾回收。所谓合并操作，即将所有老数据文件中的数据扫描一遍并生成新的数据文件，这里的合并其实就是对同一个key 的多个操作以只保留最新一个的原则进行删除，每次合并后，新生成的数据文件就不再有冗余数据了。&lt;/p&gt;
&lt;h4&gt;快速恢复&lt;/h4&gt;
&lt;p&gt;Bitcask 系统中的哈希索引存储在内存中，如果不做额外的工作，服务器断电重启重建哈希表需要扫描一遍数据文件，如果数据文件很大，这是一个非常耗时的过程。Bitcask 通过索引文件（hint file）来提高重建哈希表的速度。简单来说，索引文件就是将内存中的哈希索引表转储到磁盘生成的结果文件。Bitcask 对老数据文件进行合并操作时，会产生新的数据文件，这个过程中还会产生一个索引文件，这个索引文件记录每一条记录的哈希索引信息。与数据文件不同的是，索引文件并不存储具体的value 值，只存储value的位置（与内存哈希表一样）。这样，在重建哈希表时，就不需要扫描所有数据文件，而仅仅需要将索引文件中的数据一行行读取并重建即可，大大减少了重启后的恢复时间。&lt;/p&gt;
&lt;h2&gt;B树存储引擎&lt;/h2&gt;
&lt;p&gt;相比哈希存储引擎，B 树存储引擎不仅支持随机读取，还支持范围扫描。关系数据库中通过索引访问数据，在Mysql InnoDB 中，有一个称为&lt;code&gt;聚集索引&lt;/code&gt;的特殊索引，行的数据存于其中，组织成B+ 树（B 树的一种）数据结构。&lt;/p&gt;
&lt;h3&gt;数据结构&lt;/h3&gt;
&lt;p&gt;如下图所示，MySQL InnoDB按照页面（Page）来组织数据，每个页面对应B+树的一个节点。其中，&lt;strong&gt;叶子节点保存每行的完整数据，非叶子节点保存索引信息&lt;/strong&gt;。数据在每个节点中有序存储，数据库查询时需要从根节点开始二分查找直到叶子节点，每次读取一个节点，如果对应的页面不在内存中，需要从磁盘中读取并缓存起来。B+树的根节点是常驻内存的，因此，B+树一次检索最多需要$h-1$次磁盘IO，复杂度为$O(h)=O(logdN)$（$N$为元素个数，$d$为每个节点的出度，$h$为B+树高度）。修改操作首先需要记录提交日志，接着修改内存中的B+树。如果内存中的被修改过的页面超过一定的比率，后台线程会将这些页面刷到磁盘中持久化。当然，InnoDB实现时做了大量的优化，这部分内容我们不再讨论。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Structure of B+" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/DS1_2_zpsad1e323a.png"&gt;&lt;/p&gt;
&lt;h3&gt;缓冲区管理&lt;/h3&gt;
&lt;p&gt;缓冲区管理器负责将可用的内存划分成缓冲区，缓冲区是与页面同等大小的区域，磁盘块的内容可以传送到缓冲区中。缓冲区管理器的关键在于替换策略，即选择将哪些页面淘汰出缓冲池。常见的算法有以下两种。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LRU&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LRU算法淘汰最长时间没有读或者写过的块。这种方法要求缓冲区管理器按照页面最后一次被访问的时间组成一个链表，每次淘汰链表尾部的页面。直觉上，长时间没有读写的页面比那些最近访问过的页面有更小的最近访问的可能性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LIRS(&lt;code&gt;Low Inter-reference Recency Set&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;LRU算法在大多数情况下表现是不错的，但有一个问题：假如某一个查询做了一次全表扫描，将导致缓冲池中的大量页面（可能包含很多很快被访问的热点页面）被替换，从而污染缓冲池。现代数据库一般采用LIRS 算法，将缓冲池分为两级，数据首先进入第一级，如果数据在较短的时间内被访问两次或者以上，则成为热点数据进入第二级，每一级内部还是采用LRU替换算法。Oracle数据库中的Touch Count 算法和MySQL InnoDB 中的替换算法都采用了类似的分级思想。以MySQL InnoDB为例，InnoDB 内部的LRU 链表分为两部分：新子链表（new sublist）和老子链表（old sublist），默认情况下，前者占5/8，后者占3/8。页面首先插入到老子链表，InnoDB要求页面在老子链表停留时间超过一定值，比如1秒，才有可能被转移到新子链表。当出现全表扫描时，InnoDB将数据页面载入到老子链表，由于数据页面在老子链表中的停留时间不够，不会被转移到新子链表中，这就避免了新子链表中的页面被替换出去的情况。&lt;/p&gt;
&lt;h2&gt;LSM树存储引擎&lt;/h2&gt;
&lt;p&gt;LSM 树（Log Structured Merge Tree）的思想非常朴素，就是将对数据的修改增量保持在内存中，达到指定的大小限制后将这些修改操作批量写入磁盘，读取时需要合并磁盘中的历史数据和内存中最近的修改操作。LSM树的优势在于有效地规避了磁盘随机写入问题，但读取时可能需要访问较多的磁盘文件。本节介绍LevelDB 中的LSM 树存储引擎。&lt;/p&gt;
&lt;h3&gt;存储结构&lt;/h3&gt;
&lt;p&gt;如下图所示，LevelDB存储引擎主要包括：内存中的MemTable和不可变MemTable（Immutable MemTable，也称为Frozen MemTable，即冻结MemTable）以及磁盘上的几种主要文件：当前（Current）文件、清单（Manifest）文件、操作日志（Commit Log，也称为提交日志）文件以及SSTable 文件。当应用写入一条记录时，LevelDB 会首先将修改操作写入到操作日志文件，成功后再将修改操作应用到MemTable，这样就完成了写入操作。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Structure of LSM" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/DS1_3_zpscf2c7552.png"&gt;&lt;/p&gt;
&lt;p&gt;当MemTable占用的内存达到一个上限值后，需要将内存的数据转储到外存文件中。LevelDB会将原先的MemTable 冻结成为不可变MemTable，并生成一个新的MemTable。新到来的数据被记入新的操作日志文件和新生成的MemTable中。顾名思义，不可变MemTable中的内容是不可更改的，只能读取不能写入或者删除。LevelDB 后台线程会将不可变MemTable的数据排序后转储到磁盘，形成一个新的SSTable 文件，这个操作称为&lt;code&gt;Compaction&lt;/code&gt;。SSTable文件是内存中的数据不断进行Compaction操作后形成的，且SSTable 的所有文件是一种层级结构，第0 层为Level 0，第1 层为Level 1，以此类推。&lt;/p&gt;
&lt;p&gt;SSTable中的文件是按照记录的主键排序的，每个文件有最小的主键和最大的主键。LevelDB的清单文件记录了这些元数据，包括属于哪个层级、文件名称、最小主键和最大主键。当前文件记录了当前使用的清单文件名。在LevelDB的运行过程中，随着Compaction的进行，SSTable文件会发生变化，新的文件会产生，老的文件被废弃，此时往往会生成新的清单文件来记载这种变化，而当前文件则用来指出哪个清单文件才是当前有效的。&lt;/p&gt;
&lt;p&gt;直观上，LevelDB每次查询都需要从老到新读取每个层级的SSTable文件以及内存中的MemTable。LevelDB做了一个优化，由于LevelDB对外只支持随机读取单条记录，查询时LevelDB首先会去查看内存中的MemTable，如果MemTable包含记录的主键及其对应的值，则返回记录即可；如果MemTable没有读到该主键，则接下来到同样处于内存中的不可变Memtable中去读取；类似地，如果还是没有读到，只能依次从新到老读取磁盘中的SSTable 文件。&lt;/p&gt;
&lt;h3&gt;合并&lt;/h3&gt;
&lt;p&gt;LevelDB写入操作很简单，但是读取操作比较复杂，需要在内存以及各个层级文件中按照从新到老依次查找，代价很高。为了加快读取速度，LevelDB内部会执行Compaction操作来对已有的记录进行整理压缩，从而删除一些不再有效的记录，减少数据规模和文件数量。&lt;/p&gt;
&lt;p&gt;LevelDB的Compaction操作分为两种：minor compaction 和major compaction。Minor compaction 是指当内存中的MemTable 大小到了一定值时，将内存数据转储到SSTable文件中。每个层级下有多个SSTable，当某个层级下的SSTable文件数目超过一定设置后，levelDB会从这个层级中选择SSTable 文件，将其和高一层级的SSTable 文件合并，这就是major compaction。major compaction相当于执行一次多路归并：按照主键顺序依次迭代出所有SSTable文件中的记录，如果没有保存价值，则直接抛弃；否则，将其写入到新生成的SSTable 文件中。&lt;/p&gt;
&lt;h2&gt;事务与并发控制&lt;/h2&gt;
&lt;h3&gt;写时复制以及多版本控制&lt;sup id="sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-2-back"&gt;&lt;a class="simple-footnote" href="#sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-2" title="该部分未完全弄懂，TODO!"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;h1&gt;分布式系统&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;基本概念&lt;/h2&gt;
&lt;h3&gt;一致性&lt;/h3&gt;
&lt;p&gt;由于异常的存在,分布式存储系统设计时往往会将数据冗余存储多份,每一份称为一个副本(replica/copy)。这样,当某一个节点出现故障时,可以从其他副本上读到数据。可以这么认为,副本是分布式存储系统容错技术的唯一手段。由于多个副本的存在,如何保证副本之间的一致性是整个分布式系统的理论核心。可以从两个角度理解一致性:第一个角度是用户,或者说是客户端,即客户端读写操作是否符合某种特性;第二个角度是存储系统,即存储系统的多个副本之间是否一致,更新的顺序是否相同,等等。&lt;/p&gt;
&lt;p&gt;首先定义如下场景,这个场景包含三个组成部分:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;存储系统 :存储系统可以理解为一个黑盒子,它为我们提供了可用性和持久性的保证。&lt;/li&gt;
&lt;li&gt;客户端A:客户端A主要实现从存储系统write和read操作。&lt;/li&gt;
&lt;li&gt;客户端B和客户端C:客户端B和C是独立于A,并且B和C也相互独立的,它们同时也实现对存储系统的write和read 操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从客户端的角度来看,一致性包含如下三种情况:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;强一致性 :假如A先写入了一个值到存储系统,存储系统保证后续A,B,C的读取操作都将返回最新值。当然,如果写入操作“超时”,那么成功或者失败都是可能的,客户端A不应该做任何假设。&lt;/li&gt;
&lt;li&gt;弱一致性 :假如A先写入了一个值到存储系统,存储系统不能保证后续 A,B,C的读取操作是否能够读取到最新值。&lt;/li&gt;
&lt;li&gt;最终一致性 :最终一致性是弱一致性的一种特例。假如A首先写入一个值到存储系统,存储系统保证如果后续没有写操作更新同样的值,A,B,C的读取操作“最终”都会读取到A写入的最新值。“最终”一致性有一个&lt;code&gt;不一致窗口&lt;/code&gt;的概念,它特指从A写入值,到后续A,B,C 读取到最新值的这段时间。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不一致性窗口”的大小依赖于以下的几个因素 :交互延迟,系统的负载,以及复制协议要求同步的副本数。&lt;/p&gt;
&lt;p&gt;最终一致性描述比较粗略,其他常见的变体如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;读写 (Read-your-writes)一致性 :如果客户端 A 写入了最新的值,那么 A的后续操作都会读取到最新值。但是其他用户(比如 B 或者 C)可能要过一会才能看到。&lt;/li&gt;
&lt;li&gt;会话 (Session)一致性:要求客户端和存储系统交互的整个会话期间保证读写一致性。如果原有会话因为某种原因失效而创建了新的会话,原有会话和新会话之间的操作不保证读写一致性。&lt;/li&gt;
&lt;li&gt;单调读 (Monotonic read)一致性 :如果客户端 A 已经读取了对象的某个值,那么后续操作将不会读取到更早的值。&lt;/li&gt;
&lt;li&gt;单调写 (Monotonic write)一致性:客户端A的写操作按顺序完成,这就意味着,对于同一个客户端的操作,存储系统的多个副本需要按照与客户端相同的顺序完成。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从存储系统的角度看,一致性主要包含如下几个方面:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;副本一致性 :存储系统的多个副本之间的数据是否一致,不一致的时间窗口等;&lt;/li&gt;
&lt;li&gt;更新顺序一致性 :存储系统的多个副本之间是否按照相同的顺序执行更新操作。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一般来说,存储系统可以支持强一致性,也可以为了性能考虑只支持最终一致性。从客户端的角度看,一般要求存储系统能够支持读写一致性,会话一致性,单调读,单调写等特性,否则,使用比较麻烦,适用的场景也比较有限。&lt;/p&gt;
&lt;h3&gt;性能指标&lt;/h3&gt;
&lt;p&gt;评价分布式存储系统有一些常用的指标,下面分别介绍。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;性能&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的性能指标有 :系统的吞吐能力以及系统的响应时间。其中,系统的吞吐能力指系统在某一段时间可以处理的请求总数,通常用每秒处理的读操作数(&lt;em&gt;QPS,Query Per Second&lt;/em&gt;)或者写操作数(&lt;em&gt;TPS,Transaction Per Second&lt;/em&gt;)来衡量;系统的响应延迟,指从某个请求发出到接收到返回结果消耗的时间,通常用平均延时或者99.9%以上请求的最大延时来衡量。这两个指标往往是矛盾的,追求高吞吐的系统,往往很难做到低延迟;追求低延迟的系统,吞吐量也会受到限制。因此,设计系统时需要权衡这两个指标。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可用性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;系统的可用性(availability)是指系统在面对各种异常时可以提供正常服务的能力。系统的可用性可以用系统停服务的时间与正常服务的时间的比例来衡量,例如某系统的可用性为4个9(99.99%),相当于系统一年停服务的时间不能超过 365 × 24 ×60 /10000 = 52.56 分钟。系统可用性往往体现了系统的整体代码质量以及容错能力。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面我们说明了系统的一致性。一般来说,越是强的一致性模型,用户使用起来越简单。笔者认为,如果系统部署在同一个数据中心,只要系统设计合理,在保证强一致性的前提下,不会对性能和可用性造成太大的影响。后文中笔者在Alibaba参与开发的OceanBase 系统以及 Google 的分布式存储系统都倾向强一致性。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可扩展性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;系统的可扩展性(scalability)指分布式存储系统通过扩展集群服务器规模来提高系统存储容量、计算量和性能的能力。随着业务的发展,对底层存储系统的性能需求不断增加,比较好的方式就是通过自动增加服务器提高系统的能力。理想的分布式存储系统实现了&lt;code&gt;线性可扩展&lt;/code&gt;,也就是说,随着集群规模的增加,系统的整体性能与服务器数量呈线性关系。&lt;/p&gt;
&lt;h2&gt;数据分布&lt;/h2&gt;
&lt;p&gt;分布式系统区别于传统单机系统在于能够将数据分布到多个节点,并在多个节点之间实现负载均衡。数据分布的方式主要有两种,一种是哈希分布,如一致性哈希,代表系统为Amazon的Dynamo系统;另外一种方法是顺序分布,即每张表格上的数据按照主键整体有序,代表系统为 Google的Bigtable系统。Bigtable将一张大表根据主键切分为有序的范围,每个有序范围是一个子表。将数据分散到多台机器后,需要尽量保证多台机器之间的负载是比较均衡的。衡量机器负载涉及的因素很多,如机器Load值,CPU,内存,磁盘以及网络等资源使用情况,读写请求数及请求量,等等,分布式存储系统需要能够自动识别负载高的节点,当某台机器的负载较高时，将它服务的部分数据迁移到其他机器,实现自动负载均衡。&lt;/p&gt;
&lt;p&gt;分布式存储系统的一个基本要求就是透明性,包括&lt;strong&gt;数据分布透明性,数据迁移透明性,数据复制透明性,故障处理透明性&lt;/strong&gt;。本节介绍数据分布以及数据迁移相关的基础知识。&lt;/p&gt;
&lt;h3&gt;哈希分布&lt;/h3&gt;
&lt;p&gt;哈希取模的方法很常见,其方法是根据数据的某一种特征计算哈希值,并将哈希值与集群中的服务器建立映射关系,从而将不同哈希值的数据分布到不同的服务器上。所谓数据特征可以是key-value系统中的主键(key),也可以是其他与业务逻辑相关的值。例如,将集群中的服务器按 0 到 N-1 编号(N为服务器的数量),根据数据的主键(hash(key)%N)或者数据所属的用户id(hash(user_id)% N)计算哈希值,来决定将数据映射到哪一台服务器。如果哈希函数的散列特性很好,哈希方式可以将数据比较均匀地分布到集群中去。而且,哈希方式需要记录的元信息也非常简单,每个节点只需要知道哈希函数的计算方式以及模的服务器的个数就可以计算出处理的数据应该属于哪台机器。然而,找出一个散列特性很好的哈希函数是很难的。这是因为,如果按照主键散列,那么同一个用户 id下的数据可能被分散到多台服务器,这会使得一次操作同一个用户id下的多条记录变得困难;如果按照用户id散列,容易出现“数据倾斜”(data skew)问题,即某些大用户的数据量很大,无论集群的规模有多大,这些用户始终由一台服务器处理。处理大用户问题一般有两种方式,一种方式是手动拆分,即线下标记系统中的大用户(例如运行一次MapReduce作业),并根据这些大用户的数据量将其拆分到多台服务器上。这就相当于在哈希分布的基础上针对这些大用户特殊处理;另一种方式是自动拆分,即数据分布算法能够动态调整,自动将大用户的数据拆分到多台服务器上。&lt;/p&gt;
&lt;p&gt;传统的哈希分布算法还有一个问题:当服务器上线或者下线时,N值发生变化,数据映射完全被打乱,几乎所有的数据都需要重新分布,这将带来大量的数据迁移。一种思路是不再简单地将哈希值和服务器个数做除法取模映射,而是将哈希值与服务器的对应关系作为元数据,交给专门的元数据服务器来管理。访问数据时,首先计算哈希值,再查询元数据服务器,获得该哈希值对应的服务器。这样,集群扩容时,可以将部分哈希值分配给新加入的机器并迁移对应的数据。另一种思路就是采用一致性哈希(Distributed Hash Table,DHT) 算法。一致性哈希的具体思想如下:&lt;sup id="sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-3-back"&gt;&lt;a class="simple-footnote" href="#sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-3" title="该部分参考一致性hash算法"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h4&gt;基本场景&lt;/h4&gt;
&lt;p&gt;比如你有N个cache 服务（后面简称cache)，那么如何将一个对象object映射到N个cache上呢，你很可能会采用类似下面的通用方法计算object的hash 值，然后均匀的映射到到N个cache:&lt;/p&gt;
&lt;p&gt;$$ hash(object) \% N $$&lt;/p&gt;
&lt;p&gt;一切都运行正常，再考虑如下的两种情况:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个cache服务器m宕掉了（在实际应用中必须要考虑这种情况，这样所有映射到 cache m 的对象都会失效，怎么办，需要把 cache m 从cache 中移除，这时候 cache 是 N-1台，映射公式变成了$hash(object)\%(N-1)$；&lt;/li&gt;
&lt;li&gt;由于访问加重，需要添加 cache，这时候 cache 是 N+1 台，映射公式变成了$hash(object)\%(N+1)$；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上述两种情况意味着什么？这意味着突然之间几乎所有的cache都失效了。对于服务器而言，这是一场灾难，洪水般的访问都会直接冲向后台服务器；再来考虑第三个问题，由于硬件能力越来越强，你可能想让后面添加的节点多做点活，显然上面的 hash 算法也做不到。&lt;/p&gt;
&lt;p&gt;有什么方法可以改变这个状况呢，当当当,于是一致性哈希算法就开始登上历史的舞台。&lt;/p&gt;
&lt;h4&gt;哈希算法和单调性&lt;/h4&gt;
&lt;p&gt;Hash 算法的一个衡量指标是单调性（Monotonicity)，定义如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已&amp;gt; 分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;容易看到，上面的简单hash算法$hash(object)\%N$难以满足单调性要求。&lt;/p&gt;
&lt;h4&gt;算法原理&lt;/h4&gt;
&lt;p&gt;consistent hashing是一种hash算法，简单的说,在移除 / 添加一个 cache 时，它能够尽可能小的改变已存在 key的映射关系，尽可能的满足单调性的要求。&lt;/p&gt;
&lt;p&gt;下面就来按照5个步骤简单讲讲 consistent hashing 算法的基本原理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;环形hash空间
考虑通常的 hash 算法都是将value映射到一个32位的key值，也即是0~2^32-1次方的数值空间；我们可以将这个空间想象成一个首（0）尾（ 2^32-1 ）相接的圆环，如下图所示:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="环形hash空间" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hash1_zpsb64e03c5.jpg"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把对象映射到hash 空间&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;接下来考虑4个对象 object1~object4 ，通过 hash 函数计算出的hash值 key在环上的分布如下图所示。
hash(object1) = key1;
… …
hash(object4) = key4;&lt;/p&gt;
&lt;p&gt;&lt;img alt="对象初始hash分布" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hash2_zps95f7c0ac.jpg"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;把cache映射到hash空间
Consistent hashing 的基本思想就是将对象和cache都映射到同一个hash数值空间中，并且使用相同的 hash 算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设当前有 A,B和C共3台 cache ，那么其映射结果将如下图所示，他们在 hash 空间中，以对应的hash值排列。&lt;/p&gt;
&lt;p&gt;hash(cache A) = key A;
… …
hash(cache C) = key C;&lt;/p&gt;
&lt;p&gt;&lt;img alt="cache分布" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hash3_zps400e1a2f.jpg"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:说到这里，顺便提一下 cache 的 hash 计算，一般的方法可以使用 cache 机器的 IP 地址或者机器名作为 hash 输入。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;把对象映射到cache&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;现在cache 和对象都已经通过同一个 hash 算法映射到 hash 数值空间中了，接下来要考虑的就是如何将对象映射到 cache 上面了。&lt;/p&gt;
&lt;p&gt;在这个环形空间中，如果沿着顺时针方向从对象的key值出发，直到遇见一个cache，那么就将该对象存储在这个cache上，因为对象和cache的hash值是固定的，因此这个cache必然是唯一和确定的。这样不就找到了对象和 cache 的映射方法了吗？！&lt;/p&gt;
&lt;p&gt;依然继续上面的例子（参见上图），那么根据上面的方法，对象 object1 将被存储到cache A上；object2和object3对应到cache C；object4 对应到cache B ；&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;考察cache的变动
前面讲过，通过 hash 然后求余的方法带来的最大问题就在于不能满足单调性，当cache有所变动时，cache会失效，进而对后台服务器造成巨大的冲击，现在就来分析分析 consistent hashing算法。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(1)移除 cache&lt;/p&gt;
&lt;p&gt;考虑假设cache B挂掉了，根据上面讲到的映射方法，这时受影响的将仅是那些沿 cache B逆时针遍历直到下一个cache（cache C）之间的对象，也即是本来映射到 cache B上的那些对象。&lt;/p&gt;
&lt;p&gt;因此这里仅需要变动对象object4,将其重新映射到 cache C 上即可,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="移除Cache" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hash4_zpsf41b0bf7.jpg"&gt;&lt;/p&gt;
&lt;p&gt;(2)添加 cache&lt;/p&gt;
&lt;p&gt;再考虑添加一台新的cache D的情况，假设在这个环形hash空间中，cache D被映射在对象object2和object3之间。这时受影响的将仅是那些沿cache D逆时针遍历直到下一个cache（cache B)之间的对象（它们是也本来映射到cache C上对象的一部分),将这些对象重新映射到cache D上即可。因此这里仅需要变动对象 object2,将其重新映射到cache D上,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="添加Cache" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hash5_zpsc28b2bf6.jpg"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;虚拟节点
考量Hash算法的另一个指标是平衡性 (Balance)，定义如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;hash算法并不是保证绝对的平衡，如果 cache较少的话，对象并不能被均匀的映射到cache上，比如在上面的例子中，仅部署cache A和cache C的情况下，在4个对象中， cache A 仅存储了object1 ，而cache C则存储了object2、object3和object4；分布是很不均衡的。&lt;/p&gt;
&lt;p&gt;为了解决这种情况,consistent hashing 引入了“虚拟节点”的概念，它可以如下定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“虚拟节点”（ virtual node ）是实际节点在hash空间的复制品（replica),一实际个节点对应了若干个“虚拟节点”，这个对应&amp;gt; 个数也成为“复制个数”，“虚拟节点”在hash空间中以hash值排列。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;仍以仅部署 cache A 和 cache C的情况为例，在上面我们已经看到,cache分布并不均匀。现在我们引入虚拟节点，并设置“复制个数”为 2 ，这就意味着一共会存在 4 个“虚拟节点”， cache A1, cache A2 代表了 cache A ； cache C1, cache C2 代表了 cache C ；假设一种比较理想的情况，如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="virtual nodes" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hash6_zpse7e157d4.jpg"&gt;&lt;/p&gt;
&lt;p&gt;此时，对象到“虚拟节点”的映射关系为：
objec1-&amp;gt;cache A2；objec2-&amp;gt;cache A1；objec3-&amp;gt;cache C1；objec4-&amp;gt;cache C2；因此对象 object1 和 object2 都被映射到了 cache A 上，而 object3 和 object4 映射到了 cache C 上；平衡性有了很大提高。引入“虚拟节点”后，映射关系就从 { 对象 -&amp;gt; 节点 } 转换到了 { 对象 -&amp;gt; 虚拟节点 } 。查询物体所在cache时的映射关系如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Query Cache" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hash7_zps66159090.jpg"&gt;&lt;/p&gt;
&lt;p&gt;“虚拟节点”的hash计算可以采用对应节点的IP地址加数字后缀的方式。例如假设cache A的IP地址为 202.168.14.241。引入“虚拟节点”前，计算 cache A 的 hash 值：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Hash&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="mf"&gt;202.168.14.241&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;引入“虚拟节点”后，计算“虚拟节”点 cache A1 和 cache A2 的 hash 值：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Hash&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="mf"&gt;202.168.14.241&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;  &lt;span class="c1"&gt;// cache A1&lt;/span&gt;
&lt;span class="n"&gt;Hash&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;“&lt;/span&gt;&lt;span class="mf"&gt;202.168.14.241&lt;/span&gt;&lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="err"&gt;”&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;  &lt;span class="c1"&gt;// cache A2&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;顺序分布&lt;/h3&gt;
&lt;p&gt;哈希散列破坏了数据的有序性,只支持随机读取操作,不能够支持顺序扫描。某些系统可以在应用层做折衷,比如互联网应用经常按照用户来进行数据拆分,并通过哈希方法进行数据分布,同一个用户的数据分布到相同的存储节点,允许对同一个用户的数据执行顺序扫描,由应用层解决跨多个用户的操作问题。另外,这种方式可能出现某些用户的数据量太大的问题,由于用户的数据限定在一个存储节点,无法发挥分布式存储系统的多机并行处理能力。&lt;/p&gt;
&lt;p&gt;顺序分布在分布式表格系统中比较常见,一般的做法是将大表顺序划分为连续的范围,每个范围称为一个子表,总控服务器负责将这些子表按照一定的策略分配到存储节点上。如下图所示,用户表(User表)的主键范围为1~7000,在分布式存储系统中划分为多个子表,分别对应数据范围 1 ~ 1000,1001~2000,...6001~7000。Meta表是可选的,某些系统只有根表(Root表)一级索引,在Root表中维护用户表的位置信息,即每个User子表在哪个存储节点上。为了支持更大的集群规模,Bigtable这样的系统将索引分为两级:根表以及元数据表(Meta 表),由 Meta表维护User表的位置信息,而 Root 表用来维护Meta表的位置信息。读User表时,需要通过Meta 表查找相应的 User 子表所在的存储节点,而读取 Meta 表又需要通过 Root 表查找相应的 Meta 子表所在的存储节点。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sequence Distribution" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sequence_distribution_zps17ca6949.png"&gt;&lt;/p&gt;
&lt;p&gt;顺序分布与B+树数据结构比较类似,每个子表相当于叶子节点,随着数据的插入和删除,某些子表可能变得很大,某些变得很小,数据分布不均匀。如果采用顺序分布,系统设计时需要考虑子表的分裂与合并,这将极大地增加系统复杂度。子表分裂指当一个子表太大超过一定阀值时需要分裂为两个子表,从而有机会通过系统的负载均衡机制分散到多个存储节点。子表合并一般由数据删除引起,当相邻的两个子表都很小时,可以合并为一个子表。一般来说,单个服务节点能够服务的子表数量是有限的,比如 4000~10000 个,子表合并的目的是为了防止系统中出现过多太小的子表,减少系统中的元数据。&lt;/p&gt;
&lt;h2&gt;分布式协议&lt;/h2&gt;
&lt;p&gt;分布式系统涉及的协议很多,例如租约,复制协议,一致性协议,其中以两阶段提交协议和Paxos协议最具有代表性。两阶段提交协议用于保证跨多个节点操作的原子性,也就是说,跨多个节点的操作要么在所有节点上全部执行成功,要么全部失败。Paxos协议用于确保多个节点对某个投票(例如哪个节点为主节点)达成一致。本节介绍这两个分布式协议。&lt;/p&gt;
&lt;h3&gt;两阶段提交协议&lt;/h3&gt;
&lt;p&gt;两阶段提交协议(Two-phase Commit,2PC)经常用来实现分布式事务,在两阶段协议中,系统一般包含两类节点:一类为协调者(coordinator),通常一个系统中只有一个;另一类为事务参与者(participants,cohorts或workers),一般包含多个。协议中假设每个节点都会记录操作日志并持久化到非易失性存储介质,即使节点发生故障日志也不会丢失。顾名思义,两阶段提交协议由两个阶段组成。在正常的执行过程中,这两个阶段的执行过程如下所述:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;阶段 1 :请求阶段(Prepare Phase)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在请求阶段,协调者通知事务参与者准备提交或者取消事务,然后进入表决过程。在表决过程中,参与者将告知协调者自己的决策 :同意(事务参与者本地执行成功)或者取消(事务参与者本地执行失败)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;阶段 2 :提交阶段(Commit Phase)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在提交阶段,协调者将基于第一个阶段的投票结果进行决策:提交或者取消。当且仅当所有的参与者同意提交事务协调者才通知所有的参与者提交事务,否则协调者通知所有的参与者取消事务。参与者在接收到协调者发来的消息后将执行相应的操作。&lt;/p&gt;
&lt;p&gt;例如,A 组织 B、C 和 D 三个人去爬长城:如果所有人都同意去爬长城,那么活动将举行;如果有一人不同意去爬长城,那么活动将取消。用 2PC 算法解决该问题的过程如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先 A 将成为该活动的协调者,B、C 和 D将成为该活动的参与者。&lt;/li&gt;
&lt;li&gt;准备阶段 :A 发邮件给 B、C和D,提出下周三去爬山,问是否同意。那么此时A需要等待B、C和D的回复。B、C和D分别查看自己的日程安排表。B、C发现自己在当日没有活动安排,则发邮件告诉A他们同意下周三去爬长城。由于某种原因,D白天没有查看邮件。那么此时 A、B 和 C 均需要等待。到晚上的时候,D 发现了A的邮件,然后查看日程安排,发现下周三当天已经有别的安排,那么 D回复A说活动取消吧。&lt;/li&gt;
&lt;li&gt;此时A收到了所有活动参与者的邮件,并且A发现D下周三不能去爬山。那么A将发邮件通知B、C和D,下周三爬长城活动取消。此时 B、C 回复A“太可惜了”,D回复 A“不好意思”。至此该事务终止。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过该例子可以发现,2PC 协议存在明显的问题。假如D一直不能回复邮件,那么 A、B 和 C 将不得不处于一直等待的状态。并且 B 和 C 所持有的资源一直不能释放,即下周三不能安排其他活动。当然,A可以发邮件告诉D如果晚上六点之前不回复活动就自动取消,通过引入事务的超时机制防止资源一直不能释放的情况。更为严重的是,假如A发完邮件后生病住院了,即使B、C和D都发邮告诉A同意下周三去爬长城,如果A没有备份,事务将被阻塞,B、C和D下周三都不能安排其他活动。&lt;/p&gt;
&lt;p&gt;两阶段提交协议可能面临两种故障:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;事务参与者发生故障。给每个事务设置一个超时时间,如果某个事务参与者一直不响应,到达超时时间后整个事务失败。&lt;/li&gt;
&lt;li&gt;协调者发生故障。协调者需要将事务相关信息记录到操作日志并同步到备用协调者,假如协调者发生故障,备用协调者可以接替它完成后续的工作。如果没有备用协调者,协调者又发生了永久性故障,事务参与者将无法完成事务而一直等待下去。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总而言之,两阶段提交协议是阻塞协议,执行过程中需要锁住其他更新,且不能容错,大多数分布式存储系统都采用敬而远之的做法,放弃对分布式事务的支持。&lt;/p&gt;
&lt;h3&gt;Paxos 协议&lt;/h3&gt;
&lt;p&gt;Paxos协议用于解决多个节点之间的一致性问题。多个节点之间通过操作日志同步数据,如果只有一个节点为主节点,那么,很容易确保多个节点之间操作日志的一致性。考虑到主节点可能出现故障,系统需要选举出新的主节点。Paxos协议正是用来实现这个需求。只要保证了多个节点之间操作日志的一致性,就能够在这些节点上构建高可用的全局服务,例如分布式锁服务,全局命名和配置服务等。&lt;/p&gt;
&lt;p&gt;为了实现高可用性,主节点往往将数据以操作日志的形式同步到备节点。如果主节点发生故障,备节点会提议自己成为主节点。这里存在的问题是网络分区的时候,可能会存在多个备节点提议(Proposer,提议者)自己成为主节点。Paxos协议保证,即使同时存在多个 proposer,也能够保证所有节点最终达成一致,即选举出唯一的主节点。大多数情况下,系统只有一个proposer,他的提议也总是会很快地被大多数节点接受。Paxos 协议执行步骤如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;批准(accept):Proposer 发送accept消息要求所有其他节点(acceptor,接受者)接受某个提议值,acceptor可以接受或者拒绝。&lt;/li&gt;
&lt;li&gt;确认(acknowledge):如果超过一半的acceptor接受,意味着提议值已经生效,proposer发送acknowledge消息通知所有的acceptor提议生效。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当出现网络或者其他异常时,系统中可能存在多个proposer,他们各自发起不同的提议。这里的提议可以是一个修改操作,也可以是提议自己成为主节点。如果 proposer第一次发起的 accept 请求没有被 acceptor 中的多数派批准(例如与其他 proposer 的提议冲突),那么,需要完整地执行一轮Paxos协议。过程如下:(这个过程不靠谱,如果有时间的话会更新,没有解释Paxos协议的实质)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;准备(prepare):Proposer 首先选择一个提议序号 n 给其他的 acceptor 节点发送 prepare 消息。Acceptor 收到 prepare 消息后,如果提议的序号大于他已经回复的所有prepare消息,则acceptor将自己上次接受的提议回复给proposer,并承诺不再回复小于n的提议。&lt;/li&gt;
&lt;li&gt;批准(accept):Proposer 收到了acceptor中的多数派对prepare的回复后,就进入批准阶段。如果在之前的prepare阶段acceptor回复了上次接受的提议,那么,proposer 选择其中序号最大的提议值发给acceptor批准;否则,proposer生成一个新的提议值发给 acceptor 批准。Acceptor 在不违背他之前在 prepare 阶段的承诺的前提下,接受这个请求。&lt;/li&gt;
&lt;li&gt;确认(acknowledge):如果超过一半的 acceptor 接受,提议值生效。Proposer发送 acknowledge 消息通知所有的 acceptor 提议生效。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Paxos协议需要考虑两个问题 :正确性,即只有一个提议值会生效;可终止性,即最后总会有一个提议值生效。Paxos协议中要求每个生效的提议被 acceptor 中的多数派接受,并且每个acceptor不会接受两个不同的提议,因此可以保证正确性。Paxos协议并不能够严格保证可终止性。但是,从 Paxos协议的执行过程可以看出,只要超过一个acceptor接受了提议,proposer很快就会发现,并重新提议其中序号最大的提议值。因此,随着协议不断运行,它会往“某个提议值被多数派接受并生效”这一最终目标
靠拢。&lt;/p&gt;
&lt;h3&gt;Paxos 与 2PC&lt;/h3&gt;
&lt;p&gt;Paxos协议和2PC协议在分布式系统中所起的作用并不相同。Paxos协议用于保证同一个数据分片的多个副本之间的数据一致性。当这些副本分布到不同的数据中心时,这个需求尤其强烈。2PC协议用于保证属于多个数据分片上的操作的原子性。这些数据分片可能分布在不同的服务器上,2PC 协议保证多台服务器上的操作要么全部成功,要么全部失败。&lt;/p&gt;
&lt;p&gt;Paxos协议有两种用法:一种用法是用它来实现全局的锁服务或者命名和配置服务,例如 Google Chubby 以及Apache Zookeeper。另外一种用法是用它来将用户数据复制到多个数据中心,例如 Google Megastore 以及 Google Spanner。&lt;/p&gt;
&lt;p&gt;2PC协议最大的缺陷在于无法处理协调者宕机问题。如果协调者宕机,那么,2PC协议中的每个参与者可能都不知道事务应该提交还是回滚,整个协议被阻塞,执行过程中申请的资源都无法释放。因此,常见的做法是将2PC和Paxos协议结合起来,通过2PC保证多个数据分片上的操作的原子性,通过Paxos协议实现同一个数据分片的多个副本之间的一致性。另外,通过Paxos协议解决2PC协议中协调者宕机问题。当 2PC协议中的协调者出现故障时,通过 Paxos 协议选举出新的协调者继续提供服务。&lt;/p&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-1"&gt;关于NoSQL的详细资料参见&lt;a href="http://sebug.net/paper/databases/nosql/Nosql.html"&gt;NoSQL数据库笔谈&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-2"&gt;该部分未完全弄懂，TODO! &lt;a class="simple-footnote-back" href="#sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-3"&gt;该部分参考&lt;a href="http://blog.csdn.net/sparkliang/article/details/5279393"&gt;一致性hash算法&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-fen-bu-shi-ji-suan-yu-cun-chu-xi-lie-xu-zhang-chu-ru-men-jing-3-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="分布式系统"></category><category term="Paxos"></category><category term="一致性哈希算法"></category></entry><entry><title>机器学习系列(II):Generative models for discrete data</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html" rel="alternate"></link><updated>2014-03-04T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-03-04:ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html</id><summary type="html">&lt;h1&gt;博客若干事&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;博客更新&lt;/h2&gt;
&lt;p&gt;根据目前的学习进度、自己的空闲时间以及时间的充裕度，现将博客的更新时间定于周三，更新周期为每一周或者两周更新一次。另由于目前自己对于Latex公式还不是特别熟，所以博文中的公式可能会出现部分错误，请大家谅解。此外，博客刚刚创建，很多东西都在完善当中，包括博客的插件，博文的排版等等，这些方面之后会慢慢完善，目前已开放的功能仅基本支持博文的显示以及评论。&lt;/p&gt;
&lt;p&gt;由于机器学习领域问题一般涉及公式较多，目前采取的渲染方式是通过相应的JS插件，导致的直接后果是页面的载入速度较慢，这方面以后可能将公式转换为图片然后输出。&lt;/p&gt;
&lt;p&gt;好吧，博客方面要说的就这么多吧。&lt;/p&gt;
&lt;h2&gt;机器学习浅谈&lt;/h2&gt;
&lt;p&gt;机器学习要研究的问题无非有四:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为什么要学习?&lt;/li&gt;
&lt;li&gt;学习什么？&lt;/li&gt;
&lt;li&gt;怎么学习？&lt;/li&gt;
&lt;li&gt;怎么更好地学习？&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;也许所有的理论，所有的事无非要解决的就是这四件事吧，为什么、做什么、怎么做、怎么做好。(作者注)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;大概所有的思想、理论、模型大致是围绕这四个方向进行的，而且这四个问题都得到了较好的解决。以上这些理论比较繁杂，而且我也没完全弄懂，所以咱们慢慢啃吧。&lt;/p&gt;
&lt;p&gt;今天我们要谈的是主要是生成模型，与之对应的则是判别模型。生成模型和判别模型的区别在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成模型首先计算联合概率分布$p(x,y)$,然后据此计算$p(y|x)$;而判别模型往往直接计算$p(y|x)$;&lt;/li&gt;
&lt;li&gt;生成模型关注数据是怎么生成的，然后进行预测；判别模型不关注数据的具体生成过程，直接预测。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文主要介绍针对离散数据的生成模型，限于篇幅，本文仅对其中其中的两个模型进行介绍------Dirichlet-Multinomial Model以及朴素贝叶斯分类器，Dirichlet-Multinomial Model被广泛使用在各种语言模型中，而朴素贝叶斯分类器最为人所知的应用大概就是垃圾邮件过滤(Spam Filtering)了吧。&lt;/p&gt;
&lt;p&gt;以下我们正式开始介绍这两个模型。&lt;/p&gt;
&lt;h1&gt;Dirichlet-multinomial Model&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;我们现在要Model的问题很简单，假设现有一个$K$面的骰子，我们需要推断它出现第$k$面的概率。&lt;/p&gt;
&lt;h2&gt;Likelihood&lt;/h2&gt;
&lt;p&gt;假设我们掷骰子$N$次，$D={x_1,...,x_N}$,其中，$x_i\in {1,...,K}$.我们假设数据是独立同分布的(iid),Likelihood则有如下形式：
\begin{equation}
p(D|\theta) = \prod_{k=1}^{K}\theta_k^{N_k}
\end{equation}
其中，$N_k = \sum_{i=1}^{N}1_{y_i=k}$是第$k$面出现的次数。(1为指示函数，下同)&lt;/p&gt;
&lt;h2&gt;先验分布&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Machine Learning:A probabilistic perspective&lt;/code&gt;一书告诉我们如果先验分布和Likelihood的形式相同(共轭先验分布，conjugate prior)时，能够很好的简化我们的计算过程。基于此理，我们选择Dirichlet分布作为我们的先验分布，Dirichlet分布具有如下形式:
\begin{equation}
Dir(\theta|\alpha) = \frac{1}{B(\alpha)}\prod_{k=1}^{K} \theta_k^{\alpha_k-1}1_{x \in S_k}
\end{equation}&lt;/p&gt;
&lt;h2&gt;后验分布&lt;/h2&gt;
&lt;p&gt;将Likelihood乘以Prior，后验分布的形式告诉我们后验分布也服从Dirichlet Distribution:
\begin{equation}
p(\theta|D)     \propto p(D|\theta)p(\theta)    \
                \propto \prod_{k=1}^{K} \theta_k^{N_k}\theta_{k}^{\alpha_k-1}=\prod_{k=1}^{K}\theta_k^{\alpha_k+N_k-1} \
                =Dir(\theta|\alpha_1+N_1,...,\alpha_K+N_K)
\end{equation}
现在我们计算关于参数$\theta$的极大后验估计(MAP),其中,$\sum_{k}\theta_k=1$.
引入Lagrange乘子之后我们需要优化的目标函数为:
\begin{equation}
l(\theta,\lambda) = \sum_{k} N_klog\theta_k+\sum_{k}(\alpha_k-1)log\theta_k+\lambda(1-\sum_{k}\theta_k)
\end{equation}
为简便起见，记$N\prime_k\triangleq N_k+\alpha_k-1$.对$\lambda$求导得：
\begin{equation}
\frac{\partial l}{\partial \lambda}=(1-\sum_{k}\theta_k) = 0
\end{equation}
对$\theta_k$求导得，
\begin{equation}
\frac{\partial l}{\partial \theta_k}=\frac{N\prime_k}{\theta_k}-\lambda=0 \
N\prime_k = \lambda\theta_k
\end{equation}
由上两式得：
\begin{equation}
\sum_{k} N\prime_k = \lambda\sum_{k}\theta_k  \
N+\alpha_0-K=\lambda
\end{equation}
其中，$\alpha_0\triangleq \sum_{k=1}^{K}\alpha_k$是先验的有效样本大小。因此我们可以得出极大后验估计值为：
\begin{equation}
\hat{\theta}_k=\frac{N_k+\alpha_k-1}{N+\alpha_0-K}
\end{equation}
如果采用uniform prior $\alpha_k=1$，这时得到的最大后验估计值即与经验值相同。
\begin{equation}
\hat{\theta_k} = \frac{N_k}{N}
\end{equation}&lt;/p&gt;
&lt;h2&gt;Posterior predicative&lt;/h2&gt;
&lt;p&gt;\begin{equation}
p(X=j|D) = \int P(X=j|\theta)p(\theta|D)d\theta \
=\int P(X=j|\theta_j)[\int p(\theta_{-j},\theta_j|D)d\theta{-j}]d\theta_j \
=\int \theta_jp(\theta_j|D)d\theta_j=E[\theta_j|D] = \frac{\alpha_j+N_j}{\sum_{k}\alpha_k+N_k}=\frac{\alpha_j+N_j}{\alpha_0+N}
\end{equation}
其中，$\theta_{-j}$代表$\theta$中除$\theta_j$之外的所有分量。&lt;/p&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;上述模型的一个很重要的应用场景是语言模型，即预测一个序列中下一个可能出现的词。以下我们举一个非常简单的例子，我们假定每一个词$X_i \in {1,...,K}$都是通过$Cat(\theta)$独立取样得到的，该模型被称为bag of words model.给定一已知的样本序列，我们需要预测下一个最可能出现的是什么词?&lt;/p&gt;
&lt;p&gt;如，假设我们取样得到如下样本:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Mary&lt;/span&gt; &lt;span class="n"&gt;had&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;little&lt;/span&gt; &lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;little&lt;/span&gt; &lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;little&lt;/span&gt; &lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;Mary&lt;/span&gt; &lt;span class="n"&gt;had&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;little&lt;/span&gt; &lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;its&lt;/span&gt; &lt;span class="n"&gt;fleece&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;white&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;snow&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;另外，我们假设我们的字典中有如下词:&lt;/p&gt;
&lt;p&gt;&lt;img alt="words" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/words_zps55d01c8c.png" /&gt;&lt;/p&gt;
&lt;p&gt;这里unk代表unknown，表示未在样本中出现过的所有词。为了给上述样本中的每一行进行编码，我们先从采样样本中去掉标点符号以及&lt;code&gt;停用词&lt;/code&gt;(即没有实际意义的词，一般只是各种助词等),如，a,as,the等。此外我们还需要对所有的词进行处理仅得到其词根，如saw处理为see，running处理为run等。最后，我们对每一行进行索引编号,得到如下结果:&lt;/p&gt;
&lt;p&gt;&lt;img alt="index" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/index_zpsba0a3fb0.png?t=1393987077" /&gt;&lt;/p&gt;
&lt;p&gt;这里我们不考虑词序，仅考虑每个词在样本中出现的次数。统计得到如下结果:&lt;/p&gt;
&lt;p&gt;&lt;img alt="token_count" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/token_count_zps1876920b.png?t=1393987078" /&gt;&lt;/p&gt;
&lt;p&gt;将以上每个计数值记为$N_j$,如果对于$\theta$我们采用Dirichlet先验分布，则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(\bar{X}_j|D)=E[\theta_j|D]=\frac{\alpha_j+N_j}{\sum_t \alpha_t+N_t}=\frac{1+N_j}{10+17}
\end{equation}&lt;/p&gt;
&lt;p&gt;通过代入每一个计数值，我们便能得出每个词出现的概率。至此，我们得到了该语言模型的所有参数，进而可以进行各种预测。&lt;/p&gt;
&lt;h1&gt;Naive Bayes Classifier&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;引子&lt;/h2&gt;
&lt;p&gt;朴素贝叶斯分类是一种十分简单的分类算法，它的思想很naive,但是其实际应用效果还是不错的。所以一个模型的好坏并非在于其复杂度，而在于我们是否将它用到了正确的地方，就算一个非常Naive的模型，如果用在了恰当的地方，也能产生很好的效果。具体就机器学习算法而言，只有真正对一个算法的特性、适用条件、优缺点有非常深刻的理解，才能真正把机器学习算法或模型用好。朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。&lt;/p&gt;
&lt;p&gt;引用&lt;a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html"&gt;CodingLabs&lt;/a&gt;提到的一个例子来抛出我们的问题，并以此为基础介绍我们的模型:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;对于&lt;/span&gt;&lt;span class="n"&gt;SNS&lt;/span&gt;&lt;span class="err"&gt;社区来说，不真实账号（使用虚假身份或用户的小号）是一个普遍存在的问题，作为&lt;/span&gt;&lt;span class="n"&gt;SNS&lt;/span&gt;&lt;span class="err"&gt;社区的运营商，希望可以检测出这些不真实账号，从而在一些运营分析报告中避免这些账号的干扰，亦可以加强对&lt;/span&gt;&lt;span class="n"&gt;SNS&lt;/span&gt;&lt;span class="err"&gt;社区的了解与监管。&lt;/span&gt;
&lt;span class="err"&gt;如果通过纯人工检测，需要耗费大量的人力，效率也十分低下，如能引入自动检测机制，必将大大提升工作效率。这个问题说白了，就是要将社区中所有账号在真实账号和不真实账号两个类别上进行分类。&lt;/span&gt;
&lt;span class="err"&gt;首先设&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="err"&gt;表示真实账号，&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="err"&gt;表示不真实账号。&lt;/span&gt;
&lt;span class="err"&gt;假设我们目前确定能作为评判用户帐号是否真实的特征属性有如下几个&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;实际上在机器学习领域确定特征属性是一项特别重要且复杂的工作，我们这里为了简化，直接给出本问题的特征属性&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;F1&lt;/span&gt;&lt;span class="err"&gt;：日志数量&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="err"&gt;注册天数；&lt;/span&gt;&lt;span class="n"&gt;F2&lt;/span&gt;&lt;span class="err"&gt;：好友数量&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="err"&gt;注册天数；&lt;/span&gt;&lt;span class="n"&gt;F3&lt;/span&gt;&lt;span class="err"&gt;：是否使用真实头像。在&lt;/span&gt;&lt;span class="n"&gt;SNS&lt;/span&gt;&lt;span class="err"&gt;社区中这三项都是可以直接从数据库里得到或计算出来的。&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;对这些属性进行区间划分保证这些属性取离散值&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="err"&gt;接下来的工作是我们从数据库得到了一些新的记录，给出了如上三个特征，我们需要预测这些用户是否真实的用户。&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Introduction to Naive Bayes Classifier&lt;/h2&gt;
&lt;p&gt;Naive Bayes Classifier要解决的问题是对于具有D个特征属性，每个属性可以取${1,...,K}$中任意一个值的样本进行分类，即$x \in {1,...,K}^D$。朴素贝叶斯分类器是一个生成模型，我们需要计算关于类别的条件概率$p(x|y=c)$.朴素贝叶斯假定给定类别c的条件下，各特征属性之间是相互独立的。于是我们有:
\begin{equation}
p(x|y=c,\theta) = \prod_{j=1}^{D} p(x_j|y=c,\theta{jc})
\end{equation}
我们得到的模型即为Naive Bayes Classifier(NBC).在上面的SNS真实用户检测的例子中，C=2,D=3。&lt;/p&gt;
&lt;p&gt;Naive Bayes Classifier的基本算法流程如下所示:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Algorithm&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt; &lt;span class="n"&gt;Naive&lt;/span&gt; &lt;span class="n"&gt;Bayes&lt;/span&gt; &lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="err"&gt;算法框架&lt;/span&gt;
&lt;span class="mf"&gt;1.&lt;/span&gt;  &lt;span class="err"&gt;根据得到的样本数据计算在每一可能的类别下各属性取值的条件概率，即计算&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="mf"&gt;2.&lt;/span&gt;  &lt;span class="err"&gt;根据计算得到的条件概率计算新样本属于各个类别的概率，即计算&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="mf"&gt;3.&lt;/span&gt;  &lt;span class="err"&gt;比较计算得到的新样本属于不同类别的概率值，选择值最大的那个类别作为新样本的类别。&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这里不给出针对具体数据的计算过程，想了解具体每一步怎么算的亲们请参考&lt;a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html"&gt;算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)&lt;/a&gt;。&lt;/p&gt;
&lt;h1&gt;Mutual Information&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;NBC需要计算关于很多特征的联合概率分布，可能会导致过拟合；此外，算法的运行时间是$O(CD)$,对于有些应用来说可能计算量太大了。一个解决上述问题的普遍被采用的方案是进行特征选取，去掉和分类无关的无用属性。最简单的方法是考察每个特征与分类属性之间的相关性，并权衡复杂性以及准确度选取K个最相关的属性用于训练。该方法被称为&lt;code&gt;variable ranking,filtering,or screening&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;衡量相关性的一种方式是通过互信息，如下:
\begin{equation}
I(X,Y)=\sum_{x_j}\sum_{Y}P(x_j,y)log\frac{p(x_j,Y)}{p(x_j)p(y)}
\end{equation}
互信息可被理解为当我们观察到特征$x_j$时对于分类属性造成的熵减。对每个特征属性分别计算互信息后，选取较大的若干个用于训练即可。&lt;/p&gt;
&lt;h1&gt;Appendix I:Mutual Information&lt;/h1&gt;
&lt;h2&gt;KL divergence&lt;/h2&gt;
&lt;p&gt;衡量两个概率分布$p$和$q$差异性的一种方法是KL距离(Kullback-Leibler divergence or relative entropy).定义如下:
\begin{equation}
KL(p||q)\triangleq \sum_{k=1}^{K} p_klog\frac{p_k}{q_k}
\end{equation}
上式可以改写为:
\begin{equation}
KL(p||q) \triangleq \sum_{k}p_klogp_k-\sum_{k}p_klogq_k = -H(p)+H(p,q)
\end{equation}
其中，$H(p,q)$称为联合熵，定义为:
\begin{equation}
H(p,q)\triangleq -\sum_{k}p_klogq_k
\end{equation}
其实，联合熵可被理解为用分布$q$编码来自分布$p$的数据时所需要的最小位数，$H(p)$即是用本身分布编码本身信息所需要的最小比特位数，因此KL距离的含义即是使用$q$编码来自$p$的信息相对于分布$p$本身而言多需要的位数。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 2.1 $KL(p,q) \ge 0$,且当且仅当$p=q$时等号成立；&lt;/p&gt;
&lt;p&gt;为证明上式，我们引入琴生不等式，即任意凸函数$f$,有:
\begin{equation}
f(\sum_{i=1}^{n}\lambda_ix_i) \le \sum_{i=1}^{n}\lambda_if(x_i)
\end{equation}
其中$\lambda_i\ge 0,\sum_{i=1}^{n}\lambda_i=1$&lt;/p&gt;
&lt;p&gt;Proof:
\begin{equation}
-KL(p||q)=-\sum_{x \in A}p(x)log\frac{p(x)}{q(x)}=-\sum_{x \in A}p(x)log\frac{q(x)}{p(x)} \
\le log \sum_{x \in A}p(x)log\frac{q(x)}{p(x)}=log \sum_{x \in A} q(x) \
\le log \sum_{x \in X}q(x)=log 1=0
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;另外一个重要的推论是离散分布中一致分布的熵最大，即$H(X) \le log |X|$.&lt;/p&gt;
&lt;p&gt;\begin{equation}
0 \le KL(q||u) = \sum_{x} p(x)log \frac{p(x)}{u(x)} \
= \sum_{x}p(x)logp(x)-\sum_{x}p(x)logu(x) = -H(X)+log|X|
\end{equation}
该式是Laplace不充分理由原则的公式表示，它的含义是当没有其他理由证明其他分布好于一致分布时，应当采用一致分布。&lt;/p&gt;
&lt;h2&gt;Mutual Information&lt;/h2&gt;
&lt;p&gt;考察两个随机变量，$X$和$Y$。假如我们想知道一个变量包含关于另一变量的多少信息，我们可以计算相关系数，但那只针对实数随机变量而言。一个更通用的办法是衡量联合分布和分布乘积的相关性，即MI.定义如下：
\begin{equation}
I(X;Y) \triangleq KL((p(X,Y)||p(X)p(Y)) = \sum_{x}\sum_{y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}
\end{equation}
$I(X;Y) \ge 0 $成立且当且仅当$p(X,Y=P(X)P(Y)$时取等。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;\begin{equation}
I(X;Y) = H(X)-H(X|Y) = H(Y)-H(Y|X)
\end{equation}
其中，减式的后半部分称为条件熵，证明此处从略。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html"&gt;算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="Classfication"></category><category term="Generative Models"></category><category term="Mutual Information"></category></entry><entry><title>机器学习系列(I):决策树算法</title><link href="http://www.qingyuanxingsi.com/Decision%20Tree.html" rel="alternate"></link><updated>2014-03-03T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-03-03:Decision Tree.html</id><summary type="html">&lt;h1&gt;写在前面&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;好吧，今天我的博客在线下默默地开张了，花了好长时间才把中文显示的问题解决。言归正传，之所以开通这个博客，原因有二：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对已经学过的知识进行梳理，保证学习过程的稳步前进；&lt;/li&gt;
&lt;li&gt;敦促自己每周有一定的学习目标,以更好地推进自己的学习.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关于这个博客其他的我就不说了，如果你觉得这个博客有点用，你愿意花点时间看看，我会灰常感激滴。如果你觉得这个博客没什么用，直接忽略就好。此外，这篇博客所有内容均host在Github上，本着分享，协作的精神，如果你愿意而且有时间欢迎投稿至qingyuanxingsi@163.com,I would be much glad to receive your mails.&lt;/p&gt;
&lt;h1&gt;简介&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;二三闲话&lt;/h2&gt;
&lt;p&gt;这是本博客的第一篇博文，也是第一篇关于机器学习方面的博文，因此我想扯些闲话。就我而言，我觉得所有的机器学习算法并不只是模型本身那么简单，背后其实还有一些别的东西，从某种角度来说，它们也是模型的创立者认识世界的方式。&lt;/p&gt;
&lt;p&gt;举贝叶斯为例，从他的模型中可能能推断出他也许认为万物皆有联系，所有的事物都不是孤立的，都是相互联系，相互影响的。一个事物的改变会引起其他事物的相应变化，世界是一个相互联系的整体。另，我经常听到人们抱怨这个世界不公平，这个世界并不是他们想要的那种模样；或者说自从多年前姚晨和凌潇肃离婚之后，好多人都不再相信爱情了(just a joke）。虽然说这是生活中再平常不过的桥段，从这两个例子中，也许我们能看到另外一些东西，我们很久很久以前都对这个世界有一些先入为主的认识(&lt;strong&gt;prior&lt;/strong&gt;),我们愿意相信这个世界是公平的，爱情是非常美好的一件事。后来，慢慢的我们发现这个世界其实有很多不公平的事，我们发现这个世界里的爱情没我们想象的那么美好，我们看到了一些真实世界实实在在存在的事情(&lt;strong&gt;data&lt;/strong&gt;),于是我们对于这个世界的认识发生了改变，我们开始相信一些原来不相信的事情，对我们之前深信不疑的事情也不再那么确信。(&lt;strong&gt;posterior&lt;/strong&gt;)(关于这个模型我们下一次说吧).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;曾经相信过爱情，后来知道，原来爱情必须转化为亲情才可能持久，但是转化为亲情的爱情，犹如化入杯水中的冰块──它还是冰块吗？                    &lt;br /&gt;
曾经相信过海枯石烂作为永恒不灭的表征，后来知道，原来海其实很容易枯，石，原来很容易烂。雨水，很可能不再来，沧海，不会再成桑田。原来，自己脚下所踩的地球，很容易被毁灭。海枯石烂的永恒，原来不存在。                   &lt;br /&gt;
...                     &lt;br /&gt;
相信与不相信之间，彷佛还有令人沉吟的深度。(龙应台《相信，不相信》）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;举上面例子的目的意在说明其实机器学习算法也许并非就是些模型，就是些数学而已，它也许能给我们提供看待世界的另一种角度，也许能带给我们一些有益的思考。关于闲话就说到这儿，以后我们有时间慢慢扯。&lt;/p&gt;
&lt;h2&gt;Introduction to Decision Trees&lt;/h2&gt;
&lt;p&gt;所谓决策树，顾名思义，是一种树，一种依托于策略抉择而建立起来的树。决策树中非叶节点(非根节点)均是决策节点，决策节点的取值决定了决策树具体下一步跳到那个节点，每个决策节点的分支则分别代表了决策属性可能的取值；每一个叶节点代表了一个分类属性，即决策过程的完成。从根节点到叶节点的每一条路径代表了一个可能的决策过程。&lt;/p&gt;
&lt;p&gt;举个例子，也许大家能对决策树到底是什么有一个更为清楚直观的认识:&lt;/p&gt;
&lt;p&gt;一个非常经典的例子是一个女生找对象的过程，在女孩决定是否相亲的过程中可能产生如下对话:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;女儿：多大年纪了？&lt;/span&gt;
&lt;span class="err"&gt;母亲：&lt;/span&gt;&lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="err"&gt;。&lt;/span&gt;
&lt;span class="err"&gt;女儿：长的帅不帅？&lt;/span&gt;
&lt;span class="err"&gt;母亲：挺帅的。&lt;/span&gt;
&lt;span class="err"&gt;女儿：收入高不？&lt;/span&gt;
&lt;span class="err"&gt;母亲：不算很高，中等情况。&lt;/span&gt;
&lt;span class="err"&gt;女儿：是公务员不？&lt;/span&gt;
&lt;span class="err"&gt;母亲：是，在税务局上班呢。&lt;/span&gt;
&lt;span class="err"&gt;女儿：那好，我去见见。&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这个女孩的决策过程就是典型的分类树决策。相当于通过年龄、长相、收入和是否公务员对将男人分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么这个可以用下图表示女孩的决策逻辑：&lt;/p&gt;
&lt;p&gt;&lt;img alt="girl" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/girl_zpsd5a3cfed.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;根据奥卡姆剃刀原则(&lt;code&gt;Simpler is better&lt;/code&gt;),我们尽可能想构造得到的决策书尽可能的小。因此，如何选择上图中决策属性是所有决策树算法的核心所在。我们尽量在每一步要有限选取最有分辨能力的属性作为决策属性，以保证树尽可能的小。针对决策树，我们主要介绍两种比较典型的算法ID3以及C4.5,另外CART(Classification and Regression Tree)是另外使用的比较多的算法，商用的版本则有C5.0,它主要针对C4.5算法做了很多性能上的优化。具体针对CART以及C5.0的介绍本文将不再涉及。&lt;/p&gt;
&lt;h1&gt;ID3&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;ID3算法基本框架&lt;/h2&gt;
&lt;p&gt;ID3算法是一个由Ross Quinlan发明的用于决策树的算法。它是一个启发式算法，具体算法框架可参见《机器学习》一书中的描述，如下所示:
                       &lt;img alt="ID3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/id3_zpsaa2fe321.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;分裂属性的选取&lt;/h2&gt;
&lt;p&gt;如上图算法框架所示，判断测试某个属性为最佳的分类属性是ID3的核心问题，以下介绍两个比较重要的概念：信息熵和信息增益。&lt;/p&gt;
&lt;h3&gt;信息熵&lt;/h3&gt;
&lt;p&gt;为了精确地定义信息增益，我们先定义信息论中广泛使用的一个度量标准，称为熵(entropy),它刻画了任意样例集的纯度，另一种理解则是用来编码信息所需的最少比特位数。
\begin{equation}
Entropy(S) = -\sum_{i=1}^{c} p_ilog(p_i)
\end{equation}                                    &lt;br /&gt;
其中，$p_i$是属性S属于类别i的概率。&lt;/p&gt;
&lt;h3&gt;信息增益&lt;/h3&gt;
&lt;p&gt;已经有了熵作为衡量训练样例集合纯度的标准，现在可以定义属性分类训练数据的效力的度量标准。这个标准被称为&lt;strong&gt;“信息增益（information gain）”&lt;/strong&gt;。简单的说，一个属性的信息增益就是由于使用这个属性分割样例而导致的期望熵降低(或者说，样本按照某属性划分时造成熵减少的期望,个人结合前面理解，总结为用来衡量给定的属性区分训练样例的能力)。更精确地讲，一个属性A相对样例集合S的信息增益$Gain(S,A)$被定义为：
\begin{equation}
Gain(S,A)=Entropy(S) - \sum_{v \in S_v} \frac{|S_v|}{|S|}Entropy(S_v)
\end{equation}                 &lt;br /&gt;
其中：
    $V(A)$是属性A的值域；
    $S$是样本集合；
    $S_v$是S在属性A上取值等于v的样本集合。&lt;/p&gt;
&lt;p&gt;对于上述算法框架中迭代的每一步，针对样本集合S,我们分别算出针对每个可能的属性的信息增益值，并选择值最大的那个对应的属性作为我们该步的分裂属性即可。依次迭代，便能构造我们想要的决策树。&lt;/p&gt;
&lt;h3&gt;Python代码实现&lt;/h3&gt;
&lt;p&gt;实践出真知，磨刀霍霍，我们小小地实现一下。对于以上提到的ID3算法，基于Python我们给出了相应的源码实现，如下:(本博客中所有源码仅是算法思想的一个比较粗略的实现，很多方面还不成熟，特此说明，以后不再提及)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;operator&lt;/span&gt;

&lt;span class="n"&gt;class&lt;/span&gt; &lt;span class="n"&gt;DTree_ID3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;runDT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;classList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classify&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;max_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Max_InfoGain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="err"&gt;##&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;
        &lt;span class="n"&gt;max_fea&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;max_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;myTree&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;max_fea&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;{}}&lt;/span&gt;
        &lt;span class="n"&gt;fea_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;max_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;unique&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fea_val&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;    
        &lt;span class="n"&gt;del&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;max_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;          
            &lt;span class="n"&gt;sub_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitDataSet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         
            &lt;span class="n"&gt;myTree&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;max_fea&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;runDT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
        &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;max_fea&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;myTree&lt;/span&gt;

    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;classify&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;classCount&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;vote&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;vote&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;classCount&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;classCount&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vote&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="n"&gt;classCount&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vote&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;sortedClassCount&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classCount&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;operator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;itemgetter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;revese&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sortedClassCount&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;Max_InfoGain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data_set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="n"&gt;compute&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="n"&gt;InfoGain&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;maximal&lt;/span&gt; &lt;span class="n"&gt;one&lt;/span&gt;
        &lt;span class="n"&gt;Num_Fea&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_set&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Num_Tup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;max_IG&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;max_Fea&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Num_Fea&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;InfoGain&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_set&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_IG&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;InfoGain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;max_IG&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;InfoGain&lt;/span&gt;
                &lt;span class="n"&gt;max_Fea&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;max_Fea&lt;/span&gt;

    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;Info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;dic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tup&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
                &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="nl"&gt;else:&lt;/span&gt;
                &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

        &lt;span class="n"&gt;S_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;S_each&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;S_each&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;S_total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;S_each&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;S_total&lt;/span&gt;

    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;splitDataSet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;featureIndex&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;subDataSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;dataSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;featureIndex&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;reducedSample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;featureIndex&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
                &lt;span class="n"&gt;reducedSample&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;featureIndex&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  
                &lt;span class="n"&gt;subDataSet&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reducedSample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subDataSet&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;dataSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Cool&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;High&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Ugly&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;High&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
               &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Cool&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Low&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Cool&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Low&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
               &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Cool&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Medium&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Ugly&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Medium&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
    &lt;span class="n"&gt;featureSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Appearance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Salary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Office Guy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;dTree&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DTree_ID3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;tree&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dTree&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;runDT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;featureSet&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tree&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;C4.5&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;C4.5决策树在ID3决策树的基础之上稍作改进，并克服了其两大缺点:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用信息增益选择属性偏向于选择分枝比较多的属性，即取值多的属性;&lt;/li&gt;
&lt;li&gt;不能处理连续属性.                &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于这两个问题，C4.5都给出了具体的解决方案，以下做一个简要的阐述。&lt;/p&gt;
&lt;h2&gt;信息增益率&lt;/h2&gt;
&lt;p&gt;C4.5选取了信息增益率作为选择决策属性的依据，克服了用信息增益来选择属性时偏向选择值多的属性的不足。信息增益率定义为： 
\begin{equation}
GainRatio(S,A)=\frac{Gain(S,A)}{SplitInfo(S,A)}
\end{equation}
其中$Gain(S,A)$和ID3算法中的信息增益计算相同，而$SplitInfo(S,A)$代表了按照属性A分裂样本集合S的广度和均匀性。
\begin{equation}
SplitInfo(S,A)=-\sum_{i=1}^{c} \frac{|S_i|}{|S|}log\frac{|S_i|}{|S|}
\end{equation}
其中$S_i$表示根据属性A分割S而成的样本子集;&lt;/p&gt;
&lt;h2&gt;处理连续属性&lt;/h2&gt;
&lt;p&gt;对于离散值，C4.5和ID3的处理方法相同，对于某个属性的值连续时，假设这这个节点上的数据集合样本为total，C4.5算法进行如下处理：   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将样本数据该属性A上的具体数值按照升序排列，得到属性序列值：${A_1,A_2,A_3,...,A{total}}$&lt;/li&gt;
&lt;li&gt;在上一步生成的序列值中生成total-1个分割点。第i个分割点的取值为$A_i$和$A_{i+1}$的均值，每个分割点都将属性序列划分为两个子集;&lt;/li&gt;
&lt;li&gt;计算每个分割点的信息增益(Information Gain),得到total-1个信息增益。}&lt;/li&gt;
&lt;li&gt;对分裂点的信息增益进行修正：减去log2(N-1)/|D|，其中N为可能的分裂点个数，D为数据集合大小。&lt;/li&gt;
&lt;li&gt;选择修正后的信息增益值最大的分类点作为该属性的最佳分类点&lt;/li&gt;
&lt;li&gt;计算最佳分裂点的信息增益率(Gain Ratio)作为该属性的Gain Ratio&lt;/li&gt;
&lt;li&gt;选择Gain Ratio最大的属性作为分类属性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;决策树方法是机器学习算法中比较重要且较易于理解的一种分类算法，本文介绍了两种决策树算法，ID3和C4.5.决策树算法的核心在于分裂属性的选取上，对此，ID3采用了信息增益作为评估指标，但是ID3也有不能处理连续属性值和易于选取取值较多的属性，C4.5对这两个问题都给出了相应的解决方案。&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category></entry><entry><title>我的甲午</title><link href="http://www.qingyuanxingsi.com/wo-de-jia-wu.html" rel="alternate"></link><updated>2013-06-30T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2013-06-30:wo-de-jia-wu.html</id><summary type="html">&lt;p&gt;其实这一篇是最想写的,可是提笔却不知从何写起。还是留着以后毕业的时候填吧。&lt;/p&gt;</summary><category term="甲午"></category><category term="随笔"></category></entry><entry><title>闲话O2O:写在互联网时代的边上</title><link href="http://www.qingyuanxingsi.com/xian-hua-o2oxie-zai-hu-lian-wang-shi-dai-de-bian-shang.html" rel="alternate"></link><updated>2013-04-15T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2013-04-15:xian-hua-o2oxie-zai-hu-lian-wang-shi-dai-de-bian-shang.html</id><summary type="html">&lt;p&gt;Coming Soon!&lt;/p&gt;</summary><category term="O2O"></category><category term="互联网"></category></entry></feed>