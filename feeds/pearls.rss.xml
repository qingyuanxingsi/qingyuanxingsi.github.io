<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>苹果的味道</title><link href="http://www.qingyuanxingsi.com/" rel="alternate"></link><link href="http://www.qingyuanxingsi.com/feeds/pearls.rss.xml" rel="self"></link><id>http://www.qingyuanxingsi.com/</id><updated>2014-05-06T00:00:00+08:00</updated><entry><title>自然语言处理(序章):我爱自然语言处理(II)</title><link href="http://www.qingyuanxingsi.com/zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-ii.html" rel="alternate"></link><updated>2014-05-06T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-06:zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-ii.html</id><summary type="html">&lt;p&gt;本文紧接上一篇&lt;a href="http://www.qingyuanxingsi.com/zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i.html"&gt;自然语言处理(序章):我爱自然语言处理(I)&lt;/a&gt;,由于文章篇幅过长导致编辑器响应速度变慢,所以将其拆分为两篇,本文即为第二部分。(&lt;strong&gt;本博文引用内容版权属我爱自然语言博客作者及其引用文章作者,特此再次声明&lt;/strong&gt;)。&lt;/p&gt;
&lt;h1&gt;正态分布的前世今生&lt;/h1&gt;
&lt;hr /&gt;
&lt;blockquote&gt;
&lt;p&gt;神说，要有正态分布，就有了正态分布。&lt;/p&gt;
&lt;p&gt;神看正态分布是好的，就让随机误差就服从了正态分布。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;正态分布&lt;/h2&gt;
&lt;p&gt;学过基础统计学的同学大都对正态分布非常熟悉。这个钟型的分布曲线不但形状优雅，其密度函数写成数学表达式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{equation}&lt;/p&gt;
&lt;p&gt;也非常具有数学的美感。其标准化后的概率密度函数:&lt;/p&gt;
&lt;p&gt;\begin{equation}
f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}
\end{equation}&lt;/p&gt;
&lt;p&gt;更加的简洁漂亮,两个最重要的数学常量 $\pi$,$e$都出现在了公式之中。在我个人的审美之中，它也属于 top-N 的最美丽的数学公式之一，如果有人问我数理统计领域哪个公式最能让人感觉到上帝的存在，那我一定投正态分布的票。因为这个分布戴着神秘的面纱，在自然界中无处不在，让你在纷繁芜杂的数据背后看到隐隐的秩序。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Normal_Curve" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/normal_curve_zpse8660e23.png" /&gt;&lt;/p&gt;
&lt;p&gt;正态分布又通常被称为高斯分布，在科学领域，冠名权那是一个很高的荣誉。去过德国的兄弟们还会发现，德国的钢镚和10马克的纸币上都留有高斯的头像和正态密度曲线。正态分布被冠名高斯分布，我们也容易认为是高斯发现了正态分布，其实不然，不过高斯对于正态分布的历史地位的确立是起到了决定性的作用。&lt;/p&gt;
&lt;p&gt;&lt;img alt="10dm" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/10dm_with_gauss_curve_zps561d08ca.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="10dm_detail" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/10dm_with_gauss_curve_detail-300x217_zpsd38f3d9e.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="20_mark" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/20-mark-gauss_zpsd6e55205.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;正态曲线虽然看上去很美，却不是一拍脑袋就能想到的。我在本科学习数理统计的时候，课本一上来介绍正态分布就给出密度分布函数，却从来不说明这个分布函数是通过什么原理推导出来的。所以我一直搞不明白数学家当年是怎么找到这个概率分布曲线的，又是怎么发现误差服从这个奇妙的分布的。直到我读研究生的时候我的导师给我介绍了陈希儒院士的《数理统计简史》这本书，看了之后才了解了正态分布曲线从发现到被人们重视进而广泛应用，也是经过了几百年的历史。&lt;/p&gt;
&lt;p&gt;正态分布的这段历史是很精彩的，我们通过讲几个故事来揭开她的神秘面纱。&lt;/p&gt;
&lt;h2&gt;邂逅&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;正态曲线的首次发现第一个故事和概率论的发展密切相关，主角是棣莫弗(De Moivre) 和拉普拉斯(Laplace)。拉普拉斯是个大科学家，被称为法国的牛顿；棣莫弗名气可能不算很大，不过大家应该应该都熟悉这个名字，因为我们在高中数学学复数的时候都学过棣莫弗定理:&lt;/p&gt;
&lt;p&gt;\begin{equation}
(cosθ+isinθ)^n=cos(nθ)+isin(nθ)
\end{equation}&lt;/p&gt;
&lt;p&gt;古典概率论发源于赌博，惠更斯、帕斯卡、费马、贝努力都是古典概率的奠基人，他们那会研究的概率问题大都来自赌桌上，最早的概率论问题是赌徒梅累在1654年向帕斯卡提出的如何分赌金的问题。统计学中的总体均值之所以被称为期望(Expectation), 就是源自惠更斯、帕斯卡这些人研究平均情况下一个赌徒在赌桌上可以期望自己赢得多少钱。&lt;/p&gt;
&lt;p&gt;有一天一个哥们，也许是个赌徒，向棣莫弗提了一个和赌博相关的一个问题：A,B两人在赌场里赌博，A，B各自的获胜概率是$p,q=1−p$,赌$n$局，若 A 赢的局数$X&amp;gt;np$, 则A付给赌场$X−np$元，否则B付给赌场$np−X$元。 问赌场挣钱的期望值是多少。&lt;/p&gt;
&lt;p&gt;问题并不复杂， 本质上是一个二项分布，最后求出的理论结果是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
2npq \ b(n, p, np)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$b(n,p,i)=C_n^ip^iq^{n−i}$是常见的二项概率。 但是对具体的 n, 要把这个理论结果实际计算出数值结果可不容易， 因为其中的二项公式中有组合数.这就驱动De Moivre寻找近似计算的方法计算。&lt;/p&gt;
&lt;p&gt;与此相关联的另一个问题，是遵从二项分布的随机变量$X \sim\ B(n,p)$, 求$X$落在二项分布中心点一定范围的概率$P_d=P(|X–np|≤d)$&lt;/p&gt;
&lt;p&gt;对于$p=1/2$的情形， 棣莫弗做了一些计算并得到了一些近似结果，但是还不够漂亮，幸运的是棣莫弗和Stirling 处在同一个时代， 而且二人之间有联系，Stirling公式是在数学分析中必学的一个重要公式(事实上Stirling 公式的形式其实是棣莫弗最先发现的，但是 Stirling 改进了公式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
n! \sim \sqrt{2\pi n} (\frac{n}{e})^{n}
\end{equation}&lt;/p&gt;
&lt;p&gt;1733 年，棣莫弗很快利用 Stirling 公式进行计算并取得了重要的进展。考虑$n$是偶数的情形，令二项概率&lt;/p&gt;
&lt;p&gt;\begin{equation}
b(i) = b(n, \frac{1}{2}, i) = \binom{n}{i}(\frac{1}{2})^n
\end{equation}&lt;/p&gt;
&lt;p&gt;通过 Stirling 公式做一些简单的计算容易得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\displaystyle b(\frac{n}{2}) \sim \sqrt{\frac{2}{\pi n}}
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
\displaystyle \frac{b(\frac{n}{2}+d)}{b(\frac{n}{2})} \sim e^{-\frac{2d^2}{n}}
\end{equation}&lt;/p&gt;
&lt;p&gt;于是有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\displaystyle b(\frac{n}{2}+d) \sim \frac{2}{\sqrt{2 \pi n}}e^{-\frac{2d^2}{n}}
\end{equation}&lt;/p&gt;
&lt;p&gt;使用上式的结果，并在二项概率累加求和的过程中近似的使用定积分代替求和，很容易就能得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\displaystyle P(|\frac{X}{n} – \frac{1}{2}| \le \frac{c}{\sqrt{n}} ) \sim\int_{-2c}^{2c} \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;NOTE:以上两个公式没看懂,求大神指教。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;看，正态分布的密度函数的形式在积分公式中出现了！这也就是我们在数理统计课本上学到的二项分布的极限分布是正态分布。以上只是讨论了$p=1/2$的情形， 棣莫弗也对 p≠1/2做了一些计算，后来拉普拉斯对 $p \neq 1/2$的情况做了更多的分析，并把二项分布的正态近似推广到了任意$p$的情况。 这是第一次正态密度函数被数学家勾画出来，而且是以二项分布的极限分布的形式被推导出来的。 熟悉基础概率统计的同学们都知道这个结果其实叫棣莫弗-拉普拉斯中心极限定理。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[De Moivre-Laplace 中心极限定理]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;设随机变量$X_n(n=1,2,⋯)$服从参数为$p$的二项分布，则对任意的$x$, 恒有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\displaystyle\lim_{n\rightarrow\infty}P{ \frac{X_n – np}{\sqrt{np(1-p)}} \le x }=\int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{\frac{-t^2}{2}}dt
\end{equation}&lt;/p&gt;
&lt;p&gt;我们在大学学习数理统计的时候，学习的过程都是先学习了正态分布，然后才学习中心极限定理。而学习到正态分布的时候，直接就描述了其概率密度的数学形式，虽然数学上很漂亮，但是当时很容易困惑数学家们是如何凭空就找到这个分布的。读了陈希孺的《数理统计学简史》之后，我才明白正态分布的密度形式首次发现是在棣莫弗-拉普拉斯的中心极限定理中。数学家研究数学问题的进程很少是按照我们数学课本的安排顺序推进的，现代的数学课本都是按照数学内在的逻辑进行组织编排的，虽然逻辑结构上严谨优美，却把数学问题研究的历史痕迹抹得一干二净。DNA 双螺旋结构的发现者之一 Waston 在他的名著《DNA 双螺旋》序言中说：“科学的发现很少会像门外汉所想象的一样，按照直接了当合乎逻辑的方式进行的。”&lt;/p&gt;
&lt;p&gt;棣莫弗出他的发现后40年（大约是 1770),拉普拉斯建立了中心极限定理较一般的形式，中心极限定理后续又被其它数学家们推广到了其它任意分布的情形，而不限于二项分布。后续的统计学家发现，一系列的重要统计量，在样本量$N$趋于无穷的时候， 其极限分布都有正态的形式， 这构成了数理统计学中大样本理论的基础。&lt;/p&gt;
&lt;p&gt;棣莫弗在二项分布的计算中瞥见了正态曲线的模样，不过他并没有能展现这个曲线的美妙之处。棣莫弗的这个工作当时并没有引起人们足够的重视，原因在于棣莫弗不是个统计学家，从未从统计学的角度去考虑其工作的意义。 正态分布(当时也没有被命名为正态分布) 在当时也只是以极限分布的形式出现，并没有在统计学，尤其是误差分析中发挥作用。这也就是正态分布最终没有被冠名棣莫弗分布的重要原因。 那高斯做了啥工作导致统计学家把正态分布的这顶桂冠戴在了他的头上呢？这先得从最小二乘法的发展说起。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;[本部分待续]&lt;/strong&gt;&lt;/p&gt;
&lt;h1&gt;MCMC 和 Gibbs Sampling&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;随机模拟&lt;/h2&gt;
&lt;p&gt;随机模拟(或者统计模拟)方法有一个很酷的别名是蒙特卡罗方法(Monte Carlo Simulation)。这个方法的发展始于20世纪40年代，和原子弹制造的曼哈顿计划密切相关，当时的几个大牛，包括乌拉姆、冯.诺依曼、费米、费曼、Nicholas Metropolis，在美国洛斯阿拉莫斯国家实验室研究裂变物质的中子连锁反应的时候，开始使用统计模拟的方法,并在最早的计算机上进行编程实现。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Simulation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/simulation_zpsfd333536.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;现代的统计模拟方法最早由数学家乌拉姆提出，被Metropolis命名为蒙特卡罗方法，蒙特卡罗是著名的赌场，赌博总是和统计密切关联的，所以这个命名风趣而贴切，很快被大家广泛接受。被不过据说费米之前就已经在实验中使用了，但是没有发表。说起蒙特卡罗方法的源头，可以追溯到18世纪，布丰当年用于计算π的著名的投针实验就是蒙特卡罗模拟实验。统计采样的方法其实数学家们很早就知道，但是在计算机出现以前，随机数生成的成本很高，所以该方法也没有实用价值。随着计算机技术在二十世纪后半叶的迅猛发展，随机模拟技术很快进入实用阶段。对那些用确定算法不可行或不可能解决的问题，蒙特卡罗方法常常为人们带来希望。&lt;/p&gt;
&lt;p&gt;&lt;img alt="monte-carlo-simulation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/monte-carlo-simulation_zpsd57f8e88.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;统计模拟中有一个重要的问题就是给定一个概率分布$p(x)$，我们如何在计算机中生成它的样本。一般而言均匀分布 $Uniform(0,1)$的样本是相对容易生成的。通过线性同余发生器可以生成伪随机数，我们用确定性算法生成$[0,1]$之间的伪随机数序列后，这些序列的各种统计指标和均匀分布$Uniform(0,1)$的理论计算结果非常接近。这样的伪随机序列就有比较好的统计性质，可以被当成真实的随机数使用。&lt;/p&gt;
&lt;p&gt;&lt;img alt="sampling" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sampling_zpsb8ae4169.png" /&gt;&lt;/p&gt;
&lt;p&gt;而我们常见的概率分布，无论是连续的还是离散的分布，都可以基于$Uniform(0,1)$的样本生成。例如正态分布可以通过著名的&lt;strong&gt;Box-Muller&lt;/strong&gt;变换得到&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;[Box-Muller变换]&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果随机变量$U_1$,$U_2$独立且$U_1,U_2 \sim\ Uniform[0,1]$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
Z_0 &amp;amp; = \sqrt{-2\ln U_1} cos(2\pi U_2) \\ 
Z_1 &amp;amp; = \sqrt{-2\ln U_1} sin(2\pi U_2) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;则$Z_0,Z_1$独立且服从标准正态分布。&lt;/p&gt;
&lt;p&gt;其它几个著名的连续分布，包括指数分布、Gamma分布、t分布、F分布、Beta分布、Dirichlet分布等等,也都可以通过类似的数学变换得到；离散的分布通过均匀分布更加容易生成。更多的统计分布如何通过均匀分布的变换生成出来，大家可以参考统计计算的书，其中 Sheldon M. Ross 的&lt;strong&gt;《统计模拟》&lt;/strong&gt;是写得非常通俗易懂的一本。&lt;/p&gt;
&lt;p&gt;不过我们并不是总是这么幸运的，当$p(x)$的形式很复杂，或者$p(x)$是个高维的分布的时候，样本的生成就可能很困难了。 譬如有如下的情况:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(x) = \frac{\tilde{p}(x)}{\int \tilde{p}(x) dx}$,而$\tilde{p}(x)$我们是可以计算的，但是底下的积分式无法显式计算。&lt;/li&gt;
&lt;li&gt;$p(x,y)$是一个二维的分布函数，这个函数本身计算很困难，但是条件分布$p(x|y),p(y|x)$的计算相对简单;如果$p(x)$是高维的，这种情形就更加明显。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;此时就需要使用一些更加复杂的随机模拟的方法来生成样本。而本节中将要重点介绍的 MCMC(Markov Chain Monte Carlo) 和 Gibbs Sampling算法就是最常用的一种，这两个方法在现代贝叶斯分析中被广泛使用。要了解这两个算法，我们首先要对马氏链的平稳分布的性质有基本的认识。&lt;/p&gt;
&lt;h2&gt;马氏链及其平稳分布&lt;/h2&gt;
&lt;p&gt;马氏链的数学定义很简单:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(X_{t+1}=x|X_t, X_{t-1}, \cdots) =P(X_{t+1}=x|X_t)
\end{equation}&lt;/p&gt;
&lt;p&gt;也就是状态转移的概率只依赖于前一个状态。&lt;/p&gt;
&lt;p&gt;我们先来看马氏链的一个具体的例子。社会学家经常把人按其经济状况分成3类：下层(lower-class)、中层(middle-class)、上层(upper-class)，我们用1,2,3分别代表这三个阶层。社会学家们发现决定一个人的收入阶层的最重要的因素就是其父母的收入阶层。如果一个人的收入属于下层类别，那么他的孩子属于下层收入的概率是 0.65, 属于中层收入的概率是 0.28, 属于上层收入的概率是 0.07。事实上，从父代到子代，收入阶层的变化的转移概率如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt="table-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/table-1_zps3d0d323d.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="markov-transition" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/markov-transition_zps8213ffd9.png" /&gt;&lt;/p&gt;
&lt;p&gt;使用矩阵的表示方式，转移概率矩阵记为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P=\left[
\begin{array}{cc}
0.65 &amp;amp; 0.28 &amp;amp; 0.07 \\ 
0.15 &amp;amp; 0.67 &amp;amp; 0.18 \\ 
0.12 &amp;amp; 0.36 &amp;amp; 0.52 \\ 
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;假设当前这一代人处在下层、中层、上层的人的比例是概率分布向量 $\pi_0=[\pi_0(1),\pi_0(2),\pi_0(3)]$，那么他们的子女的分布比例将是$\pi_1=\pi_0P$, 他们的孙子代的分布比例将是 $\pi_2=\pi_1P=\pi_0P^2$, ……, 第n代子孙的收入分布比例将是$\pi_n=\pi_{n−1}P=\pi_0P^n$。&lt;/p&gt;
&lt;p&gt;假设初始概率分布为$\pi_0=[0.21,0.68,0.11]$，则我们可以计算前$n$代人的分布状况如下&lt;/p&gt;
&lt;p&gt;&lt;img alt="table-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/table-2_zps47ffb526.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;我们发现从第7代人开始，这个分布就稳定不变了，这个是偶然的吗？我们换一个初始概率分布$\pi_0=[0.75,0.15,0.1]$.试试看，继续计算前$n$代人的分布状况如下&lt;/p&gt;
&lt;p&gt;&lt;img alt="table-3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/table-3_zps3b41fb58.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;我们发现，到第9代人的时候, 分布又收敛了。最为奇特的是，两次给定不同的初始概率分布，最终都收敛到概率分布 $\pi=[0.286,0.489,0.225]$，也就是说收敛的行为和初始概率分布$\pi_0$无关。这说明这个收敛行为主要是由概率转移矩阵$P$决定的。我们计算一下$P^n$.&lt;/p&gt;
&lt;p&gt;\begin{equation}
P^{20} = P^{21} = \cdots = P^{100} = \cdots = 
\begin{bmatrix} 
0.286 &amp;amp; 0.489 &amp;amp; 0.225 \\
0.286 &amp;amp; 0.489 &amp;amp; 0.225 \\ 
0.286 &amp;amp; 0.489 &amp;amp; 0.225 \\ 
\end{bmatrix}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们发现，当$n$足够大的时候，这个$P^n$矩阵的每一行都是稳定地收敛到$\pi=[0.286,0.489,0.225]$这个概率分布。自然的，这个收敛现象并非是我们这个马氏链独有的，而是绝大多数马氏链的共同行为，关于马氏链的收敛我们有如下漂亮的定理：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;马氏链定理&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;如果一个非周期马氏链具有转移概率矩阵$P$,且它的任何两个状态是连通的，那么$\lim_{n\rightarrow\infty}P_{ij}^n$存在且与$i$无关，记$\lim_{n\rightarrow\infty}P_{ij}^n = \pi(j)$, 我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\lim_{n \rightarrow \infty} P^n =\begin{bmatrix} 
\pi(1) &amp;amp; \pi(2) &amp;amp; \cdots &amp;amp; \pi(j) &amp;amp; \cdots \\ 
\pi(1) &amp;amp; \pi(2) &amp;amp; \cdots &amp;amp; \pi(j) &amp;amp; \cdots \\ 
\cdots &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \cdots \\ 
\pi(1) &amp;amp; \pi(2) &amp;amp; \cdots &amp;amp; \pi(j) &amp;amp; \cdots \\ 
\cdots &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \cdots &amp;amp; \cdots \\ 
\end{bmatrix}    \\
\pi(j) = \sum_{i=0}^{\infty}\pi(i)P_{ij} \\
\end{split}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$\pi$是方程$\pi P=\pi$的唯一非负解。其中,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\pi = [\pi(1), \pi(2), \cdots, \pi(j),\cdots ], \quad \sum_{i=0}^{\infty} \pi_i = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;$\pi$称为马氏链的平稳分布。&lt;/p&gt;
&lt;p&gt;这个马氏链的收敛定理非常重要，&lt;strong&gt;所有的 MCMC(Markov Chain Monte Carlo) 方法都是以这个定理作为理论基础的&lt;/strong&gt;。 定理的证明相对复杂，一般的随机过程课本中也不给证明，所以我们就不用纠结它的证明了，直接用这个定理的结论就好了。我们对这个定理的内容做一些解释说明：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;该定理中马氏链的状态不要求有限，可以是有无穷多个的；&lt;/li&gt;
&lt;li&gt;定理中的“非周期“这个概念我们不打算解释了，因为我们遇到的绝大多数马氏链都是非周期的；&lt;/li&gt;
&lt;li&gt;两个状态$i,j$是连通并非指$i$可以直接一步转移到$j$($P_{ij}&amp;gt;0)$,而是指$i$可以通过有限的$n$步转移到达$j$($P^n_{ij}&amp;gt;0$)。马氏链的任何两个状态是连通的含义是指存在一个$n$,使得矩阵$P^n$中的任何一个元素的数值都大于零。&lt;/li&gt;
&lt;li&gt;我们用$X_i$表示在马氏链上跳转第$i$步后所处的状态，如果$\lim_{n\rightarrow\infty}P_{ij}^n=\pi(j)$存在，很容易证明以上定理的第二个结论。由于&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
P(X_{n+1}=j) &amp;amp; = \sum_{i=0}^\infty P(X_n=i) P(X_{n+1}=j|X_n=i) \\
&amp;amp; = \sum_{i=0}^\infty P(X_n=i) P_{ij} 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;上式两边取极限就得到$\pi(j) = \sum_{i=0}^{\infty}\pi(i)P_{ij}$.&lt;/p&gt;
&lt;p&gt;从初始概率分布$\pi_0$出发，我们在马氏链上做状态转移，记$X_i$的概率分布为$\pi_i$, 则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
X_0 &amp;amp; \sim \pi_0(x) \ 
X_i &amp;amp; \sim \pi_i(x), \quad\quad \pi_i(x) = \pi_{i-1}(x)P = \pi_0(x)P^n 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;由马氏链收敛的定理, 概率分布$\pi_i(x)$将收敛到平稳分布$\pi_(x)$。假设到第$n$步的时候马氏链收敛，则有&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
X_0 &amp;amp; \sim \pi_0(x) \\ 
X_1 &amp;amp; \sim \pi_1(x) \\ 
&amp;amp; \cdots \\ 
X_n &amp;amp; \sim \pi_n(x)=\pi(x) \\ 
X_{n+1} &amp;amp; \sim \pi(x) \\ 
X_{n+2}&amp;amp; \sim \pi(x) \\ 
&amp;amp; \cdots 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;所以$X_n,X_{n+1},X_{n+2},\cdots \sim \pi(x)$都是同分布的随机变量，当然他们并不独立。如果我们从一个具体的初始状态$x_0$开始,沿着马氏链按照概率转移矩阵做跳转，那么我们得到一个转移序列$x_0,x_1,x_2,\cdots,x_n,x_{n+1},\cdots$, 由于马氏链的收敛行为，$x_n,x_{n+1},...$ 都将是平稳分布$\pi(x)$的样本。&lt;/p&gt;
&lt;h2&gt;Markov Chain Monte Carlo&lt;/h2&gt;
&lt;p&gt;对于给定的概率分布$p(x)$,我们希望能有便捷的方式生成它对应的样本。由于马氏链能收敛到平稳分布，于是一个很漂亮的想法是：如果我们能构造一个转移矩阵为$P$的马氏链，使得该马氏链的平稳分布恰好是$p(x)$,那么我们从任何一个初始状态$x_0$出发沿着马氏链转移, 得到一个转移序列$x_0,x_1,x_2,\cdots,x_n,x_{n+1}\cdots$，如果马氏链在第$n$步已经收敛了，于是我们就得到了$p(x)$的样本$x_n,x_{n+1},\cdots$。&lt;/p&gt;
&lt;p&gt;这个绝妙的想法在1953年被Metropolis想到了，为了研究粒子系统的平稳性质，Metropolis考虑了物理学中常见的波尔兹曼分布的采样问题，首次提出了基于马氏链的蒙特卡罗方法，即Metropolis算法，并在最早的计算机上编程实现。Metropolis算法是首个普适的采样方法，并启发了一系列MCMC方法，所以人们把它视为随机模拟技术腾飞的起点。Metropolis的这篇论文被收录在《统计学中的重大突破》中，Metropolis算法也被遴选为二十世纪的十个最重要的算法之一。&lt;/p&gt;
&lt;p&gt;我们接下来介绍的MCMC算法是Metropolis算法的一个改进变种，即常用的&lt;strong&gt;Metropolis-Hastings&lt;/strong&gt;算法。由上一节的例子和定理我们看到了，马氏链的收敛性质主要由转移矩阵$P$决定,所以基于马氏链做采样的关键问题是如何构造转移矩阵$P$,使得平稳分布恰好是我们要的分布$p(x)$。如何能做到这一点呢？我们主要使用如下的定理。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;定理：&lt;strong&gt;[细致平稳条件]&lt;/strong&gt;如果非周期马氏链的转移矩阵$P$和分布$\pi(x)$满足:对于任意$i$和$j$,有:&lt;/p&gt;
&lt;p&gt;\begin{equation} 
\pi(i)P_{ij} = \pi(j)P_{ji}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;则$\pi(x)$是马氏链的平稳分布，上式被称为&lt;strong&gt;细致平稳条件(Detailed balance condition)&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;其实这个定理是显而易见的，因为细致平稳条件的物理含义就是对于任何两个状态$i$,$j$,从$i$转移出去到$j$而丢失的概率质量，恰好会被从$j$转移回$i$的概率质量补充回来，所以状态$i$上的概率质量$\pi(i)$是稳定的，从而$\pi(x)$是马氏链的平稳分布。数学上的证明也很简单，由细致平稳条件可得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
&amp;amp; \sum_{i=1}^\infty \pi(i)P_{ij} = \sum_{i=1}^\infty \pi(j)P_{ji} 
= \sum_{i=1}^\infty \pi(j)P_{ji} = \pi(j) \\ 
&amp;amp; \rightarrow \pi P = \pi 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;由于$\pi$是方程$\pi P =\pi$的解，所以$\pi$是平稳分布。&lt;/p&gt;
&lt;p&gt;假设我们已经有一个转移矩阵为$Q$的马氏链($q(i,j)$表示从状态$i$转移到状态$j$的概率，也可以写为$q(j|i)$或者$q(i \to j)$), 显然，通常情况下$p(i) q(i,j)\neq p(j)q(j,i)$也就是细致平稳条件不成立，所以$p(x)$不太可能是这个马氏链的平稳分布。我们可否对马氏链做一个改造，使得细致平稳条件成立呢？譬如，我们引入一个$\alpha(i,j)$, 我们希望:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(i)q(i,j)\alpha(i,j)=p(j)q(j,i)\alpha(j,i) \quad (∗)
\end{equation}&lt;/p&gt;
&lt;p&gt;取什么样的$\alpha(i,j)$以上等式能成立呢？最简单的，按照对称性，我们可以取:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\alpha(i,j)=p(j)q(j,i)，\alpha(j,i)=p(i)q(i,j)
\end{equation}&lt;/p&gt;
&lt;p&gt;于是(*)式就成立了。所以有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Construct_Proposal" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/construct_proposal_zpsa66cecb9.png" /&gt;&lt;/p&gt;
&lt;p&gt;于是我们把原来具有转移矩阵$Q$的一个很普通的马氏链，改造为了具有转移矩阵$Q\prime$的马氏链，而 $Q\prime$恰好满足细致平稳条件，由此马氏链$Q\prime$的平稳分布就是$p(x)$了!&lt;/p&gt;
&lt;p&gt;在改造$Q$的过程中引入的$\alpha(i,j)$称为接受率，物理意义可以理解为在原来的马氏链上，从状态$i$以$q(i,j)$的概率转跳转到状态$j$的时候，我们以$\alpha(i,j)$的概率接受这个转移，于是得到新的马氏链$Q\prime$的转移概率为$q(i,j)\alpha(i,j)$。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mcmc-transition1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mcmc-transition1_zpsf3e7c727.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;假设我们已经有一个转移矩阵$Q$(对应元素为$q(i,j)$), 把以上的过程整理一下，我们就得到了如下的用于采样概率分布$p(x)$的算法。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mcmc-algo-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mcmc-algo-1_zps4581580d.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;上述过程中$p(x),q(x|y)$说的都是离散的情形，事实上即便这两个分布是连续的，以上算法仍然是有效，于是就得到更一般的连续概率分布$p(x)$的采样算法，而$q(x|y)$就是任意一个连续二元概率分布对应的条件分布。&lt;/p&gt;
&lt;p&gt;以上的MCMC采样算法已经能很漂亮的工作了，不过它有一个小的问题：马氏链$Q$在转移的过程中的接受率 $\alpha(i,j)$可能偏小，这样采样过程中马氏链容易原地踏步，拒绝大量的跳转，这使得马氏链遍历所有的状态空间要花费太长的时间，收敛到平稳分布$p(x)的$速度太慢。有没有办法提升一些接受率呢?&lt;/p&gt;
&lt;p&gt;假设$\alpha(i,j)=0.1,\alpha(j,i)=0.2$, 此时满足细致平稳条件，于是&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(i)q(i,j)×0.1=p(j)q(j,i)×0.2
\end{equation}&lt;/p&gt;
&lt;p&gt;上式两边扩大5倍，我们改写为&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(i)q(i,j)×0.5=p(j)q(j,i)×1
\end{equation}&lt;/p&gt;
&lt;p&gt;看，我们提高了接受率，而细致平稳条件并没有打破！这启发我们可以把细致平稳条件(**) 式中的$\alpha(i,j),\alpha(j,i)$同比例放大，使得两数中最大的一个放大到1，这样我们就提高了采样中的跳转接受率。所以我们可以取:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Choose_Alpha" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/choose_alpha_zps820b59d0.png" /&gt;&lt;/p&gt;
&lt;p&gt;于是，经过对上述MCMC采样算法中接受率的微小改造，我们就得到了如下教科书中最常见的&lt;strong&gt;Metropolis-Hastings算法&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="mcmc-algo-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mcmc-algo-2_zps26a1c8bb.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;对于分布$p(x)$,我们构造转移矩阵$Q\prime$使其满足细致平稳条件:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x)Q\prime (x→y)=p(y)Q\prime(y→x)
\end{equation}&lt;/p&gt;
&lt;p&gt;此处$x$并不要求是一维的，对于高维空间的$p(\mathbf{x})$，如果满足细致平稳条件:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\mathbf{x}) Q’(\mathbf{x}\rightarrow \mathbf{y}) = p(\mathbf{y}) Q’(\mathbf{y}\rightarrow \mathbf{x})
\end{equation}&lt;/p&gt;
&lt;p&gt;那么以上的Metropolis-Hastings算法一样有效。&lt;/p&gt;
&lt;h2&gt;Gibbs Sampling&lt;/h2&gt;
&lt;p&gt;对于高维的情形，由于接受率$\alpha$的存在(通常$\alpha$&amp;lt;1), 以上Metropolis-Hastings算法的效率不够高。能否找到一个转移矩阵$Q$使得接受率$\alpha=1$呢？我们先看看二维的情形，假设有一个概率分布 $p(x,y)$, 考察$x$坐标相同的两个点$A(x_1,y_1),B(x_1,y_2)$,我们发现:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(x_1,y_1)p(y_2|x_1) &amp;amp;= p(x_1)p(y_1|x_1)p(y_2|x_1) \\
p(x_1,y_2)p(y_1|x_1) &amp;amp;= p(x_1)p(y_2|x_1)p(y_1|x_1) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;所以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation} 
p(x_1,y_1)p(y_2|x_1) = p(x_1,y_2)p(y_1|x_1)  \quad (***) 
\end{equation}&lt;/p&gt;
&lt;p&gt;即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(A)p(y_2|x_1) = p(B)p(y_1|x_1)
\end{equation}&lt;/p&gt;
&lt;p&gt;基于以上等式，我们发现，在$x=x_1$这条平行于$y$轴的直线上，如果使用条件分布$p(y|x_1)$做为任何两个点之间的转移概率，那么任何两个点之间的转移满足细致平稳条件。同样的，如果我们在$y=y_1$这条直线上任意取两个点$A(x_1,y_1),C(x_2,y_1)$,也有如下等式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(A)p(x_2|y_1)=p(C)p(x_1|y_1)
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;img alt="gibbs-transition" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs-transition_zpsd5d8548a.png" /&gt;&lt;/p&gt;
&lt;p&gt;于是我们可以如下构造平面上任意两点之间的转移概率矩阵$Q$:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
Q(A\rightarrow B) &amp;amp; = p(y_B|x_1) &amp;amp; \text{如果} \quad x_A=x_B=x_1 &amp;amp; \\ 
Q(A\rightarrow C) &amp;amp; = p(x_C|y_1) &amp;amp; \text{如果} \quad y_A=y_C=y_1 &amp;amp; \\ 
Q(A\rightarrow D) &amp;amp; = 0 &amp;amp; \text{其它} &amp;amp; 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;有了如上的转移矩阵$Q$, 我们很容易验证对平面上任意两点$X,Y$, 满足细致平稳条件:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(X)Q(X→Y)=p(Y)Q(Y→X)
\end{equation}&lt;/p&gt;
&lt;p&gt;于是这个二维空间上的马氏链将收敛到平稳分布$p(x,y)$。而这个算法就称为&lt;strong&gt;Gibbs Sampling算法&lt;/strong&gt;,是 Stuart Geman 和Donald Geman 这两兄弟于1984年提出来的，之所以叫做Gibbs Sampling 是因为他们研究了Gibbs random field, 这个算法在现代贝叶斯分析中占据重要位置。&lt;/p&gt;
&lt;p&gt;&lt;img alt="gibbs-algo-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs-algo-1_zps3efe14aa.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="two-stage-gibbs" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/two-stage-gibbs_zps30faeda1.png" /&gt;&lt;/p&gt;
&lt;p&gt;以上采样过程中，如上图所示，马氏链的转移只是轮换的沿着坐标轴$x$轴和$y$轴做转移，于是得到样本 $(x_0,y_0),(x_0,y_1),(x_1,y_1),(x_1,y_2),(x_2,y_2),\cdots$,马氏链收敛后，最终得到的样本就是$p(x,y)$的样本，而收敛之前的阶段称为 burn-in period。额外说明一下，我们看到教科书上的 Gibbs Sampling 算法大都是坐标轴轮换采样的，但是这其实是不强制要求的。最一般的情形可以是，在$t$时刻，可以在$x$轴和$y$轴之间随机的选一个坐标轴，然后按条件概率做转移，马氏链也是一样收敛的。轮换两个坐标轴只是一种方便的形式。&lt;/p&gt;
&lt;p&gt;以上的过程我们很容易推广到高维的情形，对于(***)式，如果$x_1$变为多维情形$x_1$,可以看出推导过程不变，所以细致平稳条件同样是成立的.&lt;/p&gt;
&lt;p&gt;\begin{equation} 
p(\mathbf{x_1},y_1)p(y_2|\mathbf{x_1}) = p(\mathbf{x_1},y_2)p(y_1|\mathbf{x_1}) 
\end{equation}&lt;/p&gt;
&lt;p&gt;此时转移矩阵$Q$由条件分布$p(y|x_1)$定义。上式只是说明了一根坐标轴的情形，和二维情形类似，很容易验证对所有坐标轴都有类似的结论。所以$n$维空间中对于概率分布$p(x_1,x_2,\cdots,x_n)$可以如下定义转移矩阵:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如果当前状态为$(x_1,x_2,⋯,x_n)$，马氏链转移的过程中，只能沿着坐标轴做转移。沿着$x_i$这根坐标轴做转移的时候，转移概率由条件概率$p(x_i|x_1,⋯,x_{i−1},x_{i+1},⋯,x_n)$定义;&lt;/li&gt;
&lt;li&gt;其它无法沿着单根坐标轴进行的跳转，转移概率都设置为 0。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;于是我们可以把Gibbs Smapling算法从采样二维的$p(\mathbf{x},\mathbf{y})$推广到采样$n$维的 $p(x_1,x_2,⋯,x_n)$.&lt;/p&gt;
&lt;p&gt;&lt;img alt="gibbs-algo-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs-algo-2_zps69519b9b.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;以上算法收敛后，得到的就是概率分布$p(x_1,x_2,⋯,x_n)$的样本，当然这些样本并不独立，但是我们此处要求的是采样得到的样本符合给定的概率分布，并不要求独立。同样的，在以上算法中，坐标轴轮换采样不是必须的，可以在坐标轴轮换中引入随机性，这时候转移矩阵$Q$中任何两个点的转移概率中就会包含坐标轴选择的概率，而在通常的 Gibbs Sampling 算法中，坐标轴轮换是一个确定性的过程，也就是在给定刻$t$，在一根固定的坐标轴上转移的概率是1。&lt;/p&gt;
&lt;h1&gt;文本建模&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;文本建模&lt;/h2&gt;
&lt;p&gt;我们日常生活中总是产生大量的文本，如果每一个文本存储为一篇文档，那每篇文档从人的观察来说就是有序的词的序列$d=(w_1,w_2,\cdots,w_n)$。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Corpus" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/corpus_zpsd5c55aaa.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;包含$M$篇文档的语料库统计文本建模的目的就是追问这些观察到语料库中的的词序列是如何生成的。统计学被人们描述为猜测上帝的游戏，人类产生的所有的语料文本我们都可以看成是一个伟大的上帝在天堂中抛掷骰子生成的，我们观察到的只是上帝玩这个游戏的结果 —— 词序列构成的语料，而上帝玩这个游戏的过程对我们是个黑盒子。所以在统计文本建模中，我们希望猜测出上帝是如何玩这个游戏的，具体一点，最核心的两个问题是:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;上帝都有什么样的骰子；&lt;/li&gt;
&lt;li&gt;上帝是如何抛掷这些骰子的；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一个问题就是表示模型中都有哪些参数，骰子的每一个面的概率都对应于模型中的参数；第二个问题就表示游戏规则是什么，上帝可能有各种不同类型的骰子，上帝可以按照一定的规则抛掷这些骰子从而产生词序列。 &lt;/p&gt;
&lt;h3&gt;Unigram Model&lt;/h3&gt;
&lt;p&gt;假设我们的词典中一共有$V$个词$v_1,v_2,⋯v_V$，那么最简单的Unigram Model就是认为上帝是按照如下的游戏规则产生文本的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="game_unigram_model" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/game-unigram-model_zpseddfb645.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;上帝的这个唯一的骰子各个面的概率记为$\overrightarrow{p} = (p_1, p_2, \cdots, p_V)$, 所以每次投掷骰子类似于一个抛钢镚时候的贝努利实验， 记为$w\sim Mult(w|\overrightarrow{p})$.&lt;/p&gt;
&lt;p&gt;&lt;img alt="unigram_model" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/unigram-model_zps19f87fe3.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;上帝投掷$V$个面的骰子对于一篇文档$d=\overrightarrow{w}=(w_1, w_2, \cdots, w_n)$, 该文档被生成的概率就是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\overrightarrow{w}) = p(w_1, w_2, \cdots, w_n) = p(w_1)p(w_2) \cdots p(w_n)
\end{equation}&lt;/p&gt;
&lt;p&gt;而文档和文档之间我们认为是独立的， 所以如果语料中有多篇文档$\mathcal{W}=(\overrightarrow{w_1}, \overrightarrow{w_2},…,\overrightarrow{w_m})$,则该语料的概率是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\mathcal{W})= p(\overrightarrow{w_1})p(\overrightarrow{w_2}) 
\cdots p(\overrightarrow{w_m})
\end{equation}&lt;/p&gt;
&lt;p&gt;在Unigram Model中， 我们假设了文档之间是独立可交换的，而文档中的词也是独立可交换的，所以一篇文档相当于一个袋子，里面装了一些词，而词的顺序信息就无关紧要了，这样的模型也称为词袋模型(&lt;strong&gt;Bag-of-words&lt;/strong&gt;)。&lt;/p&gt;
&lt;p&gt;假设语料中总的词数是$N$, 在所有的$N$个词中,如果我们关注每个词$v_i$的发生次数$n_i$，那么$\overrightarrow{n}=(n_1, n_2,\cdots, n_V)$正好是一个多项分布:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\overrightarrow{n}) = Mult(\overrightarrow{n}|\overrightarrow{p}, N) 
= \binom{N}{\overrightarrow{n}} \prod_{k=1}^V p_k^{n_k}
\end{equation}&lt;/p&gt;
&lt;p&gt;此时， 语料的概率是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\mathcal{W})= p(\overrightarrow{w_1})p(\overrightarrow{w_2}) \cdots p(\overrightarrow{w_m}) 
= \prod_{k=1}^V p_k^{n_k} 
\end{equation}&lt;/p&gt;
&lt;p&gt;当然，我们很重要的一个任务就是估计模型中的参数$\overrightarrow{p}$，也就是问上帝拥有的这个骰子的各个面的概率是多大，按照统计学家中频率派的观点，使用最大似然估计最大化$P(\mathcal{W})$，于是参数$p_i$的估计值就是$\hat{p_i}=\frac{n_i}{N}$.&lt;/p&gt;
&lt;p&gt;对于以上模型，贝叶斯统计学派的统计学家会有不同意见，他们会很挑剔的批评只假设上帝拥有唯一一个固定的骰子是不合理的。在贝叶斯学派看来，一切参数都是随机变量，以上模型中的骰子$\overrightarrow{p}$不是唯一固定的，它也是一个随机变量。所以按照贝叶斯学派的观点，上帝是按照以下的过程在玩游戏的:&lt;/p&gt;
&lt;p&gt;&lt;img alt="bayesian-unigram-model" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bayesian-unigram-model_zpsa4eeab8f.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;上帝的这个坛子里面，骰子可以是无穷多个，有些类型的骰子数量多，有些类型的骰子少，所以从概率分布的角度看，坛子里面的骰子$\overrightarrow{p}$服从一个概率分布$p(\overrightarrow{p})$，这个分布称为参数$\overrightarrow{p}$的先验分布。&lt;/p&gt;
&lt;p&gt;&lt;img alt="dirichlet-multinomial-unigram" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/dirichlet-multinomial-unigram_zps6e82a36d.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;以上贝叶斯学派的游戏规则的假设之下，语料$\mathcal{W}$产生的概率如何计算呢？由于我们并不知道上帝到底用了哪个骰子$\overrightarrow{p}$,所以每个骰子都是可能被使用的，只是使用的概率由先验分布$p(\overrightarrow{p})$来决定。对每一个具体的骰子$\overrightarrow{p}$,由该骰子产生数据的概率是$p(\mathcal{W}|\overrightarrow{p})$, 所以最终数据产生的概率就是对每一个骰子$\overrightarrow{p}$上产生的数据概率进行积分累加求和:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\mathcal{W}) = \int p(\mathcal{W}|\overrightarrow{p}) p(\overrightarrow{p})d\overrightarrow{p}
\end{equation}&lt;/p&gt;
&lt;p&gt;在贝叶斯分析的框架下，此处先验分布$p(\overrightarrow{p})$就可以有很多种选择了，注意到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\overrightarrow{n}) = Mult(\overrightarrow{n}|\overrightarrow{p}, N)
\end{equation}&lt;/p&gt;
&lt;p&gt;实际上是在计算一个多项分布的概率，所以对先验分布的一个比较好的选择就是多项分布对应的共轭分布,即 Dirichlet 分布:&lt;/p&gt;
&lt;p&gt;\begin{equation}
Dir(\overrightarrow{p}|\overrightarrow{\alpha})= 
\frac{1}{\Delta(\overrightarrow{\alpha})} \prod_{k=1}^V p_k^{\alpha_k -1}， 
\quad \overrightarrow{\alpha}=(\alpha_1, \cdots, \alpha_V)
\end{equation}&lt;/p&gt;
&lt;p&gt;此处，$\Delta(\overrightarrow{\alpha})$就是归一化因子$Dir(\overrightarrow{\alpha})$，即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta(\overrightarrow{\alpha}) = 
\int \prod_{k=1}^V p_k^{\alpha_k -1} d\overrightarrow{p} 
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;img alt="dirichlet-multinomial-unigram" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/dirichlet-multinomial-unigram_zps6e82a36d.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="graph-model-unigram" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/graph-model-unigram_zps3d4b6a8b.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;回顾前一个小节介绍的Drichlet分布的一些知识，其中很重要的一点就是:&lt;strong&gt;Dirichlet 先验 + 多项分布的数据 → 后验分布为 Dirichlet 分布&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;\begin{equation}
Dir(\overrightarrow{p}|\overrightarrow{\alpha}) + MultCount(\overrightarrow{n})= Dir(\overrightarrow{p}|\overrightarrow{\alpha}+\overrightarrow{n})
\end{equation}&lt;/p&gt;
&lt;p&gt;于是，在给定了参数$\overrightarrow{p}$的先验分布$Dir(\overrightarrow{p}|\overrightarrow{\alpha})$的时候，各个词出现频次的数据$\overrightarrow{n} \sim Mult(\overrightarrow{n}|\overrightarrow{p},N)$为多项分布, 所以无需计算，我们就可以推出后验分布是:&lt;/p&gt;
&lt;p&gt;\begin{equation} 
p(\overrightarrow{p}|\mathcal{W},\overrightarrow{\alpha}) 
= Dir(\overrightarrow{p}|\overrightarrow{n}+ \overrightarrow{\alpha}) 
= \frac{1}{\Delta(\overrightarrow{n}+\overrightarrow{\alpha})} 
\prod_{k=1}^V p_k^{n_k + \alpha_k -1} d\overrightarrow{p} 
\end{equation}&lt;/p&gt;
&lt;p&gt;在贝叶斯的框架下，参数$\overrightarrow{p}$如何估计呢？由于我们已经有了参数的后验分布，所以合理的方式是使用后验分布的极大值点，或者是参数在后验分布下的平均值。在该文档中，我们取平均值作为参数的估计值。使用上个小节中的结论，由于$\overrightarrow{p}$的后验分布为$Dir(\overrightarrow{p}|\overrightarrow{n} + \overrightarrow{\alpha})$，于是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(\overrightarrow{p}) = \Bigl(\frac{n_1 + \alpha_1}{\sum_{i=1}^V(n_i + \alpha_i)}, 
\frac{n_2 + \alpha_2}{\sum_{i=1}^V(n_i + \alpha_i)}, \cdots, 
\frac{n_V + \alpha_V}{\sum_{i=1}^V(n_i + \alpha_i)} \Bigr)
\end{equation}&lt;/p&gt;
&lt;p&gt;也就是说对每一个$p_i$, 我们用下式做参数估计:&lt;/p&gt;
&lt;p&gt;\begin{equation} 
\hat{p_i} = \frac{n_i + \alpha_i}{\sum_{i=1}^V(n_i + \alpha_i)} 
\end{equation}&lt;/p&gt;
&lt;p&gt;考虑到$\alpha_i$在Dirichlet 分布中的物理意义是事件的先验的伪计数，这个估计式子的含义是很直观的：每个参数的估计值是其对应事件的先验的伪计数和数据中的计数的和在整体计数中的比例。&lt;/p&gt;
&lt;p&gt;进一步，我们可以计算出文本语料的产生概率为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(\mathcal{W}|\overrightarrow{\alpha}) &amp;amp; = \int p(\mathcal{W}|\overrightarrow{p}) p(\overrightarrow{p}|\overrightarrow{\alpha})d\overrightarrow{p} \notag \\ 
&amp;amp; = \int \prod_{k=1}^V p_k^{n_k} Dir(\overrightarrow{p}|\overrightarrow{\alpha}) d\overrightarrow{p} \notag \\ 
&amp;amp; = \int \prod_{k=1}^V p_k^{n_k} \frac{1}{\Delta(\overrightarrow{\alpha})} 
\prod_{k=1}^V p_k^{\alpha_k -1} d\overrightarrow{p} \notag \\ 
&amp;amp; = \frac{1}{\Delta(\overrightarrow{\alpha})} 
\int \prod_{k=1}^V p_k^{n_k + \alpha_k -1} d\overrightarrow{p} \notag \\ 
&amp;amp; = \frac{\Delta(\overrightarrow{n}+\overrightarrow{\alpha})}{\Delta(\overrightarrow{\alpha})} 
\end{split}
\end{equation}&lt;/p&gt;
&lt;h3&gt;Topic Model 和 PLSA&lt;/h3&gt;
&lt;p&gt;以上 Unigram Model 是一个很简单的模型，模型中的假设看起来过于简单，和人类写文章产生每一个词的过程差距比较大，有没有更好的模型呢？&lt;/p&gt;
&lt;p&gt;我们可以看看日常生活中人是如何构思文章的。如果我们要写一篇文章，往往是先确定要写哪几个主题。譬如构思一篇自然语言处理相关的文章，可能$40\%$会谈论语言学,$30\%$谈论概率统计,$20\%$谈论计算机、还有$10\%$谈论其它的主题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;说到语言学，我们容易想到的词包括：语法、句子、乔姆斯基、句法分析、主语…；&lt;/li&gt;
&lt;li&gt;谈论概率统计，我们容易想到以下一些词: 概率、模型、均值、方差、证明、独立、马尔科夫链、…；&lt;/li&gt;
&lt;li&gt;谈论计算机，我们容易想到的词是： 内存、硬盘、编程、二进制、对象、算法、复杂度…；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们之所以能马上想到这些词，是因为这些词在对应的主题下出现的概率很高。我们可以很自然的看到，一篇文章通常是由多个主题构成的、而每一个主题大概可以用与该主题相关的频率最高的一些词来描述。&lt;/p&gt;
&lt;p&gt;以上这种直观的想法由Hoffman 于 1999 年给出的PLSA(Probabilistic Latent Semantic Analysis) 模型中首先进行了明确的数学化。Hoffman 认为一篇文档(Document) 可以由多个主题(Topic) 混合而成， 而每个Topic 都是词汇上的概率分布，文章中的每个词都是由一个固定的 Topic 生成的。下图是英语中几个Topic 的例子。&lt;/p&gt;
&lt;p&gt;&lt;img alt="topic_example" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/topic-examples_zps1f8f6d28.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;所有人类思考和写文章的行为都可以认为是上帝的行为，我们继续回到上帝的假设中，那么在 PLSA 模型中，Hoffman 认为上帝是按照如下的游戏规则来生成文本的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="game-plsa" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/game-plsa_zpsea8eb70a.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;以上PLSA 模型的文档生成的过程可以图形化的表示为:&lt;/p&gt;
&lt;p&gt;&lt;img alt="plsa-doc-topic-word" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/plsa-doc-topic-word_zps2dc5aea1.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;PLSA模型的文档生成过程我们可以发现在以上的游戏规则下，文档和文档之间是独立可交换的，同一个文档内的词也是独立可交换的，还是一个bag-of-words模型。游戏中的$K$个topic-word骰子，我们可以记为$\overrightarrow{\varphi}_1, \cdots, \overrightarrow{\varphi}_K$, 对于包含$M$篇文档的语料$C=(d_1, d_2, \cdots, d_M)$中的每篇文档$d_m$，都会有一个特定的doc-topic骰子$\overrightarrow{\theta}_m$，所有对应的骰子记为$\overrightarrow{\theta}_1, \cdots, \overrightarrow{\theta}_M$。为了方便，我们假设每个词$w$ 都是一个编号，对应到topic-word骰子的面。于是在 PLSA 这个模型中，第$m$篇文档$d_m$中的每个词的生成概率为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(w|d_m) = \sum_{z=1}^K p(w|z)p(z|d_m) = \sum_{z=1}^K \varphi_{zw} \theta_{mz}
\end{equation}&lt;/p&gt;
&lt;p&gt;所以整篇文档的生成概率为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\overrightarrow{w}|d_m) = \prod_{i=1}^n \sum_{z=1}^K p(w_i|z)p(z|d_m) = 
\prod_{i=1}^n \sum_{z=1}^K \varphi_{zw_i} \theta_{mz}
\end{equation}&lt;/p&gt;
&lt;p&gt;由于文档之间相互独立，我们也容易写出整个语料的生成概率。求解PLSA这个Topic Model的过程汇总，模型参数并容易求解，可以使用著名的EM算法进行求得局部最优解，由于该模型的求解并不是本文的介绍要点，有兴趣的同学参考Hoffman的原始论文，此处略去不讲。&lt;/p&gt;
&lt;h1&gt;LDA文本建模&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;游戏规则&lt;/h2&gt;
&lt;p&gt;对于上述的 PLSA 模型，贝叶斯学派显然是有意见的，doc-topic 骰子$\overrightarrow{\theta}_m$和topic-word骰子$\overrightarrow{\varphi}_k$都是模型中的参数，参数都是随机变量，怎么能没有先验分布呢？于是，类似于对Unigram Model的贝叶斯改造， 我们也可以如下在两个骰子参数前加上先验分布从而把PLSA对应的游戏过程改造为一个贝叶斯的游戏过程。由于$\overrightarrow{\varphi}_k$和$\overrightarrow{\theta}_m$都对应到多项分布，所以先验分布的一个好的选择就是Drichlet分布，于是我们就得到了&lt;strong&gt;LDA(Latent Dirichlet Allocation)&lt;/strong&gt;模型。&lt;/p&gt;
&lt;p&gt;&lt;img alt="lda_dice" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-dice_zps843a7bb2.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;在 LDA 模型中, 上帝是按照如下的规则玩文档生成的游戏的.&lt;/p&gt;
&lt;p&gt;&lt;img alt="game-lda-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/game-lda-1_zpsb9cf4135.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;假设语料库中有$M$ 篇文档，所有的的word和对应的 topic 如下表示:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
\overrightarrow{\mathbf{w}} &amp;amp; = (\overrightarrow{w}_1, \cdots, \overrightarrow{w}_M) \\ 
\overrightarrow{\mathbf{z}} &amp;amp; = (\overrightarrow{z}_1, \cdots, \overrightarrow{z}_M) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\overrightarrow{w}_m$表示第$m$篇文档中的词，$\overrightarrow{z}_m$表示这些词对应的topic编号。&lt;/p&gt;
&lt;p&gt;&lt;img alt="word_topic_example" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/word-topic-vector_zpsa89d3e3d.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;物理过程分解&lt;/h2&gt;
&lt;p&gt;使用概率图模型表示， LDA模型的游戏过程如图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="LDA概率图模型表示" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-graph-model_zps41d58402.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;这个概率图可以分解为两个主要的物理过程： &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\overrightarrow{\alpha}\rightarrow \overrightarrow{\theta}&lt;em m_n="m,n"&gt;m \rightarrow z&lt;/em&gt;$,这个过程表示在生成第$m$篇文档的时候，先从第一个坛子中抽了一个doc-topic 骰子$\overrightarrow{\theta}&lt;em m_n="m,n"&gt;m$, 然后投掷这个骰子生成了文档中第$n$个词的topic编号$z&lt;/em&gt;$； &lt;/li&gt;
&lt;li&gt;$\overrightarrow{\beta} \rightarrow \overrightarrow{\varphi}&lt;em m_n="m,n"&gt;k \rightarrow w&lt;/em&gt; | k=z_{m,n}$, 这个过程表示用如下动作生成语料中第$m$篇文档的第$n$个词：在上帝手头的$K$个topic-word骰子$\overrightarrow{\varphi}&lt;em m_n="m,n"&gt;k$中，挑选编号为$k=z&lt;/em&gt;$的那个骰子进行投掷，然后生成word$w_{m,n}$；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;理解 LDA最重要的就是理解这两个物理过程。LDA 模型在基于$K$个topic生成语料中的$M$篇文档的过程中， 由于是bag-of-words模型，有一些物理过程是相互独立可交换的。&lt;strong&gt;由此， LDA 生成模型中， $M$篇文档会对应于$M$个独立的Dirichlet-Multinomial共轭结构；$K$个topic会对应于$K$个独立的Dirichlet-Multinomial 共轭结构&lt;/strong&gt;。所以理解 LDA 所需要的所有数学就是理解Dirichlet-Multiomail共轭，其它就是理解物理过程。现在我们进入细节， 来看看 LDA 模型是如何被分解为$M+K$个Dirichlet-Multinomial 共轭结构的。&lt;/p&gt;
&lt;p&gt;由第一个物理过程，我们知道$\overrightarrow{\alpha}\rightarrow \overrightarrow{\theta}&lt;em m="m"&gt;m \rightarrow \overrightarrow{z}&lt;/em&gt;$表示生成第$m$篇文档中的所有词对应的topics，显然$\overrightarrow{\alpha}\rightarrow \overrightarrow{\theta}&lt;em m="m"&gt;m$对应于Dirichlet分布，$\overrightarrow{\theta}_m \rightarrow \overrightarrow{z}&lt;/em&gt;$对应于Multinomial分布， 所以整体是一个Dirichlet-Multinomial共轭结构；&lt;/p&gt;
&lt;p&gt;&lt;img alt="lda-dir-mult-conjugate-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-dir-mult-conjugate-1_zpsce0d98eb.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;前文介绍Bayesian Unigram Model的小节中我们对Dirichlet-Multinomial共轭结构做了一些计算。借助于该小节中的结论，我们可以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\overrightarrow{z}_m |\overrightarrow{\alpha}) = \frac{\Delta(\overrightarrow{n}_m+\overrightarrow{\alpha})}{\Delta(\overrightarrow{\alpha})}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\overrightarrow{n}&lt;em m="m"&gt;m = (n&lt;/em&gt;^{(1)}, \cdots, n_{m}^{(K)})$表示第$m$篇文档中第$k$个topic产生的词的个数。进一步，利用Dirichlet-Multiomial共轭结构，我们得到参数$\overrightarrow{\theta}_m$的后验分布恰好是$Dir(\overrightarrow{\theta}_m| \overrightarrow{n}_m + \overrightarrow{\alpha})$.&lt;/p&gt;
&lt;p&gt;由于语料中$M$篇文档的 topics生成过程相互独立，所以我们得到$M$个相互独立的Dirichlet-Multinomial共轭结构，从而我们可以得到整个语料中topics生成概率:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(\overrightarrow{\mathbf{z}} |\overrightarrow{\alpha}) &amp;amp; = \prod_{m=1}^M p(\overrightarrow{z}&lt;em m="1"&gt;m |\overrightarrow{\alpha}) \notag \\ 
&amp;amp;= \prod&lt;/em&gt;^M \frac{\Delta(\overrightarrow{n}_m+\overrightarrow{\alpha})}{\Delta(\overrightarrow{\alpha})} \quad\quad  (*) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;目前为止，我们由$M$篇文档得到了$M$个Dirichlet-Multinomial共轭结构，还有额外$K$个Dirichlet-Multinomial共轭结构在哪儿呢？在上帝按照之前的规则玩LDA游戏的时候，上帝是先完全处理完成一篇文档，再处理下一篇文档。文档中每个词的生成都要抛两次骰子，第一次抛一个doc-topic骰子得到topic, 第二次抛一个topic-word骰子得到word，每次生成每篇文档中的一个词的时候这两次抛骰子的动作是紧邻轮换进行的。如果语料中一共有$N$个词，则上帝一共要抛$2N$次骰子，轮换的抛doc-topic骰子和topic-word骰子。但实际上有一些抛骰子的顺序是可以交换的，我们可以等价的调整$2N$次抛骰子的次序：前$N$次只抛doc-topic骰子得到语料中所有词的topics,然后基于得到的每个词的topic编号，后$N$次只抛topic-word骰子生成$N$个word。于是上帝在玩 LDA 游戏的时候，可以等价的按照如下过程进行：&lt;/p&gt;
&lt;p&gt;&lt;img alt="game-lda-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/game-lda-2_zps25e3e933.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;以上游戏是先生成了语料中所有词的topic, 然后对每个词在给定topic的条件下生成 word.在语料中所有词的 topic已经生成的条件下，任何两个word的生成动作都是可交换的。于是我们把语料中的词进行交换，把具有相同topic的词放在一起:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
\overrightarrow{\mathbf{w}}’ &amp;amp;= (\overrightarrow{w}&lt;em _K_="(K)"&gt;{(1)}, \cdots, \overrightarrow{w}&lt;/em&gt;) \\ 
\overrightarrow{\mathbf{z}}’ &amp;amp;= (\overrightarrow{z}&lt;em _K_="(K)"&gt;{(1)}, \cdots, \overrightarrow{z}&lt;/em&gt;) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\overrightarrow{w}&lt;em _k_="(k)"&gt;{(k)}$表示这些词都是由第$k$个topic生成的，$\overrightarrow{z}&lt;/em&gt;$对应于这些词的topic编号，所以$\overrightarrow{z}_{(k)}$中的分量都是$k$。&lt;/p&gt;
&lt;p&gt;对应于概率图中的第二个物理过程$\overrightarrow{\beta} \rightarrow \overrightarrow{\varphi}&lt;em m_n="m,n"&gt;k \rightarrow w&lt;/em&gt; | k=z_{m,n}$，在$k=z_{m,n}$的限制下，语料中任何两个由 topic $k$生成的词都是可交换的，即便他们不再同一个文档中，所以我们此处不再考虑文档的概念，转而考虑由同一个topic生成的词。考虑如下过程 $\overrightarrow{\beta} \rightarrow \overrightarrow{\varphi}&lt;em _k_="(k)"&gt;k \rightarrow \overrightarrow{w}&lt;/em&gt;$，容易看出， 此时$\overrightarrow{\beta} \rightarrow \overrightarrow{\varphi}&lt;em _k_="(k)"&gt;k$对应于 Dirichlet分布， $\overrightarrow{\varphi}_k \rightarrow \overrightarrow{w}&lt;/em&gt;$对应于 Multinomial 分布， 所以整体也还是一个Dirichlet-Multinomial共轭结构；&lt;/p&gt;
&lt;p&gt;&lt;img alt="lda-dir-mult-conjugate-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-dir-mult-conjugate-2_zps564a3b53.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;同样的，我们可以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\overrightarrow{w}_{(k)} |\overrightarrow{\beta}) = \frac{\Delta(\overrightarrow{n}_k+\overrightarrow{\beta})}{\Delta(\overrightarrow{\beta})}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\overrightarrow{n}&lt;em k="k"&gt;k = (n&lt;/em&gt;^{(1)}, \cdots, n_{k}^{(V)})$， $n_{k}^{(t)}$表示第$k$个topic产生的词中 word $t$的个数。进一步，利用Dirichlet-Multiomial共轭结构，我们得到参数$\overrightarrow{\varphi}_k$的后验分布恰好是$Dir( \overrightarrow{\varphi}_k| \overrightarrow{n}_k + \overrightarrow{\beta})$.&lt;/p&gt;
&lt;p&gt;而语料中$K$个topics生成words的过程相互独立，所以我们得到$K$个相互独立的Dirichlet-Multinomial共轭结构，从而我们可以得到整个语料中词生成概率:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(\overrightarrow{\mathbf{w}} |\overrightarrow{\mathbf{z}},\overrightarrow{\beta}) &amp;amp;= p(\overrightarrow{\mathbf{w}}’ |\overrightarrow{\mathbf{z}}’,\overrightarrow{\beta}) \notag \\ 
&amp;amp;= \prod_{k=1}^K p(\overrightarrow{w}&lt;em _k_="(k)"&gt;{(k)} | \overrightarrow{z}&lt;/em&gt;, \overrightarrow{\beta}) \notag \\ 
&amp;amp;= \prod_{k=1}^K \frac{\Delta(\overrightarrow{n}_k+\overrightarrow{\beta})}{\Delta(\overrightarrow{\beta})}  \quad\quad (**) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;结合(*)和(**)于是我们得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(\overrightarrow{\mathbf{w}},\overrightarrow{\mathbf{z}} |\overrightarrow{\alpha}, \overrightarrow{\beta}) &amp;amp;= 
p(\overrightarrow{\mathbf{w}} |\overrightarrow{\mathbf{z}}, \overrightarrow{\beta}) p(\overrightarrow{\mathbf{z}} |\overrightarrow{\alpha}) \notag \\ 
&amp;amp;= \prod_{k=1}^K \frac{\Delta(\overrightarrow{n}&lt;em m="1"&gt;k+\overrightarrow{\beta})}{\Delta(\overrightarrow{\beta})} 
\prod&lt;/em&gt;^M \frac{\Delta(\overrightarrow{n}_m+\overrightarrow{\alpha})}{\Delta(\overrightarrow{\alpha})}  \quad\quad (***) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;此处的符号表示稍微不够严谨, 向量$\overrightarrow{n}_k$, $\overrightarrow{n}_m$都用$n$表示， 主要通过下标进行区分， $k$下标为topic编号, $m$下标为文档编号。&lt;/p&gt;
&lt;h2&gt;Gibbs Sampling&lt;/h2&gt;
&lt;p&gt;有了联合分布$p(\overrightarrow{\mathbf{w}},\overrightarrow{\mathbf{z}})$, 万能的MCMC算法就可以发挥作用了！于是我们可以考虑使用Gibbs Sampling算法对这个分布进行采样。当然由于$\overrightarrow{\mathbf{w}}$是观测到的已知数据，只有$\overrightarrow{\mathbf{z}}$是隐含的变量，所以我们真正需要采样的是分布$p(\overrightarrow{\mathbf{z}}|\overrightarrow{\mathbf{w}})$。在Gregor Heinrich 那篇很有名的LDA 模型科普文章Parameter estimation for text analysis中，是基于(***) 式推导Gibbs Sampling 公式的。此小节中我们使用不同的方式，主要是基于Dirichlet-Multinomial共轭来推导 Gibbs Sampling 公式，这样对于理解采样中的概率物理过程有帮助。&lt;/p&gt;
&lt;p&gt;语料库$\overrightarrow{\mathbf{z}}$中的第$i$个词我们记为$z_i$, 其中$i=(m,n)$是一个二维下标，对应于第$m$篇文档的第$n$个词，我们用$\neg i$表示去除下标为$i$的词。那么按照 Gibbs Sampling 算法的要求，我们要求得任一个坐标轴$i$对应的条件分布$p(z_i = k|\overrightarrow{\mathbf{z}}_{\neg i}, \overrightarrow{\mathbf{w}})$。假设已经观测到的词$w_i=t$, 则由贝叶斯法则，我们容易得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(z_i = k|\overrightarrow{\mathbf{z}}&lt;em _neg="\neg" i="i"&gt;{\neg i}, \overrightarrow{\mathbf{w}}) \propto 
p(z_i = k, w_i = t |\overrightarrow{\mathbf{z}}&lt;/em&gt;, \overrightarrow{\mathbf{w}}_{\neg i}) \\ 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;由于$z_i=k,w_i=t$只涉及到第$m$篇文档和第$k$个topic，所以上式的条件概率计算中, 实际上也只会涉及到如下两个Dirichlet-Multinomial 共轭结构:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\overrightarrow{\alpha} \rightarrow \overrightarrow{\theta}&lt;em m="m"&gt;m \rightarrow \overrightarrow{z}&lt;/em&gt;$;&lt;/li&gt;
&lt;li&gt;$\overrightarrow{\beta} \rightarrow \overrightarrow{\varphi}&lt;em _k_="(k)"&gt;k \rightarrow \overrightarrow{w}&lt;/em&gt;$;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其它的$M+K−2$个Dirichlet-Multinomial共轭结构和$z_i=k,w_i=t$是独立的。由于在语料去掉第$i$个词对应的 $(z_i,w_i)$，并不改变我们之前讨论的$M+K$个Dirichlet-Multinomial共轭结构，只是某些地方的计数会减少。所以$\overrightarrow{\theta}_m, \overrightarrow{\varphi}_k$的后验分布都是Dirichlet:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(\overrightarrow{\theta}&lt;em _neg="\neg" i="i"&gt;m|\overrightarrow{\mathbf{z}}&lt;/em&gt;, \overrightarrow{\mathbf{w}}&lt;em i="i" m_neg="m,\neg"&gt;{\neg i}) 
&amp;amp;= Dir(\overrightarrow{\theta}_m| \overrightarrow{n}&lt;/em&gt; + \overrightarrow{\alpha}) \\ 
p(\overrightarrow{\varphi}&lt;em _neg="\neg" i="i"&gt;k|\overrightarrow{\mathbf{z}}&lt;/em&gt;, \overrightarrow{\mathbf{w}}&lt;em i="i" k，_neg="k，\neg"&gt;{\neg i}) 
&amp;amp;= Dir( \overrightarrow{\varphi}_k| \overrightarrow{n}&lt;/em&gt; + \overrightarrow{\beta}) 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;使用上面两个式子，把以上想法综合一下，我们就得到了如下的Gibbs Sampling公式的推导:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
p(z_i = k|\overrightarrow{\mathbf{z}}&lt;em _neg="\neg" i="i"&gt;{\neg i}, \overrightarrow{\mathbf{w}}) &amp;amp; \propto 
p(z_i = k, w_i = t |\overrightarrow{\mathbf{z}}&lt;/em&gt;, \overrightarrow{\mathbf{w}}&lt;em _neg="\neg" i="i"&gt;{\neg i}) \\ 
&amp;amp;= \int p(z_i = k, w_i = t, \overrightarrow{\theta}_m,\overrightarrow{\varphi}_k | 
\overrightarrow{\mathbf{z}}&lt;/em&gt;, \overrightarrow{\mathbf{w}}&lt;em _neg="\neg" i="i"&gt;{\neg i}) d \overrightarrow{\theta}_m d \overrightarrow{\varphi}_k \\ 
&amp;amp;= \int p(z_i = k, \overrightarrow{\theta}_m|\overrightarrow{\mathbf{z}}&lt;/em&gt;, \overrightarrow{\mathbf{w}}&lt;em _neg="\neg" i="i"&gt;{\neg i}) 
\cdot p(w_i = t, \overrightarrow{\varphi}_k | \overrightarrow{\mathbf{z}}&lt;/em&gt;, \overrightarrow{\mathbf{w}}&lt;em _neg="\neg" i="i"&gt;{\neg i}) 
d \overrightarrow{\theta}_m d \overrightarrow{\varphi}_k \\ 
&amp;amp;= \int p(z_i = k |\overrightarrow{\theta}_m) p(\overrightarrow{\theta}_m|\overrightarrow{\mathbf{z}}&lt;/em&gt;, \overrightarrow{\mathbf{w}}&lt;em _neg="\neg" i="i"&gt;{\neg i}) 
\cdot p(w_i = t |\overrightarrow{\varphi}_k) p(\overrightarrow{\varphi}_k|\overrightarrow{\mathbf{z}}&lt;/em&gt;, \overrightarrow{\mathbf{w}}&lt;em i="i" m_neg="m,\neg"&gt;{\neg i}) 
d \overrightarrow{\theta}_m d \overrightarrow{\varphi}_k \\ 
&amp;amp;= \int p(z_i = k |\overrightarrow{\theta}_m) Dir(\overrightarrow{\theta}_m| \overrightarrow{n}&lt;/em&gt; + \overrightarrow{\alpha}) d \overrightarrow{\theta}&lt;em i="i" k_neg="k,\neg"&gt;m \\ 
&amp;amp; \hspace{0.2cm} \cdot \int p(w_i = t |\overrightarrow{\varphi}_k) Dir( \overrightarrow{\varphi}_k| \overrightarrow{n}&lt;/em&gt; + \overrightarrow{\beta}) d \overrightarrow{\varphi}&lt;em mk="mk"&gt;k \\ 
&amp;amp;= \int \theta&lt;/em&gt; Dir(\overrightarrow{\theta}&lt;em i="i" m_neg="m,\neg"&gt;m| \overrightarrow{n}&lt;/em&gt; + \overrightarrow{\alpha}) d \overrightarrow{\theta}&lt;em kt="kt"&gt;m 
\cdot \int \varphi&lt;/em&gt; Dir( \overrightarrow{\varphi}&lt;em i="i" k_neg="k,\neg"&gt;k| \overrightarrow{n}&lt;/em&gt; + \overrightarrow{\beta}) d \overrightarrow{\varphi}&lt;em mk="mk"&gt;k \\ 
&amp;amp;= E(\theta&lt;/em&gt;) \cdot E(\varphi_{kt}) \\ 
&amp;amp;= \hat{\theta}&lt;em kt="kt"&gt;{mk} \cdot \hat{\varphi}&lt;/em&gt; \\ 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;以上推导估计是整篇文章中最复杂的数学了，表面上看上去复杂，但是推导过程中的概率物理意义是简单明了的：$z_i=k,w_i=t$的概率只和两个Dirichlet-Multinomial共轭结构关联。而最终得到的$\hat{\theta}&lt;em kt="kt"&gt;{mk}, \hat{\varphi}&lt;/em&gt;$就是对应的两个Dirichlet后验分布在贝叶斯框架下的参数估计。借助于前面介绍的Dirichlet 参数估计的公式 ，我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split} 
\hat{\theta}&lt;em i="i" m_neg="m,\neg"&gt;{mk} &amp;amp;= \frac{n&lt;/em&gt;^{(k)} + \alpha_k}{\sum_{k=1}^K (n_{m,\neg i}^{(k)} + \alpha_k)} \\ 
\hat{\varphi}&lt;em i="i" k_neg="k,\neg"&gt;{kt} &amp;amp;= \frac{n&lt;/em&gt;^{(t)} + \beta_t}{\sum_{t=1}^V (n_{k,\neg i}^{(t)} + \beta_t)} 
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;于是，我们最终得到了LDA模型的Gibbs Sampling公式:&lt;/p&gt;
&lt;p&gt;\begin{equation}&lt;br /&gt;
p(z_i = k|\overrightarrow{\mathbf{z}}&lt;em i="i" m_neg="m,\neg"&gt;{\neg i}, \overrightarrow{\mathbf{w}}) \propto 
\frac{n&lt;/em&gt;^{(k)} + \alpha_k}{\sum_{k=1}^K (n_{m,\neg i}^{(k)} + \alpha_k)} 
\cdot \frac{n_{k,\neg i}^{(t)} + \beta_t}{\sum_{t=1}^V (n_{k,\neg i}^{(t)} + \beta_t)} 
\end{equation}&lt;/p&gt;
&lt;p&gt;这个公式是很漂亮的， 右边其实就是$p(topic|doc)⋅p(word|topic)$,这个概率其实是doc→topic→word的路径概率，由于topic 有$K$个，所以Gibbs Sampling 公式的物理意义其实就是在这$K$条路径中进行采样。&lt;/p&gt;
&lt;p&gt;&lt;img alt="doc-topic-word" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gibbs-path-search_zpsb83099d8.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;Training and Inference&lt;/h2&gt;
&lt;p&gt;有了LDA模型，当然我们的目标有两个:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;估计模型中的参数$\overrightarrow{\varphi}_1, \cdots, \overrightarrow{\varphi}_K$和$\overrightarrow{\theta}_1, \cdots, \overrightarrow{\theta}_M$；&lt;/li&gt;
&lt;li&gt;对于新来的一篇文档$doc_{new}$，我们能够计算这篇文档的topic分布$\overrightarrow{\theta}_{new}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;有了Gibbs Sampling公式， 我们就可以基于语料训练LDA模型，并应用训练得到的模型对新的文档进行topic 语义分析。训练的过程就是获取语料中的$(z,w)$的样本，而模型中的所有的参数都可以基于最终采样得到的样本进行估计。训练的流程很简单:&lt;/p&gt;
&lt;p&gt;&lt;img alt="LDA Training" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-training_zpsa31be49e.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;对于Gibbs Sampling算法实现的细节，请参考Gregor Heinrich的 Parameter estimation for text analysis 中对算法的描述，以及PLDA(http://code.google.com/p/plda)的代码实现，此处不再赘述。&lt;/p&gt;
&lt;p&gt;由这个topic-word频率矩阵我们可以计算每一个$p(word|topic)$概率，从而算出模型参数$\overrightarrow{\varphi}_1, \cdots, \overrightarrow{\varphi}_K$, 这就是上帝用的$K$个topic-word骰子。当然，语料中的文档对应的骰子参数$\overrightarrow{\theta}_1, \cdots, \overrightarrow{\theta}_M$在以上训练过程中也是可以计算出来的，只要在Gibbs Sampling收敛之后，统计每篇文档中的topic的频率分布，我们就可以计算每一个$p(topic|doc)$概率，于是就可以计算出每一个$\overrightarrow{\theta}_m$。由于参数$\overrightarrow{\theta}_m$是和训练语料中的每篇文档相关的，对于我们理解新的文档并无用处，所以工程上最终存储LDA模型时候一般没有必要保留。通常，在LDA模型训练的过程中，我们是取Gibbs Sampling收敛之后的$n$个迭代的结果进行平均来做参数估计，这样模型质量更高。&lt;/p&gt;
&lt;p&gt;有了LDA的模型，对于新来的文档 $doc_{new}$, 我们如何做该文档的topic语义分布的计算呢？基本上inference的过程和training的过程完全类似。对于新的文档， 我们只要认为Gibbs Sampling公式中的$\hat{\varphi}&lt;em new="new"&gt;{kt}$部分是稳定不变的，是由训练语料得到的模型提供的，所以采样过程中我们只要估计该文档的topic分布$\overrightarrow{\theta}&lt;/em&gt;$就好了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="LDA Inference" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lda-inference_zpsaa5c9320.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;后记LDA&lt;/h2&gt;
&lt;p&gt;对于专业做机器学习的兄弟而言，只能算是一个简单的Topic Model。但是对于互联网中做数据挖掘、语义分析的工程师，LDA 的门槛并不低。 LDA 典型的属于这样一种机器学习模型：要想理解它，需要比较多的数学背景，要在工程上进行实现，却相对简单。 Gregor Heinrich 的LDA 模型科普文章 Parameter estimation for text analysis 写得非常的出色，这是学习 LDA 的必看文章。不过即便是这篇文章，对于工程师也是有门槛的。我写的这个科普最好对照 Gregor Heinrich 的这篇文章来看， 我用的数学符号也是尽可能和这篇文章保持一致。这份LDA 科普是基于给组内兄弟做报告的 ppt 整理而成的，说是科普其实也不简单，涉及到的数学还是太多。在工业界也混了几年，经常感觉到工程师对于学术界的玩的模型有很强的学习和尝试的欲望，只是学习成本往往太高。所以我写 LDA 的初衷就是写给工业界的工程师们看的，希望把学术界玩的一些模型用相对通俗的方式介绍给工程师；如果这个科普对于读研究生的一些兄弟姐妹也有所启发，只能说那是一个 side effect :-)。我个人很喜欢LDA ，它是在文本建模中一个非常优雅的模型，相比于很多其它的贝叶斯模型， LDA 在数学推导上简洁优美。学术界自 2003 年以来也输出了很多基于LDA 的 Topic Model 的变体，要想理解这些更加高级的 Topic Model, 首先需要很好的理解标准的 LDA 模型。在工业界， Topic Model 在 Google、Baidu 等大公司的产品的语义分析中都有着重要的应用；所以Topic Model 对于工程师而言，这是一个很有应用价值、值得学习的模型。我接触 Topic Model 的时间不长，主要是由于2年前和 PLDA 的作者 Wangyi 一起合作的过程中，从他身上学到了很多 Topic Model 方面的知识。关于 LDA 的相关知识，其实可以写的还有很多：如何提高 LDA Gibbs Sampling 的速度、如何优化超参数、如何做大规模并行化、LDA 的应用、LDA 的各种变体…… 不过我的主要目标还是科普如何理解标准的LDA 模型。学习一个模型的时候我喜欢追根溯源，常常希望把模型中的每一个数学推导的细节搞明白，把公式的物理意义想清楚，不过数学推导本身并不是我想要的，把数学推导还原为物理过程才是我乐意做的事。最后引用一下物理学家费曼的名言结束 LDA 的数学科普：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What I cannot create, I do not understand. — Richard Feynman&lt;/p&gt;
&lt;/blockquote&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="NLP"></category></entry><entry><title>小小收藏夹[持续更新中]</title><link href="http://www.qingyuanxingsi.com/xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html" rel="alternate"></link><updated>2014-05-05T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-05:xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html</id><summary type="html">&lt;h1&gt;NLP&lt;sup id="sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1-back"&gt;&lt;a class="simple-footnote" href="#sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1" title="Natural Language Processing"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;Relation Extration&lt;ul&gt;
&lt;li&gt;Hand-written approach more suitable for structured data,such as a telephone book,Facebook or eBay;&lt;/li&gt;
&lt;li&gt;Supervised Method;得到所有的命名实体组,使用一个分类器(&lt;em&gt;features&lt;/em&gt;)判断它们是否是关联的,如果是,则使用第二个分类器判断它们之间的关联关系具体是什么; &lt;/li&gt;
&lt;li&gt;Semi-Supervised(Relation Bootstrapping/Distant Supervised Learning) and unsupervised methods(Open Information Extraction);Strapping方法感觉很巧妙,个人很喜欢;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;SVM&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;对于SVM这么高端大气上档次的东西,当然要单独列出来。今天其他东西实在看不下去了,所以把Pluskid之前写的一系列讲SVM的文章再挖出来看看。以下是目录以及对每篇的简单说明:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=632"&gt;支持向量机: Maximum Margin Classifier&lt;/a&gt;;文中主要介绍了两个距离,&lt;em&gt;Functional Margin&lt;/em&gt; $\hat{\gamma}$和&lt;em&gt;Geometrical Margin&lt;/em&gt; $\tilde{\gamma}$.它们之间满足$\hat{\gamma} = ||w||\tilde{\gamma}$.我们固定$\hat{\gamma} = 1$,通过最大化$\frac{1}{||w||}$来得到&lt;strong&gt;Maximum Margin Classifier&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=682"&gt;支持向量机: Support Vector&lt;/a&gt;;简要介绍了Support Vector是指什么,另外对线性可分的情况利用Duality进行了推导并得出了两个比较重要的结论:&lt;ul&gt;
&lt;li&gt;对新点的预测只需要计算与训练点之间的内积即可;&lt;/li&gt;
&lt;li&gt;非支持向量不参与模型的计算过程之中。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=685"&gt;支持向量机: Kernel&lt;/a&gt;;&lt;strong&gt;Kernel&lt;/strong&gt;的基本思想。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=692"&gt;支持向量机：Outliers&lt;/a&gt;;通过引入松弛变量处理Outliers,而实际上最后的优化形式只是加上$\alpha_i \leq C$的限制。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=696"&gt;支持向量机：Numerical Optimization&lt;/a&gt;;以非常通俗易懂的方式介绍了一下&lt;strong&gt;SMO(Sequential Minimal Optimization)&lt;/strong&gt;,赞一个。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Deep Learning&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;稀疏编码(&lt;em&gt;Sparse Coding&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8777094"&gt;Deep Learning（深度学习）学习笔记整理系列之（五)&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep Learning总结&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8782018"&gt;Deep Learning（深度学习）学习笔记整理系列之（八)&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Pocket&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://v.youku.com/v_show/id_XNzAwMTI0MDgw.html"&gt;罗辑思维 2014：右派为什么这么横 10&lt;/a&gt;;视频主要介绍了保守主义的三个特征,同时分析了人们在面临选择的时候的不同思维方式,个人觉得这一点很有借鉴意义，建议一看!&lt;/li&gt;
&lt;li&gt;&lt;a href="http://v.youku.com/v_show/id_XNzAzMTkyNDky.html"&gt;罗辑思维 2014：迷茫时代的明白人 11&lt;/a&gt;;活在当下。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;算法&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;今天看了一下网上流传的传说中的高大上的所谓的&lt;code&gt;十大海量数据处理算法&lt;/code&gt;,看了一下,实际上没有什么东西,唯独&lt;strong&gt;Bloom Filter&lt;/strong&gt;看着还挺好玩的,所以以下给出一个通俗易懂的链接。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.cnblogs.com/heaad/archive/2011/01/02/1924195.html"&gt;那些优雅的数据结构(1) : BloomFilter——大规模数据处理利器&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在过去的若干年里,有一个心结一直萦绕在我的心头挥之不去,它存在于我的脑海里，我的梦里，我的歌声里,TA就是&lt;strong&gt;B树&lt;/strong&gt;(好吧,其实是因为没有机会好好地研究一下它啦)。以下给出两个链接,它们主要介绍了B树的基本概念,性质以及针对B树的插入、删除操作,两个PPT还是相当直观的,应该能够比较直观地了解B树这个数据结构!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://cecs.wright.edu/~tkprasad/courses/cs707/L04-X-B-Trees.ppt"&gt;B-Trees&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://zh.scribd.com/doc/18210/B-TREE-TUTORIAL-PPT"&gt;B TREE TUTORIAL PPT&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;今天又重新看了一下这个写的很不错的&lt;strong&gt;A*算法&lt;/strong&gt;,恩,这篇文章想来是极好的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.raywenderlich.com/zh-hans/21503/a%E6%98%9F%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D"&gt;A星寻路算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;复习一下之前做智能提示时用到的&lt;strong&gt;Trie Tree&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.cs.umd.edu/class/fall2005/cmsc132/lecs/lec29.ppt"&gt;Indexed Search Tree (Trie) - Computer Science Department&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;看了一下传说中的数据挖掘十大算法,好像就&lt;strong&gt;Apriori算法&lt;/strong&gt;不是特别熟吧,所以重新看了一遍;个人觉得如果我早出生若干年,这种程度的算法也是能想出来的吧(我指思想).好吧,我认为着重要理解的有如下两点:&lt;ul&gt;
&lt;li&gt;Support;其实也就是某种组合在所有Transaction中出现的频度。&lt;/li&gt;
&lt;li&gt;Confidence;当生成关联规则$A\to B$时,有$confidence = \frac{Count(A,B)}{Count(A)}$,背后的Intuition就是如果我买了$A$,大概会有多大的可能买$B$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.cs.sjsu.edu/faculty/lee/cs157b/Gaurang%20Negandhi--Apriori%20Algorithm%20Presentation.ppt"&gt;Apriori Algorithm Review for Finals&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最近需要对1G的文本数据进行处理,所以想了解一下现行的分布式计算框架的应用场景,从而选择合适的框架用于这个任务,期间看到以下两篇文章写的很不错,特此摘录。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://langyu.iteye.com/blog/1407194"&gt;对实时分析与离线分析的思考&lt;/a&gt;
&lt;a href="http://langyu.iteye.com/blog/1407194"&gt;对实时分析与离线分析的思考(二)&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最近脑子总是不断宕机,宕机了就什么也看不了了,刚看了一篇一个人讲自己怎么学习算法的博文,感觉还不错(只是看了作者的经历,他看过的那些书还没来得及看)。以下给出链接:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://zh.lucida.me/blog/on-learning-algorithms/"&gt;我的算法学习之路&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Machine Learning&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;看的有点累了,不想看&lt;em&gt;EM&lt;/em&gt;算法复杂的数学公式推导了,所以找到之前看过的一篇,回顾一下,等以后想看了再详细介绍&lt;em&gt;Mixture Models&lt;/em&gt;和&lt;em&gt;EM&lt;/em&gt;算法吧!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blog.pluskid.org/?p=39"&gt;漫谈 Clustering (3): Gaussian Mixture Model&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;近期为了理解卷积,于是到处找资料,无意中发现了这一篇神一般的理解。(&lt;strong&gt;墙裂推荐&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.guokr.com/post/342476/"&gt;关于卷积的一个血腥的讲解，看完给跪了&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PCA数据预处理&lt;em&gt;Whitening&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;在使用PCA前需要对数据进行预处理，首先是均值化，即对每个特征维，都减掉该维的平均值，然后就是将不同维的数据范围归一化到同一范围，方法一般都是&lt;strong&gt;除以最大值&lt;/strong&gt;。但是比较奇怪的是，在对自然图像进行均值处理时并不是不是减去该维的平均值，而是减去这张图片本身的平均值。因为PCA的预处理是按照不同应用场合来定的。&lt;/p&gt;
&lt;p&gt;自然图像指的是人眼经常看见的图像，其符合某些统计特征。一般实际过程中，只要是拿正常相机拍的，没有加入很多人工创作进去的图片都可以叫做是自然图片，因为很多算法对这些图片的输入类型还是比较鲁棒的。在对自然图像进行学习时，其实不需要太关注对图像做方差归一化，因为自然图像每一部分的统计特征都相似，只需做均值为0化就ok了。不过对其它的图片进行训练时，比如首先字识别等，就需要进行方差归一化了。&lt;/p&gt;
&lt;p&gt;有一个观点需要注意，那就是&lt;strong&gt;PCA并不能阻止过拟合现象&lt;/strong&gt;。表明上看PCA是降维了，因为在同样多的训练样本数据下，其特征数变少了，应该是更不容易产生过拟合现象。但是在实际操作过程中，这个方法阻止过拟合现象效果很小，主要还是通过&lt;strong&gt;规则项&lt;/strong&gt;来进行阻止过拟合的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whitening&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whitening&lt;/strong&gt;的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。数据的Whitening必须满足两个条件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不同特征间相关性最小，接近0；&lt;/li&gt;
&lt;li&gt;所有特征的方差相等（不一定为1）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的白化操作有PCA whitening和ZCA whitening。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PCA whitening&lt;/em&gt;是指将数据x经过PCA降维为z后，可以看出z中每一维是独立的，满足whitening白化的第一个条件，这是只需要将z中的每一维都除以标准差就得到了每一维的方差为1，也就是说方差相等。公式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_{PCAwhite,i} = \frac{x_{rot,i}}{\sqrt{\lambda_i}}
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ZCA whitening&lt;/em&gt;是指数据x先经过PCA变换为z，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足whtienning的第一个条件，特征间相互独立。然后同样进行方差为1的操作，最后将得到的矩阵左乘一个特征向量矩阵U即可。ZCA whitening公式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_{ZCAwhite} = Ux_{PCAwhite}
\end{equation}&lt;/p&gt;
&lt;p&gt;参考&lt;a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/21/2973231.html"&gt;Deep learning：十(PCA和whitening)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;最近一直在看&lt;strong&gt;高斯过程&lt;/strong&gt;,挺难理解的,好吧,咱们慢慢来,先给个链接。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.eurandom.tue.nl/events/workshops/2010/YESIV/Prog-Abstr_files/Ghahramani-lecture2.pdf"&gt;Introduction to Gaussian Process&lt;/a&gt;
* 今天看自然语言处理Standford公开课的时候看到最大熵模型(Maximum Entropy Models),视频讲的实在太罗嗦了,在网上找了找,下面这个PPT貌似还挺不错的。(原始PPT有部分错误,以下网盘共享文件是部分修正后版本,可能还会有错误,欢迎指出)&lt;/p&gt;
&lt;p&gt;&lt;a href="http://pan.baidu.com/s/1gdze7h5"&gt;Maximum Entropy Model&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;PGM&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;以下给出讲解PGM比较深入浅出的一系列Lecture Slides。&lt;/p&gt;
&lt;h2&gt;PART I:Introduction to PGM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-1-Introduction-Combined.pdf"&gt;Introduction and Overview&lt;/a&gt;;主要介绍了PGM的背景以及Factor的基础知识。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Bayes-Nets.pdf"&gt;Bayesian Network Fundamentals&lt;/a&gt;;简要介绍了什么是Bayesian Network、Reasoning Patterns以及Influence Flow.最后简要介绍了一下Naive Bayes Classifier.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Naive Bayes Classifier" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/naive_bayes_model_zps09771da2.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Template-Models.pdf"&gt;Template Models&lt;/a&gt;;主要介绍了Template Models,包括Bayesian Network(HMM)以及Plate Models;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-CPDs.pdf"&gt;Structured CPDs&lt;/a&gt;;介绍了几种CPD表示的其他常见形式,包括:&lt;ul&gt;
&lt;li&gt;Deterministic CPDs&lt;/li&gt;
&lt;li&gt;Tree-structured CPDs&lt;/li&gt;
&lt;li&gt;Logistic CPDs &amp;amp; generalizations&lt;/li&gt;
&lt;li&gt;Noisy OR/AND&lt;/li&gt;
&lt;li&gt;Linear Gaussian &amp;amp; generalizations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Markov-Nets.pdf"&gt;Markov Network Fundamentals&lt;/a&gt;;本部分涵盖的内容有Markov Network,General Gibbs Distribution,CRF,Log-Linear Models.(&lt;strong&gt;Logistic Models is a simple CRF;CRF does not need to concern about the correlation between features!&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;PART II:PGM Inference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Variable-Elimination.pdf"&gt;Variable Elimination&lt;/a&gt;;简要介绍了如何在Bayesian Network以及Ｍarkov Network中执行VE算法;接着对其复杂度进行了分析;最后从图的视角重新审视了一下VE算法.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Variable Elimination" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/variable_elimination_zps6fdc76a6.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Belief-Propagation.pdf"&gt;Belief Propagation(I)&lt;/a&gt; &amp;amp; &lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Belief-Propagation.pdf"&gt;Belief Propagation(II)&lt;/a&gt;;其基本内容如下:&lt;ul&gt;
&lt;li&gt;Belief Propagation算法基本流程;&lt;/li&gt;
&lt;li&gt;Cluster Graph的基本性质(&lt;code&gt;BP does poorly when we have strong correlations!&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;BP算法的基本性质;&lt;/li&gt;
&lt;li&gt;Clique Tree Algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Belief Propagation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/belief_propogation_zps866416cc.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-MAP.pdf"&gt;MAP Estimation&lt;/a&gt;;关于MAP Inference的基础知识。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Sampling.pdf"&gt;Sampling Methods&lt;/a&gt;;Basic Sampling Methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;PART III:PGM Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Parameters-Part-1.pdf"&gt;Learning: Parameter Estimation, Part 1&lt;/a&gt; &amp;amp; &lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Parameters-Part-2.pdf"&gt;Learning: Parameter Estimation, Part 2&lt;/a&gt;;Parameter Estimation for BN and MN;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-BN-Structures.pdf"&gt;Structure Learning&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Incomplete-Data.pdf"&gt;Learning With Incomplete Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:该课程网址见&lt;a href="https://class.coursera.org/pgm-003"&gt;PGM&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ising Model&lt;/li&gt;
&lt;li&gt;Dual Decomposition&lt;/li&gt;
&lt;li&gt;Decision Making&lt;/li&gt;
&lt;li&gt;Bayesian Scores&lt;/li&gt;
&lt;li&gt;Learning With Incomplete Data&lt;/li&gt;
&lt;li&gt;Lassos&lt;/li&gt;
&lt;li&gt;凸QP&lt;/li&gt;
&lt;li&gt;Duality&lt;/li&gt;
&lt;li&gt;KKT条件&lt;/li&gt;
&lt;li&gt;支持向量机番外篇I:&lt;a href="http://blog.pluskid.org/?p=702"&gt;支持向量机：Duality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;支持向量机番外篇II:&lt;a href="http://blog.pluskid.org/?p=723"&gt;支持向量机：Kernel II&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Apriori算法细节&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1"&gt;&lt;a href="https://class.coursera.org/nlp/lecture"&gt;Natural Language Processing&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="算法"></category><category term="Fun"></category><category term="Staff"></category><category term="收藏夹"></category><category term="Bloom Filter"></category><category term="B Trees"></category><category term="Data Structure"></category><category term="Algorithm"></category><category term="PGM"></category></entry><entry><title>自然语言处理(序章):我爱自然语言处理(I)</title><link href="http://www.qingyuanxingsi.com/zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i.html" rel="alternate"></link><updated>2014-05-05T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-05:zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i.html</id><summary type="html">&lt;p&gt;昨天浏览了一下&lt;a href="http://www.52nlp.cn"&gt;我爱自然语言处理&lt;/a&gt;站点上的全部文章,然后基本过滤下来自己感兴趣的90篇左右的文章,这一阵子就先把这90篇文章认认真真看完吧,总结看的过程中自己感兴趣而且重要的点,遂成此文。&lt;strong&gt;本文中所有资料属我爱自然语言处理及博客原文引用作者所有,特此声明&lt;/strong&gt;。&lt;/p&gt;
&lt;h1&gt;齐夫定律(Zipf’s Law)&lt;/h1&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Zipf's Law&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;在任何一个自然语言里第$n$个最常用的单词的频率与$n$近似成反比(The frequency of use of the nth-most-frequently-used word in any natural language is approximately inversely proportional to n).更正式地,我们可以说:存在一个常量$k$,使得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
f \times r =k
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$f$表示单词出现的频度,$r$表示单词出现次数的排名(RANK).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Zipf" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/zipf_zpsae557119.png"&gt;&lt;/p&gt;
&lt;p&gt;北京大学姜望琪老师的《Zipf与省力原则》讲得很好，部分摘录如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;省力原则(the Principle of Least Effort)，又称经济原则(the Economy Principle)，可以概括为：以最小的代价换取最大的收益。这是指导人类行为的一条根本性原则。在现代学术界，第一个明确提出这条原则的是美国学者 George Kingsley Zipf。　　&lt;/li&gt;
&lt;li&gt;George Kingsley Zipf1902年1月出生于一个德裔家庭（其祖父十九世纪中叶移居美国)。1924年，他以优异成绩毕业于哈佛学院。1925年在德国波恩、柏林学习。1929年完成Relative Frequency as a Determinant of Phonetic Change，获得哈佛比较语文学博士学位。然后，他开始在哈佛教授德语。1931年与Joyce Waters Brown结婚。1932年出版Selected Studies of the Principle of Relative Frequency in Language。1935年出版The Psycho- Biology of Language：An Introduction to Dynamic Philology。1939年被聘为讲师。1949年出版Human Behavior and the Principle of Least Effort：An Introduction to Human Ecology。1950年9月因患癌症病逝。　　&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zipf在1949年的书里提出了一条指导人类行为的基本原则——省力原则。Zipf在序言里指出，如果我们把人类行为纯粹看作一种自然现象，如果我们像研究蜜蜂的社会行为、鸟类的筑巢习惯一样研究人类行为，那么，我们就有可能揭示其背后的基本原则。这是他提出“省力原则”的大背景。当Zipf在众多互不相干的现象里都发现类似Zipf定律的规律性以后，他就开始思考造成这种规律性的原因。这是导致他提出“省力原则”的直接因素。在开始正式论证以前，Zipf首先澄清了“省力原则”的字面意义。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一，这是一种平均量。一个人一生要经历很多事情，他在一件事情上的省力可能导致在另一件事情上的费力。反过来，在一件事情上的费力，又可能导致在另一件事情上的省力。&lt;/li&gt;
&lt;li&gt;第二，这是一种概率。一个人很难在事先百分之百地肯定某种方法一定能让他省力，他只能有一个大概的估计。因为用词研究是理解整个言语过程的关键，而后者又是理解整个人类生态学的关键，他的具体论证从用词经济开始。Zipf认为，用词经济可以从两个角度来讨论：说话人的角度和听话人的角度。从说话人的角度看，用一个词表达所有的意义是最经济的。这样，说话人不需要花费气力去掌握更多的词汇，也不需要考虑如何从一堆词汇中选择一个合适的词。这种“单一词词汇量”就像木工的一种多用工具，集锯刨钻锤于一身，可以满足多种用途。但是，从听话人角度看，这种“单一词词汇量”是最费力的。他要决定这个词在某个特定场合到底是什么意思，而这几乎是不可能的。相反，对听话人来说，最省力的是每个词都只有一个意义，词汇的形式和意义之间完全一一对应。这两种经济原则是互相冲突、互相矛盾的。Zipf把它们叫做一条言语流中的两股对立的力量：“单一化力量”（the Force of Unification）和“多样化力量”（the Force of Diversification）。他认为，这两股力量只有达成妥协，达成一种平衡，才能实现真正的省力。事实正像预计的那样。请看Zipf的论证：假如只有单一化力量，那么任何语篇的单词数量（number）都会是1，而它的出现次数（frequency）会是100%。另一方面，假如只有多样化力量，那么每个单词的出现次数都会接近1，而单词总数量则由语篇的长度决定。这就是说， &lt;em&gt;number&lt;/em&gt;和&lt;em&gt;frequency&lt;/em&gt;是衡量词汇平衡程度的两个参数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;中文分词&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;对于英文而言,由于词自然一般有非常自然的分隔符(空格或标点符号等),因此对于英文而言基本不涉及分词这个任务,而对于中文而言,因为中文没有非常明显的自然分隔符,而且很多自然语言处理任务很大程度上依赖于分词质量,因此中文分词是中文自然语言处理中非常基础且重要的一个任务,以下对中文分词中涉及的基本算法做一个简要的介绍:&lt;/p&gt;
&lt;h2&gt;最长正向匹配算法&lt;/h2&gt;
&lt;p&gt;最长正向匹配算法的基本流程如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="MAX_SEGMENTATION" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/max_segmentation_zpsadc70b2d.png"&gt;&lt;/p&gt;
&lt;p&gt;逆向匹配法思想与正向一样，只是从右向左切分，这里举一个例子：&lt;/p&gt;
&lt;p&gt;输入例句:S1=”计算语言学课程有意思”;&lt;/p&gt;
&lt;p&gt;定义:最大词长MaxLen = 5；S2="";分隔符="/"；&lt;/p&gt;
&lt;p&gt;假设存在词表:计算语言学,课程,意思,...；最大逆向匹配分词算法过程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;S2=””；S1不为空，从S1右边取出候选子串W=”课程有意思”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有意思”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”有意思”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”意思”&lt;/li&gt;
&lt;li&gt;查词表，“意思”在词表中，将W加入到S2中，S2=” 意思/”，并将W从S1中去掉，此时S1=”计算语言学课程有”；&lt;/li&gt;
&lt;li&gt;S1不为空，于是从S1左边取出候选子串W=”言学课程有”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程有”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程有”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”程有”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”有”，这W是单字，将W加入到S2中，S2=“/有/意思”，并将W从S1中去掉，此时S1=”计算语言学课程”；&lt;/li&gt;
&lt;li&gt;S1不为空，于是从S1左边取出候选子串W=”语言学课程”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”言学课程”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”学课程”；&lt;/li&gt;
&lt;li&gt;查词表，W不在词表中，将W最左边一个字去掉，得到W=”课程”；&lt;/li&gt;
&lt;li&gt;查词表，“意思”在词表中，将W加入到S2中，S2=“ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=”计算语言学”；&lt;/li&gt;
&lt;li&gt;S1不为空，于是从S1左边取出候选子串W=”计算语言学”；&lt;/li&gt;
&lt;li&gt;查词表，“计算语言学”在词表中，将W加入到S2中，S2=“计算语言学/ 课程/ 有/ 意思/”，并将W从S1中去掉，此时S1=””；&lt;/li&gt;
&lt;li&gt;S1为空，输出S2作为分词结果，分词过程结束。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;至于怎么实现,&lt;a href="http://yangshangchuan.iteye.com/blog/2031813"&gt;中文分词算法之基于词典的正向最大匹配算法&lt;/a&gt;一文中对针对JAVA的实现有非常详尽的性能分析,其实吧,个人觉得算法无非是在时间和空间间的权衡,对Hash式存储结构而言,一般来讲,空间开销是很大的,而时间上可以做的很好;对于类似于Trie树的数据结构,在某种程度上能节省一定的空间,但肯定比Hash类数据结构慢点。这里我们就不纠结数据结构和性能的差异了,我们使用STL set&lt;sup id="sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-1-back"&gt;&lt;a class="simple-footnote" href="#sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-1" title="STL中set的简单学习"&gt;1&lt;/a&gt;&lt;/sup&gt;实现上述功能。&lt;/p&gt;
&lt;p&gt;以下给出逆向最长匹配算法C++源码(&lt;strong&gt;代码中词典的初始化只用了几个词,实际中可从词表文件中读取并构造一个词典,此处代码只是为了演示算法框架&lt;/strong&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="vi"&gt;#include&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;iostream&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;span class="vi"&gt;#include&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;

&lt;span class="nx"&gt;using&lt;/span&gt; &lt;span class="nx"&gt;namespace&lt;/span&gt; &lt;span class="nx"&gt;std&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="cm"&gt;/**&lt;/span&gt;
&lt;span class="cm"&gt; *A simple inverse match algorithm&lt;/span&gt;
&lt;span class="cm"&gt; *@author:qingyuanxingsi&lt;/span&gt;
&lt;span class="cm"&gt; *@date:2014-05-04&lt;/span&gt;
&lt;span class="cm"&gt; *@version:1.0&lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;
&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="nx"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="nx"&gt;argc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nx"&gt;char&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="nx"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;//Max word length&lt;/span&gt;
    &lt;span class="nx"&gt;const&lt;/span&gt; &lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;max_len&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nx"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;string&lt;/span&gt; &lt;span class="n"&gt;split_sequence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"/"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nx"&gt;const&lt;/span&gt; &lt;span class="kt"&gt;string&lt;/span&gt; &lt;span class="n"&gt;to_split&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;"计算语言学真有意思啊"&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="kt"&gt;string&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="c1"&gt;//Initialize the dict&lt;/span&gt;
    &lt;span class="nx"&gt;dict.insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"计算语言学"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="nx"&gt;dict.insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;"意思"&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="c1"&gt;//Split the Chinese Sequence&lt;/span&gt;
    &lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;tail&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;to_split.length&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="nb"&gt;for&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;int&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nx"&gt;max_len&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
      &lt;span class="kt"&gt;string&lt;/span&gt; &lt;span class="n"&gt;temp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;to_split.substr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="na"&gt;-i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
      &lt;span class="c1"&gt;//If single word&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;temp.length&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
        &lt;span class="nx"&gt;cout&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;split_sequence&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;temp&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="nx"&gt;tail&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;max_len&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;max_len&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;dict.find&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="nx"&gt;dict.end&lt;/span&gt;&lt;span class="p"&gt;()){&lt;/span&gt;
        &lt;span class="nx"&gt;cout&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;split_sequence&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;temp&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;endl&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="nx"&gt;tail&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt;&lt;span class="nx"&gt;i&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="nx"&gt;max_len&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;tail&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;max_len&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;  
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;NOTE:这种机械的分词方法实际上是远远满足不了我们的需要的,对于某些特定的句子不管采用正向最长匹配还是逆向最长匹配都会产生错误切分。比如说&lt;strong&gt;"结婚的和尚未结婚的"&lt;/strong&gt;,采用正向最长匹配就得不到正确的分词结果,逆向最长匹配也类同。类似的分词方法还有&lt;strong&gt;最小词数法&lt;/strong&gt;等。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;基于以上简单的中文分词算法，很多学者进行了改进,我爱自然语言网站上介绍了一个叫MMSEG的系统,个人不是很感兴趣,有兴趣的同学可参考如下链接:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%B3%95%E6%89%A9%E5%B1%951"&gt;中文分词入门之最大匹配法扩展1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%B3%95%E6%89%A9%E5%B1%952"&gt;中文分词入门之最大匹配法扩展2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%85%A5%E9%97%A8%E4%B9%8B%E7%AF%87%E5%A4%96"&gt;中文分词入门之篇外&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;基于字标注的中文分词&lt;sup id="sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-2-back"&gt;&lt;a class="simple-footnote" href="#sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-2" title="本部分更多细节请参考我爱自然语言处理博客!"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;以往的分词方法，无论是基于规则的还是基于统计的，一般都依赖于一个事先编制的词表(词典)。自动分词过程就是通过词表和相关信息来做出词语切分的决策。与此相反，基于字标注的分词方法实际上是构词方法。即把分词过程视为字在字串中的标注问题。由于每个字在构造一个特定的词语时都占据着一个确定的构词位置(即词位)，假如规定每个字最多只有四个构词位置：即B(词首)，M (词中)，E(词尾)和S(单独成词)，那么下面句子(甲)的分词结果就可以直接表示成如(乙)所示的逐字标注形式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(甲)分词结果：／上海／计划／N／本／世纪／末／实现／人均／国内／生产／总值／五千美元／。&lt;/p&gt;
&lt;p&gt;(乙)字标注形式：上/B 海／E 计／B 划／E N／S 本／s 世／B 纪／E 末／S 实／B 现／E 人／B 均／E 国／B 内／E生／B产／E总／B值／E 五／B千／M 美／M 元／E 。／S&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先需要说明，这里说到的“字”不只限于汉字。考虑到中文真实文本中不可避免地会包含一定数量的非汉字字符，本文所说的“字”，也包括外文字母、阿拉伯数字和标点符号等字符。所有这些字符都是构词的基本单元。当然，汉字依然是这个单元集合中数量最多的一类字符。&lt;/p&gt;
&lt;p&gt;把分词过程视为字的标注问题的一个重要优势在于，&lt;strong&gt;它能够平衡地看待词表词和未登录词的识别问题&lt;/strong&gt;。在这种分词技术中，文本中的词表词和未登录词都是用统一的字标注过程来实现的。在学习架构上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词(如人名、地名、机构名)识别模块。这使得分词系统的设计大大简化。在字标注过程中，所有的字根据预定义的特征进行词位特性的学习，获得一个概率模型。然后，在待分字串上，根据字与字之间的结合紧密程度，得到一个词位的标注结果。最后，根据词位定义直接获得最终的分词结果。总而言之，在这样一个分词过程中，分词成为字重组的简单过程。然而这一简单处理带来的分词结果却是令人满意的。&lt;/p&gt;
&lt;p&gt;在&lt;a href="http://www.52nlp.cn/two-innovative-ideas-in-natural-language-processing-area"&gt;《自然语言处理领域的两种创新观念》&lt;/a&gt;中，张俊林博士谈了两种创新模式：&lt;strong&gt;一种创新是研究模式的颠覆，另外一种创新是应用创新&lt;/strong&gt;，前者需要NLP领域出现爱因斯坦式的革新人物，后者则是强调用同样的核心技术做不一样的应用。&lt;/p&gt;
&lt;p&gt;在自然语言处理领域，多数创新都属于后者，譬如统计机器翻译，Brown就是学习和借鉴了贾里尼克将语音识别看成通信问题的思想，将信源信道模型应用到了机器翻译之中，从而开辟了SMT这一全新领域。而Nianwen Xue将词性标注的思想应用到中文分词领域，成就了字标注的中文分词方法（Chinese Word Segmentation as Character Tagging），同样取得了巨大的成功。&lt;/p&gt;
&lt;p&gt;既然基于字标注的中文分词方法是将中文分词当作词性标注的问题来对待，那么就必须有标注对象和标注集了。形象一点，从这个方法的命名上我们就可以推断出它的标注是基本的汉字（还包括一定数量的非汉字字符），而标注集则比较灵活，这些标注集都是依据汉字在汉语词中的位置设计的，最简单的是2-tag，譬如将词首标记设计为B，而将词的其他位置标记设计为I，那么“中国”就可以标记为“中/B 国/I”，“海南岛”则可以标记为“海/B 南/I 岛/I”，相应地，对于如下分好词的句子：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;瓦西里斯&lt;/span&gt; &lt;span class="err"&gt;的&lt;/span&gt; &lt;span class="err"&gt;船只&lt;/span&gt; &lt;span class="err"&gt;中&lt;/span&gt; &lt;span class="err"&gt;有&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="err"&gt;驶&lt;/span&gt; &lt;span class="err"&gt;向&lt;/span&gt; &lt;span class="err"&gt;远东&lt;/span&gt; &lt;span class="err"&gt;，&lt;/span&gt; &lt;span class="err"&gt;每个&lt;/span&gt; &lt;span class="err"&gt;月&lt;/span&gt; &lt;span class="err"&gt;几乎&lt;/span&gt; &lt;span class="err"&gt;都&lt;/span&gt; &lt;span class="err"&gt;有&lt;/span&gt; &lt;span class="err"&gt;两三条&lt;/span&gt; &lt;span class="err"&gt;船&lt;/span&gt; &lt;span class="err"&gt;停靠&lt;/span&gt; &lt;span class="err"&gt;中国&lt;/span&gt; &lt;span class="err"&gt;港口&lt;/span&gt; &lt;span class="err"&gt;。&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;基于2-tag(B,I)的标注就是：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;瓦&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;西&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;里&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;斯&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;的&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;船&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;只&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;中&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;有&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;４&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;０&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;％&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;驶&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;向&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;远&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;东&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;，&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;每&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;个&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;月&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;几&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;乎&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;都&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;有&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;两&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;三&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;条&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;船&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;停&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;靠&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;中&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;国&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;港&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt; &lt;span class="err"&gt;口&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;I&lt;/span&gt; &lt;span class="err"&gt;。&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;B&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;除了2-tag，还有4-tag、6-tag等，都是依据字在词中的位置设计的，本文主要目的是从实践的角度介绍基于字标注的中文分词方法设计，以达到抛砖引玉的作用，因此我们仅选用2-tag（B，I）标注集进行实验说明。有了标注对象和标注集，那么又如何进行中文分词呢？因为字标注本质上是采用POS Tagging的思想,只不过要TAG的基本单元现在变成字了而已,因此我们可以这样做:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;获取已分词语料,将其转化为字的形式并采用某种标注集根据分词信息对其进行标注;&lt;/li&gt;
&lt;li&gt;将得到的语料作为训练集输入到最大熵模型或者HMM模型中进行训练(&lt;strong&gt;可以使用Citar&lt;/strong&gt;);&lt;/li&gt;
&lt;li&gt;利用训练后模型对未分词语料进行字标注,最后还原成分词结果即可。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:利用现有开源工具时,如果能够构建适用于中文字标注的特征集合,然后再进行训练,可能会取得更好的结果。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;鲁棒性NLP系统(观点)&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;一个 real life 自然语言处理系统，其质量和可用度除了传统的 data quality 的衡量指标查准度（precision）和查全度（recall）外，还有更为重要的三大指标：&lt;strong&gt;海量处理能力（scalability）, 深度（depth）和鲁棒性（robustness）&lt;/strong&gt;。本部分就简单谈一下鲁棒性。&lt;/p&gt;
&lt;p&gt;为了取得语言处理的鲁棒性（robustness），一个行之有效的方法是实现四个形容词的所指：&lt;strong&gt;词汇主义（lexicalist）; 自底而上（bottom-up）; 调适性（adaptive）；和数据制导（data-driven）&lt;/strong&gt;。这四条是相互关联的，但各自重点和视角不同。系统设计和开发上贯彻这四项基本原则， 是取得坚固性的良好保证。有了坚固性，系统对于不同领域的语言，甚至对极不规范的社会媒体中的语言现象，都可以应对。这是很多实用系统的必要条件。&lt;/p&gt;
&lt;p&gt;先说&lt;strong&gt;词汇主义策略&lt;/strong&gt;。词汇主义的语言处理策略是学界和业界公认的一个有效的方法。具体说来就是在系统中增加词汇制导的个性规则的总量。自然语言的现象是如此复杂，几乎所有的规则都有例外，词汇制导是必由之路。从坚固性而言，更是如此。基本的事实是，语言现象中的所谓子语言（sublanguage），譬如专业用语，网络用语，青少年用语，他们之间的最大区别是在词汇以及词汇的用法上。一般来说，颗粒度大的普遍语法规则在各子语言中依然有效。因此，采用词汇主义策略，可以有效地解决子语言的分析问题，从而提高系统的鲁棒性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;自底而上的分析方法&lt;/strong&gt;。这种方法对于自浅而深的管式系统最自然。系统从单词出发，一步一步形成越来越大的句法单位，同时解析句法成分之间的关系。其结果是自动识别（构建）出来的句法结构树。很多人都知道社会媒体的混乱性，这些语言充满了错别字和行话，语法错误也随处可见。错别字和行话由词汇主义策略去对付，语法错误则可以借助自底而上的分析方法。其中的道理就是，即便是充满了语法错误的社会媒体语言，其实并不是说这些不规范的语言完全不受语法规则的束缚，无章可循。事实绝不是如此，否则人也不可理解，达不到语言交流的目的。完全没有语法的“语言”可以想象成一个随机发生器，随机抽取字典或词典的条目发射出来，这样的字串与我们见到的最糟糕的社会媒体用语也是截然不同的。事实上，社会媒体类的不规范语言（degraded text）就好比一个躁动不安的逆反期青年嬉皮士，他们在多数时候是守法的，不过情绪不够稳定，不时会”突破”一下规章法律。具体到语句，其对应的情形就是，每句话里面的多数短语或从句是合法的，可是短语（或从句）之间常常会断了链子。这种情形对于自底而上的系统，并不构成大的威胁。因为系统会尽其所能，一步一步组合可以预测（解构）的短语和从句，直到断链的所在。这样一来，一个句子可能形成几个小的句法子树（sub-tree），子树之内的关系是明确的。朋友会问：既然有断链，既然子树没有形成一个完整的句法树来涵盖所分析的语句，就不能说系统真正鲁棒了，自然语言理解就有缺陷。抽象地说，这话不错。但是在实际使用中，问题远远不是想象的那样严重。其道理就是，语言分析并非目标，语言分析只是实现目标的一个手段和基础。对于多数应用型自然语言系统来说，目标是信息抽取（Information Extraction），是这些预先定义的抽取目标在支持应用（app）。抽取模块的屁股通常坐在分析的结构之上，典型的抽取规则 by nature 是基于子树匹配的，这是因为语句可以是繁复的，但是抽取的目标相对单纯，对于与目标不相关的结构，匹配规则无需cover。这样的子树匹配分两种情形，其一是抽取子树（subtree1）的规则完全匹配在语句分析的子树（subtree2）之内（i.e. subtree2 &amp;gt; subtree1），这种匹配不受断链的任何影响，因此最终抽取目标的质量不受损失。只有第二种情形，即抽取子树恰好坐落在分析语句的断链上，抽取不能完成，因而印象了抽取质量。值得强调的是，一般来说，情形2的出现概率远低于情形1，因此自底而上的分析基本保证了语言结构分析的鲁棒性，从而保障了最终目标信息抽取的达成。其实，对于 worst case scenario 的情形2，我们也不是没有办法补救。补救的办法就是在分析的后期把断链 patch 起来，虽然系统无法确知断链的句法关系的性质，但是patched过的断链形成了一个完整的句法树，为抽取模块的补救创造了条件。此话怎讲？具体说来就是，只要系统的设计和开发者坚持&lt;strong&gt;调适性开发&lt;/strong&gt;抽取模块（adaptive extraction）的原则，部分抽取子树的规则完全可以建立在被patched的断链之上，从而在不规范的语句中达成抽取。其中的奥妙就是某样榜戏中所说的墙内损失墙外补，用到这里就是结构不足词汇补。展开来说就是，任何子树匹配不外乎check两种条件约束，一是节点之间的关系句法关系的条件（主谓，动宾，等等），另外就是节点本身的词汇条件（产品，组织，人，动物，等等）。这些抽取条件可以相互补充，句法关系的条件限制紧了，节点词汇的条件就可以放宽；反之亦然。即便对于完全合法规范的语句，由于语言分析器不可避免的缺陷而可能导致的断链（世界上除了上帝以外不存在完美的系统），以及词汇语义的模糊性，开发者为了兼顾查准率和查全率，也会在抽取子树的规则上有意平衡节点词汇的条件和句法关系的条件。如果预知系统要用于不规范的语言现象上，那么我们完全可以特制一些规则，利用强化词汇节点的条件来放宽对于节点句法关系的条件约束。其结果就是适调了patched的断链，依然达成抽取。说了一箩筐，总而言之，言而总之，对于语法不规范的语言现象，自底而上的分析策略是非常有效的，加上调适性开发，可以保证最终的抽取目标基本不受影响。&lt;/p&gt;
&lt;p&gt;调适性上面已经提到，作为一个管式系统的开发原则，这一条很重要，它是克服错误放大（error propagation）的反制。理想化的系统，模块之间的接口是单纯明确的，铁路警察，各管一段，步步推进，天衣无缝。但是实际的系统，特别是自然语言系统，情况很不一样，良莠不齐，正误夹杂，后面的模块必须设计到有足够的容错能力，针对可能的偏差做调适才不至于一错再错，步步惊心。如果错误是consistent/predictable 的，后面的模块可以矫枉过正，以毒攻毒，错错为正。还有一点就是歧义的保存（keeping ambiguity untouched）策略。很多时候，前面的模块往往条件不成熟，这时候尽可能保持歧义，运用系统内部的调适性开发在后面的模块处理歧义，往往是有效的。&lt;/p&gt;
&lt;p&gt;最后，&lt;strong&gt;数据制导&lt;/strong&gt;的开发原则，怎样强调都不过分。语言海洋无边无涯，多数语言学家好像一个爱玩水的孩子，跳进海洋往往坐井观天，乐不思蜀。见树木不见森林，一条路走到黑，是很多语言学家的天生缺陷。如果由着他们的性子来，系统的overhead越来越大，效果可能越来越小。数据制导是迫使语言学家回到现实，开发真正有现实和统计意义的系统的一个保证。这样的保证应该制度化，这牵涉到开发语料库（dev corpus）的选取，baseline 的建立和维护，unit testing 和regression testing 等开发操作规范的制定以及 data quality QA 的配合。理想的数据制导还应该包括引入机器学习的方法，来筛选制约具有统计意义的语言现象反馈给语言学家。从稍微长远一点看，自动分类用户的数据反馈，实现某种程度的粗颗粒度的自学习，建立半自动人际交互式开发环境，这是手工开发和机器学习以长补短的很有意义的思路。以上所述，每一条都是经验的总结，背后有成百上千的实例可以详加解说。不过，网文也不是科普投稿，没时间去细细具体解说了。做过的自然有同感和呼应，没做过的也许不明白，等做几年就自然明白了，又不是高精尖的火箭技术。&lt;/p&gt;
&lt;h1&gt;无约束最优化&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;看了一下我爱自然语言处理博客上关于无约束优化的几篇文章,可能是自己水平很烂的原因,感觉怪怪的,好像有点不对劲,还是自己查相关资料吧。以下给出那几篇的链接.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/unconstrained-optimization-one"&gt;无约束最优化一&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/unconstrained-optimization-two"&gt;无约束最优化二&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/unconstrained-optimization-three"&gt;无约束最优化三&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/unconstrained-optimization-four"&gt;无约束最优化四&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/unconstrained-optimization-five"&gt;无约束最优化五&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;资源集锦&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://clair.eecs.umich.edu/aan/index.php"&gt;ACL Anthology Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.ldc.upenn.edu/"&gt;LDC (Linguistic Data Consortium)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://web-ngram.research.microsoft.com/info/quickstart.htm"&gt;Microsoft N-gram Service&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://start.csail.mit.edu/index.php"&gt;Start Question-Answering System&lt;/a&gt;;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Start_China" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/start_china_zps584efc72.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://wing.comp.nus.edu.sg:8080/SMSCorpus/"&gt;Collecting SMS Messages for a Public Research Corpus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.statmt.org/moses/"&gt;Moses|统计机器翻译&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;TODO Board:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;TBL&lt;/em&gt;(&lt;strong&gt;参考《自然语言处理综论》第8章&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最大熵求解算法IIS等&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.52nlp.cn/%E5%88%9D%E5%AD%A6%E8%80%85%E6%8A%A5%E9%81%93%EF%BC%882%EF%BC%89%EF%BC%9A%E5%AE%9E%E7%8E%B0-1-gram%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95"&gt;1-Gram Python分词实现&lt;/a&gt;;之后自己实现一个3-gram Language Model吧!&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/"&gt;Topic modeling made just simple enough&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-1"&gt;&lt;a href="http://www.cppblog.com/shongbee2/archive/2009/04/05/79011.html"&gt;STL中set的简单学习&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-2"&gt;本部分更多细节请参考我爱自然语言处理博客! &lt;a class="simple-footnote-back" href="#sf-zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="NLP"></category></entry><entry><title>当最近邻遇到LSH</title><link href="http://www.qingyuanxingsi.com/dang-zui-jin-lin-yu-dao-lsh.html" rel="alternate"></link><updated>2014-05-01T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-01:dang-zui-jin-lin-yu-dao-lsh.html</id><summary type="html">&lt;p&gt;貌似感冒了,脑子昏昏沉沉的,啥都想不了,无意中发现这么一个高端大气上档次的算法---局部敏感哈希方法。于是Google了一下,发现这篇&lt;a href="http://www.strongczq.com/2012/04/locality-sensitive-hashinglsh%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%8A%95%E5%BD%B1%E6%B3%95.html"&gt;Locality Sensitive Hashing(LSH)之随机投影法&lt;/a&gt;关于局部敏感哈希算法的介绍还不错,于是摘录如下:&lt;/p&gt;
&lt;h1&gt;概述&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;LSH&lt;/strong&gt;是由文献[1]提出的一种用于高效求解最近邻搜索问题的Hash算法。LSH算法的基本思想是利用一个hash函数把集合中的元素映射成hash值，使得相似度越高的元素hash值相等的概率也越高。LSH算法使用的关键是针对某一种相似度计算方法，找到一个具有以上描述特性的hash函数。LSH所要求的hash函数的准确数学定义比较复杂，以下给出一种通俗的定义方式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于集合$S$，集合内元素间相似度的计算公式为$sim(a,b)$。如果存在一个hash函数$h()$满足以下条件：存在一个相似度$s$到概率$p$的单调递增映射关系，使得$S$中的任意两个满足$sim(a,b)\geq s$的元素$a$和$b$，$h(a)=h(b)$的概率大于等于$p$。那么$h()$就是该集合的一个LSH算法hash函数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一般来说在最近邻搜索中，元素间的关系可以用相似度或者距离来衡量。如果用距离来衡量，那么距离一般与相似度之间存在单调递减的关系。以上描述如果使用距离来替代相似度需要在单调关系上做适当修改。&lt;/p&gt;
&lt;p&gt;根据元素相似度计算方式的不同，LSH有许多不同的hash算法。两种比较常见的hash算法是&lt;strong&gt;随机投影法&lt;/strong&gt;和min-hash算法。本文即将介绍的随机投影法适用于集合元素可以表示成向量的形式，并且相似度计算是基于向量之间夹角的应用场景，如余弦相似度。min-hash法在参考文献[2]中有相关介绍。&lt;/p&gt;
&lt;h1&gt;随机投影法(Random projection)&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;假设集合$S$中的每个元素都是一个$n$维的向量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\vec{x} ={v_1,v_2,\cdots,v_n}
\end{equation}&lt;/p&gt;
&lt;p&gt;集合中两个元素$\vec{v}$和$\vec{u}$之间的相似度定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
sim(\vec{v},\vec{u})=\frac{\vec{v}*\vec{u}}{|\vec{v}||\vec{u}|}
\end{equation}&lt;/p&gt;
&lt;p&gt;对于以上元素集合$S$的随机投影法hash函数$h()$可以定义为如下：&lt;/p&gt;
&lt;p&gt;在$n$维空间中随机选取一个非零向量$\vec{x}={x_1, x_2, \ldots, x_n}$。考虑以该向量为法向量且经过坐标系原点的超平面，该超平面把整个$n$维空间分成了两部分，将法向量所在的空间称为正空间，另一空间为负空间。那么集合$S$中位于正空间的向量元素hash值为1，位于负空间的向量元素hash值为0。判断向量属于哪部分空间的一种简单办法是判断向量与法向量之间的夹角为锐角还是钝角，因此具体的定义公式可以写为&lt;/p&gt;
&lt;p&gt;&lt;img alt="Formula 1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/formula_2_zpsd4647f5a.png" /&gt;&lt;/p&gt;
&lt;p&gt;根据以上定义，假设向量$\vec{v}$和$\vec{u}$之间的夹角为$\theta$，由于法向量$\vec{x}$是随机选取的，那么这两个向量未被该超平面分割到两侧（即hash值相等）的概率应该为：$p(\theta)=1-\frac{\theta}{\pi}$。假设两个向量的相似度值为$s$，那么根据$\theta=arccos(s)$,有&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(s)=1-\frac{arccos(s)}{\pi}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此，存在相似度$s$到概率$p$的单调递增映射关系，使得对于任意相似度大于等于$s$的两个元素，它们hash值相等的概率大于等于$p(s)$。所以，以上定义的hash值计算方法符合LSH算法的要求。&lt;/p&gt;
&lt;p&gt;以上所描述的$h()$函数虽然符合LSH算法的要求，但是实用性不高。因为该hash函数只产生了两个hash值，没有达到hash函数将元素分散到多个分组的目的。为了增加不同hash值的个数，可以多次生成独立的函数$h()$，只有当两个元素的多个$h()$值都相等时才算拥有相同的hash值。根据该思路可以定义如下的hash函数$H()$：&lt;/p&gt;
&lt;p&gt;\begin{equation}
H(\vec{v})=(h_b(\vec{v})h_{b-1}(\vec{v})\ldots h_1(\vec{v}))_2
\end{equation}&lt;/p&gt;
&lt;p&gt;其中每个$h_i(\vec{v})$表示一个独立的$h()$函数，$H()$函数值的二进制表现形式中每一位都是一个$h()$函数的结果。
以$H()$为hash函数的话，两个相似度为$s$的元素具有相同hash值的概率公式为&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(s)=(1-\frac{arccos(s)}{\pi})^b
\end{equation}&lt;/p&gt;
&lt;p&gt;hash值的个数为$2^b$。很容易看出$H()$函数同样也是符合LSH算法要求的。一般随机按投影算法选用的hash函数就是$H()$。其中参数$b$的取值会在后面小节中讨论。&lt;/p&gt;
&lt;h1&gt;随机投影法在最近邻搜索中的应用&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;最近邻搜索&lt;/h2&gt;
&lt;p&gt;最近邻搜索可以简单的定义为：对于$m$个元素的集合$T$，为一个待查询元素$q$找到集合中相似度最高的$k$个元素。&lt;/p&gt;
&lt;p&gt;最近邻搜索最简单的实现方法为：计算$q$与集合$T$中每一个元素的相似度，使用一个具有$k$个元素的大顶堆（优先队列）保存相似度计算结果（相似度值为key）。这种实现方法每一次查询都要遍历整个集合$T$来计算相似度，当$m$很大并且查询的频率很高的时候这种暴力搜索的方法无法满足性能要求。&lt;/p&gt;
&lt;p&gt;当最近邻搜索的近邻要求并不是那么严格的时候，即允许top k近邻的召回率不一定为1（但是越高越好），那么可以考虑借助于LSH算法。&lt;/p&gt;
&lt;h2&gt;随机投影法提高执行速度&lt;/h2&gt;
&lt;p&gt;这里我们介绍当集合$T$的元素和查询元素$q$为同维度向量(维度为$n$)，并且元素相似度计算方法为余弦相似度时，使用随机投影法来提高最近邻搜索的执行速度。具体的实现方法为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;预处理阶段&lt;/strong&gt;:使用hash函数$H(*)$计算集合$T$中所有元素的hash值，将集合$T$分成一个个分组，每个分组内的元素hash值均相等。用合适的数据结构保存这些hash值到分组的映射关系（如HashMap）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;查询阶段&lt;/strong&gt;:计算查询元素$q$的hash值$H(q)$，取集合$T$中所有hash值为$H(q)$的分组，以该分组内的所有元素作为候选集合，在候选该集合内使用简单的最近邻搜索方法寻找最相似的$k$个元素。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该方法的执行效率取决于$H(*)$的hash值个数$2^b$，也就是分组的个数。理想情况下，如果集合$T$中的向量元素在空间中分布的足够均匀，那么每一个hash值对应的元素集合大小大致为$\frac{m} {2^b}$。当$m$远大于向量元素的维度时，每次查询的速度可以提高到$2^b$倍。&lt;/p&gt;
&lt;p&gt;根据以上分析$H()$中$b$的取值越大算法的执行速度的提升越多，并且是指数级别的提升。但是，在这种情况下$H()$函数下的概率公式$p(s)$，&lt;strong&gt;实际上表示与查询元素$q$的相似度为$s$的元素的召回率&lt;/strong&gt;。当$b$的取值越大时，top k元素的召回率必然会下降。因此算法执行速度的提升需要召回率的下降作为代价。例如：当$b$等于10时，如果要保证某个元素的召回率不小于0.9，那么该元素与查询元素$q$的相似度必须不小于0.9999998。&lt;/p&gt;
&lt;h2&gt;提高召回率改进&lt;/h2&gt;
&lt;p&gt;为了在保证召回率的前提下尽可能提高算法的执行效率，一般可以进行如下改进：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;预处理阶段&lt;/strong&gt;:生成$t$个独立的hash函数$H_i(∗)$，根据这$t$个不同的hash函数，对集合$T$进行$t$种不同的分组，每一种分组方式下，同一个分组的元素在对应hash函数下具有相同的hash值。用合适的数据结构保存这些映射关系（如使用$t$个HashMap来保存）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;查询阶段&lt;/strong&gt;:对于每一个hash函数$H_i(∗)$，计算查询元素$q$的hash值$H_i(q)$，将集合$T$中$H_i(∗)$所对应的分组方式下hash值为$H_i(q)$的分组添加到该次查询的候选集合中。然后，在该候选集合内使用简单的最近邻搜索方法寻找最相似的$k$个元素。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上改进使得集合中元素与查询元素$q$的$t$个hash值中，只要任意一个相等，那么该集合元素就会被加入到候选集中。那么，相似度为$s$的元素的召回率为&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(s)=1-(1-(1-\frac{arccos(s)}{\pi})^b)^t
\end{equation}&lt;/p&gt;
&lt;p&gt;在执行效率上，预处理阶段由于需要计算$t$个hash函数的值，所以执行时间上升为$t$倍。查询阶段，如果单纯考虑候选集合大小对执行效率的影响，在最坏的情况下，$t$个hash值获得的列表均不相同，候选集集合大小的期望值为$\frac{t∗m}{2^b}$，查询速度下降至$1 \over t$，与简单近邻搜索相比查询速度提升为$\frac{2^b}{t}$倍。&lt;/p&gt;
&lt;p&gt;下图是召回率公式$p(s)=1-(1-(1-\frac{arccos(s)}{\pi})^b)^t$在不同的$b$和$t$取值下的$s-p$曲线。我们通过这些曲线来分析这里引入参数$t$的意义。4条蓝色的线以及最右边红色的线表示当$t$取值为1（相当于没有引入$t$），而$b$的取值从1变化到5的过程，从图中可以看出随着$b$的增大，不同相似度下的召回率都下降的非常厉害，特别的，当相似度接近1时曲线的斜率很大，也就说在高相似度的区域，召回率对相似度的变化非常敏感。10条红色的线从右到左表示$b$的取值为5不变，$t$的取值从1到10的过程，从图中可以看出，随着$t$的增大，曲线的形状发生了变化，高相似度区域的召回率变得下降的非常平缓，而最陡峭的地方渐渐的被移动到相对较低的相似度区域。因此，从以上曲线的变化特点可以看出，引入适当的参数$t$使得高相似度区域在一段较大的范围内仍然能够保持很高的召回率从而满足实际应用的需求。&lt;/p&gt;
&lt;p&gt;&lt;img alt="SP Curve" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sp_curve_zpse1b5dbe6.png" /&gt;&lt;/p&gt;
&lt;h2&gt;参数选取&lt;/h2&gt;
&lt;p&gt;根据以上分析，$H(*)$函数的参数$b$越大查询效率越高，但是召回率越低；参数$t$越大查询效率越低但是召回率越高。因此选择适当参数$b$和$t$来折中查询效率与召回率之间的矛盾是应用好随机投影法的关键。下面提供一种在实际应用中选取$b$和$t$的参考方法。&lt;/p&gt;
&lt;p&gt;根据实际应用的需要确定一对$(s,p)$，表示相似度大于等于$s$的元素，召回率的最低要求为$p$。然后将召回率公式表示成$b-t$之间的函数关系$t=\log_{1-(1-\frac{acos(s)}{pi})^b}{(1-p)}$。根据$(s,p)$的取值，画出$b-t$的关系曲线。如$s=0.8,p=0.95$时的$b-t$曲线如下图所示。考虑具体应用中的实际情况，在该曲线上选取一组使得执行效率可以达到最优的$(b,t)$组合。&lt;/p&gt;
&lt;p&gt;&lt;img alt="BT_Curve" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bt_curve_zps5aba4948.png" /&gt;&lt;/p&gt;
&lt;h2&gt;关于最近邻文本搜索&lt;/h2&gt;
&lt;p&gt;在最近邻文本搜索中，一般待检索的文本或查询文本，都已被解析成一系列带有权重的关键词，然后通过余弦相似度公式计算两个文本之间的相似度。这种应用场景下的最近邻搜索与以上所提到的最近邻搜索问题相比存在以下两个特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果把每个文本的带权重关键词表都看作是一个向量元素的话，每个关键词都是向量的一个维度，关键词权重为该维度的值。理论上可能关键词的个数并不确定（所有单词的组合都可能是一个关键词），因此该向量元素的维数实际上是不确定的。&lt;/li&gt;
&lt;li&gt;由于关键词权重肯定是大于零的，所以向量元素的每一个维度的值都是非负的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于第一个特点，我们需要选取一个包含$n$个关键词的关键词集合，在进行文本相似度计算时只考虑属于该集合的关键词。也就是说，每一个文本都视为是一个$n$维度的向量，关键词权重体现为对应维度的值。该关键词集合可以有很多种生成办法，比如可以是网站上具有一定搜索频率的关键词集合，总的来说该关键词集合应当能够涵盖所有有意义并且具有一定使用频率的关键词。通常$n$的取值会比较大，如几十万到几百万，由于在使用随机投影算法时，每一个生成的随机向量维度都为$n$，这种情况下需要特别考虑利用这些高维随机向量对执行效率造成的影响，在确定$b、t$参数时需要考虑到这方面的影响。&lt;/p&gt;
&lt;p&gt;对于第二个特点，由于向量元素各维度值都非负，那么这些元素在高维空间中只会出现在特定的区域中。比如当$n$为3时，只会出现在第一象限中。一个直观的感觉是在生成随机向量的时候，会不会生成大量的无用切割平面（与第一个象限空间不相交，使得所有元素都位于切割平面的同侧）。这些切割平面对应的$H(*)$函数hash值中的二进制位恒定为1或者0，对于提高算法执行速度没有帮助。以下说明这种担心是没有必要的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;切割平面与第一象限空间不相交等价于其法向量的每一个维度值都有相同的符号（都为正或者负），否则总能在第一象限空间中找到两个向量与法向量的乘积符号不同，也就是在切割平面的两侧。那么，随机生成的n维向量所有维度值都同号的概率为$\frac{1}{2^{n−1}}$，当$n$的取值很大时，该概率可以忽略不计。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;[1] P. Indyk and R. Motwani. Approximate Nearest Neighbor:Towards Removing the Curse of Dimensionality. In Proc. of the 30th Annual ACM Symposium on Theory of Computing, 1998, pp. 604–613.&lt;/p&gt;
&lt;p&gt;[2] Google News Personalization: Scalable Online Collaborative Filtering.&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="算法"></category><category term="Algorithm"></category><category term="LSH"></category><category term="局部敏感哈希算法"></category></entry><entry><title>Play With Cardinality Estimation</title><link href="http://www.qingyuanxingsi.com/play-with-cardinality-estimation.html" rel="alternate"></link><updated>2014-04-14T00:00:00+08:00</updated><author><name>CodingLabs</name></author><id>tag:www.qingyuanxingsi.com,2014-04-14:play-with-cardinality-estimation.html</id><summary type="html">&lt;p&gt;记得之前某周例会的时候一个博士师兄抛出一个小问题:在大数据环境下,如何估计一个可能含有重复元素的集合中不同元素的数目,当时其实没有怎么在意。这两天因为看CNN的东西实在无法完全理解,所以到处逛了逛(&lt;code&gt;好吧,我每次逛了逛都能发现特别好玩的算法呀&lt;/code&gt;),于是不经意间发现了解决上述问题的一些现有算法,很是高兴呀。&lt;/p&gt;
&lt;p&gt;在开始今天的相关介绍之前,咱们扯点闲话吧,个人不是特别喜欢纯科研的科研,如果一个算法或者一个数据结构以至于一个理论不能应用到实际生活中去,不能解决实际生活中的某个问题的话,个人认为这种理论或者算法/数据结构的研究就是无意义的。个人还是比较倾向于好玩的科研吧,一方面研究的东西自己觉得有意思,另一方面又能应用到实际项目或生活实际中去,成为一个研究好玩问题的研究人员估计就是我毕生最大的志向了吧,呵呵。好吧,其实说这么多只是为了说明基数估计这个东西真的很好玩呀。(&lt;strong&gt;以后只要在Pearls目录下的博文均收集自他人博客,原始链接见脚注,版权属于原作者所有,无意侵犯,特此说明,以后不再说明&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;言归正传,开始我们正式的介绍。&lt;/p&gt;
&lt;h1&gt;基本概念&lt;sup id="sf-play-with-cardinality-estimation-1-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-1" title="解读Cardinality Estimation算法（第一部分：基本概念）"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;基数计数(&lt;strong&gt;Cardinality Counting&lt;/strong&gt;）是实际应用中一种常见的计算场景，在数据分析、网络监控及数据库优化等领域都有相关需求。精确的基数计数算法由于种种原因，在面对大数据场景时往往力不从心，因此如何在误差可控的情况下对基数进行估计就显得十分重要。目前常见的基数估计算法有Linear Counting、LogLog Counting、HyperLogLog Counting及Adaptive Counting等。这几种算法都是基于概率统计理论所设计的概率算法，它们克服了精确基数计数算法的诸多弊端（如内存需求过大或难以合并等），同时可以通过一定手段将误差控制在所要求的范围内。&lt;/p&gt;
&lt;p&gt;以下我们主要介绍一下基数估计(Cardinality Estimation)的基本概念。&lt;/p&gt;
&lt;h2&gt;基数的定义&lt;/h2&gt;
&lt;p&gt;简单来说，基数（Cardinality，也译作势），是指一个集合（这里的集合允许存在重复元素，与集合论对集合严格的定义略有不同，如不做特殊说明，本文中提到的集合均允许存在重复元素）中不同元素的个数。例如看下面的集合：&lt;/p&gt;
&lt;p&gt;${1,2,3,4,5,2,3,9,7}$&lt;/p&gt;
&lt;p&gt;这个集合有9个元素，但是2和3各出现了两次，因此不重复的元素为1,2,3,4,5,9,7，所以这个集合的基数是7。&lt;/p&gt;
&lt;p&gt;如果两个集合具有相同的基数，我们说这两个集合等势。基数和等势的概念在有限集范畴内比较直观，但是如果扩展到无限集则会比较复杂，一个无限集可能会与其真子集等势（例如整数集和偶数集是等势的）。不过在这个系列文章中，我们仅讨论有限集的情况，关于无限集合基数的讨论，有兴趣的同学可以参考实变分析相关内容。&lt;/p&gt;
&lt;p&gt;容易证明，如果一个集合是有限集，则其基数是一个自然数。&lt;/p&gt;
&lt;h2&gt;基数的应用实例&lt;/h2&gt;
&lt;p&gt;下面通过一个实例说明基数在电商数据分析中的应用。&lt;/p&gt;
&lt;p&gt;假设一个淘宝网店在其店铺首页放置了10个宝贝链接，分别从Item01到Item10为这十个链接编号。店主希望可以在一天中随时查看从今天零点开始到目前这十个宝贝链接分别被多少个独立访客点击过。所谓独立访客（&lt;code&gt;Unique Visitor，简称UV&lt;/code&gt;）是指有多少个自然人，例如，即使我今天点了五次Item01，我对Item01的UV贡献也是1，而不是5。&lt;/p&gt;
&lt;p&gt;用术语说这实际是一个实时数据流统计分析问题。&lt;/p&gt;
&lt;p&gt;要实现这个统计需求。需要做到如下三点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对独立访客做标识&lt;/li&gt;
&lt;li&gt;在访客点击链接时记录下链接编号及访客标记&lt;/li&gt;
&lt;li&gt;对每一个要统计的链接维护一个数据结构和一个当前UV值，当某个链接发生一次点击时，能迅速定位此用户在今天是否已经点过此链接，如果没有则此链接的UV增加1&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面分别介绍三个步骤的实现方案&lt;/p&gt;
&lt;h3&gt;对独立访客做标识&lt;/h3&gt;
&lt;p&gt;客观来说，目前还没有能在互联网上准确对一个自然人进行标识的方法，通常采用的是近似方案。例如通过登录用户+cookie跟踪的方式：当某个用户已经登录，则采用会员ID标识；对于未登录用户，则采用跟踪cookie的方式进行标识。为了简单起见，我们假设完全采用跟踪cookie的方式对独立访客进行标识。&lt;/p&gt;
&lt;h3&gt;记录链接编号及访客标记&lt;/h3&gt;
&lt;p&gt;这一步可以通过Javascript埋点及记录accesslog完成，具体原理和实现方案可以参考博文&lt;a href="http://blog.codinglabs.org/articles/how-web-analytics-data-collection-system-work.html"&gt;网站统计中的数据收集原理及实现&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;实时UV计算&lt;/h3&gt;
&lt;p&gt;可以看到，如果将每个链接被点击的日志中访客标识字段看成一个集合，那么此链接当前的UV也就是这个集合的基数，因此UV计算本质上就是一个基数计数问题。&lt;/p&gt;
&lt;p&gt;在实时计算流中，我们可以认为任何一次链接点击均触发如下逻辑（伪代码描述）：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;cand_counting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item_no&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_id&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;item_no&lt;/span&gt; &lt;span class="n"&gt;visitor&lt;/span&gt; &lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;add&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;item_no&lt;/span&gt; &lt;span class="n"&gt;visitor&lt;/span&gt; &lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;cand&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;item_no&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;逻辑非常简单，每当有一个点击事件发生，就去相应的链接被访集合中寻找此访客是否已经在里面，如果没有则将此用户标识加入集合，并将此链接的UV加1。&lt;/p&gt;
&lt;p&gt;虽然逻辑非常简单，但是在实际实现中尤其面临大数据场景时还是会遇到诸多困难，下面一节我会介绍两种目前被业界普遍使用的精确算法实现方案，并通过分析说明当数据量增大时它们面临的问题。&lt;/p&gt;
&lt;h2&gt;传统的基数计数实现&lt;/h2&gt;
&lt;p&gt;接着上面的例子，我们看一下目前常用的基数计数的实现方法。&lt;/p&gt;
&lt;h3&gt;基于B树的基数计数&lt;/h3&gt;
&lt;p&gt;对上面的伪代码做一个简单分析，会发现关键操作有两个：查找-迅速定位当前访客是否已经在集合中，插入-将新的访客标识插入到访客集合中。因此，需要为每一个需要统计UV的点（此处就是十个宝贝链接）维护一个查找效率较高的数据结构，又因为实时数据流的关系，这个数据结构需要尽量在内存中维护，因此这个数据结构在空间复杂度上也要比较适中。综合考虑一种传统的做法是在实时计算引擎采用了B树来组织这个集合。下图是一个示意图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="B Tree" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/B_tree_zpsda8ce41d.png"&gt;&lt;/p&gt;
&lt;p&gt;之所以选用B树是因为B树的查找和插入相关高效，同时空间复杂度也可以接受（关于B树具体的性能分析请参考&lt;a href="http://en.wikipedia.org/wiki/B-tree"&gt;B_Tree&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;这种实现方案为一个基数计数器维护一棵B树，由于B树在查找效率、插入效率和内存使用之间非常平衡，所以算是一种可以接受的解决方案。但是当数据量特别巨大时，例如要同时统计几万个链接的UV，如果要将几万个链接一天的访问记录全部维护在内存中，这个内存使用量也是相当可观的（假设每个B树占用1M内存，10万个B树就是100G！）。一种方案是在某个时间点将内存数据结构写入磁盘（双十一和双十二大促时一淘数据部的效果平台是每分钟将数据写入HBase）然后将内存中的计数器和数据结构清零，但是B树并不能高效的进行合并，这就使得内存数据落地成了非常大的难题。&lt;/p&gt;
&lt;p&gt;另一个需要数据结构合并的场景是查看并集的基数，例如在上面的例子中，如果我想查看Item1和Item2的总UV，是没有办法通过这种B树的结构快速得到的。当然可以为每一种可能的组合维护一棵B树。不过通过简单的分析就可以知道这个方案基本不可行。N个元素集合的非空幂集数量为2N−1，因此要为10个链接维护1023棵B树，而随着链接的增加这个数量会以幂指级别增长。&lt;/p&gt;
&lt;h3&gt;基于Bitmap的基数计数&lt;/h3&gt;
&lt;p&gt;为了克服B树不能高效合并的问题，一种替代方案是使用Bitmap表示集合。也就是使用一个很长的bit数组表示集合，将bit位顺序编号，bit为1表示此编号在集合中，为0表示不在集合中。例如“00100110”表示集合 {2，5，6}。bitmap中1的数量就是这个集合的基数。&lt;/p&gt;
&lt;p&gt;显然，与B树不同Bitmap可以高效的进行合并，只需进行按位或（or）运算就可以，而位运算在计算机中的运算效率是很高的。但是Bitmap方式也有自己的问题，就是内存使用问题。&lt;/p&gt;
&lt;p&gt;很容易发现，Bitmap的长度与集合中元素个数无关，而是与基数的上限有关。例如在上面的例子中，假如要计算上限为1亿的基数，则需要12.5M字节的Bitmap，十个链接就需要125M。关键在于，这个内存使用与集合元素数量无关，即使一个链接仅仅有一个1UV，也要为其分配12.5M字节。&lt;/p&gt;
&lt;p&gt;由此可见，虽然Bitmap方式易于合并，却由于内存使用问题而无法广泛用于大数据场景。&lt;/p&gt;
&lt;h1&gt;Linear Counting&lt;sup id="sf-play-with-cardinality-estimation-2-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-2" title="解读Cardinality Estimation算法（第二部分：Linear Counting）"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;通过上面的介绍我们知道传统的精确基数计数算法在数据量大时会存在一定瓶颈，瓶颈主要来自于&lt;strong&gt;数据结构合并和内存使用&lt;/strong&gt;两个方面。因此出现了很多基数估计的概率算法，这些算法虽然计算出的结果不是精确的，但误差可控，重要的是这些算法所使用的数据结构易于合并，同时比传统方法大大节省内存。&lt;/p&gt;
&lt;p&gt;作为本文的第二部分，我们讨论Linear Counting算法。&lt;/p&gt;
&lt;h2&gt;简介&lt;/h2&gt;
&lt;p&gt;Linear Counting（以下简称LC）在1990年的一篇论文“A linear-time probabilistic counting algorithm for database applications”中被提出。作为一个早期的基数估计算法，LC在空间复杂度方面并不算优秀，实际上LC的空间复杂度与上文中简单Bitmap方法是一样的（但是有个常数项级别的降低），都是$O(N_{max})$，因此目前很少单独使用LC。不过作为Adaptive Counting等算法的基础，研究一下LC还是比较有价值的。&lt;/p&gt;
&lt;h2&gt;基本算法&lt;/h2&gt;
&lt;h3&gt;思路&lt;/h3&gt;
&lt;p&gt;LC的基本思路是：设有一哈希函数$H$，其哈希结果空间有$m$个值（最小值$0$，最大值$m-1$），并且哈希结果服从均匀分布。使用一个长度为$m$的Bitmap，每个bit为一个桶，均初始化为0，设一个集合的基数为$n$，此集合所有元素通过$H$哈希到Bitmap中，如果某一个元素被哈希到第$k$个比特并且第$k$个比特为$0$，则将其置为$1$。当集合所有元素哈希完成后，设Bitmap中还有$u$个bit为$0$。则：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=−mlog_u m
\end{equation}&lt;/p&gt;
&lt;p&gt;为$n$的一个估计，且为最大似然估计（MLE）。&lt;/p&gt;
&lt;p&gt;示意图如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="LC Hash" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hash_lc_zpsad47853b.png"&gt;&lt;/p&gt;
&lt;h3&gt;推导及证明&lt;/h3&gt;
&lt;p&gt;由上文对$H$的定义已知$n$个不同元素的哈希值服从独立均匀分布。设$A_j$为事件“经过$n$个不同元素哈希后，第$j$个桶值为0”，则：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(A_j)=(1−{1 \over m})n
\end{equation}&lt;/p&gt;
&lt;p&gt;又每个桶是独立的，则$u$的期望为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(u)=\sum_{j=1}^mP(A_j)=m(1-\frac{1}{m})^n=m((1+\frac{1}{-m})^{-m})^{-n/m}
\end{equation}&lt;/p&gt;
&lt;p&gt;当$n$和$m$趋于无穷大时，其值约为$me^{-{n \over m}}$&lt;/p&gt;
&lt;p&gt;令：&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(u)=me^{-n/m}
\end{equation}&lt;/p&gt;
&lt;p&gt;得：&lt;/p&gt;
&lt;p&gt;\begin{equation}
n=−mlog \frac{E(u)}{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;显然每个桶的值服从参数相同0-1分布，因此$u$服从二项分布。由概率论知识可知，当$n$很大时，可以用正态分布逼近二项分布，因此可以认为当$n$和$m$趋于无穷大时$u$渐进服从正态分布。&lt;/p&gt;
&lt;p&gt;因此$u$的概率密度函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f(x) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}}
\end{equation}&lt;/p&gt;
&lt;p&gt;由于我们观察到的空桶数$u$是从正态分布中随机抽取的一个样本，因此它就是$\mu$的最大似然估计（正态分布的期望的最大似然估计是样本均值）。&lt;/p&gt;
&lt;p&gt;又由如下定理：&lt;/p&gt;
&lt;p&gt;设$f(x)$是可逆函数,$\hat{x}$是$x$的最大似然估计，则$f(\hat{x})$是$f(x)$的最大似然估计。
且$-mlog\frac{x}{m}$是可逆函数，则$\hat{n}=-mlog\frac{u}{m}$是$-mlog\frac{E(u)}{m}=n$的最大似然估计。&lt;/p&gt;
&lt;h2&gt;偏差分析&lt;/h2&gt;
&lt;p&gt;下面不加证明给出如下结论：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
Bias(\frac{\hat{n}}{n}) &amp;amp;=E(\frac{\hat{n}}{n})-1=\frac{e^t-t-1}{2n} \\
StdError(\frac{\hat{n}}{n}) &amp;amp;=\frac{\sqrt{m}(e^t-t-1)^{1/2}}{n}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$t=n/m$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:以上结论的推导在“A linear-time probabilistic counting algorithm for database applications”可以找到。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;算法应用&lt;/h2&gt;
&lt;p&gt;在应用LC算法时，主要需要考虑的是Bitmap长度$m$的选择。这个选择主要受两个因素的影响：基数$n$的量级以及容许的误差。这里假设估计基数$n$的量级大约为$N$，允许的误差为$\epsilon$，则$m$的选择需要遵循如下约束。&lt;/p&gt;
&lt;h3&gt;误差控制&lt;/h3&gt;
&lt;p&gt;这里以标准差作为误差。由上面标准差公式可以推出，当基数的量级为$N$，容许误差为$\epsilon$时，有如下限制：&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; \frac{e^t-t-1}{(\epsilon t)^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;将量级和容许误差带入上式，就可以得出$m$的最小值。&lt;/p&gt;
&lt;h3&gt;满桶控制&lt;/h3&gt;
&lt;p&gt;由LC的描述可以看到，如果$m$比$n$小太多，则很有可能所有桶都被哈希到了，此时$u$的值为0，LC的估计公式就不起作用了（变成无穷大）。因此$m$的选择除了要满足上面误差控制的需求外，还要保证满桶的概率非常小。&lt;/p&gt;
&lt;p&gt;上面已经说过，$u$满足二项分布，而当$n$非常大，$p$非常小时，可以用泊松分布近似逼近二项分布。因此这里我们可以认为$u$服从泊松分布（注意，上面我们说$u$也可以近似服从正态分布，这并不矛盾，实际上泊松分布和正态分布分别是二项分布的离散型和连续型概率逼近，且泊松分布以正态分布为极限）：&lt;/p&gt;
&lt;p&gt;当$n、m$趋于无穷大时：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Pr(u=k)=(\frac{\lambda^k}{k!})e^{-\lambda}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Pr(u=0)&amp;lt;e^{-5}=0.007
\end{equation}&lt;/p&gt;
&lt;p&gt;由于泊松分布的方差为$\lambda$，因此只要保证$u$的期望偏离$0$点$\sqrt{5}$的标准差就可以保证满桶的概率不大于$0.7%$。因此可得：&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; 5(e^t-t-1)
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:上式没看懂,望看懂的童鞋不吝赐教！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;综上所述，当基数量级为$N$，可接受误差为$\epsilon$，则$m$的选取应该遵从&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; \beta(e^t-t-1)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\beta = max(5, 1/(\epsilon t)^2)$&lt;/p&gt;
&lt;p&gt;下图是论文作者预先计算出的关于不同基数量级和误差情况下，$m$的选择表：&lt;/p&gt;
&lt;p&gt;&lt;img alt="m choice" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/m_choice_zpsb78fb0f8.png"&gt;&lt;/p&gt;
&lt;p&gt;可以看出精度要求越高，则Bitmap的长度越大。随着$m$和$n$的增大，$m$大约为$n$的十分之一。因此LC所需要的空间只有传统的Bitmap直接映射方法的1/10，但是从渐进复杂性的角度看，空间复杂度仍为$O(N_{max})$。&lt;/p&gt;
&lt;h3&gt;合并&lt;/h3&gt;
&lt;p&gt;LC非常方便于合并，合并方案与传统Bitmap映射方法无异，都是通过按位或的方式。&lt;/p&gt;
&lt;h1&gt;LogLog Counting&lt;sup id="sf-play-with-cardinality-estimation-3-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-3" title="解读Cardinality Estimation算法（第三部分：LogLog Counting）"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;上一部分介绍的Linear Counting算法相较于直接映射Bitmap的方法能大大节省内存（大约只需后者1/10的内存），但毕竟只是一个常系数级的降低，空间复杂度仍然为$O(N_max)$。而本文要介绍的LogLog Counting却只有$O(log_2(log_2(N_{max})))$。例如，假设基数的上限为1亿，原始Bitmap方法需要12.5M内存，而LogLog Counting只需不到1K内存（640字节）就可以在标准误差不超过4%的精度下对基数进行估计，效果可谓十分惊人。&lt;/p&gt;
&lt;p&gt;本部分将介绍LogLog Counting。&lt;/p&gt;
&lt;h2&gt;简介&lt;/h2&gt;
&lt;p&gt;LogLog Counting（以下简称LLC）出自论文“Loglog Counting of Large Cardinalities”。LLC的空间复杂度仅有$O(log_2(log_2(N_{max})))$，使得通过KB级内存估计数亿级别的基数成为可能，因此目前在处理大数据的基数计算问题时，所采用算法基本为LLC或其几个变种。下面来具体看一下这个算法。&lt;/p&gt;
&lt;h2&gt;基本算法&lt;/h2&gt;
&lt;h3&gt;均匀随机化&lt;/h3&gt;
&lt;p&gt;与LC一样，在使用LLC之前需要选取一个哈希函数$H$应用于所有元素，然后对哈希值进行基数估计。$H$必须满足如下条件（定性的）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H$的结果具有很好的均匀性，也就是说无论原始集合元素的值分布如何，其哈希结果的值几乎服从均匀分布（完全服从均匀分布是不可能的，D.Knuth已经证明不可能通过一个哈希函数将一组不服从均匀分布的数据映射为绝对均匀分布，但是很多哈希函数可以生成几乎服从均匀分布的结果，这里我们忽略这种理论上的差异，认为哈希结果就是服从均匀分布）。&lt;/li&gt;
&lt;li&gt;$H$的碰撞几乎可以忽略不计。也就是说我们认为对于不同的原始值，其哈希结果相同的概率非常小以至于可以忽略不计。&lt;/li&gt;
&lt;li&gt;$H$的哈希结果是固定长度的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上对哈希函数的要求是随机化和后续概率分析的基础。后面的分析均认为是针对哈希后的均匀分布数据进行。&lt;/p&gt;
&lt;h3&gt;思想来源&lt;/h3&gt;
&lt;p&gt;下面非正式的从直观角度描述LLC算法的思想来源。&lt;/p&gt;
&lt;p&gt;设$a$为待估集合（哈希后）中的一个元素，由上面对$H$的定义可知，$a$可以看做一个长度固定的比特串（也就是$a$的二进制表示），设$H$哈希后的结果长度为$L$比特，我们将这$L$个比特位从左到右分别编号为$1、2、…、L$：&lt;/p&gt;
&lt;p&gt;&lt;img alt="LLC Structure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/llc_structure_zpsefad7ee9.png"&gt;&lt;/p&gt;
&lt;p&gt;又因为$a$是从服从均与分布的样本空间中随机抽取的一个样本，因此$a$每个比特位服从如下分布且相互独立。&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(x=k)=\left \lbrace
\begin{array}{cc}
0.5 &amp;amp; (k=0) \\ 
0.5 &amp;amp; (k=1)
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;通俗说就是$a$的每个比特位为0和1的概率各为0.5，且相互之间是独立的。&lt;/p&gt;
&lt;p&gt;设$\rho(a)$为$a$的比特串中第一个“1”出现的位置，显然$1≤\rho(a)≤L$，这里我们忽略比特串全为0的情况（概率为$1/2^L$）。如果我们遍历集合中所有元素的比特串，取$\rho_{max}$为所有$\rho(a)$的最大值。&lt;/p&gt;
&lt;p&gt;此时我们可以将$2^{\rho_{max}}$作为基数的一个粗糙估计，即：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n} = 2^{\rho_{max}}
\end{equation}&lt;/p&gt;
&lt;p&gt;下面解释为什么可以这样估计。注意如下事实：&lt;/p&gt;
&lt;p&gt;由于比特串每个比特都独立且服从0-1分布，因此从左到右扫描上述某个比特串寻找第一个“1”的过程从统计学角度看是一个伯努利过程，例如，可以等价看作不断投掷一个硬币（每次投掷正反面概率皆为0.5），直到得到一个正面的过程。在一次这样的过程中，投掷一次就得到正面的概率为$1/2$，投掷两次得到正面的概率是$1/2^2$，…，投掷k次才得到第一个正面的概率为$1/2^k$。&lt;/p&gt;
&lt;p&gt;现在考虑如下两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;进行n次伯努利过程，所有投掷次数都不大于$k$的概率是多少？&lt;/li&gt;
&lt;li&gt;进行n次伯努利过程，至少有一次投掷次数等于$k$的概率是多少？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;首先看第一个问题，在一次伯努利过程中，投掷次数大于$k$的概率为$1/2^k$，即连续掷出$k$个反面的概率。因此，在一次过程中投掷次数不大于$k$的概率为$1−1/2^k$。因此，$n$次伯努利过程投掷次数均不大于$k$的概率为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X \leq k)=(1-1/2^k)^n
\end{equation}&lt;/p&gt;
&lt;p&gt;显然第二个问题的答案是：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X \neq k)=1-(1-1/2^{k-1})^n
\end{equation}&lt;/p&gt;
&lt;p&gt;从以上分析可以看出，当$n \ll 2^k$，$P_n(X \neq k)$的概率几乎为0，同时，当$n \gg k$时，$P_n(X \leq k)$的概率也几乎为0。用自然语言概括上述结论就是：当伯努利过程次数远远小于$2^k$时，至少有一次过程投掷次数等于$k$的概率几乎为0；当伯努利过程次数远远大于$2^k$时，没有一次过程投掷次数大于$k$的概率也几乎为0。&lt;/p&gt;
&lt;p&gt;如果将上面描述做一个对应：一次伯努利过程对应一个元素的比特串，反面对应0，正面对应1，投掷次数$k$对应第一个“1”出现的位置，我们就得到了下面结论：&lt;/p&gt;
&lt;p&gt;设一个集合的基数为$n$，$\rho_{max}$为所有元素中首个“1”的位置最大的那个元素的“1”的位置，如果$n$远远小于$2^{\rho_{max}}$，则我们得到$\rho_{max}$为当前值的概率几乎为0（它应该更小），同样的，如果$n$远远大于$2^{\rho_{max}}$，则我们得到$\rho_{max}$为当前值的概率也几乎为0（它应该更大），因此$2^{\rho_{max}}$可以作为基数$n$的一个粗糙估计。&lt;/p&gt;
&lt;h3&gt;分桶平均&lt;/h3&gt;
&lt;p&gt;上述分析给出了LLC的基本思想，不过如果直接使用上面的单一估计量进行基数估计会由于偶然性而存在较大误差。因此，LLC采用了分桶平均的思想来消减误差。具体来说，就是将哈希空间平均分成$m$份，每份称之为一个桶（bucket）。对于每一个元素，其哈希值的前$k$比特作为桶编号，其中$2^k=m$，而后$L-k$个比特作为真正用于基数估计的比特串。桶编号相同的元素被分配到同一个桶，在进行基数估计时，首先计算每个桶内元素最大的第一个“1”的位置，设为$M[i]$，然后对这$m$个值取平均后再进行估计，即：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=2^{\frac{1}{m}\sum{M[i]}}
\end{equation}&lt;/p&gt;
&lt;p&gt;这相当于物理试验中经常使用的多次试验取平均的做法，可以有效消减因偶然性带来的误差。&lt;/p&gt;
&lt;p&gt;下面举一个例子说明分桶平均怎么做。&lt;/p&gt;
&lt;p&gt;假设$H$的哈希长度为16bit，分桶数$m$定为32。设一个元素哈希值的比特串为“0001001010001010”，由于$m$为32，因此前5个bit为桶编号，所以这个元素应该归入“00010”即2号桶（桶编号从0开始，最大编号为$m-1$），而剩下部分是“01010001010”且显然ρ(01010001010)=2，所以桶编号为“00010”的元素最大的$\rho$即为$M[2]$的值。&lt;/p&gt;
&lt;h3&gt;偏差修正&lt;/h3&gt;
&lt;p&gt;上述经过分桶平均后的估计量看似已经很不错了，不过通过数学分析可以知道这并不是基数n的无偏估计。因此需要修正成无偏估计。这部分的具体数学分析在“Loglog Counting of Large Cardinalities”中，过程过于艰涩这里不再具体详述，有兴趣的朋友可以参考原论文。这里只简要提一下分析框架：&lt;/p&gt;
&lt;p&gt;首先上文已经得出：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X \leq k)=(1-1/2^k)^n
\end{equation}&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X = k)=(1-1/2^k)^n - (1-1/2^{k-1})^n
\end{equation}&lt;/p&gt;
&lt;p&gt;这是一个未知通项公式的递推数列，研究这种问题的常用方法是使用生成函数（generating function）。通过运用指数生成函数和poissonization得到上述估计量的Poisson期望和方差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\varepsilon _n&amp;amp;\sim [(\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^m+\epsilon _n]n \\
\nu _n&amp;amp;\sim [(\Gamma (-2/m)\frac{1-2^{2/m}}{log2})^m - (\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^{2m}+\eta _n]n^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$|\epsilon _n|$和$|\eta _n|$不超过$10^{−6}$。&lt;/p&gt;
&lt;p&gt;最后通过depoissonization得到一个渐进无偏估计量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=\alpha _m 2^{\frac{1}{m}\sum{M[i]}}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Equation_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/equation_one_zps76f405b2.png"&gt;&lt;/p&gt;
&lt;p&gt;其中$m$是分桶数。这就是LLC最终使用的估计量。&lt;/p&gt;
&lt;h3&gt;误差分析&lt;/h3&gt;
&lt;p&gt;不加证明给出如下结论：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E_n(\hat{n})/n &amp;amp;= 1 + \theta_{1,n} + o(1) \\
\sqrt{Var_n(E)}/n &amp;amp;= \beta_m / \sqrt{m} + \theta_{2,n} + o(1)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$|\theta_{1,n}|$和$|\theta_{2,n}|$不超过$10^{−6}$。&lt;/p&gt;
&lt;p&gt;当$m$不太小（不小于64）时，$\beta$大约为1.30。因此：&lt;/p&gt;
&lt;p&gt;\begin{equation}
StdError(\hat{n}/n) \approx \frac{1.30}{\sqrt{m}}
\end{equation}&lt;/p&gt;
&lt;h2&gt;算法应用&lt;/h2&gt;
&lt;h3&gt;误差控制&lt;/h3&gt;
&lt;p&gt;在应用LLC时，主要需要考虑的是分桶数$m$，而这个$m$主要取决于误差。根据上面的误差分析，如果要将误差控制在$\epsilon$之内，则：&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; (\frac{1.30}{\epsilon})^2
\end{equation}&lt;/p&gt;
&lt;h3&gt;内存使用分析&lt;/h3&gt;
&lt;p&gt;内存使用与$m$的大小及哈希值得长度（或说基数上限）有关。假设$H$的值为32bit，由于$\rho_{max} \leq 32$，因此每个桶需要5bit空间存储这个桶的$\rho_{max}$，$m$个桶就是$5 \times m/8$字节。例如基数上限为一亿（约227），当分桶数$m$为1024时，每个桶的基数上限约为227/210=217，而$log_2(log_2(217))=4.09$，因此每个桶需要5bit，需要字节数就是$5×1024/8=640$，误差为$1.30 / \sqrt{1024} = 0.040625$，也就是约为$4%$。&lt;/p&gt;
&lt;h3&gt;合并&lt;/h3&gt;
&lt;p&gt;与LC不同，LLC的合并是以桶为单位而不是bit为单位，由于LLC只需记录桶的$\rho_{max}$，因此合并时取相同桶编号数值最大者为合并后此桶的数值即可。&lt;/p&gt;
&lt;h1&gt;HyperLogLog Counting及Adaptive Counting&lt;sup id="sf-play-with-cardinality-estimation-4-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-4" title="解读Cardinality Estimation算法（第四部分：HyperLogLog Counting及Adaptive Counting）"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;在前一部分，我们了解了LogLog Counting。LLC算法的空间复杂度为$O(log_2(log_2(N_{max})))$，并且具有较高的精度，因此非常适合用于大数据场景的基数估计。不过LLC也有自己的问题，就是当基数不太大时，估计值的误差会比较大。这主要是因为当基数不太大时，可能存在一些空桶，这些空桶的$\rho_{max}$为0。由于LLC的估计值依赖于各桶$\rho_{max}$的几何平均数，而几何平均数对于特殊值（这里就是指0）非常敏感，因此当存在一些空桶时，LLC的估计效果就变得较差。&lt;/p&gt;
&lt;p&gt;本部分将要介绍的HyperLogLog Counting及Adaptive Counting算法均是对LLC算法的改进，可以有效克服LLC对于较小基数估计效果差的缺点。&lt;/p&gt;
&lt;h2&gt;评价基数估计算法的精度&lt;/h2&gt;
&lt;p&gt;首先我们来分析一下LLC的问题。一般来说LLC最大问题在于当基数不太大时，估计效果比较差。上文说过，LLC的渐近标准误差为$1.30/\sqrt{m}$，看起来貌似只和分桶数$m$有关，那么为什么基数的大小也会导致效果变差呢？这就需要重点研究一下如何评价基数估计算法的精度，以及“渐近标准误差”的意义是什么。&lt;/p&gt;
&lt;h3&gt;标准误差&lt;/h3&gt;
&lt;p&gt;首先需要明确标准误差的意义。例如标准误差为0.02，到底表示什么意义。&lt;/p&gt;
&lt;p&gt;标准误差是针对一个统计量（或估计量）而言。在分析基数估计算法的精度时，我们关心的统计量是$\hat{n}/n$。注意这个量分子分母均为一组抽样的统计量。下面正式描述一下这个问题。&lt;/p&gt;
&lt;p&gt;设$S$是我们要估计基数的可重复有限集合。$S$中每个元素都是来自值服从均匀分布的样本空间的一个独立随机抽样样本。这个集合共有$C$个元素，但其基数不一定是$C$，因为其中可能存在重复元素。设$f_n$为定义在$S$上的函数：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f_n(S)=CardinalityofS
\end{equation}&lt;/p&gt;
&lt;p&gt;同时定义$\hat{f_n}$也是定义在$S$上的函数：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f_\hat{n}(S)=LogLog\;estimate\;value\;of\;S
\end{equation}&lt;/p&gt;
&lt;p&gt;我们想得到的第一个函数值，但是由于第一个函数值不好计算，所以我们计算同样集合的第二个函数值来作为第一个函数值得估计。因此最理想的情况是对于任意一个集合两个函数值是相等的，如果这样估计就是100%准确了。不过显然没有这么好的事，因此我们退而求其次，只希望fn^(S)是一个无偏估计，即：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f_\hat{n}(S)
\end{equation}&lt;/p&gt;
&lt;p&gt;这个在上一篇文章中已经说明了。同时也可以看到，$\frac{f_\hat{n}(S)}{f_n(S)}$实际上是一个随机变量，并且服从正态分布。对于正态分布随机变量，一般可以通过标准差$\sigma$度量其稳定性，直观来看，标准差越小，则整体分布越趋近于均值，所以估计效果就越好。这是定性的，那么定量来看标准误差$\sigma$到底表达了什么意思呢。它的意义是这样的：&lt;/p&gt;
&lt;p&gt;对于无偏正态分布而言，随机变量的一次随机取值落在均值一个标准差范围内的概率是68.2%，而落在两个和三个标准差范围内的概率分别为95.4%和99.6%，如下图所示（图片来自维基百科）：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Norm" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/norm_zps0655fdfe.png"&gt;&lt;/p&gt;
&lt;p&gt;因此，假设标准误差是0.02（2%），它实际的意义是：假设真实基数为$n$，$n$与估计值之比落入(0.98, 1.02)的概率是68.2%，落入(0.96,1.04)的概率是95.4%，落入(0.94,1.06)的概率是99.6%。显然这个比值越大则估计值越不准，因此对于0.02的标准误差，这个比值大于1.06或小于0.94的概率不到0.004。&lt;/p&gt;
&lt;p&gt;再直观一点，假设真实基数为10000，则一次估计值有99.6%的可能不大于10600且不小于9400。&lt;/p&gt;
&lt;h3&gt;组合计数与渐近分析&lt;/h3&gt;
&lt;p&gt;如果LLC能够做到绝对服从$1.30/\sqrt{m}$，那么也算很好了，因为我们只要通过控制分桶数$m$就可以得到一个一致的标准误差。这里的一致是指标准误差与基数无关。不幸的是并不是这样，上面已经说过，这是一个“渐近”标注误差。下面解释一下什么叫渐近。&lt;/p&gt;
&lt;p&gt;在计算数学中，有一个非常有用的分支就是组合计数。组合计数简单来说就是分析自然数的组合函数随着自然数的增长而增长的量级。可能很多人已经意识到这个听起来很像算法复杂度分析。没错，算法复杂度分析就是组合计数在算法领域的应用。&lt;/p&gt;
&lt;p&gt;举个例子，设$A$是一个有$n$个元素的集合（这里$A$是严格的集合，不存在重复元素），则$A$的幂集（即由$A$的所有子集组成的集合）有$2^n$个元素。&lt;/p&gt;
&lt;p&gt;上述关于幂集的组合计数是一个非常整齐一致的组合计数，也就是不管$n$多大，A的幂集总有$2^n$个元素。&lt;/p&gt;
&lt;p&gt;可惜的是现实中一般的组合计数都不存在如此干净一致的解。LLC的偏差和标准差其实都是组合函数，但是论文中已经分析出，LLC的偏差和标准差都是渐近组合计数，也就是说，随着$n$趋向于无穷大，标准差趋向于$1.30/\sqrt{m}$，而不是说$n$多大时其值都一致为$1.30/\sqrt{m}$。另外，其无偏性也是渐近的，只有当$n$远远大于$m$时，其估计值才近似无偏。因此当$n$不太大时，LLC的效果并不好。&lt;/p&gt;
&lt;p&gt;庆幸的是，同样通过统计分析方法，我们可以得到$n$具体小到什么程度我们就不可忍受了，另外就是当$n$太小时可不可以用别的估计方法替代LLC来弥补LLC这个缺陷。HyperLogLog Counting及Adaptive Counting都是基于这个思想实现的。&lt;/p&gt;
&lt;h2&gt;Adaptive Counting&lt;/h2&gt;
&lt;p&gt;Adaptive Counting（简称AC）在“Fast and accurate traffic matrix measurement using adaptive cardinality counting”一文中被提出。其思想也非常简单直观：实际上AC只是简单将LC和LLC组合使用，根据基数量级决定是使用LC还是LLC。具体是通过分析两者的标准差，给出一个阈值，根据阈值选择使用哪种估计。&lt;/p&gt;
&lt;h3&gt;基本算法&lt;/h3&gt;
&lt;p&gt;如果分析一下LC和LLC的存储结构，可以发现两者是兼容的，区别仅仅在于LLC关心每个桶的$\rho_{max}$，而LC仅关心此桶是否为空。因此只要简单认为$\rho_{max}$值不为0的桶为非空，桶为空就可以使用LLC的数据结构做LC估计了。&lt;/p&gt;
&lt;p&gt;而我们已经知道，LC在基数不太大时效果好，基数太大时会失效；LLC恰好相反，因此两者有很好的互补性。&lt;/p&gt;
&lt;p&gt;回顾一下，LC的标准误差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
SE_{lc}(\hat{n}/n)=\sqrt{e^t-t-1}/(t\sqrt{m})
\end{equation}&lt;/p&gt;
&lt;p&gt;LLC的标准误差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
SE_{llc}(\hat{n}/n)=1.30/\sqrt{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;将两个公式联立：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sqrt{e^t-t-1}/(t\sqrt{m})=1.30/\sqrt{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;解得$t \approx 2.89$。注意$m$被消掉了，说明这个阈值与$m$无关。其中$t=n/m$。&lt;/p&gt;
&lt;p&gt;设$\beta$为空桶率，根据LC的估算公式，带入上式可得：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\beta = e^{-t} \approx 0.051
\end{equation}&lt;/p&gt;
&lt;p&gt;因此可以知道，当空桶率大于0.051时，LC的标准误差较小，而当小于0.051时，LLC的标准误差较小。
完整的AC算法如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=\left \lbrace 
\begin{array}{cc}
\alpha_m m2^{\frac{1}{m}\sum{M}} &amp;amp; if &amp;amp; 0 \leq \beta &amp;lt; 0.051 \\ 
-mlog(\beta) &amp;amp; if &amp;amp; 0.051 \leq \beta \leq 1 \end{array} 
\right.
\end{equation}&lt;/p&gt;
&lt;h3&gt;误差分析&lt;/h3&gt;
&lt;p&gt;因为AC只是LC和LLC的简单组合，所以误差分析可以依照LC和LLC进行。值得注意的是，当β&amp;lt;0.051时，LLC最大的偏差不超过0.17%，因此可以近似认为是无偏的。&lt;/p&gt;
&lt;h2&gt;HyperLogLog Counting&lt;/h2&gt;
&lt;p&gt;HyperLogLog Counting（以下简称HLLC）的基本思想也是在LLC的基础上做改进，不过相对于AC来说改进的比较多，所以相对也要复杂一些。本文不做具体细节分析，具体细节请参考“HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm”这篇论文。&lt;/p&gt;
&lt;h3&gt;基本算法&lt;/h3&gt;
&lt;p&gt;HLLC的第一个改进是使用调和平均数替代几何平均数。注意LLC是对各个桶取算数平均数，而算数平均数最终被应用到2的指数上，所以总体来看LLC取得是几何平均数。由于几何平均数对于离群值（例如这里的0）特别敏感，因此当存在离群值时，LLC的偏差就会很大，这也从另一个角度解释了为什么n不太大时LLC的效果不太好。这是因为n较小时，可能存在较多空桶，而这些特殊的离群值强烈干扰了几何平均数的稳定性。&lt;/p&gt;
&lt;p&gt;因此，HLLC使用调和平均数来代替几何平均数，调和平均数的定义如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
H = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + ... + \frac{1}{x_n}} = \frac{n}{\sum_{i=1}^n \frac{1}{x_i}}
\end{equation}&lt;/p&gt;
&lt;p&gt;调和平均数可以有效抵抗离群值的扰动。使用调和平均数代替几何平均数后，估计公式变为如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=\frac{\alpha_m m^2}{\sum{2^{-M}}}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\alpha_m=(m\int _0^\infty (log_2(\frac{2+u}{1+u}))^m du)^{-1}
\end{equation}&lt;/p&gt;
&lt;h3&gt;偏差分析&lt;/h3&gt;
&lt;p&gt;根据论文中的分析结论，与LLC一样HLLC是渐近无偏估计，且其渐近标准差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
SE_{hllc}(\hat{n}/n)=1.04/\sqrt{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此在存储空间相同的情况下，HLLC比LLC具有更高的精度。例如，对于分桶数$m$为$2^13$（8k字节）时，LLC的标准误差为1.4%，而HLLC为1.1%。&lt;/p&gt;
&lt;h3&gt;分段偏差修正&lt;/h3&gt;
&lt;p&gt;在HLLC的论文中，作者在实现建议部分还给出了在$n$相对于$m$较小或较大时的偏差修正方案。具体来说，设$E$为估计值：&lt;/p&gt;
&lt;p&gt;当$E≤{5 \over 2}m$时，使用LC进行估计。&lt;/p&gt;
&lt;p&gt;当$\frac{5}{2}m &amp;lt; E \leq \frac{1}{30}2^{32}$时，使用上面给出的HLLC公式进行估计。&lt;/p&gt;
&lt;p&gt;当$E&amp;gt;\frac{1}{30}2^{32}$时，估计公式则为$\hat{n}=-2^{32}log(1-E/2^{32})$。&lt;/p&gt;
&lt;p&gt;关于分段偏差修正效果分析也可以在原论文中找到。&lt;/p&gt;
&lt;h1&gt;写在后面&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;原博客中作者对这几种算法进行了实验比较,因为个人对实验不是很感兴趣,现只摘录作者的&lt;strong&gt;个人建议&lt;/strong&gt;(对实验结果有兴趣的同学请参考&lt;a href="http://blog.codinglabs.org/articles/cardinality-estimate-exper.html"&gt;五种常用基数估计算法效果实验及实践建议&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear Counting和LogLog Counting由于分别在基数较大和基数较小（阈值可解析分析，具体方法和公式请参考后文列出的相关论文）时存在严重的失效，因此不适合在实际中单独使用。一种例外是，如果对节省存储空间要求不强烈，不要求空间复杂度为常数（Linear Counting的空间复杂度为$O(n)$，其它算法均为$O(1)$），则在保证Bitmap全满概率很小的条件下，Linear Counting的效果要优于其它算法。&lt;/li&gt;
&lt;li&gt;总体来看，不论哪种算法，提高分桶数都可以降低偏差和方差，因此总体来看基数估计算法中分桶数的选择是最重要的一个权衡——在精度和存储空间间的权衡。&lt;/li&gt;
&lt;li&gt;实际中，Adaptive Counting或HyperLogLog Counting都是不错的选择，前者偏差较小，后者对离群点容忍性更好，方差较小。&lt;/li&gt;
&lt;li&gt;Google的HyperLogLog Counting++算法属于实验性改进，缺乏严格的数学分析基础，通用性存疑，不宜在实际中贸然使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后感谢CodingLabs撰写的精彩博文,这两周就写这3篇博文吧，两周后再见。尼玛,都2:16了,大家晚安，睡了。&lt;/p&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-play-with-cardinality-estimation-1"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-i.html"&gt;解读Cardinality Estimation算法（第一部分：基本概念）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-play-with-cardinality-estimation-2"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-ii.html"&gt;解读Cardinality Estimation算法（第二部分：Linear Counting）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-play-with-cardinality-estimation-3"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iii.html"&gt;解读Cardinality Estimation算法（第三部分：LogLog Counting）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-3-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-play-with-cardinality-estimation-4"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iv.html"&gt;解读Cardinality Estimation算法（第四部分：HyperLogLog Counting及Adaptive Counting）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-4-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="基数估计"></category><category term="Cardinality Estimation"></category><category term="Big Data"></category></entry><entry><title>也说2048:Minimax算法以及Alpha-Beta剪枝</title><link href="http://www.qingyuanxingsi.com/ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi.html" rel="alternate"></link><updated>2014-04-06T00:00:00+08:00</updated><author><name>CodingLabs</name></author><id>tag:www.qingyuanxingsi.com,2014-04-06:ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi.html</id><summary type="html">&lt;p&gt;今天看机器学习Logistic Regression,脑子实在转不过来了,于是到处游荡了一下,然后不经意间发现了这篇特别好玩的文章&lt;a href="http://blog.codinglabs.org/articles/2048-ai-analysis.html"&gt;2048-AI程序算法分析&lt;/a&gt;,于是摘录如下。(&lt;strong&gt;版权属原作者所有,特此声明&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;针对目前火爆的2048游戏，有人实现了一个AI程序，可以以较大概率（高于90%）赢得游戏，想一睹该AI程序的童鞋们请移步&lt;a href="http://ov3y.github.io/2048-AI/"&gt;2048AI实现&lt;/a&gt;,点击Auto-run按钮即可运行。此外作者在&lt;a href="http://stackoverflow.com/questions/22342854/what-is-the-optimal-algorithm-for-the-game-2048"&gt;stackoverflow上简要介绍了AI的算法框架和实现思路&lt;/a&gt;。但是这个回答主要集中在启发函数的选取上，对AI用到的核心算法并没有仔细说明。这篇文章将主要分为两个部分，第一部分介绍其中用到的基础算法，即Minimax和Alpha-beta剪枝；第二部分分析作者具体的实现。&lt;/p&gt;
&lt;p&gt;&lt;img alt="2048" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/2048_zps4f3b681e.png"&gt;&lt;/p&gt;
&lt;h1&gt;基础算法&lt;/h1&gt;
&lt;p&gt;2048本质上可以抽象成信息对称双人对弈模型（玩家向四个方向中的一个移动，然后计算机在某个空格中填入2或4）。这里“信息对称”是指在任一时刻对弈双方对格局的信息完全一致，移动策略仅依赖对接下来格局的推理。作者使用的核心算法为对弈模型中常用的带Alpha-beta剪枝的Minimax。这个算法也常被用于如国际象棋等信息对称对弈AI中。&lt;/p&gt;
&lt;h2&gt;Minimax&lt;/h2&gt;
&lt;p&gt;下面先介绍不带剪枝的Minimax。首先本文将通过一个简单的例子说明Minimax算法的思路和决策方式。&lt;/p&gt;
&lt;h3&gt;问题&lt;/h3&gt;
&lt;p&gt;现在考虑这样一个游戏：有三个盘子A、B和C，每个盘子分别放有三张纸币。A放的是1、20、50；B放的是5、10、100；C放的是1、5、20。单位均为“元”。有甲、乙两人，两人均对三个盘子和上面放置的纸币有可以任意查看。游戏分三步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;甲从三个盘子中选取一个。&lt;/li&gt;
&lt;li&gt;乙从甲选取的盘子中拿出两张纸币交给甲。&lt;/li&gt;
&lt;li&gt;甲从乙所给的两张纸币中选取一张，拿走。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中甲的目标是最后拿到的纸币面值尽量大，乙的目标是让甲最后拿到的纸币面值尽量小。&lt;/p&gt;
&lt;p&gt;下面用Minimax算法解决这个问题。&lt;/p&gt;
&lt;h3&gt;基本思路&lt;/h3&gt;
&lt;p&gt;一般解决博弈类问题的自然想法是将格局组织成一棵树，树的每一个节点表示一种格局，而父子关系表示由父格局经过一步可以到达子格局。Minimax也不例外，它通过对以当前格局为根的格局树搜索来确定下一步的选择。而一切格局树搜索算法的核心都是对每个格局价值的评价。Minimax算法基于以下朴素思想确定格局价值：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimax是一种悲观算法，即假设对手每一步都会将我方引入从当前看理论上价值最小的格局方向，即对手具有完美决策能力。因此我方的策略应该是选择那些对方所能达到的让我方最差情况中最好的，也就是让对方在完美决策下所对我造成的损失最小。&lt;/li&gt;
&lt;li&gt;Minimax不找理论最优解，因为理论最优解往往依赖于对手是否足够愚蠢，Minimax中我方完全掌握主动，如果对方每一步决策都是完美的，则我方可以达到预计的最小损失格局，如果对方没有走出完美决策，则我方可能达到比预计的最悲观情况更好的结局。总之我方就是要在最坏情况中选择最好的。
上面的表述有些抽象，下面看具体示例。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;解题&lt;/h3&gt;
&lt;p&gt;下图是上述示例问题的格局树：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Situation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/01_zps1cf40396.png"&gt;&lt;/p&gt;
&lt;p&gt;注意，由于示例问题格局数非常少，我们可以给出完整的格局树。这种情况下我可以找到Minimax算法的全局最优解。而真实情况中，格局树非常庞大，即使是计算机也不可能给出完整的树，因此我们往往只搜索一定深度，这时只能找到局部最优解。&lt;/p&gt;
&lt;p&gt;我们从甲的角度考虑。其中正方形节点表示轮到我方（甲），而三角形表示轮到对方（乙）。经过三轮对弈后（我方-对方-我方），将进入终局。黄色叶结点表示所有可能的结局。从甲方看，由于最终的收益可以通过纸币的面值评价，我们自然可以用结局中甲方拿到的纸币面值表示终格局的价值。&lt;/p&gt;
&lt;p&gt;下面考虑倒数第二层节点，在这些节点上，轮到我方选择，所以我们应该引入可选择的最大价值格局，因此每个节点的价值为其子节点的最大值：&lt;/p&gt;
&lt;p&gt;&lt;img alt="02" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/02_zps80320dc2.png"&gt;&lt;/p&gt;
&lt;p&gt;这些轮到我方的节点叫做max节点，max节点的值是其子节点最大值。&lt;/p&gt;
&lt;p&gt;倒数第三层轮到对方选择，假设对方会尽力将局势引入让我方价值最小的格局，因此这些节点的价值取决于子节点的最小值。这些轮到对方的节点叫做min节点。&lt;/p&gt;
&lt;p&gt;最后，根节点是max节点，因此价值取决于叶子节点的最大值。最终完整赋值的格局树如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="03" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/03_zps171fcc9a.png"&gt;&lt;/p&gt;
&lt;p&gt;总结一下Minimax算法的步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先确定最大搜索深度D，D可能达到终局，也可能是一个中间格局。&lt;/li&gt;
&lt;li&gt;在最大深度为D的格局树叶子节点上，使用预定义的价值评价函数对叶子节点价值进行评价。&lt;/li&gt;
&lt;li&gt;自底向上为非叶子节点赋值。其中max节点取子节点最大值，min节点取子节点最小值。&lt;/li&gt;
&lt;li&gt;每次轮到我方时（此时必处在格局树的某个max节点），选择价值等于此max节点价值的那个子节点路径。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在上面的例子中，根节点的价值为20，表示如果对方每一步都完美决策，则我方按照上述算法可最终拿到20元，这是我方在Minimax算法下最好的决策。格局转换路径如下图红色路径所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt="04" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/04_zpsaa4d7848.png"&gt;&lt;/p&gt;
&lt;p&gt;对于真实问题中的Minimax，再次强调几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;真实问题一般无法构造出完整的格局树，所以需要确定一个最大深度D，每次最多从当前格局向下计算D层。&lt;/li&gt;
&lt;li&gt;因为上述原因，Minimax一般是寻找一个局部最优解而不是全局最优解，搜索深度越大越可能找到更好的解，但计算耗时会呈指数级膨胀。&lt;/li&gt;
&lt;li&gt;也是因为无法一次构造出完整的格局树，所以真实问题中Minimax一般是边对弈边计算局部格局树，而不是只计算一次，但已计算的中间结果可以缓存。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Alpha-beta剪枝&lt;sup id="sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1" title="以下描述看不懂可参考Step by Step:Alpha-Beta Cutting Example"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;简单的Minimax算法有一个很大的问题就是计算复杂性。由于所需搜索的节点数随最大深度呈指数膨胀，而算法的效果往往和深度相关，因此这极大限制了算法的效果。&lt;/p&gt;
&lt;p&gt;Alpha-beta剪枝是对Minimax的补充和改进。采用Alpha-beta剪枝后，我们可不必构造和搜索最大深度D内的所有节点，在构造过程中，如果发现当前格局再往下不能找到更好的解，我们就停止在这个格局及以下的搜索，也就是剪枝。&lt;/p&gt;
&lt;p&gt;Alpha-beta基于这样一种朴素的思想：&lt;strong&gt;时时刻刻记得当前已经知道的最好选择，如果从当前格局搜索下去，不可能找到比已知最优解更好的解，则停止这个格局分支的搜索（剪枝），回溯到父节点继续搜索&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Alpha-beta算法可以看成变种的Minimax，基本方法是从根节点开始采用深度优先的方式构造格局树，在构造每个节点时，都会读取此节点的alpha和beta两个值，其中alpha表示搜索到当前节点时已知的最好选择的下界，而beta表示从这个节点往下搜索最坏结局的上界。由于我们假设对手会将局势引入最坏结局之一，因此当beta小于alpha时，表示从此处开始不论最终结局是哪一个，其上限价值也要低于已知的最优解，也就是说已经不可能此处向下找到更好的解，所以就会剪枝。&lt;/p&gt;
&lt;p&gt;下面同样以上述示例介绍Alpha-beta剪枝算法的工作原理。我们从根节点开始，详述使用Alpha-beta的每一个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根节点的alpha和beta分别被初始化为$-\infty$，和$+\infty$。&lt;/li&gt;
&lt;li&gt;深度优先搜索第一个孩子，不是叶子节点，所以alpha和beta继承自父节点，分别为$-\infty$，和$+\infty$&lt;/li&gt;
&lt;li&gt;搜索第三层的第一个孩子，同上。&lt;/li&gt;
&lt;li&gt;搜索第四层，到达叶子节点，采用评价函数得到此节点的评价值为1。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="05" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/05_zps5136a83f.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;此叶节点的父节点为max节点，因此更新其alpha值为1，表示此节点取值的下界为1。&lt;/li&gt;
&lt;li&gt;再看另外一个子节点，值为20，大于当前alpha值，因此将alpha值更新为20。&lt;/li&gt;
&lt;li&gt;此时第三层最左节点所有子树搜索完毕，作为max节点，更新其真实值为当前alpha值：20。&lt;/li&gt;
&lt;li&gt;由于其父节点（第二层最左节点）为min节点，因此更新其父节点beta值为20，表示这个节点取值最多为20。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="06" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/06_zps09a0fcab.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;搜索第二层最左节点的第二个孩子及其子树，按上述逻辑，得到值为50（&lt;strong&gt;注意第二层最左节点的beta值要传递给孩子&lt;/strong&gt;）。由于50大于20，不更新min节点的beta值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="07" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/07_zps5a2b5a81.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;搜索第二层最左节点的第三个孩子。当看完第一个叶子节点后，发现第三个孩子的alpha=beta，此时表示这个节点下不会再有更好解，于是剪枝。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="08" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/08_zpsbee7e438.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;继续搜索B分支，当搜索完B分支的第一个孩子后，发现此时B分支的alpha为20，beta为10。这表示B分支节点的最大取值不会超过10，而我们已经在A分支取到20，此时满足alpha大于等于beta的剪枝条件，因此将B剪枝。并将B分支的节点值设为10，注意，这个10不一定是这个节点的真实值，而只是上线，B节点的真实值可能是5，可能是1，可能是任何小于10的值。但是已经无所谓了，反正我们知道这个分支不会好过A分支，因此可以放弃了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="09" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/09_zpsf2b60883.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在C分支搜索时遇到了与B分支相同的情况。因此将C分支剪枝。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="10" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/10_zps1254e8ee.png"&gt;&lt;/p&gt;
&lt;p&gt;此时搜索全部完毕，而我们也得到了这一步的策略：应该走A分支。&lt;/p&gt;
&lt;p&gt;可以看到相比普通Minimax要搜索18个叶子节点相比，这里只搜索了9个。采用Alpha-beta剪枝，可以在相同时间内加大Minimax的搜索深度，因此可以获得更好的效果。并且Alpha-beta的解和普通Minimax的解是一致的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:在查找讲解Alpha-Beta剪枝算法的过程中看到了这样一种表述,我个人觉得这么理解可能更容易看懂这个算法:Alpha是对于Max节点而言从该节点到根节点路径上最好的Option,而Beta是对于Min节点而言从该节点到根节点路径上最好的Option;Max节点的目标是最大化所得收益,而Min节点目标则是最小化Max节点所得收益.(MORE SEE AT &lt;a href="http://www.youtube.com/watch?v=xBXHtz4Gbdo"&gt;Step by Step: Alpha Beta Pruning|GoAgent翻墙&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;针对2048游戏的实现&lt;/h1&gt;
&lt;p&gt;下面看一下ov3y同学针对2048实现的AI。原程序见于&lt;a href="https://github.com/ov3y/2048-AI"&gt;github&lt;/a&gt;，主要程序都在&lt;a href="https://github.com/ov3y/2048-AI/blob/master/js/ai.js"&gt;ai.js&lt;/a&gt;中。&lt;/p&gt;
&lt;h2&gt;建模&lt;/h2&gt;
&lt;p&gt;上面说过Minimax和Alpha-beta都是针对信息对称的轮流对弈问题，这里作者是这样抽象游戏的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我方：游戏玩家。每次可以选择上、下、左、右四个行棋策略中的一种（某些格局会少于四种，因为有些方向不可走）。行棋后方块按照既定逻辑移动及合并，格局转换完成。&lt;/li&gt;
&lt;li&gt;对方：计算机。在当前任意空格子里放置一个方块，方块的数值可以是2或4。放置新方块后，格局转换完成。&lt;/li&gt;
&lt;li&gt;胜利条件：出现某个方块的数值为“2048”。&lt;/li&gt;
&lt;li&gt;失败条件：格子全满，且无法向四个方向中任何一个方向移动（均不能触发合并）。如此2048游戏就被建模成一个信息对称的双人对弈问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;格局评价&lt;/h2&gt;
&lt;p&gt;作为算法的核心，如何评价当前格局的价值是重中之重。在2048中，除了终局外，中间格局并无非常明显的价值评价指标，因此需要用一些启发式的指标来评价格局。那些分数高的“好”格局是容易引向胜利的格局，而分低的“坏”格局是容易引向失败的格局。&lt;/p&gt;
&lt;p&gt;作者采用了如下几个启发式指标。&lt;/p&gt;
&lt;h3&gt;单调性&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;单调性&lt;/code&gt;指方块从左到右、从上到下均遵从递增或递减。一般来说，越单调的格局越好。下面是一个具有良好单调格局的例子：&lt;/p&gt;
&lt;p&gt;&lt;img alt="单调性" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/11_zps7ce37f15.png"&gt;&lt;/p&gt;
&lt;h3&gt;平滑性&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;平滑性&lt;/code&gt;是指每个方块与其直接相邻方块数值的差，其中差越小越平滑。例如2旁边是4就比2旁边是128平滑。一般认为越平滑的格局越好。下面是一个具有极端平滑性的例子：&lt;/p&gt;
&lt;p&gt;&lt;img alt="平滑性" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/12_zpsb53c5ef7.png"&gt;&lt;/p&gt;
&lt;h3&gt;空格数&lt;/h3&gt;
&lt;p&gt;这个很好理解，因为一般来说，空格子越少对玩家越不利。所以我们认为空格越多的格局越好。&lt;/p&gt;
&lt;h3&gt;孤立空格数&lt;/h3&gt;
&lt;p&gt;这个指标评价空格被分开的程度，空格越分散则格局越差。&lt;/p&gt;
&lt;p&gt;具体来说，2048-AI在评价格局时，对这些启发指标采用了加权策略。具体代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// static evaluation function&lt;/span&gt;
&lt;span class="no"&gt;AI&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prototype&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;emptyCells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;availableCells&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;smoothWeight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="c1"&gt;//monoWeight   = 0.0,&lt;/span&gt;
        &lt;span class="c1"&gt;//islandWeight = 0.0,&lt;/span&gt;
        &lt;span class="n"&gt;mono2Weight&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;emptyWeight&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;maxWeight&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;smoothness&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;smoothWeight&lt;/span&gt;
        &lt;span class="c1"&gt;//+ this.grid.monotonicity() * monoWeight&lt;/span&gt;
        &lt;span class="c1"&gt;//- this.grid.islands() * islandWeight&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;monotonicity2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mono2Weight&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emptyCells&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;emptyWeight&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maxValue&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;maxWeight&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;有兴趣的同学可以调整一下权重看看有什么效果。&lt;/p&gt;
&lt;h2&gt;对对方选择的剪枝&lt;/h2&gt;
&lt;p&gt;在这个程序中，除了采用Alpha-beta剪枝外，在min节点还采用了另一种剪枝，即只考虑对方走出让格局最差的那一步（而实际2048中计算机的选择是随机的），而不是搜索全部对方可能的走法。这是因为对方所有可能的选择为“空格数×2”，如果全部搜索的话会严重限制搜索深度。
相关剪枝代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// try a 2 and 4 in each cell and measure how annoying it is&lt;/span&gt;
&lt;span class="c1"&gt;// with metrics from eval&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[];&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;availableCells&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="mh"&gt;2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="mh"&gt;4&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;push&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="k"&gt;cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;tile&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Tile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parseInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;10&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insertTile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tile&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;smoothness&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;islands&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;removeTile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;cell&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;// now just pick out the most annoying moves&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;maxScore&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mh"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mh"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]));&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// 2 and 4&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;maxScore&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;push&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nl"&gt;position:&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nl"&gt;value:&lt;/span&gt; &lt;span class="n"&gt;parseInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;搜索深度&lt;/h2&gt;
&lt;p&gt;在2048-AI的实现中，并没有限制搜索的最大深度，而是限制每次“思考”的时间。这里设定了一个超时时间，默认为100ms，在这个时间内，会从1开始，搜索到所能达到的深度。相关代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// performs iterative deepening over the alpha-beta search&lt;/span&gt;
&lt;span class="no"&gt;AI&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prototype&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterativeDeep&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;()).&lt;/span&gt;&lt;span class="n"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;newBest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mh"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;newBest&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;move&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mh"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="c1"&gt;//console.log('BREAKING EARLY');&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;newBest&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;()).&lt;/span&gt;&lt;span class="n"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;minSearchTime&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="c1"&gt;//console.log('depth', --depth);&lt;/span&gt;
    &lt;span class="c1"&gt;//console.log(this.translate(best.move));&lt;/span&gt;
    &lt;span class="c1"&gt;//console.log(best);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;因此这个算法实现的效果实际上依赖于执行javascript引擎机器的性能。当然可以通过增加超时时间来达到更好的效果，但此时每一步行走速度会相应变慢。&lt;/p&gt;
&lt;h2&gt;算法的改进&lt;/h2&gt;
&lt;p&gt;目前这个实现作者声称成功合成2048的概率超过90%，但是合成4096甚至8192的概率并不高。作者在&lt;a href="https://github.com/ov3y/2048-AI/blob/master/README.md"&gt;github项目的REAMDE&lt;/a&gt;中同时给出了一些优化建议，这些建议包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缓存结果。目前这个实现并没有对已搜索的树做缓存，每一步都要重新开始搜索。&lt;/li&gt;
&lt;li&gt;多线程搜索。由于javascript引擎的单线程特性，这一点很难做到，但如果在其它平台上也许也可考虑并行技术。&lt;/li&gt;
&lt;li&gt;更好的启发函数。也许可以总结出一些更好的启发函数来评价格局价值。&lt;/li&gt;
&lt;/ul&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1"&gt;以下描述看不懂可参考&lt;a href="http://www.youtube.com/watch?v=xBXHtz4Gbdo"&gt;Step by Step:Alpha-Beta Cutting Example&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="AI"></category><category term="2048"></category><category term="Minimax Algorithm"></category><category term="Alpha-Beta Pruning"></category><category term="Algorithm"></category></entry></feed>