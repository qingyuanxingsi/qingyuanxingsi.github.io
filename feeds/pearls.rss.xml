<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>苹果的味道</title><link href="http://www.qingyuanxingsi.com/" rel="alternate"></link><link href="http://www.qingyuanxingsi.com/feeds/pearls.rss.xml" rel="self"></link><id>http://www.qingyuanxingsi.com/</id><updated>2014-05-04T00:00:00+08:00</updated><entry><title>小小收藏夹[持续更新中]</title><link href="http://www.qingyuanxingsi.com/xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html" rel="alternate"></link><updated>2014-05-04T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-04:xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html</id><summary type="html">&lt;h1&gt;NLP&lt;sup id="sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1-back"&gt;&lt;a class="simple-footnote" href="#sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1" title="Natural Language Processing"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;Relation Extration&lt;ul&gt;
&lt;li&gt;Hand-written approach more suitable for structured data,such as a telephone book,Facebook or eBay;&lt;/li&gt;
&lt;li&gt;Supervised Method;得到所有的命名实体组,使用一个分类器(&lt;em&gt;features&lt;/em&gt;)判断它们是否是关联的,如果是,则使用第二个分类器判断它们之间的关联关系具体是什么; &lt;/li&gt;
&lt;li&gt;Semi-Supervised(Relation Bootstrapping/Distant Supervised Learning) and unsupervised methods(Open Information Extraction);Strapping方法感觉很巧妙,个人很喜欢;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;SVM&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;对于SVM这么高端大气上档次的东西,当然要单独列出来。今天其他东西实在看不下去了,所以把Pluskid之前写的一系列讲SVM的文章再挖出来看看。以下是目录以及对每篇的简单说明:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=632"&gt;支持向量机: Maximum Margin Classifier&lt;/a&gt;;文中主要介绍了两个距离,&lt;em&gt;Functional Margin&lt;/em&gt; $\hat{\gamma}$和&lt;em&gt;Geometrical Margin&lt;/em&gt; $\tilde{\gamma}$.它们之间满足$\hat{\gamma} = ||w||\tilde{\gamma}$.我们固定$\hat{\gamma} = 1$,通过最大化$\frac{1}{||w||}$来得到&lt;strong&gt;Maximum Margin Classifier&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=682"&gt;支持向量机: Support Vector&lt;/a&gt;;简要介绍了Support Vector是指什么,另外对线性可分的情况利用Duality进行了推导并得出了两个比较重要的结论:&lt;ul&gt;
&lt;li&gt;对新点的预测只需要计算与训练点之间的内积即可;&lt;/li&gt;
&lt;li&gt;非支持向量不参与模型的计算过程之中。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=685"&gt;支持向量机: Kernel&lt;/a&gt;;&lt;strong&gt;Kernel&lt;/strong&gt;的基本思想。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=692"&gt;支持向量机：Outliers&lt;/a&gt;;通过引入松弛变量处理Outliers,而实际上最后的优化形式只是加上$\alpha_i \leq C$的限制。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=696"&gt;支持向量机：Numerical Optimization&lt;/a&gt;;以非常通俗易懂的方式介绍了一下&lt;strong&gt;SMO(Sequential Minimal Optimization)&lt;/strong&gt;,赞一个。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Deep Learning&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;稀疏编码(&lt;em&gt;Sparse Coding&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8777094"&gt;Deep Learning（深度学习）学习笔记整理系列之（五)&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep Learning总结&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8782018"&gt;Deep Learning（深度学习）学习笔记整理系列之（八)&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Pocket&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://v.youku.com/v_show/id_XNzAwMTI0MDgw.html"&gt;罗辑思维 2014：右派为什么这么横 10&lt;/a&gt;;视频主要介绍了保守主义的三个特征,同时分析了人们在面临选择的时候的不同思维方式,个人觉得这一点很有借鉴意义，建议一看!&lt;/li&gt;
&lt;li&gt;&lt;a href="http://v.youku.com/v_show/id_XNzAzMTkyNDky.html"&gt;罗辑思维 2014：迷茫时代的明白人 11&lt;/a&gt;;活在当下。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;算法&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;今天看了一下网上流传的传说中的高大上的所谓的&lt;code&gt;十大海量数据处理算法&lt;/code&gt;,看了一下,实际上没有什么东西,唯独&lt;strong&gt;Bloom Filter&lt;/strong&gt;看着还挺好玩的,所以以下给出一个通俗易懂的链接。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.cnblogs.com/heaad/archive/2011/01/02/1924195.html"&gt;那些优雅的数据结构(1) : BloomFilter——大规模数据处理利器&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在过去的若干年里,有一个心结一直萦绕在我的心头挥之不去,它存在于我的脑海里，我的梦里，我的歌声里,TA就是&lt;strong&gt;B树&lt;/strong&gt;(好吧,其实是因为没有机会好好地研究一下它啦)。以下给出两个链接,它们主要介绍了B树的基本概念,性质以及针对B树的插入、删除操作,两个PPT还是相当直观的,应该能够比较直观地了解B树这个数据结构!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://cecs.wright.edu/~tkprasad/courses/cs707/L04-X-B-Trees.ppt"&gt;B-Trees&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://zh.scribd.com/doc/18210/B-TREE-TUTORIAL-PPT"&gt;B TREE TUTORIAL PPT&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;今天又重新看了一下这个写的很不错的&lt;strong&gt;A*算法&lt;/strong&gt;,恩,这篇文章想来是极好的。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.raywenderlich.com/zh-hans/21503/a%E6%98%9F%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D"&gt;A星寻路算法介绍&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;复习一下之前做智能提示时用到的&lt;strong&gt;Trie Tree&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="https://www.cs.umd.edu/class/fall2005/cmsc132/lecs/lec29.ppt"&gt;Indexed Search Tree (Trie) - Computer Science Department&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;看了一下传说中的数据挖掘十大算法,好像就&lt;strong&gt;Apriori算法&lt;/strong&gt;不是特别熟吧,所以重新看了一遍;个人觉得如果我早出生若干年,这种程度的算法也是能想出来的吧(我指思想).好吧,我认为着重要理解的有如下两点:&lt;ul&gt;
&lt;li&gt;Support;其实也就是某种组合在所有Transaction中出现的频度。&lt;/li&gt;
&lt;li&gt;Confidence;当生成关联规则$A\to B$时,有$confidence = \frac{Count(A,B)}{Count(A)}$,背后的Intuition就是如果我买了$A$,大概会有多大的可能买$B$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.cs.sjsu.edu/faculty/lee/cs157b/Gaurang%20Negandhi--Apriori%20Algorithm%20Presentation.ppt"&gt;Apriori Algorithm Review for Finals&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最近需要对1G的文本数据进行处理,所以想了解一下现行的分布式计算框架的应用场景,从而选择合适的框架用于这个任务,期间看到以下两篇文章写的很不错,特此摘录。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://langyu.iteye.com/blog/1407194"&gt;对实时分析与离线分析的思考&lt;/a&gt;
&lt;a href="http://langyu.iteye.com/blog/1407194"&gt;对实时分析与离线分析的思考(二)&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Machine Learning&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;看的有点累了,不想看&lt;em&gt;EM&lt;/em&gt;算法复杂的数学公式推导了,所以找到之前看过的一篇,回顾一下,等以后想看了再详细介绍&lt;em&gt;Mixture Models&lt;/em&gt;和&lt;em&gt;EM&lt;/em&gt;算法吧!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://blog.pluskid.org/?p=39"&gt;漫谈 Clustering (3): Gaussian Mixture Model&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;近期为了理解卷积,于是到处找资料,无意中发现了这一篇神一般的理解。(&lt;strong&gt;墙裂推荐&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.guokr.com/post/342476/"&gt;关于卷积的一个血腥的讲解，看完给跪了&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PCA数据预处理&lt;em&gt;Whitening&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;在使用PCA前需要对数据进行预处理，首先是均值化，即对每个特征维，都减掉该维的平均值，然后就是将不同维的数据范围归一化到同一范围，方法一般都是&lt;strong&gt;除以最大值&lt;/strong&gt;。但是比较奇怪的是，在对自然图像进行均值处理时并不是不是减去该维的平均值，而是减去这张图片本身的平均值。因为PCA的预处理是按照不同应用场合来定的。&lt;/p&gt;
&lt;p&gt;自然图像指的是人眼经常看见的图像，其符合某些统计特征。一般实际过程中，只要是拿正常相机拍的，没有加入很多人工创作进去的图片都可以叫做是自然图片，因为很多算法对这些图片的输入类型还是比较鲁棒的。在对自然图像进行学习时，其实不需要太关注对图像做方差归一化，因为自然图像每一部分的统计特征都相似，只需做均值为0化就ok了。不过对其它的图片进行训练时，比如首先字识别等，就需要进行方差归一化了。&lt;/p&gt;
&lt;p&gt;有一个观点需要注意，那就是&lt;strong&gt;PCA并不能阻止过拟合现象&lt;/strong&gt;。表明上看PCA是降维了，因为在同样多的训练样本数据下，其特征数变少了，应该是更不容易产生过拟合现象。但是在实际操作过程中，这个方法阻止过拟合现象效果很小，主要还是通过&lt;strong&gt;规则项&lt;/strong&gt;来进行阻止过拟合的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whitening&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Whitening&lt;/strong&gt;的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。数据的Whitening必须满足两个条件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不同特征间相关性最小，接近0；&lt;/li&gt;
&lt;li&gt;所有特征的方差相等（不一定为1）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常见的白化操作有PCA whitening和ZCA whitening。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;PCA whitening&lt;/em&gt;是指将数据x经过PCA降维为z后，可以看出z中每一维是独立的，满足whitening白化的第一个条件，这是只需要将z中的每一维都除以标准差就得到了每一维的方差为1，也就是说方差相等。公式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_{PCAwhite,i} = \frac{x_{rot,i}}{\sqrt{\lambda_i}}
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;em&gt;ZCA whitening&lt;/em&gt;是指数据x先经过PCA变换为z，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足whtienning的第一个条件，特征间相互独立。然后同样进行方差为1的操作，最后将得到的矩阵左乘一个特征向量矩阵U即可。ZCA whitening公式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_{ZCAwhite} = Ux_{PCAwhite}
\end{equation}&lt;/p&gt;
&lt;p&gt;参考&lt;a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/21/2973231.html"&gt;Deep learning：十(PCA和whitening)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;最近一直在看&lt;strong&gt;高斯过程&lt;/strong&gt;,挺难理解的,好吧,咱们慢慢来,先给个链接。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href="http://www.eurandom.tue.nl/events/workshops/2010/YESIV/Prog-Abstr_files/Ghahramani-lecture2.pdf"&gt;Introduction to Gaussian Process&lt;/a&gt;
* 今天看自然语言处理Standford公开课的时候看到最大熵模型(Maximum Entropy Models),视频讲的实在太罗嗦了,在网上找了找,下面这个PPT貌似还挺不错的。(原始PPT有部分错误,以下网盘共享文件是部分修正后版本,可能还会有错误,欢迎指出)&lt;/p&gt;
&lt;p&gt;&lt;a href="http://pan.baidu.com/s/1gdze7h5"&gt;Maximum Entropy Model&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;PGM&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;以下给出讲解PGM比较深入浅出的一系列Lecture Slides。&lt;/p&gt;
&lt;h2&gt;PART I:Introduction to PGM&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-1-Introduction-Combined.pdf"&gt;Introduction and Overview&lt;/a&gt;;主要介绍了PGM的背景以及Factor的基础知识。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Bayes-Nets.pdf"&gt;Bayesian Network Fundamentals&lt;/a&gt;;简要介绍了什么是Bayesian Network、Reasoning Patterns以及Influence Flow.最后简要介绍了一下Naive Bayes Classifier.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Naive Bayes Classifier" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/naive_bayes_model_zps09771da2.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Template-Models.pdf"&gt;Template Models&lt;/a&gt;;主要介绍了Template Models,包括Bayesian Network(HMM)以及Plate Models;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-CPDs.pdf"&gt;Structured CPDs&lt;/a&gt;;介绍了几种CPD表示的其他常见形式,包括:&lt;ul&gt;
&lt;li&gt;Deterministic CPDs&lt;/li&gt;
&lt;li&gt;Tree-structured CPDs&lt;/li&gt;
&lt;li&gt;Logistic CPDs &amp;amp; generalizations&lt;/li&gt;
&lt;li&gt;Noisy OR/AND&lt;/li&gt;
&lt;li&gt;Linear Gaussian &amp;amp; generalizations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Markov-Nets.pdf"&gt;Markov Network Fundamentals&lt;/a&gt;;本部分涵盖的内容有Markov Network,General Gibbs Distribution,CRF,Log-Linear Models.(&lt;strong&gt;Logistic Models is a simple CRF;CRF does not need to concern about the correlation between features!&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;PART II:PGM Inference&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Variable-Elimination.pdf"&gt;Variable Elimination&lt;/a&gt;;简要介绍了如何在Bayesian Network以及Ｍarkov Network中执行VE算法;接着对其复杂度进行了分析;最后从图的视角重新审视了一下VE算法.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Variable Elimination" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/variable_elimination_zps6fdc76a6.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Belief-Propagation.pdf"&gt;Belief Propagation(I)&lt;/a&gt; &amp;amp; &lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Belief-Propagation.pdf"&gt;Belief Propagation(II)&lt;/a&gt;;其基本内容如下:&lt;ul&gt;
&lt;li&gt;Belief Propagation算法基本流程;&lt;/li&gt;
&lt;li&gt;Cluster Graph的基本性质(&lt;code&gt;BP does poorly when we have strong correlations!&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;BP算法的基本性质;&lt;/li&gt;
&lt;li&gt;Clique Tree Algorithm.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Belief Propagation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/belief_propogation_zps866416cc.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-MAP.pdf"&gt;MAP Estimation&lt;/a&gt;;关于MAP Inference的基础知识。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Sampling.pdf"&gt;Sampling Methods&lt;/a&gt;;Basic Sampling Methods.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;PART III:PGM Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Parameters-Part-1.pdf"&gt;Learning: Parameter Estimation, Part 1&lt;/a&gt; &amp;amp; &lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Parameters-Part-2.pdf"&gt;Learning: Parameter Estimation, Part 2&lt;/a&gt;;Parameter Estimation for BN and MN;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-BN-Structures.pdf"&gt;Structure Learning&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Incomplete-Data.pdf"&gt;Learning With Incomplete Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:该课程网址见&lt;a href="https://class.coursera.org/pgm-003"&gt;PGM&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ising Model&lt;/li&gt;
&lt;li&gt;Dual Decomposition&lt;/li&gt;
&lt;li&gt;Decision Making&lt;/li&gt;
&lt;li&gt;Bayesian Scores&lt;/li&gt;
&lt;li&gt;Learning With Incomplete Data&lt;/li&gt;
&lt;li&gt;Lassos&lt;/li&gt;
&lt;li&gt;凸QP&lt;/li&gt;
&lt;li&gt;Duality&lt;/li&gt;
&lt;li&gt;KKT条件&lt;/li&gt;
&lt;li&gt;支持向量机番外篇I:&lt;a href="http://blog.pluskid.org/?p=702"&gt;支持向量机：Duality&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;支持向量机番外篇II:&lt;a href="http://blog.pluskid.org/?p=723"&gt;支持向量机：Kernel II&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Apriori算法细节&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1"&gt;&lt;a href="https://class.coursera.org/nlp/lecture"&gt;Natural Language Processing&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="算法"></category><category term="Fun"></category><category term="Staff"></category><category term="收藏夹"></category><category term="Bloom Filter"></category><category term="B Trees"></category><category term="Data Structure"></category><category term="Algorithm"></category><category term="PGM"></category></entry><entry><title>自然语言处理(序章):我爱自然语言处理</title><link href="http://www.qingyuanxingsi.com/zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li.html" rel="alternate"></link><updated>2014-05-04T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-04:zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li.html</id><summary type="html">&lt;p&gt;昨天浏览了一下&lt;a href="http://www.52nlp.cn"&gt;我爱自然语言处理&lt;/a&gt;站点上的全部文章,然后基本过滤下来自己感兴趣的90篇左右的文章,这一阵子就先把这90篇文章认认真真看完吧,总结看的过程中自己看兴趣的点,遂成此文。&lt;strong&gt;本文中所有资料属我爱自然语言处理及博客原文引用作者所有,特此声明&lt;/strong&gt;。&lt;/p&gt;
&lt;h1&gt;齐夫定律(Zipf’s Law)&lt;/h1&gt;
&lt;hr /&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Zipf's Law&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;在任何一个自然语言里第$n$个最常用的单词的频率与$n$近似成反比(The frequency of use of the nth-most-frequently-used word in any natural language is approximately inversely proportional to n).更正式地,我们可以说:存在一个常量$k$,使得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
f \times r =k
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$f$表示单词出现的频度,$r$表示单词出现次数的排名(RANK).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img alt="Zipf" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/zipf_zpsae557119.png" /&gt;&lt;/p&gt;
&lt;p&gt;北京大学姜望琪老师的《Zipf与省力原则》讲得很好，部分摘录如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;省力原则(the Principle of Least Effort)，又称经济原则(the Economy Principle)，可以概括为：以最小的代价换取最大的收益。这是指导人类行为的一条根本性原则。在现代学术界，第一个明确提出这条原则的是美国学者 George Kingsley Zipf。　　&lt;/li&gt;
&lt;li&gt;George Kingsley Zipf1902年1月出生于一个德裔家庭（其祖父十九世纪中叶移居美国)。1924年，他以优异成绩毕业于哈佛学院。1925年在德国波恩、柏林学习。1929年完成Relative Frequency as a Determinant of Phonetic Change，获得哈佛比较语文学博士学位。然后，他开始在哈佛教授德语。1931年与Joyce Waters Brown结婚。1932年出版Selected Studies of the Principle of Relative Frequency in Language。1935年出版The Psycho- Biology of Language：An Introduction to Dynamic Philology。1939年被聘为讲师。1949年出版Human Behavior and the Principle of Least Effort：An Introduction to Human Ecology。1950年9月因患癌症病逝。　　&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zipf在1949年的书里提出了一条指导人类行为的基本原则——省力原则。Zipf在序言里指出，如果我们把人类行为纯粹看作一种自然现象，如果我们像研究蜜蜂的社会行为、鸟类的筑巢习惯一样研究人类行为，那么，我们就有可能揭示其背后的基本原则。这是他提出“省力原则”的大背景。当Zipf在众多互不相干的现象里都发现类似Zipf定律的规律性以后，他就开始思考造成这种规律性的原因。这是导致他提出“省力原则”的直接因素。在开始正式论证以前，Zipf首先澄清了“省力原则”的字面意义。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;第一，这是一种平均量。一个人一生要经历很多事情，他在一件事情上的省力可能导致在另一件事情上的费力。反过来，在一件事情上的费力，又可能导致在另一件事情上的省力。&lt;/li&gt;
&lt;li&gt;第二，这是一种概率。一个人很难在事先百分之百地肯定某种方法一定能让他省力，他只能有一个大概的估计。因为用词研究是理解整个言语过程的关键，而后者又是理解整个人类生态学的关键，他的具体论证从用词经济开始。Zipf认为，用词经济可以从两个角度来讨论：说话人的角度和听话人的角度。从说话人的角度看，用一个词表达所有的意义是最经济的。这样，说话人不需要花费气力去掌握更多的词汇，也不需要考虑如何从一堆词汇中选择一个合适的词。这种“单一词词汇量”就像木工的一种多用工具，集锯刨钻锤于一身，可以满足多种用途。但是，从听话人角度看，这种“单一词词汇量”是最费力的。他要决定这个词在某个特定场合到底是什么意思，而这几乎是不可能的。相反，对听话人来说，最省力的是每个词都只有一个意义，词汇的形式和意义之间完全一一对应。这两种经济原则是互相冲突、互相矛盾的。Zipf把它们叫做一条言语流中的两股对立的力量：“单一化力量”（the Force of Unification）和“多样化力量”（the Force of Diversification）。他认为，这两股力量只有达成妥协，达成一种平衡，才能实现真正的省力。事实正像预计的那样。请看Zipf的论证：假如只有单一化力量，那么任何语篇的单词数量（number）都会是1，而它的出现次数（frequency）会是100%。另一方面，假如只有多样化力量，那么每个单词的出现次数都会接近1，而单词总数量则由语篇的长度决定。这就是说， &lt;em&gt;number&lt;/em&gt;和&lt;em&gt;frequency&lt;/em&gt;是衡量词汇平衡程度的两个参数。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="NLP"></category></entry><entry><title>当最近邻遇到LSH</title><link href="http://www.qingyuanxingsi.com/dang-zui-jin-lin-yu-dao-lsh.html" rel="alternate"></link><updated>2014-05-01T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-01:dang-zui-jin-lin-yu-dao-lsh.html</id><summary type="html">&lt;p&gt;貌似感冒了,脑子昏昏沉沉的,啥都想不了,无意中发现这么一个高端大气上档次的算法---局部敏感哈希方法。于是Google了一下,发现这篇&lt;a href="http://www.strongczq.com/2012/04/locality-sensitive-hashinglsh%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%8A%95%E5%BD%B1%E6%B3%95.html"&gt;Locality Sensitive Hashing(LSH)之随机投影法&lt;/a&gt;关于局部敏感哈希算法的介绍还不错,于是摘录如下:&lt;/p&gt;
&lt;h1&gt;概述&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;LSH&lt;/strong&gt;是由文献[1]提出的一种用于高效求解最近邻搜索问题的Hash算法。LSH算法的基本思想是利用一个hash函数把集合中的元素映射成hash值，使得相似度越高的元素hash值相等的概率也越高。LSH算法使用的关键是针对某一种相似度计算方法，找到一个具有以上描述特性的hash函数。LSH所要求的hash函数的准确数学定义比较复杂，以下给出一种通俗的定义方式：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对于集合$S$，集合内元素间相似度的计算公式为$sim(a,b)$。如果存在一个hash函数$h()$满足以下条件：存在一个相似度$s$到概率$p$的单调递增映射关系，使得$S$中的任意两个满足$sim(a,b)\geq s$的元素$a$和$b$，$h(a)=h(b)$的概率大于等于$p$。那么$h()$就是该集合的一个LSH算法hash函数。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一般来说在最近邻搜索中，元素间的关系可以用相似度或者距离来衡量。如果用距离来衡量，那么距离一般与相似度之间存在单调递减的关系。以上描述如果使用距离来替代相似度需要在单调关系上做适当修改。&lt;/p&gt;
&lt;p&gt;根据元素相似度计算方式的不同，LSH有许多不同的hash算法。两种比较常见的hash算法是&lt;strong&gt;随机投影法&lt;/strong&gt;和min-hash算法。本文即将介绍的随机投影法适用于集合元素可以表示成向量的形式，并且相似度计算是基于向量之间夹角的应用场景，如余弦相似度。min-hash法在参考文献[2]中有相关介绍。&lt;/p&gt;
&lt;h1&gt;随机投影法(Random projection)&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;假设集合$S$中的每个元素都是一个$n$维的向量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\vec{x} ={v_1,v_2,\cdots,v_n}
\end{equation}&lt;/p&gt;
&lt;p&gt;集合中两个元素$\vec{v}$和$\vec{u}$之间的相似度定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
sim(\vec{v},\vec{u})=\frac{\vec{v}*\vec{u}}{|\vec{v}||\vec{u}|}
\end{equation}&lt;/p&gt;
&lt;p&gt;对于以上元素集合$S$的随机投影法hash函数$h()$可以定义为如下：&lt;/p&gt;
&lt;p&gt;在$n$维空间中随机选取一个非零向量$\vec{x}={x_1, x_2, \ldots, x_n}$。考虑以该向量为法向量且经过坐标系原点的超平面，该超平面把整个$n$维空间分成了两部分，将法向量所在的空间称为正空间，另一空间为负空间。那么集合$S$中位于正空间的向量元素hash值为1，位于负空间的向量元素hash值为0。判断向量属于哪部分空间的一种简单办法是判断向量与法向量之间的夹角为锐角还是钝角，因此具体的定义公式可以写为&lt;/p&gt;
&lt;p&gt;&lt;img alt="Formula 1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/formula_2_zpsd4647f5a.png" /&gt;&lt;/p&gt;
&lt;p&gt;根据以上定义，假设向量$\vec{v}$和$\vec{u}$之间的夹角为$\theta$，由于法向量$\vec{x}$是随机选取的，那么这两个向量未被该超平面分割到两侧（即hash值相等）的概率应该为：$p(\theta)=1-\frac{\theta}{\pi}$。假设两个向量的相似度值为$s$，那么根据$\theta=arccos(s)$,有&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(s)=1-\frac{arccos(s)}{\pi}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此，存在相似度$s$到概率$p$的单调递增映射关系，使得对于任意相似度大于等于$s$的两个元素，它们hash值相等的概率大于等于$p(s)$。所以，以上定义的hash值计算方法符合LSH算法的要求。&lt;/p&gt;
&lt;p&gt;以上所描述的$h()$函数虽然符合LSH算法的要求，但是实用性不高。因为该hash函数只产生了两个hash值，没有达到hash函数将元素分散到多个分组的目的。为了增加不同hash值的个数，可以多次生成独立的函数$h()$，只有当两个元素的多个$h()$值都相等时才算拥有相同的hash值。根据该思路可以定义如下的hash函数$H()$：&lt;/p&gt;
&lt;p&gt;\begin{equation}
H(\vec{v})=(h_b(\vec{v})h_{b-1}(\vec{v})\ldots h_1(\vec{v}))_2
\end{equation}&lt;/p&gt;
&lt;p&gt;其中每个$h_i(\vec{v})$表示一个独立的$h()$函数，$H()$函数值的二进制表现形式中每一位都是一个$h()$函数的结果。
以$H()$为hash函数的话，两个相似度为$s$的元素具有相同hash值的概率公式为&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(s)=(1-\frac{arccos(s)}{\pi})^b
\end{equation}&lt;/p&gt;
&lt;p&gt;hash值的个数为$2^b$。很容易看出$H()$函数同样也是符合LSH算法要求的。一般随机按投影算法选用的hash函数就是$H()$。其中参数$b$的取值会在后面小节中讨论。&lt;/p&gt;
&lt;h1&gt;随机投影法在最近邻搜索中的应用&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;最近邻搜索&lt;/h2&gt;
&lt;p&gt;最近邻搜索可以简单的定义为：对于$m$个元素的集合$T$，为一个待查询元素$q$找到集合中相似度最高的$k$个元素。&lt;/p&gt;
&lt;p&gt;最近邻搜索最简单的实现方法为：计算$q$与集合$T$中每一个元素的相似度，使用一个具有$k$个元素的大顶堆（优先队列）保存相似度计算结果（相似度值为key）。这种实现方法每一次查询都要遍历整个集合$T$来计算相似度，当$m$很大并且查询的频率很高的时候这种暴力搜索的方法无法满足性能要求。&lt;/p&gt;
&lt;p&gt;当最近邻搜索的近邻要求并不是那么严格的时候，即允许top k近邻的召回率不一定为1（但是越高越好），那么可以考虑借助于LSH算法。&lt;/p&gt;
&lt;h2&gt;随机投影法提高执行速度&lt;/h2&gt;
&lt;p&gt;这里我们介绍当集合$T$的元素和查询元素$q$为同维度向量(维度为$n$)，并且元素相似度计算方法为余弦相似度时，使用随机投影法来提高最近邻搜索的执行速度。具体的实现方法为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;预处理阶段&lt;/strong&gt;:使用hash函数$H(*)$计算集合$T$中所有元素的hash值，将集合$T$分成一个个分组，每个分组内的元素hash值均相等。用合适的数据结构保存这些hash值到分组的映射关系（如HashMap）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;查询阶段&lt;/strong&gt;:计算查询元素$q$的hash值$H(q)$，取集合$T$中所有hash值为$H(q)$的分组，以该分组内的所有元素作为候选集合，在候选该集合内使用简单的最近邻搜索方法寻找最相似的$k$个元素。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;该方法的执行效率取决于$H(*)$的hash值个数$2^b$，也就是分组的个数。理想情况下，如果集合$T$中的向量元素在空间中分布的足够均匀，那么每一个hash值对应的元素集合大小大致为$\frac{m} {2^b}$。当$m$远大于向量元素的维度时，每次查询的速度可以提高到$2^b$倍。&lt;/p&gt;
&lt;p&gt;根据以上分析$H()$中$b$的取值越大算法的执行速度的提升越多，并且是指数级别的提升。但是，在这种情况下$H()$函数下的概率公式$p(s)$，&lt;strong&gt;实际上表示与查询元素$q$的相似度为$s$的元素的召回率&lt;/strong&gt;。当$b$的取值越大时，top k元素的召回率必然会下降。因此算法执行速度的提升需要召回率的下降作为代价。例如：当$b$等于10时，如果要保证某个元素的召回率不小于0.9，那么该元素与查询元素$q$的相似度必须不小于0.9999998。&lt;/p&gt;
&lt;h2&gt;提高召回率改进&lt;/h2&gt;
&lt;p&gt;为了在保证召回率的前提下尽可能提高算法的执行效率，一般可以进行如下改进：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;预处理阶段&lt;/strong&gt;:生成$t$个独立的hash函数$H_i(∗)$，根据这$t$个不同的hash函数，对集合$T$进行$t$种不同的分组，每一种分组方式下，同一个分组的元素在对应hash函数下具有相同的hash值。用合适的数据结构保存这些映射关系（如使用$t$个HashMap来保存）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;查询阶段&lt;/strong&gt;:对于每一个hash函数$H_i(∗)$，计算查询元素$q$的hash值$H_i(q)$，将集合$T$中$H_i(∗)$所对应的分组方式下hash值为$H_i(q)$的分组添加到该次查询的候选集合中。然后，在该候选集合内使用简单的最近邻搜索方法寻找最相似的$k$个元素。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上改进使得集合中元素与查询元素$q$的$t$个hash值中，只要任意一个相等，那么该集合元素就会被加入到候选集中。那么，相似度为$s$的元素的召回率为&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(s)=1-(1-(1-\frac{arccos(s)}{\pi})^b)^t
\end{equation}&lt;/p&gt;
&lt;p&gt;在执行效率上，预处理阶段由于需要计算$t$个hash函数的值，所以执行时间上升为$t$倍。查询阶段，如果单纯考虑候选集合大小对执行效率的影响，在最坏的情况下，$t$个hash值获得的列表均不相同，候选集集合大小的期望值为$\frac{t∗m}{2^b}$，查询速度下降至$1 \over t$，与简单近邻搜索相比查询速度提升为$\frac{2^b}{t}$倍。&lt;/p&gt;
&lt;p&gt;下图是召回率公式$p(s)=1-(1-(1-\frac{arccos(s)}{\pi})^b)^t$在不同的$b$和$t$取值下的$s-p$曲线。我们通过这些曲线来分析这里引入参数$t$的意义。4条蓝色的线以及最右边红色的线表示当$t$取值为1（相当于没有引入$t$），而$b$的取值从1变化到5的过程，从图中可以看出随着$b$的增大，不同相似度下的召回率都下降的非常厉害，特别的，当相似度接近1时曲线的斜率很大，也就说在高相似度的区域，召回率对相似度的变化非常敏感。10条红色的线从右到左表示$b$的取值为5不变，$t$的取值从1到10的过程，从图中可以看出，随着$t$的增大，曲线的形状发生了变化，高相似度区域的召回率变得下降的非常平缓，而最陡峭的地方渐渐的被移动到相对较低的相似度区域。因此，从以上曲线的变化特点可以看出，引入适当的参数$t$使得高相似度区域在一段较大的范围内仍然能够保持很高的召回率从而满足实际应用的需求。&lt;/p&gt;
&lt;p&gt;&lt;img alt="SP Curve" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sp_curve_zpse1b5dbe6.png" /&gt;&lt;/p&gt;
&lt;h2&gt;参数选取&lt;/h2&gt;
&lt;p&gt;根据以上分析，$H(*)$函数的参数$b$越大查询效率越高，但是召回率越低；参数$t$越大查询效率越低但是召回率越高。因此选择适当参数$b$和$t$来折中查询效率与召回率之间的矛盾是应用好随机投影法的关键。下面提供一种在实际应用中选取$b$和$t$的参考方法。&lt;/p&gt;
&lt;p&gt;根据实际应用的需要确定一对$(s,p)$，表示相似度大于等于$s$的元素，召回率的最低要求为$p$。然后将召回率公式表示成$b-t$之间的函数关系$t=\log_{1-(1-\frac{acos(s)}{pi})^b}{(1-p)}$。根据$(s,p)$的取值，画出$b-t$的关系曲线。如$s=0.8,p=0.95$时的$b-t$曲线如下图所示。考虑具体应用中的实际情况，在该曲线上选取一组使得执行效率可以达到最优的$(b,t)$组合。&lt;/p&gt;
&lt;p&gt;&lt;img alt="BT_Curve" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bt_curve_zps5aba4948.png" /&gt;&lt;/p&gt;
&lt;h2&gt;关于最近邻文本搜索&lt;/h2&gt;
&lt;p&gt;在最近邻文本搜索中，一般待检索的文本或查询文本，都已被解析成一系列带有权重的关键词，然后通过余弦相似度公式计算两个文本之间的相似度。这种应用场景下的最近邻搜索与以上所提到的最近邻搜索问题相比存在以下两个特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果把每个文本的带权重关键词表都看作是一个向量元素的话，每个关键词都是向量的一个维度，关键词权重为该维度的值。理论上可能关键词的个数并不确定（所有单词的组合都可能是一个关键词），因此该向量元素的维数实际上是不确定的。&lt;/li&gt;
&lt;li&gt;由于关键词权重肯定是大于零的，所以向量元素的每一个维度的值都是非负的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于第一个特点，我们需要选取一个包含$n$个关键词的关键词集合，在进行文本相似度计算时只考虑属于该集合的关键词。也就是说，每一个文本都视为是一个$n$维度的向量，关键词权重体现为对应维度的值。该关键词集合可以有很多种生成办法，比如可以是网站上具有一定搜索频率的关键词集合，总的来说该关键词集合应当能够涵盖所有有意义并且具有一定使用频率的关键词。通常$n$的取值会比较大，如几十万到几百万，由于在使用随机投影算法时，每一个生成的随机向量维度都为$n$，这种情况下需要特别考虑利用这些高维随机向量对执行效率造成的影响，在确定$b、t$参数时需要考虑到这方面的影响。&lt;/p&gt;
&lt;p&gt;对于第二个特点，由于向量元素各维度值都非负，那么这些元素在高维空间中只会出现在特定的区域中。比如当$n$为3时，只会出现在第一象限中。一个直观的感觉是在生成随机向量的时候，会不会生成大量的无用切割平面（与第一个象限空间不相交，使得所有元素都位于切割平面的同侧）。这些切割平面对应的$H(*)$函数hash值中的二进制位恒定为1或者0，对于提高算法执行速度没有帮助。以下说明这种担心是没有必要的：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;切割平面与第一象限空间不相交等价于其法向量的每一个维度值都有相同的符号（都为正或者负），否则总能在第一象限空间中找到两个向量与法向量的乘积符号不同，也就是在切割平面的两侧。那么，随机生成的n维向量所有维度值都同号的概率为$\frac{1}{2^{n−1}}$，当$n$的取值很大时，该概率可以忽略不计。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;[1] P. Indyk and R. Motwani. Approximate Nearest Neighbor:Towards Removing the Curse of Dimensionality. In Proc. of the 30th Annual ACM Symposium on Theory of Computing, 1998, pp. 604–613.&lt;/p&gt;
&lt;p&gt;[2] Google News Personalization: Scalable Online Collaborative Filtering.&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="算法"></category><category term="Algorithm"></category><category term="LSH"></category><category term="局部敏感哈希算法"></category></entry><entry><title>Play With Cardinality Estimation</title><link href="http://www.qingyuanxingsi.com/play-with-cardinality-estimation.html" rel="alternate"></link><updated>2014-04-14T00:00:00+08:00</updated><author><name>CodingLabs</name></author><id>tag:www.qingyuanxingsi.com,2014-04-14:play-with-cardinality-estimation.html</id><summary type="html">&lt;p&gt;记得之前某周例会的时候一个博士师兄抛出一个小问题:在大数据环境下,如何估计一个可能含有重复元素的集合中不同元素的数目,当时其实没有怎么在意。这两天因为看CNN的东西实在无法完全理解,所以到处逛了逛(&lt;code&gt;好吧,我每次逛了逛都能发现特别好玩的算法呀&lt;/code&gt;),于是不经意间发现了解决上述问题的一些现有算法,很是高兴呀。&lt;/p&gt;
&lt;p&gt;在开始今天的相关介绍之前,咱们扯点闲话吧,个人不是特别喜欢纯科研的科研,如果一个算法或者一个数据结构以至于一个理论不能应用到实际生活中去,不能解决实际生活中的某个问题的话,个人认为这种理论或者算法/数据结构的研究就是无意义的。个人还是比较倾向于好玩的科研吧,一方面研究的东西自己觉得有意思,另一方面又能应用到实际项目或生活实际中去,成为一个研究好玩问题的研究人员估计就是我毕生最大的志向了吧,呵呵。好吧,其实说这么多只是为了说明基数估计这个东西真的很好玩呀。(&lt;strong&gt;以后只要在Pearls目录下的博文均收集自他人博客,原始链接见脚注,版权属于原作者所有,无意侵犯,特此说明,以后不再说明&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;言归正传,开始我们正式的介绍。&lt;/p&gt;
&lt;h1&gt;基本概念&lt;sup id="sf-play-with-cardinality-estimation-1-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-1" title="解读Cardinality Estimation算法（第一部分：基本概念）"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;基数计数(&lt;strong&gt;Cardinality Counting&lt;/strong&gt;）是实际应用中一种常见的计算场景，在数据分析、网络监控及数据库优化等领域都有相关需求。精确的基数计数算法由于种种原因，在面对大数据场景时往往力不从心，因此如何在误差可控的情况下对基数进行估计就显得十分重要。目前常见的基数估计算法有Linear Counting、LogLog Counting、HyperLogLog Counting及Adaptive Counting等。这几种算法都是基于概率统计理论所设计的概率算法，它们克服了精确基数计数算法的诸多弊端（如内存需求过大或难以合并等），同时可以通过一定手段将误差控制在所要求的范围内。&lt;/p&gt;
&lt;p&gt;以下我们主要介绍一下基数估计(Cardinality Estimation)的基本概念。&lt;/p&gt;
&lt;h2&gt;基数的定义&lt;/h2&gt;
&lt;p&gt;简单来说，基数（Cardinality，也译作势），是指一个集合（这里的集合允许存在重复元素，与集合论对集合严格的定义略有不同，如不做特殊说明，本文中提到的集合均允许存在重复元素）中不同元素的个数。例如看下面的集合：&lt;/p&gt;
&lt;p&gt;${1,2,3,4,5,2,3,9,7}$&lt;/p&gt;
&lt;p&gt;这个集合有9个元素，但是2和3各出现了两次，因此不重复的元素为1,2,3,4,5,9,7，所以这个集合的基数是7。&lt;/p&gt;
&lt;p&gt;如果两个集合具有相同的基数，我们说这两个集合等势。基数和等势的概念在有限集范畴内比较直观，但是如果扩展到无限集则会比较复杂，一个无限集可能会与其真子集等势（例如整数集和偶数集是等势的）。不过在这个系列文章中，我们仅讨论有限集的情况，关于无限集合基数的讨论，有兴趣的同学可以参考实变分析相关内容。&lt;/p&gt;
&lt;p&gt;容易证明，如果一个集合是有限集，则其基数是一个自然数。&lt;/p&gt;
&lt;h2&gt;基数的应用实例&lt;/h2&gt;
&lt;p&gt;下面通过一个实例说明基数在电商数据分析中的应用。&lt;/p&gt;
&lt;p&gt;假设一个淘宝网店在其店铺首页放置了10个宝贝链接，分别从Item01到Item10为这十个链接编号。店主希望可以在一天中随时查看从今天零点开始到目前这十个宝贝链接分别被多少个独立访客点击过。所谓独立访客（&lt;code&gt;Unique Visitor，简称UV&lt;/code&gt;）是指有多少个自然人，例如，即使我今天点了五次Item01，我对Item01的UV贡献也是1，而不是5。&lt;/p&gt;
&lt;p&gt;用术语说这实际是一个实时数据流统计分析问题。&lt;/p&gt;
&lt;p&gt;要实现这个统计需求。需要做到如下三点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;对独立访客做标识&lt;/li&gt;
&lt;li&gt;在访客点击链接时记录下链接编号及访客标记&lt;/li&gt;
&lt;li&gt;对每一个要统计的链接维护一个数据结构和一个当前UV值，当某个链接发生一次点击时，能迅速定位此用户在今天是否已经点过此链接，如果没有则此链接的UV增加1&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面分别介绍三个步骤的实现方案&lt;/p&gt;
&lt;h3&gt;对独立访客做标识&lt;/h3&gt;
&lt;p&gt;客观来说，目前还没有能在互联网上准确对一个自然人进行标识的方法，通常采用的是近似方案。例如通过登录用户+cookie跟踪的方式：当某个用户已经登录，则采用会员ID标识；对于未登录用户，则采用跟踪cookie的方式进行标识。为了简单起见，我们假设完全采用跟踪cookie的方式对独立访客进行标识。&lt;/p&gt;
&lt;h3&gt;记录链接编号及访客标记&lt;/h3&gt;
&lt;p&gt;这一步可以通过Javascript埋点及记录accesslog完成，具体原理和实现方案可以参考博文&lt;a href="http://blog.codinglabs.org/articles/how-web-analytics-data-collection-system-work.html"&gt;网站统计中的数据收集原理及实现&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;实时UV计算&lt;/h3&gt;
&lt;p&gt;可以看到，如果将每个链接被点击的日志中访客标识字段看成一个集合，那么此链接当前的UV也就是这个集合的基数，因此UV计算本质上就是一个基数计数问题。&lt;/p&gt;
&lt;p&gt;在实时计算流中，我们可以认为任何一次链接点击均触发如下逻辑（伪代码描述）：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;cand_counting&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;item_no&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;user_id&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;item_no&lt;/span&gt; &lt;span class="n"&gt;visitor&lt;/span&gt; &lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;add&lt;/span&gt; &lt;span class="n"&gt;user_id&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;item_no&lt;/span&gt; &lt;span class="n"&gt;visitor&lt;/span&gt; &lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;cand&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;item_no&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;逻辑非常简单，每当有一个点击事件发生，就去相应的链接被访集合中寻找此访客是否已经在里面，如果没有则将此用户标识加入集合，并将此链接的UV加1。&lt;/p&gt;
&lt;p&gt;虽然逻辑非常简单，但是在实际实现中尤其面临大数据场景时还是会遇到诸多困难，下面一节我会介绍两种目前被业界普遍使用的精确算法实现方案，并通过分析说明当数据量增大时它们面临的问题。&lt;/p&gt;
&lt;h2&gt;传统的基数计数实现&lt;/h2&gt;
&lt;p&gt;接着上面的例子，我们看一下目前常用的基数计数的实现方法。&lt;/p&gt;
&lt;h3&gt;基于B树的基数计数&lt;/h3&gt;
&lt;p&gt;对上面的伪代码做一个简单分析，会发现关键操作有两个：查找-迅速定位当前访客是否已经在集合中，插入-将新的访客标识插入到访客集合中。因此，需要为每一个需要统计UV的点（此处就是十个宝贝链接）维护一个查找效率较高的数据结构，又因为实时数据流的关系，这个数据结构需要尽量在内存中维护，因此这个数据结构在空间复杂度上也要比较适中。综合考虑一种传统的做法是在实时计算引擎采用了B树来组织这个集合。下图是一个示意图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="B Tree" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/B_tree_zpsda8ce41d.png"&gt;&lt;/p&gt;
&lt;p&gt;之所以选用B树是因为B树的查找和插入相关高效，同时空间复杂度也可以接受（关于B树具体的性能分析请参考&lt;a href="http://en.wikipedia.org/wiki/B-tree"&gt;B_Tree&lt;/a&gt;）。&lt;/p&gt;
&lt;p&gt;这种实现方案为一个基数计数器维护一棵B树，由于B树在查找效率、插入效率和内存使用之间非常平衡，所以算是一种可以接受的解决方案。但是当数据量特别巨大时，例如要同时统计几万个链接的UV，如果要将几万个链接一天的访问记录全部维护在内存中，这个内存使用量也是相当可观的（假设每个B树占用1M内存，10万个B树就是100G！）。一种方案是在某个时间点将内存数据结构写入磁盘（双十一和双十二大促时一淘数据部的效果平台是每分钟将数据写入HBase）然后将内存中的计数器和数据结构清零，但是B树并不能高效的进行合并，这就使得内存数据落地成了非常大的难题。&lt;/p&gt;
&lt;p&gt;另一个需要数据结构合并的场景是查看并集的基数，例如在上面的例子中，如果我想查看Item1和Item2的总UV，是没有办法通过这种B树的结构快速得到的。当然可以为每一种可能的组合维护一棵B树。不过通过简单的分析就可以知道这个方案基本不可行。N个元素集合的非空幂集数量为2N−1，因此要为10个链接维护1023棵B树，而随着链接的增加这个数量会以幂指级别增长。&lt;/p&gt;
&lt;h3&gt;基于Bitmap的基数计数&lt;/h3&gt;
&lt;p&gt;为了克服B树不能高效合并的问题，一种替代方案是使用Bitmap表示集合。也就是使用一个很长的bit数组表示集合，将bit位顺序编号，bit为1表示此编号在集合中，为0表示不在集合中。例如“00100110”表示集合 {2，5，6}。bitmap中1的数量就是这个集合的基数。&lt;/p&gt;
&lt;p&gt;显然，与B树不同Bitmap可以高效的进行合并，只需进行按位或（or）运算就可以，而位运算在计算机中的运算效率是很高的。但是Bitmap方式也有自己的问题，就是内存使用问题。&lt;/p&gt;
&lt;p&gt;很容易发现，Bitmap的长度与集合中元素个数无关，而是与基数的上限有关。例如在上面的例子中，假如要计算上限为1亿的基数，则需要12.5M字节的Bitmap，十个链接就需要125M。关键在于，这个内存使用与集合元素数量无关，即使一个链接仅仅有一个1UV，也要为其分配12.5M字节。&lt;/p&gt;
&lt;p&gt;由此可见，虽然Bitmap方式易于合并，却由于内存使用问题而无法广泛用于大数据场景。&lt;/p&gt;
&lt;h1&gt;Linear Counting&lt;sup id="sf-play-with-cardinality-estimation-2-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-2" title="解读Cardinality Estimation算法（第二部分：Linear Counting）"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;通过上面的介绍我们知道传统的精确基数计数算法在数据量大时会存在一定瓶颈，瓶颈主要来自于&lt;strong&gt;数据结构合并和内存使用&lt;/strong&gt;两个方面。因此出现了很多基数估计的概率算法，这些算法虽然计算出的结果不是精确的，但误差可控，重要的是这些算法所使用的数据结构易于合并，同时比传统方法大大节省内存。&lt;/p&gt;
&lt;p&gt;作为本文的第二部分，我们讨论Linear Counting算法。&lt;/p&gt;
&lt;h2&gt;简介&lt;/h2&gt;
&lt;p&gt;Linear Counting（以下简称LC）在1990年的一篇论文“A linear-time probabilistic counting algorithm for database applications”中被提出。作为一个早期的基数估计算法，LC在空间复杂度方面并不算优秀，实际上LC的空间复杂度与上文中简单Bitmap方法是一样的（但是有个常数项级别的降低），都是$O(N_{max})$，因此目前很少单独使用LC。不过作为Adaptive Counting等算法的基础，研究一下LC还是比较有价值的。&lt;/p&gt;
&lt;h2&gt;基本算法&lt;/h2&gt;
&lt;h3&gt;思路&lt;/h3&gt;
&lt;p&gt;LC的基本思路是：设有一哈希函数$H$，其哈希结果空间有$m$个值（最小值$0$，最大值$m-1$），并且哈希结果服从均匀分布。使用一个长度为$m$的Bitmap，每个bit为一个桶，均初始化为0，设一个集合的基数为$n$，此集合所有元素通过$H$哈希到Bitmap中，如果某一个元素被哈希到第$k$个比特并且第$k$个比特为$0$，则将其置为$1$。当集合所有元素哈希完成后，设Bitmap中还有$u$个bit为$0$。则：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=−mlog_u m
\end{equation}&lt;/p&gt;
&lt;p&gt;为$n$的一个估计，且为最大似然估计（MLE）。&lt;/p&gt;
&lt;p&gt;示意图如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="LC Hash" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hash_lc_zpsad47853b.png"&gt;&lt;/p&gt;
&lt;h3&gt;推导及证明&lt;/h3&gt;
&lt;p&gt;由上文对$H$的定义已知$n$个不同元素的哈希值服从独立均匀分布。设$A_j$为事件“经过$n$个不同元素哈希后，第$j$个桶值为0”，则：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(A_j)=(1−{1 \over m})n
\end{equation}&lt;/p&gt;
&lt;p&gt;又每个桶是独立的，则$u$的期望为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(u)=\sum_{j=1}^mP(A_j)=m(1-\frac{1}{m})^n=m((1+\frac{1}{-m})^{-m})^{-n/m}
\end{equation}&lt;/p&gt;
&lt;p&gt;当$n$和$m$趋于无穷大时，其值约为$me^{-{n \over m}}$&lt;/p&gt;
&lt;p&gt;令：&lt;/p&gt;
&lt;p&gt;\begin{equation}
E(u)=me^{-n/m}
\end{equation}&lt;/p&gt;
&lt;p&gt;得：&lt;/p&gt;
&lt;p&gt;\begin{equation}
n=−mlog \frac{E(u)}{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;显然每个桶的值服从参数相同0-1分布，因此$u$服从二项分布。由概率论知识可知，当$n$很大时，可以用正态分布逼近二项分布，因此可以认为当$n$和$m$趋于无穷大时$u$渐进服从正态分布。&lt;/p&gt;
&lt;p&gt;因此$u$的概率密度函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f(x) = {1 \over \sigma\sqrt{2\pi} }\,e^{- {{(x-\mu )^2 \over 2\sigma^2}}}
\end{equation}&lt;/p&gt;
&lt;p&gt;由于我们观察到的空桶数$u$是从正态分布中随机抽取的一个样本，因此它就是$\mu$的最大似然估计（正态分布的期望的最大似然估计是样本均值）。&lt;/p&gt;
&lt;p&gt;又由如下定理：&lt;/p&gt;
&lt;p&gt;设$f(x)$是可逆函数,$\hat{x}$是$x$的最大似然估计，则$f(\hat{x})$是$f(x)$的最大似然估计。
且$-mlog\frac{x}{m}$是可逆函数，则$\hat{n}=-mlog\frac{u}{m}$是$-mlog\frac{E(u)}{m}=n$的最大似然估计。&lt;/p&gt;
&lt;h2&gt;偏差分析&lt;/h2&gt;
&lt;p&gt;下面不加证明给出如下结论：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
Bias(\frac{\hat{n}}{n}) &amp;amp;=E(\frac{\hat{n}}{n})-1=\frac{e^t-t-1}{2n} \\
StdError(\frac{\hat{n}}{n}) &amp;amp;=\frac{\sqrt{m}(e^t-t-1)^{1/2}}{n}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$t=n/m$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:以上结论的推导在“A linear-time probabilistic counting algorithm for database applications”可以找到。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;算法应用&lt;/h2&gt;
&lt;p&gt;在应用LC算法时，主要需要考虑的是Bitmap长度$m$的选择。这个选择主要受两个因素的影响：基数$n$的量级以及容许的误差。这里假设估计基数$n$的量级大约为$N$，允许的误差为$\epsilon$，则$m$的选择需要遵循如下约束。&lt;/p&gt;
&lt;h3&gt;误差控制&lt;/h3&gt;
&lt;p&gt;这里以标准差作为误差。由上面标准差公式可以推出，当基数的量级为$N$，容许误差为$\epsilon$时，有如下限制：&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; \frac{e^t-t-1}{(\epsilon t)^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;将量级和容许误差带入上式，就可以得出$m$的最小值。&lt;/p&gt;
&lt;h3&gt;满桶控制&lt;/h3&gt;
&lt;p&gt;由LC的描述可以看到，如果$m$比$n$小太多，则很有可能所有桶都被哈希到了，此时$u$的值为0，LC的估计公式就不起作用了（变成无穷大）。因此$m$的选择除了要满足上面误差控制的需求外，还要保证满桶的概率非常小。&lt;/p&gt;
&lt;p&gt;上面已经说过，$u$满足二项分布，而当$n$非常大，$p$非常小时，可以用泊松分布近似逼近二项分布。因此这里我们可以认为$u$服从泊松分布（注意，上面我们说$u$也可以近似服从正态分布，这并不矛盾，实际上泊松分布和正态分布分别是二项分布的离散型和连续型概率逼近，且泊松分布以正态分布为极限）：&lt;/p&gt;
&lt;p&gt;当$n、m$趋于无穷大时：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Pr(u=k)=(\frac{\lambda^k}{k!})e^{-\lambda}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Pr(u=0)&amp;lt;e^{-5}=0.007
\end{equation}&lt;/p&gt;
&lt;p&gt;由于泊松分布的方差为$\lambda$，因此只要保证$u$的期望偏离$0$点$\sqrt{5}$的标准差就可以保证满桶的概率不大于$0.7%$。因此可得：&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; 5(e^t-t-1)
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:上式没看懂,望看懂的童鞋不吝赐教！&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;综上所述，当基数量级为$N$，可接受误差为$\epsilon$，则$m$的选取应该遵从&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; \beta(e^t-t-1)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\beta = max(5, 1/(\epsilon t)^2)$&lt;/p&gt;
&lt;p&gt;下图是论文作者预先计算出的关于不同基数量级和误差情况下，$m$的选择表：&lt;/p&gt;
&lt;p&gt;&lt;img alt="m choice" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/m_choice_zpsb78fb0f8.png"&gt;&lt;/p&gt;
&lt;p&gt;可以看出精度要求越高，则Bitmap的长度越大。随着$m$和$n$的增大，$m$大约为$n$的十分之一。因此LC所需要的空间只有传统的Bitmap直接映射方法的1/10，但是从渐进复杂性的角度看，空间复杂度仍为$O(N_{max})$。&lt;/p&gt;
&lt;h3&gt;合并&lt;/h3&gt;
&lt;p&gt;LC非常方便于合并，合并方案与传统Bitmap映射方法无异，都是通过按位或的方式。&lt;/p&gt;
&lt;h1&gt;LogLog Counting&lt;sup id="sf-play-with-cardinality-estimation-3-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-3" title="解读Cardinality Estimation算法（第三部分：LogLog Counting）"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;上一部分介绍的Linear Counting算法相较于直接映射Bitmap的方法能大大节省内存（大约只需后者1/10的内存），但毕竟只是一个常系数级的降低，空间复杂度仍然为$O(N_max)$。而本文要介绍的LogLog Counting却只有$O(log_2(log_2(N_{max})))$。例如，假设基数的上限为1亿，原始Bitmap方法需要12.5M内存，而LogLog Counting只需不到1K内存（640字节）就可以在标准误差不超过4%的精度下对基数进行估计，效果可谓十分惊人。&lt;/p&gt;
&lt;p&gt;本部分将介绍LogLog Counting。&lt;/p&gt;
&lt;h2&gt;简介&lt;/h2&gt;
&lt;p&gt;LogLog Counting（以下简称LLC）出自论文“Loglog Counting of Large Cardinalities”。LLC的空间复杂度仅有$O(log_2(log_2(N_{max})))$，使得通过KB级内存估计数亿级别的基数成为可能，因此目前在处理大数据的基数计算问题时，所采用算法基本为LLC或其几个变种。下面来具体看一下这个算法。&lt;/p&gt;
&lt;h2&gt;基本算法&lt;/h2&gt;
&lt;h3&gt;均匀随机化&lt;/h3&gt;
&lt;p&gt;与LC一样，在使用LLC之前需要选取一个哈希函数$H$应用于所有元素，然后对哈希值进行基数估计。$H$必须满足如下条件（定性的）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$H$的结果具有很好的均匀性，也就是说无论原始集合元素的值分布如何，其哈希结果的值几乎服从均匀分布（完全服从均匀分布是不可能的，D.Knuth已经证明不可能通过一个哈希函数将一组不服从均匀分布的数据映射为绝对均匀分布，但是很多哈希函数可以生成几乎服从均匀分布的结果，这里我们忽略这种理论上的差异，认为哈希结果就是服从均匀分布）。&lt;/li&gt;
&lt;li&gt;$H$的碰撞几乎可以忽略不计。也就是说我们认为对于不同的原始值，其哈希结果相同的概率非常小以至于可以忽略不计。&lt;/li&gt;
&lt;li&gt;$H$的哈希结果是固定长度的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以上对哈希函数的要求是随机化和后续概率分析的基础。后面的分析均认为是针对哈希后的均匀分布数据进行。&lt;/p&gt;
&lt;h3&gt;思想来源&lt;/h3&gt;
&lt;p&gt;下面非正式的从直观角度描述LLC算法的思想来源。&lt;/p&gt;
&lt;p&gt;设$a$为待估集合（哈希后）中的一个元素，由上面对$H$的定义可知，$a$可以看做一个长度固定的比特串（也就是$a$的二进制表示），设$H$哈希后的结果长度为$L$比特，我们将这$L$个比特位从左到右分别编号为$1、2、…、L$：&lt;/p&gt;
&lt;p&gt;&lt;img alt="LLC Structure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/llc_structure_zpsefad7ee9.png"&gt;&lt;/p&gt;
&lt;p&gt;又因为$a$是从服从均与分布的样本空间中随机抽取的一个样本，因此$a$每个比特位服从如下分布且相互独立。&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(x=k)=\left \lbrace
\begin{array}{cc}
0.5 &amp;amp; (k=0) \\ 
0.5 &amp;amp; (k=1)
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;通俗说就是$a$的每个比特位为0和1的概率各为0.5，且相互之间是独立的。&lt;/p&gt;
&lt;p&gt;设$\rho(a)$为$a$的比特串中第一个“1”出现的位置，显然$1≤\rho(a)≤L$，这里我们忽略比特串全为0的情况（概率为$1/2^L$）。如果我们遍历集合中所有元素的比特串，取$\rho_{max}$为所有$\rho(a)$的最大值。&lt;/p&gt;
&lt;p&gt;此时我们可以将$2^{\rho_{max}}$作为基数的一个粗糙估计，即：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n} = 2^{\rho_{max}}
\end{equation}&lt;/p&gt;
&lt;p&gt;下面解释为什么可以这样估计。注意如下事实：&lt;/p&gt;
&lt;p&gt;由于比特串每个比特都独立且服从0-1分布，因此从左到右扫描上述某个比特串寻找第一个“1”的过程从统计学角度看是一个伯努利过程，例如，可以等价看作不断投掷一个硬币（每次投掷正反面概率皆为0.5），直到得到一个正面的过程。在一次这样的过程中，投掷一次就得到正面的概率为$1/2$，投掷两次得到正面的概率是$1/2^2$，…，投掷k次才得到第一个正面的概率为$1/2^k$。&lt;/p&gt;
&lt;p&gt;现在考虑如下两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;进行n次伯努利过程，所有投掷次数都不大于$k$的概率是多少？&lt;/li&gt;
&lt;li&gt;进行n次伯努利过程，至少有一次投掷次数等于$k$的概率是多少？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;首先看第一个问题，在一次伯努利过程中，投掷次数大于$k$的概率为$1/2^k$，即连续掷出$k$个反面的概率。因此，在一次过程中投掷次数不大于$k$的概率为$1−1/2^k$。因此，$n$次伯努利过程投掷次数均不大于$k$的概率为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X \leq k)=(1-1/2^k)^n
\end{equation}&lt;/p&gt;
&lt;p&gt;显然第二个问题的答案是：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X \neq k)=1-(1-1/2^{k-1})^n
\end{equation}&lt;/p&gt;
&lt;p&gt;从以上分析可以看出，当$n \ll 2^k$，$P_n(X \neq k)$的概率几乎为0，同时，当$n \gg k$时，$P_n(X \leq k)$的概率也几乎为0。用自然语言概括上述结论就是：当伯努利过程次数远远小于$2^k$时，至少有一次过程投掷次数等于$k$的概率几乎为0；当伯努利过程次数远远大于$2^k$时，没有一次过程投掷次数大于$k$的概率也几乎为0。&lt;/p&gt;
&lt;p&gt;如果将上面描述做一个对应：一次伯努利过程对应一个元素的比特串，反面对应0，正面对应1，投掷次数$k$对应第一个“1”出现的位置，我们就得到了下面结论：&lt;/p&gt;
&lt;p&gt;设一个集合的基数为$n$，$\rho_{max}$为所有元素中首个“1”的位置最大的那个元素的“1”的位置，如果$n$远远小于$2^{\rho_{max}}$，则我们得到$\rho_{max}$为当前值的概率几乎为0（它应该更小），同样的，如果$n$远远大于$2^{\rho_{max}}$，则我们得到$\rho_{max}$为当前值的概率也几乎为0（它应该更大），因此$2^{\rho_{max}}$可以作为基数$n$的一个粗糙估计。&lt;/p&gt;
&lt;h3&gt;分桶平均&lt;/h3&gt;
&lt;p&gt;上述分析给出了LLC的基本思想，不过如果直接使用上面的单一估计量进行基数估计会由于偶然性而存在较大误差。因此，LLC采用了分桶平均的思想来消减误差。具体来说，就是将哈希空间平均分成$m$份，每份称之为一个桶（bucket）。对于每一个元素，其哈希值的前$k$比特作为桶编号，其中$2^k=m$，而后$L-k$个比特作为真正用于基数估计的比特串。桶编号相同的元素被分配到同一个桶，在进行基数估计时，首先计算每个桶内元素最大的第一个“1”的位置，设为$M[i]$，然后对这$m$个值取平均后再进行估计，即：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=2^{\frac{1}{m}\sum{M[i]}}
\end{equation}&lt;/p&gt;
&lt;p&gt;这相当于物理试验中经常使用的多次试验取平均的做法，可以有效消减因偶然性带来的误差。&lt;/p&gt;
&lt;p&gt;下面举一个例子说明分桶平均怎么做。&lt;/p&gt;
&lt;p&gt;假设$H$的哈希长度为16bit，分桶数$m$定为32。设一个元素哈希值的比特串为“0001001010001010”，由于$m$为32，因此前5个bit为桶编号，所以这个元素应该归入“00010”即2号桶（桶编号从0开始，最大编号为$m-1$），而剩下部分是“01010001010”且显然ρ(01010001010)=2，所以桶编号为“00010”的元素最大的$\rho$即为$M[2]$的值。&lt;/p&gt;
&lt;h3&gt;偏差修正&lt;/h3&gt;
&lt;p&gt;上述经过分桶平均后的估计量看似已经很不错了，不过通过数学分析可以知道这并不是基数n的无偏估计。因此需要修正成无偏估计。这部分的具体数学分析在“Loglog Counting of Large Cardinalities”中，过程过于艰涩这里不再具体详述，有兴趣的朋友可以参考原论文。这里只简要提一下分析框架：&lt;/p&gt;
&lt;p&gt;首先上文已经得出：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X \leq k)=(1-1/2^k)^n
\end{equation}&lt;/p&gt;
&lt;p&gt;因此：&lt;/p&gt;
&lt;p&gt;\begin{equation}
P_n(X = k)=(1-1/2^k)^n - (1-1/2^{k-1})^n
\end{equation}&lt;/p&gt;
&lt;p&gt;这是一个未知通项公式的递推数列，研究这种问题的常用方法是使用生成函数（generating function）。通过运用指数生成函数和poissonization得到上述估计量的Poisson期望和方差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\varepsilon _n&amp;amp;\sim [(\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^m+\epsilon _n]n \\
\nu _n&amp;amp;\sim [(\Gamma (-2/m)\frac{1-2^{2/m}}{log2})^m - (\Gamma (-1/m)\frac{1-2^{1/m}}{log2})^{2m}+\eta _n]n^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$|\epsilon _n|$和$|\eta _n|$不超过$10^{−6}$。&lt;/p&gt;
&lt;p&gt;最后通过depoissonization得到一个渐进无偏估计量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=\alpha _m 2^{\frac{1}{m}\sum{M[i]}}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Equation_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/equation_one_zps76f405b2.png"&gt;&lt;/p&gt;
&lt;p&gt;其中$m$是分桶数。这就是LLC最终使用的估计量。&lt;/p&gt;
&lt;h3&gt;误差分析&lt;/h3&gt;
&lt;p&gt;不加证明给出如下结论：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E_n(\hat{n})/n &amp;amp;= 1 + \theta_{1,n} + o(1) \\
\sqrt{Var_n(E)}/n &amp;amp;= \beta_m / \sqrt{m} + \theta_{2,n} + o(1)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$|\theta_{1,n}|$和$|\theta_{2,n}|$不超过$10^{−6}$。&lt;/p&gt;
&lt;p&gt;当$m$不太小（不小于64）时，$\beta$大约为1.30。因此：&lt;/p&gt;
&lt;p&gt;\begin{equation}
StdError(\hat{n}/n) \approx \frac{1.30}{\sqrt{m}}
\end{equation}&lt;/p&gt;
&lt;h2&gt;算法应用&lt;/h2&gt;
&lt;h3&gt;误差控制&lt;/h3&gt;
&lt;p&gt;在应用LLC时，主要需要考虑的是分桶数$m$，而这个$m$主要取决于误差。根据上面的误差分析，如果要将误差控制在$\epsilon$之内，则：&lt;/p&gt;
&lt;p&gt;\begin{equation}
m &amp;gt; (\frac{1.30}{\epsilon})^2
\end{equation}&lt;/p&gt;
&lt;h3&gt;内存使用分析&lt;/h3&gt;
&lt;p&gt;内存使用与$m$的大小及哈希值得长度（或说基数上限）有关。假设$H$的值为32bit，由于$\rho_{max} \leq 32$，因此每个桶需要5bit空间存储这个桶的$\rho_{max}$，$m$个桶就是$5 \times m/8$字节。例如基数上限为一亿（约227），当分桶数$m$为1024时，每个桶的基数上限约为227/210=217，而$log_2(log_2(217))=4.09$，因此每个桶需要5bit，需要字节数就是$5×1024/8=640$，误差为$1.30 / \sqrt{1024} = 0.040625$，也就是约为$4%$。&lt;/p&gt;
&lt;h3&gt;合并&lt;/h3&gt;
&lt;p&gt;与LC不同，LLC的合并是以桶为单位而不是bit为单位，由于LLC只需记录桶的$\rho_{max}$，因此合并时取相同桶编号数值最大者为合并后此桶的数值即可。&lt;/p&gt;
&lt;h1&gt;HyperLogLog Counting及Adaptive Counting&lt;sup id="sf-play-with-cardinality-estimation-4-back"&gt;&lt;a class="simple-footnote" href="#sf-play-with-cardinality-estimation-4" title="解读Cardinality Estimation算法（第四部分：HyperLogLog Counting及Adaptive Counting）"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;在前一部分，我们了解了LogLog Counting。LLC算法的空间复杂度为$O(log_2(log_2(N_{max})))$，并且具有较高的精度，因此非常适合用于大数据场景的基数估计。不过LLC也有自己的问题，就是当基数不太大时，估计值的误差会比较大。这主要是因为当基数不太大时，可能存在一些空桶，这些空桶的$\rho_{max}$为0。由于LLC的估计值依赖于各桶$\rho_{max}$的几何平均数，而几何平均数对于特殊值（这里就是指0）非常敏感，因此当存在一些空桶时，LLC的估计效果就变得较差。&lt;/p&gt;
&lt;p&gt;本部分将要介绍的HyperLogLog Counting及Adaptive Counting算法均是对LLC算法的改进，可以有效克服LLC对于较小基数估计效果差的缺点。&lt;/p&gt;
&lt;h2&gt;评价基数估计算法的精度&lt;/h2&gt;
&lt;p&gt;首先我们来分析一下LLC的问题。一般来说LLC最大问题在于当基数不太大时，估计效果比较差。上文说过，LLC的渐近标准误差为$1.30/\sqrt{m}$，看起来貌似只和分桶数$m$有关，那么为什么基数的大小也会导致效果变差呢？这就需要重点研究一下如何评价基数估计算法的精度，以及“渐近标准误差”的意义是什么。&lt;/p&gt;
&lt;h3&gt;标准误差&lt;/h3&gt;
&lt;p&gt;首先需要明确标准误差的意义。例如标准误差为0.02，到底表示什么意义。&lt;/p&gt;
&lt;p&gt;标准误差是针对一个统计量（或估计量）而言。在分析基数估计算法的精度时，我们关心的统计量是$\hat{n}/n$。注意这个量分子分母均为一组抽样的统计量。下面正式描述一下这个问题。&lt;/p&gt;
&lt;p&gt;设$S$是我们要估计基数的可重复有限集合。$S$中每个元素都是来自值服从均匀分布的样本空间的一个独立随机抽样样本。这个集合共有$C$个元素，但其基数不一定是$C$，因为其中可能存在重复元素。设$f_n$为定义在$S$上的函数：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f_n(S)=CardinalityofS
\end{equation}&lt;/p&gt;
&lt;p&gt;同时定义$\hat{f_n}$也是定义在$S$上的函数：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f_\hat{n}(S)=LogLog\;estimate\;value\;of\;S
\end{equation}&lt;/p&gt;
&lt;p&gt;我们想得到的第一个函数值，但是由于第一个函数值不好计算，所以我们计算同样集合的第二个函数值来作为第一个函数值得估计。因此最理想的情况是对于任意一个集合两个函数值是相等的，如果这样估计就是100%准确了。不过显然没有这么好的事，因此我们退而求其次，只希望fn^(S)是一个无偏估计，即：&lt;/p&gt;
&lt;p&gt;\begin{equation}
f_\hat{n}(S)
\end{equation}&lt;/p&gt;
&lt;p&gt;这个在上一篇文章中已经说明了。同时也可以看到，$\frac{f_\hat{n}(S)}{f_n(S)}$实际上是一个随机变量，并且服从正态分布。对于正态分布随机变量，一般可以通过标准差$\sigma$度量其稳定性，直观来看，标准差越小，则整体分布越趋近于均值，所以估计效果就越好。这是定性的，那么定量来看标准误差$\sigma$到底表达了什么意思呢。它的意义是这样的：&lt;/p&gt;
&lt;p&gt;对于无偏正态分布而言，随机变量的一次随机取值落在均值一个标准差范围内的概率是68.2%，而落在两个和三个标准差范围内的概率分别为95.4%和99.6%，如下图所示（图片来自维基百科）：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Norm" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/norm_zps0655fdfe.png"&gt;&lt;/p&gt;
&lt;p&gt;因此，假设标准误差是0.02（2%），它实际的意义是：假设真实基数为$n$，$n$与估计值之比落入(0.98, 1.02)的概率是68.2%，落入(0.96,1.04)的概率是95.4%，落入(0.94,1.06)的概率是99.6%。显然这个比值越大则估计值越不准，因此对于0.02的标准误差，这个比值大于1.06或小于0.94的概率不到0.004。&lt;/p&gt;
&lt;p&gt;再直观一点，假设真实基数为10000，则一次估计值有99.6%的可能不大于10600且不小于9400。&lt;/p&gt;
&lt;h3&gt;组合计数与渐近分析&lt;/h3&gt;
&lt;p&gt;如果LLC能够做到绝对服从$1.30/\sqrt{m}$，那么也算很好了，因为我们只要通过控制分桶数$m$就可以得到一个一致的标准误差。这里的一致是指标准误差与基数无关。不幸的是并不是这样，上面已经说过，这是一个“渐近”标注误差。下面解释一下什么叫渐近。&lt;/p&gt;
&lt;p&gt;在计算数学中，有一个非常有用的分支就是组合计数。组合计数简单来说就是分析自然数的组合函数随着自然数的增长而增长的量级。可能很多人已经意识到这个听起来很像算法复杂度分析。没错，算法复杂度分析就是组合计数在算法领域的应用。&lt;/p&gt;
&lt;p&gt;举个例子，设$A$是一个有$n$个元素的集合（这里$A$是严格的集合，不存在重复元素），则$A$的幂集（即由$A$的所有子集组成的集合）有$2^n$个元素。&lt;/p&gt;
&lt;p&gt;上述关于幂集的组合计数是一个非常整齐一致的组合计数，也就是不管$n$多大，A的幂集总有$2^n$个元素。&lt;/p&gt;
&lt;p&gt;可惜的是现实中一般的组合计数都不存在如此干净一致的解。LLC的偏差和标准差其实都是组合函数，但是论文中已经分析出，LLC的偏差和标准差都是渐近组合计数，也就是说，随着$n$趋向于无穷大，标准差趋向于$1.30/\sqrt{m}$，而不是说$n$多大时其值都一致为$1.30/\sqrt{m}$。另外，其无偏性也是渐近的，只有当$n$远远大于$m$时，其估计值才近似无偏。因此当$n$不太大时，LLC的效果并不好。&lt;/p&gt;
&lt;p&gt;庆幸的是，同样通过统计分析方法，我们可以得到$n$具体小到什么程度我们就不可忍受了，另外就是当$n$太小时可不可以用别的估计方法替代LLC来弥补LLC这个缺陷。HyperLogLog Counting及Adaptive Counting都是基于这个思想实现的。&lt;/p&gt;
&lt;h2&gt;Adaptive Counting&lt;/h2&gt;
&lt;p&gt;Adaptive Counting（简称AC）在“Fast and accurate traffic matrix measurement using adaptive cardinality counting”一文中被提出。其思想也非常简单直观：实际上AC只是简单将LC和LLC组合使用，根据基数量级决定是使用LC还是LLC。具体是通过分析两者的标准差，给出一个阈值，根据阈值选择使用哪种估计。&lt;/p&gt;
&lt;h3&gt;基本算法&lt;/h3&gt;
&lt;p&gt;如果分析一下LC和LLC的存储结构，可以发现两者是兼容的，区别仅仅在于LLC关心每个桶的$\rho_{max}$，而LC仅关心此桶是否为空。因此只要简单认为$\rho_{max}$值不为0的桶为非空，桶为空就可以使用LLC的数据结构做LC估计了。&lt;/p&gt;
&lt;p&gt;而我们已经知道，LC在基数不太大时效果好，基数太大时会失效；LLC恰好相反，因此两者有很好的互补性。&lt;/p&gt;
&lt;p&gt;回顾一下，LC的标准误差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
SE_{lc}(\hat{n}/n)=\sqrt{e^t-t-1}/(t\sqrt{m})
\end{equation}&lt;/p&gt;
&lt;p&gt;LLC的标准误差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
SE_{llc}(\hat{n}/n)=1.30/\sqrt{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;将两个公式联立：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sqrt{e^t-t-1}/(t\sqrt{m})=1.30/\sqrt{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;解得$t \approx 2.89$。注意$m$被消掉了，说明这个阈值与$m$无关。其中$t=n/m$。&lt;/p&gt;
&lt;p&gt;设$\beta$为空桶率，根据LC的估算公式，带入上式可得：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\beta = e^{-t} \approx 0.051
\end{equation}&lt;/p&gt;
&lt;p&gt;因此可以知道，当空桶率大于0.051时，LC的标准误差较小，而当小于0.051时，LLC的标准误差较小。
完整的AC算法如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=\left \lbrace 
\begin{array}{cc}
\alpha_m m2^{\frac{1}{m}\sum{M}} &amp;amp; if &amp;amp; 0 \leq \beta &amp;lt; 0.051 \\ 
-mlog(\beta) &amp;amp; if &amp;amp; 0.051 \leq \beta \leq 1 \end{array} 
\right.
\end{equation}&lt;/p&gt;
&lt;h3&gt;误差分析&lt;/h3&gt;
&lt;p&gt;因为AC只是LC和LLC的简单组合，所以误差分析可以依照LC和LLC进行。值得注意的是，当β&amp;lt;0.051时，LLC最大的偏差不超过0.17%，因此可以近似认为是无偏的。&lt;/p&gt;
&lt;h2&gt;HyperLogLog Counting&lt;/h2&gt;
&lt;p&gt;HyperLogLog Counting（以下简称HLLC）的基本思想也是在LLC的基础上做改进，不过相对于AC来说改进的比较多，所以相对也要复杂一些。本文不做具体细节分析，具体细节请参考“HyperLogLog: the analysis of a near-optimal cardinality estimation algorithm”这篇论文。&lt;/p&gt;
&lt;h3&gt;基本算法&lt;/h3&gt;
&lt;p&gt;HLLC的第一个改进是使用调和平均数替代几何平均数。注意LLC是对各个桶取算数平均数，而算数平均数最终被应用到2的指数上，所以总体来看LLC取得是几何平均数。由于几何平均数对于离群值（例如这里的0）特别敏感，因此当存在离群值时，LLC的偏差就会很大，这也从另一个角度解释了为什么n不太大时LLC的效果不太好。这是因为n较小时，可能存在较多空桶，而这些特殊的离群值强烈干扰了几何平均数的稳定性。&lt;/p&gt;
&lt;p&gt;因此，HLLC使用调和平均数来代替几何平均数，调和平均数的定义如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
H = \frac{n}{\frac{1}{x_1} + \frac{1}{x_2} + ... + \frac{1}{x_n}} = \frac{n}{\sum_{i=1}^n \frac{1}{x_i}}
\end{equation}&lt;/p&gt;
&lt;p&gt;调和平均数可以有效抵抗离群值的扰动。使用调和平均数代替几何平均数后，估计公式变为如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{n}=\frac{\alpha_m m^2}{\sum{2^{-M}}}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\alpha_m=(m\int _0^\infty (log_2(\frac{2+u}{1+u}))^m du)^{-1}
\end{equation}&lt;/p&gt;
&lt;h3&gt;偏差分析&lt;/h3&gt;
&lt;p&gt;根据论文中的分析结论，与LLC一样HLLC是渐近无偏估计，且其渐近标准差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
SE_{hllc}(\hat{n}/n)=1.04/\sqrt{m}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此在存储空间相同的情况下，HLLC比LLC具有更高的精度。例如，对于分桶数$m$为$2^13$（8k字节）时，LLC的标准误差为1.4%，而HLLC为1.1%。&lt;/p&gt;
&lt;h3&gt;分段偏差修正&lt;/h3&gt;
&lt;p&gt;在HLLC的论文中，作者在实现建议部分还给出了在$n$相对于$m$较小或较大时的偏差修正方案。具体来说，设$E$为估计值：&lt;/p&gt;
&lt;p&gt;当$E≤{5 \over 2}m$时，使用LC进行估计。&lt;/p&gt;
&lt;p&gt;当$\frac{5}{2}m &amp;lt; E \leq \frac{1}{30}2^{32}$时，使用上面给出的HLLC公式进行估计。&lt;/p&gt;
&lt;p&gt;当$E&amp;gt;\frac{1}{30}2^{32}$时，估计公式则为$\hat{n}=-2^{32}log(1-E/2^{32})$。&lt;/p&gt;
&lt;p&gt;关于分段偏差修正效果分析也可以在原论文中找到。&lt;/p&gt;
&lt;h1&gt;写在后面&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;原博客中作者对这几种算法进行了实验比较,因为个人对实验不是很感兴趣,现只摘录作者的&lt;strong&gt;个人建议&lt;/strong&gt;(对实验结果有兴趣的同学请参考&lt;a href="http://blog.codinglabs.org/articles/cardinality-estimate-exper.html"&gt;五种常用基数估计算法效果实验及实践建议&lt;/a&gt;):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Linear Counting和LogLog Counting由于分别在基数较大和基数较小（阈值可解析分析，具体方法和公式请参考后文列出的相关论文）时存在严重的失效，因此不适合在实际中单独使用。一种例外是，如果对节省存储空间要求不强烈，不要求空间复杂度为常数（Linear Counting的空间复杂度为$O(n)$，其它算法均为$O(1)$），则在保证Bitmap全满概率很小的条件下，Linear Counting的效果要优于其它算法。&lt;/li&gt;
&lt;li&gt;总体来看，不论哪种算法，提高分桶数都可以降低偏差和方差，因此总体来看基数估计算法中分桶数的选择是最重要的一个权衡——在精度和存储空间间的权衡。&lt;/li&gt;
&lt;li&gt;实际中，Adaptive Counting或HyperLogLog Counting都是不错的选择，前者偏差较小，后者对离群点容忍性更好，方差较小。&lt;/li&gt;
&lt;li&gt;Google的HyperLogLog Counting++算法属于实验性改进，缺乏严格的数学分析基础，通用性存疑，不宜在实际中贸然使用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后感谢CodingLabs撰写的精彩博文,这两周就写这3篇博文吧，两周后再见。尼玛,都2:16了,大家晚安，睡了。&lt;/p&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-play-with-cardinality-estimation-1"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-i.html"&gt;解读Cardinality Estimation算法（第一部分：基本概念）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-play-with-cardinality-estimation-2"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-ii.html"&gt;解读Cardinality Estimation算法（第二部分：Linear Counting）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-play-with-cardinality-estimation-3"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iii.html"&gt;解读Cardinality Estimation算法（第三部分：LogLog Counting）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-3-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-play-with-cardinality-estimation-4"&gt;&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iv.html"&gt;解读Cardinality Estimation算法（第四部分：HyperLogLog Counting及Adaptive Counting）&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-play-with-cardinality-estimation-4-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="基数估计"></category><category term="Cardinality Estimation"></category><category term="Big Data"></category></entry><entry><title>也说2048:Minimax算法以及Alpha-Beta剪枝</title><link href="http://www.qingyuanxingsi.com/ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi.html" rel="alternate"></link><updated>2014-04-06T00:00:00+08:00</updated><author><name>CodingLabs</name></author><id>tag:www.qingyuanxingsi.com,2014-04-06:ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi.html</id><summary type="html">&lt;p&gt;今天看机器学习Logistic Regression,脑子实在转不过来了,于是到处游荡了一下,然后不经意间发现了这篇特别好玩的文章&lt;a href="http://blog.codinglabs.org/articles/2048-ai-analysis.html"&gt;2048-AI程序算法分析&lt;/a&gt;,于是摘录如下。(&lt;strong&gt;版权属原作者所有,特此声明&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;针对目前火爆的2048游戏，有人实现了一个AI程序，可以以较大概率（高于90%）赢得游戏，想一睹该AI程序的童鞋们请移步&lt;a href="http://ov3y.github.io/2048-AI/"&gt;2048AI实现&lt;/a&gt;,点击Auto-run按钮即可运行。此外作者在&lt;a href="http://stackoverflow.com/questions/22342854/what-is-the-optimal-algorithm-for-the-game-2048"&gt;stackoverflow上简要介绍了AI的算法框架和实现思路&lt;/a&gt;。但是这个回答主要集中在启发函数的选取上，对AI用到的核心算法并没有仔细说明。这篇文章将主要分为两个部分，第一部分介绍其中用到的基础算法，即Minimax和Alpha-beta剪枝；第二部分分析作者具体的实现。&lt;/p&gt;
&lt;p&gt;&lt;img alt="2048" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/2048_zps4f3b681e.png"&gt;&lt;/p&gt;
&lt;h1&gt;基础算法&lt;/h1&gt;
&lt;p&gt;2048本质上可以抽象成信息对称双人对弈模型（玩家向四个方向中的一个移动，然后计算机在某个空格中填入2或4）。这里“信息对称”是指在任一时刻对弈双方对格局的信息完全一致，移动策略仅依赖对接下来格局的推理。作者使用的核心算法为对弈模型中常用的带Alpha-beta剪枝的Minimax。这个算法也常被用于如国际象棋等信息对称对弈AI中。&lt;/p&gt;
&lt;h2&gt;Minimax&lt;/h2&gt;
&lt;p&gt;下面先介绍不带剪枝的Minimax。首先本文将通过一个简单的例子说明Minimax算法的思路和决策方式。&lt;/p&gt;
&lt;h3&gt;问题&lt;/h3&gt;
&lt;p&gt;现在考虑这样一个游戏：有三个盘子A、B和C，每个盘子分别放有三张纸币。A放的是1、20、50；B放的是5、10、100；C放的是1、5、20。单位均为“元”。有甲、乙两人，两人均对三个盘子和上面放置的纸币有可以任意查看。游戏分三步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;甲从三个盘子中选取一个。&lt;/li&gt;
&lt;li&gt;乙从甲选取的盘子中拿出两张纸币交给甲。&lt;/li&gt;
&lt;li&gt;甲从乙所给的两张纸币中选取一张，拿走。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中甲的目标是最后拿到的纸币面值尽量大，乙的目标是让甲最后拿到的纸币面值尽量小。&lt;/p&gt;
&lt;p&gt;下面用Minimax算法解决这个问题。&lt;/p&gt;
&lt;h3&gt;基本思路&lt;/h3&gt;
&lt;p&gt;一般解决博弈类问题的自然想法是将格局组织成一棵树，树的每一个节点表示一种格局，而父子关系表示由父格局经过一步可以到达子格局。Minimax也不例外，它通过对以当前格局为根的格局树搜索来确定下一步的选择。而一切格局树搜索算法的核心都是对每个格局价值的评价。Minimax算法基于以下朴素思想确定格局价值：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minimax是一种悲观算法，即假设对手每一步都会将我方引入从当前看理论上价值最小的格局方向，即对手具有完美决策能力。因此我方的策略应该是选择那些对方所能达到的让我方最差情况中最好的，也就是让对方在完美决策下所对我造成的损失最小。&lt;/li&gt;
&lt;li&gt;Minimax不找理论最优解，因为理论最优解往往依赖于对手是否足够愚蠢，Minimax中我方完全掌握主动，如果对方每一步决策都是完美的，则我方可以达到预计的最小损失格局，如果对方没有走出完美决策，则我方可能达到比预计的最悲观情况更好的结局。总之我方就是要在最坏情况中选择最好的。
上面的表述有些抽象，下面看具体示例。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;解题&lt;/h3&gt;
&lt;p&gt;下图是上述示例问题的格局树：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Situation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/01_zps1cf40396.png"&gt;&lt;/p&gt;
&lt;p&gt;注意，由于示例问题格局数非常少，我们可以给出完整的格局树。这种情况下我可以找到Minimax算法的全局最优解。而真实情况中，格局树非常庞大，即使是计算机也不可能给出完整的树，因此我们往往只搜索一定深度，这时只能找到局部最优解。&lt;/p&gt;
&lt;p&gt;我们从甲的角度考虑。其中正方形节点表示轮到我方（甲），而三角形表示轮到对方（乙）。经过三轮对弈后（我方-对方-我方），将进入终局。黄色叶结点表示所有可能的结局。从甲方看，由于最终的收益可以通过纸币的面值评价，我们自然可以用结局中甲方拿到的纸币面值表示终格局的价值。&lt;/p&gt;
&lt;p&gt;下面考虑倒数第二层节点，在这些节点上，轮到我方选择，所以我们应该引入可选择的最大价值格局，因此每个节点的价值为其子节点的最大值：&lt;/p&gt;
&lt;p&gt;&lt;img alt="02" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/02_zps80320dc2.png"&gt;&lt;/p&gt;
&lt;p&gt;这些轮到我方的节点叫做max节点，max节点的值是其子节点最大值。&lt;/p&gt;
&lt;p&gt;倒数第三层轮到对方选择，假设对方会尽力将局势引入让我方价值最小的格局，因此这些节点的价值取决于子节点的最小值。这些轮到对方的节点叫做min节点。&lt;/p&gt;
&lt;p&gt;最后，根节点是max节点，因此价值取决于叶子节点的最大值。最终完整赋值的格局树如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="03" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/03_zps171fcc9a.png"&gt;&lt;/p&gt;
&lt;p&gt;总结一下Minimax算法的步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先确定最大搜索深度D，D可能达到终局，也可能是一个中间格局。&lt;/li&gt;
&lt;li&gt;在最大深度为D的格局树叶子节点上，使用预定义的价值评价函数对叶子节点价值进行评价。&lt;/li&gt;
&lt;li&gt;自底向上为非叶子节点赋值。其中max节点取子节点最大值，min节点取子节点最小值。&lt;/li&gt;
&lt;li&gt;每次轮到我方时（此时必处在格局树的某个max节点），选择价值等于此max节点价值的那个子节点路径。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;在上面的例子中，根节点的价值为20，表示如果对方每一步都完美决策，则我方按照上述算法可最终拿到20元，这是我方在Minimax算法下最好的决策。格局转换路径如下图红色路径所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt="04" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/04_zpsaa4d7848.png"&gt;&lt;/p&gt;
&lt;p&gt;对于真实问题中的Minimax，再次强调几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;真实问题一般无法构造出完整的格局树，所以需要确定一个最大深度D，每次最多从当前格局向下计算D层。&lt;/li&gt;
&lt;li&gt;因为上述原因，Minimax一般是寻找一个局部最优解而不是全局最优解，搜索深度越大越可能找到更好的解，但计算耗时会呈指数级膨胀。&lt;/li&gt;
&lt;li&gt;也是因为无法一次构造出完整的格局树，所以真实问题中Minimax一般是边对弈边计算局部格局树，而不是只计算一次，但已计算的中间结果可以缓存。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Alpha-beta剪枝&lt;sup id="sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1" title="以下描述看不懂可参考Step by Step:Alpha-Beta Cutting Example"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;简单的Minimax算法有一个很大的问题就是计算复杂性。由于所需搜索的节点数随最大深度呈指数膨胀，而算法的效果往往和深度相关，因此这极大限制了算法的效果。&lt;/p&gt;
&lt;p&gt;Alpha-beta剪枝是对Minimax的补充和改进。采用Alpha-beta剪枝后，我们可不必构造和搜索最大深度D内的所有节点，在构造过程中，如果发现当前格局再往下不能找到更好的解，我们就停止在这个格局及以下的搜索，也就是剪枝。&lt;/p&gt;
&lt;p&gt;Alpha-beta基于这样一种朴素的思想：&lt;strong&gt;时时刻刻记得当前已经知道的最好选择，如果从当前格局搜索下去，不可能找到比已知最优解更好的解，则停止这个格局分支的搜索（剪枝），回溯到父节点继续搜索&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;Alpha-beta算法可以看成变种的Minimax，基本方法是从根节点开始采用深度优先的方式构造格局树，在构造每个节点时，都会读取此节点的alpha和beta两个值，其中alpha表示搜索到当前节点时已知的最好选择的下界，而beta表示从这个节点往下搜索最坏结局的上界。由于我们假设对手会将局势引入最坏结局之一，因此当beta小于alpha时，表示从此处开始不论最终结局是哪一个，其上限价值也要低于已知的最优解，也就是说已经不可能此处向下找到更好的解，所以就会剪枝。&lt;/p&gt;
&lt;p&gt;下面同样以上述示例介绍Alpha-beta剪枝算法的工作原理。我们从根节点开始，详述使用Alpha-beta的每一个步骤：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;根节点的alpha和beta分别被初始化为$-\infty$，和$+\infty$。&lt;/li&gt;
&lt;li&gt;深度优先搜索第一个孩子，不是叶子节点，所以alpha和beta继承自父节点，分别为$-\infty$，和$+\infty$&lt;/li&gt;
&lt;li&gt;搜索第三层的第一个孩子，同上。&lt;/li&gt;
&lt;li&gt;搜索第四层，到达叶子节点，采用评价函数得到此节点的评价值为1。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="05" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/05_zps5136a83f.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;此叶节点的父节点为max节点，因此更新其alpha值为1，表示此节点取值的下界为1。&lt;/li&gt;
&lt;li&gt;再看另外一个子节点，值为20，大于当前alpha值，因此将alpha值更新为20。&lt;/li&gt;
&lt;li&gt;此时第三层最左节点所有子树搜索完毕，作为max节点，更新其真实值为当前alpha值：20。&lt;/li&gt;
&lt;li&gt;由于其父节点（第二层最左节点）为min节点，因此更新其父节点beta值为20，表示这个节点取值最多为20。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="06" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/06_zps09a0fcab.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;搜索第二层最左节点的第二个孩子及其子树，按上述逻辑，得到值为50（&lt;strong&gt;注意第二层最左节点的beta值要传递给孩子&lt;/strong&gt;）。由于50大于20，不更新min节点的beta值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="07" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/07_zps5a2b5a81.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;搜索第二层最左节点的第三个孩子。当看完第一个叶子节点后，发现第三个孩子的alpha=beta，此时表示这个节点下不会再有更好解，于是剪枝。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="08" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/08_zpsbee7e438.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;继续搜索B分支，当搜索完B分支的第一个孩子后，发现此时B分支的alpha为20，beta为10。这表示B分支节点的最大取值不会超过10，而我们已经在A分支取到20，此时满足alpha大于等于beta的剪枝条件，因此将B剪枝。并将B分支的节点值设为10，注意，这个10不一定是这个节点的真实值，而只是上线，B节点的真实值可能是5，可能是1，可能是任何小于10的值。但是已经无所谓了，反正我们知道这个分支不会好过A分支，因此可以放弃了。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="09" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/09_zpsf2b60883.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在C分支搜索时遇到了与B分支相同的情况。因此将C分支剪枝。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="10" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/10_zps1254e8ee.png"&gt;&lt;/p&gt;
&lt;p&gt;此时搜索全部完毕，而我们也得到了这一步的策略：应该走A分支。&lt;/p&gt;
&lt;p&gt;可以看到相比普通Minimax要搜索18个叶子节点相比，这里只搜索了9个。采用Alpha-beta剪枝，可以在相同时间内加大Minimax的搜索深度，因此可以获得更好的效果。并且Alpha-beta的解和普通Minimax的解是一致的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:在查找讲解Alpha-Beta剪枝算法的过程中看到了这样一种表述,我个人觉得这么理解可能更容易看懂这个算法:Alpha是对于Max节点而言从该节点到根节点路径上最好的Option,而Beta是对于Min节点而言从该节点到根节点路径上最好的Option;Max节点的目标是最大化所得收益,而Min节点目标则是最小化Max节点所得收益.(MORE SEE AT &lt;a href="http://www.youtube.com/watch?v=xBXHtz4Gbdo"&gt;Step by Step: Alpha Beta Pruning|GoAgent翻墙&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;针对2048游戏的实现&lt;/h1&gt;
&lt;p&gt;下面看一下ov3y同学针对2048实现的AI。原程序见于&lt;a href="https://github.com/ov3y/2048-AI"&gt;github&lt;/a&gt;，主要程序都在&lt;a href="https://github.com/ov3y/2048-AI/blob/master/js/ai.js"&gt;ai.js&lt;/a&gt;中。&lt;/p&gt;
&lt;h2&gt;建模&lt;/h2&gt;
&lt;p&gt;上面说过Minimax和Alpha-beta都是针对信息对称的轮流对弈问题，这里作者是这样抽象游戏的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我方：游戏玩家。每次可以选择上、下、左、右四个行棋策略中的一种（某些格局会少于四种，因为有些方向不可走）。行棋后方块按照既定逻辑移动及合并，格局转换完成。&lt;/li&gt;
&lt;li&gt;对方：计算机。在当前任意空格子里放置一个方块，方块的数值可以是2或4。放置新方块后，格局转换完成。&lt;/li&gt;
&lt;li&gt;胜利条件：出现某个方块的数值为“2048”。&lt;/li&gt;
&lt;li&gt;失败条件：格子全满，且无法向四个方向中任何一个方向移动（均不能触发合并）。如此2048游戏就被建模成一个信息对称的双人对弈问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;格局评价&lt;/h2&gt;
&lt;p&gt;作为算法的核心，如何评价当前格局的价值是重中之重。在2048中，除了终局外，中间格局并无非常明显的价值评价指标，因此需要用一些启发式的指标来评价格局。那些分数高的“好”格局是容易引向胜利的格局，而分低的“坏”格局是容易引向失败的格局。&lt;/p&gt;
&lt;p&gt;作者采用了如下几个启发式指标。&lt;/p&gt;
&lt;h3&gt;单调性&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;单调性&lt;/code&gt;指方块从左到右、从上到下均遵从递增或递减。一般来说，越单调的格局越好。下面是一个具有良好单调格局的例子：&lt;/p&gt;
&lt;p&gt;&lt;img alt="单调性" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/11_zps7ce37f15.png"&gt;&lt;/p&gt;
&lt;h3&gt;平滑性&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;平滑性&lt;/code&gt;是指每个方块与其直接相邻方块数值的差，其中差越小越平滑。例如2旁边是4就比2旁边是128平滑。一般认为越平滑的格局越好。下面是一个具有极端平滑性的例子：&lt;/p&gt;
&lt;p&gt;&lt;img alt="平滑性" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/12_zpsb53c5ef7.png"&gt;&lt;/p&gt;
&lt;h3&gt;空格数&lt;/h3&gt;
&lt;p&gt;这个很好理解，因为一般来说，空格子越少对玩家越不利。所以我们认为空格越多的格局越好。&lt;/p&gt;
&lt;h3&gt;孤立空格数&lt;/h3&gt;
&lt;p&gt;这个指标评价空格被分开的程度，空格越分散则格局越差。&lt;/p&gt;
&lt;p&gt;具体来说，2048-AI在评价格局时，对这些启发指标采用了加权策略。具体代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// static evaluation function&lt;/span&gt;
&lt;span class="no"&gt;AI&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prototype&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;emptyCells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;availableCells&lt;/span&gt;&lt;span class="p"&gt;().&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;smoothWeight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="c1"&gt;//monoWeight   = 0.0,&lt;/span&gt;
        &lt;span class="c1"&gt;//islandWeight = 0.0,&lt;/span&gt;
        &lt;span class="n"&gt;mono2Weight&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;emptyWeight&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;2.7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;maxWeight&lt;/span&gt;    &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;smoothness&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;smoothWeight&lt;/span&gt;
        &lt;span class="c1"&gt;//+ this.grid.monotonicity() * monoWeight&lt;/span&gt;
        &lt;span class="c1"&gt;//- this.grid.islands() * islandWeight&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;monotonicity2&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;mono2Weight&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;emptyCells&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;emptyWeight&lt;/span&gt;
        &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maxValue&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;maxWeight&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="p"&gt;};&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;有兴趣的同学可以调整一下权重看看有什么效果。&lt;/p&gt;
&lt;h2&gt;对对方选择的剪枝&lt;/h2&gt;
&lt;p&gt;在这个程序中，除了采用Alpha-beta剪枝外，在min节点还采用了另一种剪枝，即只考虑对方走出让格局最差的那一步（而实际2048中计算机的选择是随机的），而不是搜索全部对方可能的走法。这是因为对方所有可能的选择为“空格数×2”，如果全部搜索的话会严重限制搜索深度。
相关剪枝代码如下：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// try a 2 and 4 in each cell and measure how annoying it is&lt;/span&gt;
&lt;span class="c1"&gt;// with metrics from eval&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;candidates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[];&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;availableCells&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="mh"&gt;2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[],&lt;/span&gt; &lt;span class="mh"&gt;4&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;push&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="k"&gt;cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;tile&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Tile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;cell&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;parseInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;10&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insertTile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tile&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;smoothness&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;islands&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
        &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;removeTile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;cell&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;// now just pick out the most annoying moves&lt;/span&gt;
&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;maxScore&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mh"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;Math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;null&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mh"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]));&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="c1"&gt;// 2 and 4&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;].&lt;/span&gt;&lt;span class="n"&gt;length&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;maxScore&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;candidates&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;push&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nl"&gt;position:&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="nl"&gt;value:&lt;/span&gt; &lt;span class="n"&gt;parseInt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;搜索深度&lt;/h2&gt;
&lt;p&gt;在2048-AI的实现中，并没有限制搜索的最大深度，而是限制每次“思考”的时间。这里设定了一个超时时间，默认为100ms，在这个时间内，会从1开始，搜索到所能达到的深度。相关代码：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c1"&gt;// performs iterative deepening over the alpha-beta search&lt;/span&gt;
&lt;span class="no"&gt;AI&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;prototype&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iterativeDeep&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;()).&lt;/span&gt;&lt;span class="n"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;depth&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;var&lt;/span&gt; &lt;span class="n"&gt;newBest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;this&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mh"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mh"&gt;0&lt;/span&gt; &lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mh"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;newBest&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;move&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mh"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="c1"&gt;//console.log('BREAKING EARLY');&lt;/span&gt;
            &lt;span class="k"&gt;break&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="n"&gt;best&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;newBest&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="n"&gt;depth&lt;/span&gt;&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Date&lt;/span&gt;&lt;span class="p"&gt;()).&lt;/span&gt;&lt;span class="n"&gt;getTime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;minSearchTime&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="c1"&gt;//console.log('depth', --depth);&lt;/span&gt;
    &lt;span class="c1"&gt;//console.log(this.translate(best.move));&lt;/span&gt;
    &lt;span class="c1"&gt;//console.log(best);&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;best&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;因此这个算法实现的效果实际上依赖于执行javascript引擎机器的性能。当然可以通过增加超时时间来达到更好的效果，但此时每一步行走速度会相应变慢。&lt;/p&gt;
&lt;h2&gt;算法的改进&lt;/h2&gt;
&lt;p&gt;目前这个实现作者声称成功合成2048的概率超过90%，但是合成4096甚至8192的概率并不高。作者在&lt;a href="https://github.com/ov3y/2048-AI/blob/master/README.md"&gt;github项目的REAMDE&lt;/a&gt;中同时给出了一些优化建议，这些建议包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;缓存结果。目前这个实现并没有对已搜索的树做缓存，每一步都要重新开始搜索。&lt;/li&gt;
&lt;li&gt;多线程搜索。由于javascript引擎的单线程特性，这一点很难做到，但如果在其它平台上也许也可考虑并行技术。&lt;/li&gt;
&lt;li&gt;更好的启发函数。也许可以总结出一些更好的启发函数来评价格局价值。&lt;/li&gt;
&lt;/ul&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1"&gt;以下描述看不懂可参考&lt;a href="http://www.youtube.com/watch?v=xBXHtz4Gbdo"&gt;Step by Step:Alpha-Beta Cutting Example&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-ye-shuo-2048minimaxsuan-fa-yi-ji-alpha-betajian-zhi-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="AI"></category><category term="2048"></category><category term="Minimax Algorithm"></category><category term="Alpha-Beta Pruning"></category><category term="Algorithm"></category></entry></feed>