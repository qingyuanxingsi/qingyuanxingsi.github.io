<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>苹果的味道</title><link href="http://www.qingyuanxingsi.com/" rel="alternate"></link><link href="http://www.qingyuanxingsi.com/feeds/notes.rss.xml" rel="self"></link><id>http://www.qingyuanxingsi.com/</id><updated>2014-05-02T00:00:00+08:00</updated><entry><title>机器学习拾遗(II):大杂烩</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-shi-yi-iida-za-hui.html" rel="alternate"></link><updated>2014-05-02T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-02:ji-qi-xue-xi-shi-yi-iida-za-hui.html</id><summary type="html">&lt;p&gt;本文主要对之前看过的&lt;em&gt;Machine Learning:A Probabilistic Perspective&lt;/em&gt;一书中有关离散数据生成模型、高斯模型以及线性模型的相关部分进行梳理。&lt;/p&gt;
&lt;h1&gt;Closed Form&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1" title="Closed-form expression"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;在阅读&lt;em&gt;Machine Learning:A Probabilistic Perspective&lt;/em&gt;这本书的时候经常看到&lt;strong&gt;Closed Form&lt;/strong&gt;这个词。以下给出一简单说明:&lt;/p&gt;
&lt;p&gt;如果一个表达式可以表示成&lt;strong&gt;有限的&lt;/strong&gt;某些特定的&lt;strong&gt;Ｗell-known&lt;/strong&gt;函数的形式，那么我们说这个表达式是一个&lt;strong&gt;Closed Form&lt;/strong&gt;表达式。一般而言,这些&lt;strong&gt;Well-known&lt;/strong&gt;函数包括基本的函数，如常数、单个变量$x$、基本的算术操作、开方、指数对数函数、三角反三角函数等。如果一个问题可以得到一&lt;strong&gt;Closed Form&lt;/strong&gt;的解,我们通常就说这个问题是可解的。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Closed–form&lt;/strong&gt;表达式是解析表达式的一个非常重要的子类,解析表达式可包含有限或无限个&lt;strong&gt;Well-known&lt;/strong&gt;函数。和更为广泛的解析表达式不同,&lt;strong&gt;Closed–form&lt;/strong&gt;表达式并不包含无限级数和递归分数(&lt;em&gt;Continued Fractions&lt;/em&gt;)形式，也不能包含积分和极限。&lt;/p&gt;
&lt;p&gt;一个方程或方程组当且仅当它的一个解可写成&lt;strong&gt;Closed-Form&lt;/strong&gt;形式时,我们才说该方程或方程组具有一&lt;strong&gt;Closed-form solution&lt;/strong&gt;.类似地,一个方程或方程组当且仅当它的一个解可写成解析形式时,我们才说该方程或方程组具有一解析解。&lt;/p&gt;
&lt;h1&gt;Gaussian Models&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-2-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-2" title="Machine Learning:A Probabilistic Perspective Chapter 4"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;Diagonal LDA&lt;/h2&gt;
&lt;p&gt;在GLA中,如果我们限定不同类间共享$\Sigma$,并对每类使用一对角协方差矩阵，我们得到的模型则称为&lt;strong&gt;Diagonal LDA Model&lt;/strong&gt;.对应的判别函数如下所示:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\delta_c(x) = log\ p(x,y=c|\theta) = -\sum_{j=1}^D \frac{(x_j-\mu_{cj})^2}{2\sigma_j^2}+log\ \pi_c
\end{equation}&lt;/p&gt;
&lt;p&gt;一般我们令$\hat\mu_{cj} = \bar{x}_{cj}$且$\hat\sigma_j^2=s_j^2$,$s_j^2$称为&lt;em&gt;Pooled empirical variance&lt;/em&gt;,定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
s_j^2 = \frac{\sum_{c=1}^C \sum_{i:y_i=c} (x_{ij} - \bar{x}_{cj})^2}{N-C}
\end{equation}&lt;/p&gt;
&lt;p&gt;在高维情形下,该模型比LDA和RDA效果都要好。&lt;/p&gt;
&lt;h2&gt;方差MAP估计&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;ML&lt;/em&gt;(指Machine Learning:A Probabilistic Perspective,以后如果不作说明,也指该书)一书中关于$\Sigma$的估计写的很复杂,于是基本上没看那部分,关于$\Sigma$的MAP估计可参考以下链接:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/"&gt;Regularized Gaussian Covariance Estimation&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Linear Regression&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-3-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-3" title="Machine Learning:A Probabilistic Perspective Chapter 7"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;Bayesian Linear Regression&lt;/h2&gt;
&lt;p&gt;给定训练集$D={(x_i,y_i)|i=1,\cdots,n}$,我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Design_Matrix" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/design_matrix_zpse58dd2c8.png"&gt;&lt;/p&gt;
&lt;p&gt;对于Linear Regression,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
f(x) &amp;amp; = x^Tw \in R,x,w \in R^D   \\
y &amp;amp;= f(x) + \epsilon = x^Tw+\epsilon \in R \\
\epsilon &amp;amp;\sim\ N(0,\sigma^2) \in R
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们首先计算Likelihood:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
P(y|X,w) &amp;amp;= \prod_{i=1}^n P(y_i|x_i^Tw) \\
&amp;amp;= \prod_{i=1}^n N_{yi}(x_i^Tw,\sigma^2) \\
&amp;amp;= \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}exp[\frac{-(y_i-x_i^Tw)^2}{2\sigma^2}] \\
&amp;amp;= \frac{1}{(2\pi\sigma^2)^{n/2}}exp[\frac{-1}{2\sigma^2}||y-X^Tw||^2] \\
&amp;amp;= N_y(X^Tw,\sigma^2I_n)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;另我们假定Prior服从$w \sim\ N_w(0,\Sigma_p)$&lt;/p&gt;
&lt;p&gt;则我们可得Posterior:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
P(w|X,y) &amp;amp;= \frac{P(y|X,w)P(w)}{P(y|X)}  \\
&amp;amp;= \frac{P(y|X,w)P(w)}{\int P(y|X,w)dw}  \\
&amp;amp;= \frac{N_y(X^Tw,\sigma^2I_n)N_w(0,\Sigma_p)}{\int N_y(X^Tw,\sigma^2I_n)N_w(0,\Sigma_p)dw} \\
&amp;amp;\sim\ N_y(X^Tw,\sigma^2I_n)N_w(0,\Sigma_p)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;更进一步地,我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="MAP_Estimation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/map_weight_zps49ddf4b7.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:Posterior协方差矩阵并不依赖观察值$y$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们希望使用以上得到的Posterior计算$f$在点$x_{*}$处的值,我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Posterior Predicative" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/predicative_weight_zps31d23a10.png"&gt;&lt;/p&gt;
&lt;h1&gt;Logistic Regression&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-4-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-4" title="Machine Learning:A Probabilistic Perspective Chapter 8"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;ML&lt;/em&gt;一书中给出的形式并不便于计算Gradient以及Hessian,以下根据李航&lt;strong&gt;统计学习方法&lt;/strong&gt;中的相关内容给出以下较易计算的形式。&lt;/p&gt;
&lt;p&gt;对于给定的训练集$T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$,其中,$x_i \in R^n,y_i \in {0,1}$,我们令:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(Y=1|x) = \mu(x),P(Y=0|x) = 1-\mu(x)
\end{equation}&lt;/p&gt;
&lt;p&gt;对数似然函数为:
\begin{equation}
\begin{split}
\ell(w) &amp;amp;= \sum_{i=1}^N [y_i log \mu(x_i)+(1-y_i)log(1-\mu(x_i))] \\
&amp;amp;= \sum_{i=1}^N [y_i log \frac{\mu(x_i)}{1-\mu(x_i)}+log(1-\mu(x_i))] \\
&amp;amp;= \sum_{i=1}^N [y_i(w^Tx_i)-log(1+exp(w^Tx_i)]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;为了使用最小值,在&lt;em&gt;ML&lt;/em&gt;一书中用到的是Negetive Log Likelihood(NLL),特此说明。&lt;/p&gt;
&lt;p&gt;由此我们得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
g &amp;amp;= \frac{df(w)}{dw} = \sum_i (\mu_i-y_i)x_i = X^T(\mu-y) \\
H &amp;amp;= \frac{dg(w)^T}{dw} = \sum_i (\bigtriangledown_w \mu_i)x_i^T \\
&amp;amp;= \sum_i \mu_i(1-\mu_i)x_ix_i^T \\
&amp;amp;= X^TSX
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$S \triangleq diag(\mu_i(1-\mu_i))$.得到梯度和Hessian矩阵之后,以下我们就能使用最速下降、牛顿法、BFGS等方法对Logistic Regression进行求解了。&lt;/p&gt;
&lt;h1&gt;EM&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;本部分与&lt;em&gt;ML&lt;/em&gt;一书第十一章无关,只是为了更清楚地了解&lt;strong&gt;EM算法&lt;/strong&gt;。以下给出一个挺不错的PPT:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://people.csail.mit.edu/fergus/research/tutorial_em.ppt"&gt;Tutorial EM&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Posterior Predicative for Multinomial-Dirichlet Models&lt;/em&gt;&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-5-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-5" title="Machine Learning:A Probabilistic Perspective Chapter 3"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Regularized LDA&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Nearest shrunken centroids classifier&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Numerically stable computation[7.5.2]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Connection with PCA[7.5.3]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Estimating $\sigma^2$[7.6.3]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Evidence Procedure[7.6.4]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Conjugate Gradient(CG)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;BFGS(LBFGS)&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Multi-class Logistic Regression and Maximum Entropy Classfier&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Bayesian Logistic Regression[8.4]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Online Learning[8.5]&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Generative vs Discriminative Classfiers[8.6]&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1"&gt;&lt;a href="http://en.wikipedia.org/wiki/Closed-form_expression"&gt;Closed-form expression&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-2"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 4&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-3"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 7&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-3-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-4"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 8&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-4-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-5"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 3&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-5-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="Machine Learning"></category><category term="Gaussian Models"></category><category term="Linear Models"></category><category term="Notes"></category></entry><entry><title>机器学习拾遗(I):概率图模型[待续]</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu.html" rel="alternate"></link><updated>2014-05-01T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-01:ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu.html</id><summary type="html">&lt;p&gt;今天突然发现&lt;em&gt;Machine Learning:A Probabilistic Perspective&lt;/em&gt;一书中看过的章节中还有一些东西没完全弄懂,或者有些东西干脆就漏掉了,所以这一阵子打算把看过的部分梳理一下,慢慢来,脚踏实地地把这本书吃透,至少达到让自己满意的理解水平吧。作为机器学习拾遗的第一篇,我们主要从这本书中涉及概率图模型的章节入手,以期对概率图模型有一个更为深入的了解,并把之前遗漏的部分知识点捡起来。&lt;/p&gt;
&lt;h1&gt;Undirected Graphical Models&lt;sup id="sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1" title="Machine Learning:A Probabilistic Perspective Chapter 10"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Family&lt;/strong&gt;;For a directed graph,the family of a node is the node and its parents,$fam(s) = {s} \cup pa(s)$.&lt;/li&gt;
&lt;li&gt;Influence(Decision Diagrams);人的行为是理智的,人们在决策时总希望自己所能得到的期望效用最大化(&lt;em&gt;Expected Utility&lt;/em&gt;),当我们对不同行动下的期望效用进行比较后就很容易做出对我们而言最好的&lt;em&gt;Action&lt;/em&gt;.然而,现实生活中信息对于决策具有非常重要的意义，当我们获得一些其他的有用的信息时，可能做出更为明智的决定。得知一定的决策信息后，一般而言，我们的最大期望效用会提高,而此时的最大期望效用值相对之前的提升量则称为&lt;strong&gt;Value of Perfect Information,VPI(完美信息价值)&lt;/strong&gt;。而实际上在现实生活中我们获取这些信息并不是不无代价的,因此 理智人能做出的理智选择是,当信息的VPI要大于获取信息的代价时,我们选择获取额外的信息，否则则不用获取该信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Decision Diagrams" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/decision_diagram_zps163a6ef8.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP(&lt;em&gt;Markov Decision Process&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 10&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="Machine Learning"></category><category term="Graphical Models"></category><category term="Notes"></category></entry></feed>