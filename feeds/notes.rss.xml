<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>苹果的味道</title><link href="http://www.qingyuanxingsi.com/" rel="alternate"></link><link href="http://www.qingyuanxingsi.com/feeds/notes.rss.xml" rel="self"></link><id>http://www.qingyuanxingsi.com/</id><updated>2014-05-02T00:00:00+08:00</updated><entry><title>机器学习拾遗(II):大杂烩</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-shi-yi-iida-za-hui.html" rel="alternate"></link><updated>2014-05-02T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-02:ji-qi-xue-xi-shi-yi-iida-za-hui.html</id><summary type="html">&lt;p&gt;本文主要对之前看过的&lt;em&gt;Machine Learning:A Probabilistic Perspective&lt;/em&gt;一书中有关离散数据生成模型、高斯模型以及线性模型等相关部分进行梳理。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Posterior Predicative for Multinomial-Dirichlet Models&lt;/em&gt;&lt;sup id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1" title="Machine Learning:A Probabilistic Perspective Chapter 3"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 3&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-iida-za-hui-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="Machine Learning"></category><category term="Gaussian Models"></category><category term="Linear Models"></category><category term="Notes"></category></entry><entry><title>机器学习拾遗(I):概率图模型[待续]</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu.html" rel="alternate"></link><updated>2014-05-01T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-01:ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu.html</id><summary type="html">&lt;p&gt;今天突然发现&lt;em&gt;Machine Learning:A Probabilistic Perspective&lt;/em&gt;一书中看过的章节中还有一些东西没完全弄懂,或者有些东西干脆就漏掉了,所以这一阵子打算把看过的部分梳理一下,慢慢来,脚踏实地地把这本书吃透,至少达到让自己满意的理解水平吧。作为机器学习拾遗的第一篇,我们主要从这本书中涉及概率图模型的章节入手,以期对概率图模型有一个更为深入的了解,并把之前遗漏的部分知识点捡起来。&lt;/p&gt;
&lt;h1&gt;Undirected Graphical Models&lt;sup id="sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1" title="Machine Learning:A Probabilistic Perspective Chapter 10"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Family&lt;/strong&gt;;For a directed graph,the family of a node is the node and its parents,$fam(s) = {s} \cup pa(s)$.&lt;/li&gt;
&lt;li&gt;Influence(Decision Diagrams);人的行为是理智的,人们在决策时总希望自己所能得到的期望效用最大化(&lt;em&gt;Expected Utility&lt;/em&gt;),当我们对不同行动下的期望效用进行比较后就很容易做出对我们而言最好的&lt;em&gt;Action&lt;/em&gt;.然而,现实生活中信息对于决策具有非常重要的意义，当我们获得一些其他的有用的信息时，可能做出更为明智的决定。得知一定的决策信息后，一般而言，我们的最大期望效用会提高,而此时的最大期望效用值相对之前的提升量则称为&lt;strong&gt;Value of Perfect Information,VPI(完美信息价值)&lt;/strong&gt;。而实际上在现实生活中我们获取这些信息并不是不无代价的,因此 理智人能做出的理智选择是,当信息的VPI要大于获取信息的代价时,我们选择获取额外的信息，否则则不用获取该信息。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Decision Diagrams" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/decision_diagram_zps163a6ef8.png"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MDP(&lt;em&gt;Markov Decision Process&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1"&gt;&lt;em&gt;Machine Learning:A Probabilistic Perspective Chapter 10&lt;/em&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-shi-yi-igai-lu-tu-mo-xing-dai-xu-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="Machine Learning"></category><category term="Graphical Models"></category><category term="Notes"></category></entry></feed>