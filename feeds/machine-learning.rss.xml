<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>苹果的味道</title><link href="http://www.qingyuanxingsi.com/" rel="alternate"></link><link href="http://www.qingyuanxingsi.com/feeds/machine-learning.rss.xml" rel="self"></link><id>http://www.qingyuanxingsi.com/</id><updated>2014-05-31T00:00:00+08:00</updated><entry><title>计算机视觉(III):霍夫变换直线检测</title><link href="http://www.qingyuanxingsi.com/ji-suan-ji-shi-jue-iiihuo-fu-bian-huan-zhi-xian-jian-ce.html" rel="alternate"></link><updated>2014-05-31T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-31:ji-suan-ji-shi-jue-iiihuo-fu-bian-huan-zhi-xian-jian-ce.html</id><summary type="html">&lt;h1 id="_1"&gt;霍夫变换直线检测基本原理&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;Hough变换是图像处理中从图像中识别几何形状的基本方法之一，即它可以检测已知形状的目标，而且受噪声和曲线间断的影响小。
Hough变换的基本思想是利用点-线的对偶性。如下图所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt="电线对偶性" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/70B97EBF5BF950766027_zpsa30da4aa.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;从上图中可看出，$x-y$坐标和$k-b$坐标有点-线的对偶性。$x-y$坐标中的点$P_1$、$P_2$对应于$k-b$坐标中的$L_1、L_2$；而$k-b$坐标中的点$P_0$对应于$x-y$坐标中的线$L_0$。&lt;/p&gt;
&lt;p&gt;由于$x-y$坐标中的垂直线的$k$值为无穷大，给计算带来不便，故使用极坐标变换解决这一问题。直角坐标$X-Y$中的一点$(x,y)$，经过极坐标变换为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\rho = x * cos \theta + y * sin \theta
\end{equation}&lt;/p&gt;
&lt;p&gt;在极坐标$\theta-\rho$中变为一条正弦曲线，$\theta$取(0-180°)。可以证明，直角坐标$X-Y$中直线上的点经过Hough变换后，它们的正弦曲线在极坐标$\theta-\rho$有一个公共交点，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="极坐标系" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hough_curve_zps8378d511.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;也就是说，极坐标$\theta-\rho$上的一点$(\theta,\rho)$，对应于直角坐标$X-Y$中的一条直线，而且它们是一一对应的。为了检测出直角坐标$X-Y$中由点所构成的直线，可以将极坐标$\theta-\rho$量化成许多小格。根据直角坐标中每个点的坐标$(x,y)$，在$\theta = 0-180°$内以小格的步长计算各个$\rho$值，所得值落在某个小格内，便使该小格的累加记数器加1。当直角坐标中全部的点都变换后，对小格进行检验，计数值最大的小格，其$(\theta,\rho)$值对应于直角坐标中所求直线。&lt;/p&gt;
&lt;h1 id="_2"&gt;代码示例&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;以下我们给出一个使用OpenCV检测直线的代码示例:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include "opencv2/core/core.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/imgproc/imgproc.hpp"

using namespace std;
using namespace cv;

int main(int argc, char** argv)
{
     Mat src = imread("/home/qingyuanxingsi/workspace/C++/contents/jinmen.jpg");
     if(src.empty())
     {
         cout &amp;lt;&amp;lt; "can not open the image" &amp;lt;&amp;lt; endl;
         return -1;
     }

     Mat dst, cdst;
     //Canny operator retrieve edges
     Canny(src, dst, 50, 200, 3);
     cvtColor(dst, cdst, CV_GRAY2BGR);



     vector&amp;lt;Vec4i&amp;gt; lines;
     //Probablistic Hough Transformation
     HoughLinesP(dst, lines, 1, CV_PI/180, 50, 50, 10 );
      for( size_t i = 0; i &amp;lt; lines.size(); i++ )
      {
        Vec4i l = lines[i];
        line(cdst, Point(l[0], l[1]), Point(l[2], l[3]), Scalar(255,0,0), 2, CV_AA);
      }
     imshow("source", src);
     imshow("detected lines", cdst);
     imwrite("line.jpg",cdst);

     waitKey();

     return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;原图使用金门大桥图片，如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt="金门大桥" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/jinmen_zpsfd6d69ea.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;检测到的直线图像如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="霍夫变换直线检测" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/line_zpsc5550ab6.jpg" /&gt;&lt;/p&gt;
&lt;h1 id="_3"&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://hi.baidu.com/tangsu2009/item/88475289bb40035a840fabda"&gt;Hough变换的基本原理&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="计算机视觉"></category><category term="直线检测"></category></entry><entry><title>计算机视觉(II):边缘检测</title><link href="http://www.qingyuanxingsi.com/ji-suan-ji-shi-jue-iibian-yuan-jian-ce.html" rel="alternate"></link><updated>2014-05-27T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-27:ji-suan-ji-shi-jue-iibian-yuan-jian-ce.html</id><summary type="html">&lt;blockquote&gt;
&lt;p&gt;本文中的介绍相对粗浅，自己实现一遍这些算法可能会有更为深入的了解，理论部分也不是很充分，没有那种"边缘检测不过如此"的感觉，以后可能就这个问题进行更为深入的挖掘(如果时间允许的话).此外，本文中对Canny边缘的&lt;strong&gt;非极大值抑制&lt;/strong&gt;也没有给出解释，有机会我们会详细讨论。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id="_1"&gt;边缘&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;边缘(edge)&lt;/strong&gt;是指图像局部强度变化最显著的部分。主要存在于目标与目标、目标与背景、区域与区域(包括不同色彩)之间，是图像分割、纹理特征和形状特征等图像分析的重要基础。&lt;/p&gt;
&lt;p&gt;图像强度的显著变化可分为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;阶跃变化函数，即图像强度在不连续处的两边的像素灰度值有着显著的差异；&lt;/li&gt;
&lt;li&gt;线条（屋顶）变化函数，即图像强度突然从一个值变化到另一个值，保持一较小行程后又回到原来的值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;图像的边缘有方向和幅度两个属性,沿边缘方向像素变化平缓,垂直于边缘方向像素变化剧烈.边缘上的这种变化可以用微分算子检测出来,通常用一阶或二阶导数来检测边缘。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Edge_Detection_I" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/edge_detection_1_zps4e28524a.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;注:(a)(b)分别是阶跃函数和屋顶函数的二维图像；(c)(d)是阶跃和屋顶函数的函数图象；(e)(f)对应一阶导数；（g)(h)是二阶导数&lt;/strong&gt;。&lt;/p&gt;
&lt;h1 id="_2"&gt;一阶导数法：梯度算子&lt;/h1&gt;
&lt;p&gt;对于左图，左侧的边是正的（由暗到亮），右侧的边是负的（由亮到暗）。对于右图，结论相反。常数部分为零。用来检测边是否存在。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Edge_Detection_II" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/edge_detection_2_zpsb552892a.png" /&gt;&lt;/p&gt;
&lt;h2 id="gradient-operators"&gt;梯度算子 Gradient operators&lt;/h2&gt;
&lt;p&gt;函数$f(x,y)$在$(x,y)$处的梯度为一个向量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\bigtriangledown f = [\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}]^T
\end{equation}&lt;/p&gt;
&lt;p&gt;计算这个向量的大小为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
|\bigtriangledown f| = mag(\bigtriangledown f) = [(\frac{\partial f}{\partial x})^2+(\frac{\partial f}{\partial y})^2]^{1 \over 2}
\end{equation}&lt;/p&gt;
&lt;p&gt;近似为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
|\bigtriangledown f| \thickapprox |G_x| + |G_y|
\end{equation}&lt;/p&gt;
&lt;p&gt;梯度的方向角为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
a(x,y) = arctan(\frac{G_y}{G_x})
\end{equation}&lt;/p&gt;
&lt;h2 id="sobel"&gt;Sobel算子&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Sobel算子&lt;/strong&gt;的表示：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
G_x &amp;amp;= (z_7+2z_8+z_9) - (z_1+2z_2+z_3) \\
G_y &amp;amp;= (z_3+2z_6+z_9) - (z_1+2z_4+z_7)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;梯度幅值：&lt;/p&gt;
&lt;p&gt;\begin{equation}
|\bigtriangledown f| \thickapprox |G_x| + |G_y|
\end{equation}&lt;/p&gt;
&lt;p&gt;用卷积模板来实现：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Conv Template" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/edge_detection_3_zps392719ee.png" /&gt;&lt;/p&gt;
&lt;h1 id="_3"&gt;二阶微分法：拉普拉斯&lt;/h1&gt;
&lt;p&gt;二阶微分在亮的一边是负的，在暗的一边是正的。常数部分为零。可以用来确定边的准确位置，以及像素在亮的一侧还是暗的一侧。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Edge Detection IV" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/edge_detection_4_zpsfcf520a3.png" /&gt;&lt;/p&gt;
&lt;h2 id="laplace"&gt;LapLace 拉普拉斯算子&lt;/h2&gt;
&lt;p&gt;二维函数$f(x,y)$的拉普拉斯是一个二阶的微分，定义为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\bigtriangledown^2 f = \frac{\partial^2 f}{\partial x^2}+\frac{\partial^2 f}{\partial y^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\frac{\partial^2 f}{\partial x^2} &amp;amp;= f[x+1,y]-2f[x,y]+f[x-1,y] \\
\frac{\partial^2 f}{\partial y^2} &amp;amp;= f[x,y+1]-2f[x,y]+f[x,y-1]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;可以用多种方式将其表示为数字形式。对于一个3*3的区域，经验上被推荐最多的形式是：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\bigtriangledown^2 f = 4z_5 - (z_2+z_4+z_6+z_8)
\end{equation}&lt;/p&gt;
&lt;p&gt;定义数字形式的拉普拉斯要求系数之和必为0&lt;/p&gt;
&lt;p&gt;&lt;img alt="Laplace" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/edge_detection_5_zps88cd3030.png" /&gt;&lt;/p&gt;
&lt;p&gt;以下给出两种边缘检测算子的代码示例:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include "opencv2/core/core.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/imgproc/imgproc.hpp"

using namespace std;
using namespace cv;

int main()
{
    Mat src, src_gray;
    Mat grad;

    int scale = 1;
    int delta = 0;
    int ddepth = CV_16S;
    int kernel_size = 3;

    /// Load an image
    src = imread("/home/qingyuanxingsi/workspace/C++/contents/lufei.jpg");

    if( !src.data )
    { return -1; }

    GaussianBlur( src, src, Size(3,3), 0, 0, BORDER_DEFAULT );
    cvtColor( src, src_gray, CV_RGB2GRAY );


    /////////////////////////// Sobe l////////////////////////////////////
    /// Generate grad_x and grad_y
    Mat grad_x, grad_y;
    Mat abs_grad_x, abs_grad_y;
    /// Gradient X
    //Scharr( src_gray, grad_x, ddepth, 1, 0, scale, delta, BORDER_DEFAULT );
    //Calculates the first, second, third, or mixed image derivatives using an extended Sobel operator.
    Sobel( src_gray, grad_x, ddepth, 1, 0, 3, scale, delta, BORDER_DEFAULT );
    convertScaleAbs( grad_x, abs_grad_x );
    /// Gradient Y
    //Scharr( src_gray, grad_y, ddepth, 0, 1, scale, delta, BORDER_DEFAULT );
    Sobel( src_gray, grad_y, ddepth, 0, 1, 3, scale, delta, BORDER_DEFAULT );
    convertScaleAbs( grad_y, abs_grad_y );
     /// Total Gradient (approximate)
    addWeighted( abs_grad_x, 0.5, abs_grad_y, 0.5, 0, grad );

    namedWindow("Sobel", CV_WINDOW_AUTOSIZE );
    imshow("Sobel", grad );
    imwrite("Sobel.png",grad);

    /////////////////////////////////// Laplace ///////////////////////////////
    Mat abs_dst,dst;
    Laplacian( src_gray, dst, ddepth, kernel_size, scale, delta, BORDER_DEFAULT );
    convertScaleAbs( dst, abs_dst );
    namedWindow("Laplacian", CV_WINDOW_AUTOSIZE );
    imshow("Laplacian", abs_dst );
    imwrite("Laplace.png",grad);

    waitKey(0);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;源图我们采用海贼王路飞图像:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lufei" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lufei_zps83311ff7.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;Sobel算子处理后得到如下边缘:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sobel" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/Sobel_zps3e362cbb.png" /&gt;&lt;/p&gt;
&lt;p&gt;Sobel算子可以直接计算$G_x$ 、$G_y$可以检测到边的存在，以及从暗到亮，从亮到暗的变化。仅计算$|G_x|$，产生最强的响应是正交于$x$轴的边；$|G_y|$则是正交于$y$轴的边。&lt;/p&gt;
&lt;p&gt;Laplacian算子则得到如下边缘:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Laplacian" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/Laplace_zps1b1cf9bb.png" /&gt;&lt;/p&gt;
&lt;p&gt;拉普拉斯对噪声敏感，会产生双边效果。不能检测出边的方向。&lt;strong&gt;通常不直接用于边的检测，只起辅助的角色&lt;/strong&gt;，检测一个像素是在边的亮的一边还是暗的一边利用零跨越，确定边的位置。&lt;/p&gt;
&lt;h1 id="canny"&gt;Canny边缘检测&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;在计算机视觉边缘检测算法实现时，实际上就是将图像与某边缘检测模板做卷积的过程。针对Canny而言，其卷积模板相对来说比较简单：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Canny Template" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/canny_template_zps34034b29.png" /&gt;&lt;/p&gt;
&lt;p&gt;我们将左上角视为$f[i,j]$,则其$x$,$y$方向的偏导数矩阵,梯度幅值以及方向的表达式分别为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
P[i,j] &amp;amp;= (f[i,j+1]+f[i+1,j+1)-f[i][j]-f[i+1,j])/2 \\
Q[i,j] &amp;amp;= (f[i,j]+f[i][j+1]-f[i+1,j]-f[i+1,j+1])/2 \\
M[i,j] &amp;amp;= \sqrt{P]i,j]^2+Q[i,j]^2} \\
\theta[i,j] &amp;amp;= arctan(\frac{Q[i,j]}{P[i,j]})
\end{split}
\end{equation}&lt;/p&gt;
&lt;h2 id="todo"&gt;对梯度幅值进行非极大值抑制[TODO]&lt;/h2&gt;
&lt;h2 id="_4"&gt;用双阈值算法检测和连接边缘&lt;/h2&gt;
&lt;p&gt;Canny算法中减少假边缘数量的方法是采用双阈值法。选择两个阈值，根据高阈值得到一个边缘图像，这样一个图像含有很少的假边缘，但是由于阈值较高，产生的图像边缘可能不闭合，为解决这样一个问题采用了另外一个低阈值。&lt;/p&gt;
&lt;p&gt;在高阈值图像中把边缘链接成轮廓，当到达轮廓的端点时，该算法会在断点的8邻域点中寻找满足低阈值的点，再根据此点收集新的边缘，直到整个图像边缘闭合。&lt;/p&gt;
&lt;p&gt;以上即为Canny边缘检测算法的原理分析，接下来我们给出一个OpenCV代码示例。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include "opencv2/core/core.hpp"
#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/highgui/highgui.hpp"

using namespace std;
using namespace cv;

Mat src, src_gray;
Mat dst, detected_edges;
int edgeThresh = 1;
int lowThreshold;
int const max_lowThreshold = 100;
int ratio = 3;
int kernel_size = 3;
char* window_name = "Canny";

void CannyThreshold(int, void*)
{
    /// Reduce noise with a kernel 3x3
    blur( src_gray, detected_edges, Size(3,3) );
    /// Canny detector
    Canny( detected_edges, detected_edges, lowThreshold, lowThreshold*ratio, kernel_size );
    dst = Scalar::all(0);
    src.copyTo( dst, detected_edges);
    imshow( window_name, dst );
    if(lowThreshold == 0){
        imwrite("canny_0.png",dst);
    }
    if(lowThreshold==50){
        imwrite("canny_50.png",dst);
    }
    if(lowThreshold==100){
        imwrite("canny_100.png",dst);
    }
}

int main( )
{
  src = imread( "/home/qingyuanxingsi/workspace/C++/contents/lufei.jpg" );
  if( !src.data )
    { return -1; }
  dst.create( src.size(), src.type() );
  cvtColor( src, src_gray, CV_BGR2GRAY );
  namedWindow( window_name, CV_WINDOW_AUTOSIZE );
  createTrackbar( "Min Threshold:", window_name, &amp;amp;lowThreshold, max_lowThreshold, CannyThreshold );
  CannyThreshold(0, 0);
  waitKey(0);
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过设定不同的阈值，我们分别得到不同的边缘检测结果：&lt;/p&gt;
&lt;p&gt;&lt;img alt="threshold_0" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/canny_0_zps09475230.png" /&gt;&lt;/p&gt;
&lt;p&gt;当我们把阈值均设置为0时,我们得到的是原图像的边缘阵列；&lt;/p&gt;
&lt;p&gt;以下分别当低阈值分别设置为50和100时的结果:&lt;/p&gt;
&lt;p&gt;&lt;img alt="threshold_50" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/canny_50_zpsa24f5b6c.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="threshold_100" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/canny_100_zps8dff2e1d.png" /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;TODO BOARD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sobel算子$G_x$,$G_y$定义确认;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h1 id="_5"&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/xiaowei_cqu/article/details/7829481"&gt;【OpenCV】边缘检测：Sobel、拉普拉斯算子&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/likezhaobin/article/details/6892176"&gt;Canny边缘检测算法原理及其VC实现详解(一)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/xiaowei_cqu/article/details/7839140"&gt;【OpenCV】Canny 边缘检测&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="计算机视觉"></category><category term="图像处理"></category></entry><entry><title>计算机视觉(I):图像处理基础初步</title><link href="http://www.qingyuanxingsi.com/ji-suan-ji-shi-jue-itu-xiang-chu-li-ji-chu-chu-bu.html" rel="alternate"></link><updated>2014-05-24T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-24:ji-suan-ji-shi-jue-itu-xiang-chu-li-ji-chu-chu-bu.html</id><summary type="html">&lt;p&gt;还有一个月左右本科生涯就结束了,毕业设计目前也做的差不多了,趁着这段时间学点图像处理的东东,貌似应该可能会有点意思吧。&lt;/p&gt;
&lt;h1 id="_1"&gt;直方图均衡化&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;直方图均衡化的作用是&lt;strong&gt;图像增强&lt;/strong&gt;(关于直方图均衡化的背景等请参考&lt;strong&gt;参考文献[3]&lt;/strong&gt;)。&lt;/p&gt;
&lt;p&gt;关于直方图均衡化有两个问题比较难懂:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一是为什么要选用累积分布函数;&lt;/li&gt;
&lt;li&gt;二是为什么使用累积分布函数处理后像素值会均匀分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一个问题。均衡化过程中，必须要保证两个条件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;像素无论怎么映射，一定要保证原来的大小关系不变，较亮的区域，依旧是较亮的，较暗依旧暗，只是对比度增大，绝对不能明暗颠倒；&lt;/li&gt;
&lt;li&gt;如果是八位图像，那么像素映射函数的值域应在0和255之间的，不能越界。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;综合以上两个条件，累积分布函数是个好的选择，因为累积分布函数是单调增函数（控制大小关系），并且值域是0到1（控制越界问题），所以直方图均衡化中使用的是累积分布函数。&lt;/p&gt;
&lt;p&gt;第二个问题。累积分布函数具有一些好的性质，那么如何运用累积分布函数使得直方图均衡化？比较概率分布函数和累积分布函数，前者的二维图像是参差不齐的，后者是单调递增的。直方图均衡化过程中，映射方法是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
s_k = \sum_{j=0}^k \frac{n_j}{n},k=0,1,2\cdots,L-1
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$n$是图像中像素的总和，$n_j$是当前灰度级的像素个数，$L$是图像中可能的灰度级总数。&lt;/p&gt;
&lt;p&gt;下面我们来看看通过上述公式怎样实现的拉伸的。假设有如下图像：&lt;/p&gt;
&lt;p&gt;&lt;img alt="img_src" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hist1_zps2c68d1d4.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;得图像的统计信息如下图所示，并根据统计信息完成灰度值映射：&lt;/p&gt;
&lt;p&gt;&lt;img alt="HIST INFO" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hist2_zps208dde74.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;映射后的图像如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt="img_dst" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hist3_zpsc1020cfe.jpg" /&gt;.&lt;/p&gt;
&lt;p&gt;以下我们给出对彩色图像进行直方图均衡化的具体代码,对于彩色图像而言,我们需要先将图像分割成多个通道,然后对通道进行直方图均衡化，最后将得到的均衡化后的通道图合并为目标图像。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;opencv2/core/core.hpp&amp;gt;
#include &amp;lt;opencv2/highgui/highgui.hpp&amp;gt;
#include &amp;lt;opencv/cv.h&amp;gt;

int main(int argc, char** argv)
{
    int i;
    IplImage* src = cvLoadImage("/home/qingyuanxingsi/workspace/C++/contents/lena.jpg", 1 );
    IplImage* imgChannel[4] = { 0, 0, 0, 0 };
    IplImage* dst = cvCreateImage(cvGetSize( src ), IPL_DEPTH_8U, 3);

    if( src )
    {
        for( i = 0; i &amp;lt; src -&amp;gt; nChannels; i++ )
        {
            //Single Channel Image required for hist equalization
            imgChannel[i] = cvCreateImage( cvGetSize( src ), IPL_DEPTH_8U, 1 );
        }
        //Split the channels
        cvSplit( src, imgChannel[0], imgChannel[1], imgChannel[2], imgChannel[3] );//BGRA
        for( i = 0; i &amp;lt; dst -&amp;gt; nChannels; i++ )
        {
            //Hist Equalize
            cvEqualizeHist( imgChannel[i], imgChannel[i] );
        }

        //Merge the channels
        cvMerge( imgChannel[0], imgChannel[1], imgChannel[2], imgChannel[3], dst );
        cvNamedWindow( "src", 1 );
        cvShowImage( "src", src );
        cvNamedWindow( "Equalize", 1 );
        cvShowImage( "Equalize", dst );

        //Save the images
        cvSaveImage("src.jpg",src);
        cvSaveImage("histEqualize.jpg",dst);

        cvWaitKey(0);
        //Release Resources
        for( i = 0; i &amp;lt; src -&amp;gt; nChannels; i++ )
        {
            if( imgChannel[i] )
            {
                cvReleaseImage( &amp;amp;imgChannel[i] );
                //imgChannel[i] = 0;
            }
        }
        cvReleaseImage( &amp;amp;dst );
    }

    return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上述代码中我们采用的图片为著名的&lt;strong&gt;Lena&lt;/strong&gt;,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="LENA" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/src_zps4d025e5f.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;进行直方图均衡化后得到的图像如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="LENA_EQUALIZED" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/histEqualize_zps433e20fa.jpg" /&gt;&lt;/p&gt;
&lt;h1 id="_2"&gt;线性滤波&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id="_3"&gt;相关理论与概念&lt;/h2&gt;
&lt;h3 id="_4"&gt;关于平滑处理&lt;/h3&gt;
&lt;p&gt;“平滑处理“（smoothing）也称“模糊处理”（bluring），是一项简单且使用频率很高的图像处理方法。平滑处理的用途有很多，最常见的是用来减少图像上的噪点或者失真。在涉及到降低图像分辨率时，平滑处理是非常好用的方法。&lt;/p&gt;
&lt;h3 id="_5"&gt;图像滤波与滤波器&lt;/h3&gt;
&lt;p&gt;首先我们看一下图像滤波的概念。&lt;strong&gt;图像滤波，即在尽量保留图像细节特征的条件下对目标图像的噪声进行抑制，是图像预处理中不可缺少的操作，其处理效果的好坏将直接影响到后续图像处理和分析的有效性和可靠性。&lt;/strong&gt;消除图像中的噪声成分叫作图像的平滑化或滤波操作。信号或图像的能量大部分集中在幅度谱的低频和中频段是很常见的，而在较高频段，感兴趣的信息经常被噪声淹没。因此一个能降低高频成分幅度的滤波器就能够减弱噪声的影响。&lt;/p&gt;
&lt;p&gt;图像滤波的目的有两个:一是抽出对象的特征作为图像识别的特征模式;另一个是为适应图像处理的要求，消除图像数字化时所混入的噪声。&lt;/p&gt;
&lt;p&gt;而对滤波处理的要求也有两条:一是不能损坏图像的轮廓及边缘等重要信息;二是使图像清晰视觉效果好。&lt;/p&gt;
&lt;p&gt;平滑滤波是低频增强的空间域滤波技术。它的目的有两类：一类是模糊；另一类是消除噪音。（各种“两"，：））&lt;/p&gt;
&lt;p&gt;空间域的平滑滤波一般采用简单平均法进行，就是求邻近像元点的平均亮度值。邻域的大小与平滑的效果直接相关，邻域越大平滑的效果越好，但邻域过大，平滑会使边缘信息损失的越大，从而使输出的图像变得模糊，因此需合理选择邻域的大小。&lt;/p&gt;
&lt;p&gt;关于滤波器，一种形象的比喻法是：我们可以把滤波器想象成一个包含加权系数的窗口，当使用这个滤波器平滑处理图像时，就把这个窗口放到图像之上，透过这个窗口来看我们得到的图像。滤波器的种类有很多，在OpenCV中，提供了如下五种常用的图像平滑处理操作方法，且他们分别被封装在单独的函数中，使用起来非常方便：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;方框滤波——boxblur函数&lt;/li&gt;
&lt;li&gt;均值滤波（邻域平均滤波)——blur函数&lt;/li&gt;
&lt;li&gt;高斯滤波——GaussianBlur函数&lt;/li&gt;
&lt;li&gt;中值滤波——medianBlur函数&lt;/li&gt;
&lt;li&gt;双边滤波——bilateralFilter函数&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_6"&gt;线性滤波器简介&lt;/h3&gt;
&lt;p&gt;线性滤波器：线性滤波器经常用于剔除输入信号中不想要的频率或者从许多频率中选择一个想要的频率。几种常见的线性滤波器：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;允许低频率通过的低通滤波器。&lt;/li&gt;
&lt;li&gt;允许高频率通过的高通滤波器。&lt;/li&gt;
&lt;li&gt;允许一定范围频率通过的带通滤波器。&lt;/li&gt;
&lt;li&gt;阻止一定范围频率通过并且允许其它频率通过的带阻滤波器。&lt;/li&gt;
&lt;li&gt;允许所有频率通过、仅仅改变相位关系的全通滤波器。&lt;/li&gt;
&lt;li&gt;阻止一个狭窄频率范围通过的特殊带阻滤波器，陷波滤波器（Band-stop filter）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_7"&gt;关于滤波和模糊&lt;/h3&gt;
&lt;p&gt;关于滤波和模糊，大家往往在初次接触的时候会弄混淆，“一会儿说滤波，一会儿又说模糊，什么玩意儿啊”。没关系，在这里，我们就来辨别一下，为大家扫清障碍。&lt;/p&gt;
&lt;p&gt;我们上文已经提到过，滤波是将信号中特定波段频率滤除的操作，是抑制和防止干扰的一项重要措施。为了方便说明，就拿我们经常用的高斯滤波来作例子吧。我们知道，滤波可分低通滤波和高通滤波两种。而高斯滤波是指用高斯函数作为滤波函数的滤波操作，至于是不是模糊，要看是高斯低通还是高斯高通，&lt;strong&gt;低通就是模糊，高通就是锐化&lt;/strong&gt;。其实说白了是很简单的，对吧:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高斯滤波是指用高斯函数作为滤波函数的滤波操作。&lt;/li&gt;
&lt;li&gt;高斯模糊就是高斯低通滤波。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="_8"&gt;邻域算子与线性邻域滤波&lt;/h3&gt;
&lt;p&gt;邻域算子（局部算子）是利用给定像素周围的像素值的决定此像素的最终输出值的一种算子。而线性邻域滤波是一种常用的邻域算子，像素的输出值取决于输入像素的加权和，具体过程如下图。&lt;/p&gt;
&lt;p&gt;邻域算子除了用于局部色调调整以外，还可以用于图像滤波，实现图像的平滑和锐化，图像边缘增强或者图像噪声的去除。本部分，我们介绍的主角是线性邻域滤波算子，即用不同的权重去结合一个小邻域内的像素，来得到应有的处理效果。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Linear Filter" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/linear_filter_1_zps55e01741.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;图注：邻域滤波（卷积）：左边图像与中间图像的卷积产生右边图像。目标图像中蓝色标记的像素是利用原图像中红色标记的像素计算得到的。&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;线性滤波处理的输出像素值$g(i,j)$是输入像素值$f(i+k,j+l)$的加权和:&lt;/p&gt;
&lt;p&gt;\begin{equation}
g(i,j) = \sum_{k,l} f(i+k,j+l) \times h(k,l)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中我们称$h$为“核”，滤波器的加权系数，即滤波器的“滤波系数”。&lt;/p&gt;
&lt;p&gt;上面的式子可以简单写作：&lt;/p&gt;
&lt;p&gt;\begin{equation}
g = f \otimes h
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$f$表示输入像素值，$h$表示加权系数“核“，$g$表示输出像素值。以上我们提到过的方框滤波、均值滤波以及高斯滤波均属于线性滤波(对OpenCV各函数源码以及细节感兴趣的朋友请参考&lt;strong&gt;参考文献[4]&lt;/strong&gt;)。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;均值滤波&lt;/strong&gt;是典型的线性滤波算法，主要方法为邻域平均法，即用一片图像区域的各个像素的均值来代替原图像中的各个像素值。一般需要在图像上对目标像素给出一个模板（核），该模板包括了其周围的临近像素（比如以目标像素为中心的周围8（3x3-1）个像素，构成一个滤波模板，即去掉目标像素本身）。再用模板中的全体像素的平均值来代替原来像素值。即对待处理的当前像素点$(x，y)$，选择一个模板，该模板由其近邻的若干像素组成，求模板中所有像素的均值，再把该均值赋予当前像素点$(x，y)$，作为处理后图像在该点上的灰度值$g(x,y)$，即$g（x，y）=1/m\sum f(x，y)$,其中$m$为该模板中包含当前像素在内的像素总个数。&lt;strong&gt;均值滤波本身存在着固有的缺陷，即它不能很好地保护图像细节，在图像去噪的同时也破坏了图像的细节部分，从而使图像变得模糊，不能很好地去除噪声点&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;方框滤波&lt;/strong&gt;则分为归一化和非归一化两种,归一化版本等同于均值滤波,非归一化版本求和之后不平均即可(对核未进行归一化)。归一化就是把要处理的量都缩放到一个范围内,比如(0,1)，以便统一处理和直观量化。而非归一化（Unnormalized）的方框滤波则用于计算每个像素邻域内的积分特性，比如密集光流算法（&lt;em&gt;dense optical flow algorithms&lt;/em&gt;）中用到的图像倒数的协方差矩阵（&lt;em&gt;covariance matrices of image derivatives&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;高斯滤波&lt;/strong&gt;也是一种线性平滑滤波，适用于消除高斯噪声，广泛应用于图像处理的减噪过程。通俗的讲，高斯滤波就是对整幅图像进行加权平均的过程，每一个像素点的值，都由其本身和邻域内的其他像素值经过加权平均后得到。高斯滤波的具体操作是：用一个模板（或称卷积、掩模）扫描图像中的每一个像素，用模板确定的邻域内像素的加权平均灰度值去替代模板中心像素点的值。&lt;/p&gt;
&lt;p&gt;大家常常说高斯滤波最有用的滤波操作，虽然它用起来，效率往往不是最高的。&lt;/p&gt;
&lt;p&gt;高斯模糊技术生成的图像，其视觉效果就像是经过一个半透明屏幕在观察图像，这与镜头焦外成像效果散景以及普通照明阴影中的效果都明显不同。高斯平滑也用于计算机视觉算法中的预先处理阶段，以增强图像在不同比例大小下的图像效果。从数学的角度来看，图像的高斯模糊过程就是图像与正态分布做卷积。由于正态分布又叫作高斯分布，所以这项技术就叫作高斯模糊。
图像与圆形方框模糊做卷积将会生成更加精确的焦外成像效果。由于高斯函数的傅立叶变换是另外一个高斯函数，所以高斯模糊对于图像来说就是一个低通滤波操作。&lt;/p&gt;
&lt;p&gt;高斯滤波器是一类根据高斯函数的形状来选择权值的线性平滑滤波器。高斯平滑滤波器对于抑制服从正态分布的噪声非常有效。一维零均值高斯函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
G(x) = exp(-x^2/2\sigma^2)
\end{equation}
其中，高斯分布参数$\sigma$决定了高斯函数的宽度。对于图像处理来说，常用二维零均值离散高斯函数作平滑滤波器。
二维高斯函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
G_0(x,y) = Ae^{\frac{-(x-\mu_x)^2}{2\sigma_x^2}+\frac{-(y-\mu_y)^2}{2\sigma_y^2}}
\end{equation}&lt;/p&gt;
&lt;p&gt;针对如上提到的三种滤波算子,以下我们给出OpenCV源码示例,图片采用下图:&lt;/p&gt;
&lt;p&gt;&lt;img alt="AngelaBaby" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/angelababy_zps187c89ba.jpg" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include "opencv2/core/core.hpp"
#include"opencv2/highgui/highgui.hpp"
#include"opencv2/imgproc/imgproc.hpp"

using namespace cv;

int main( )
{
       Mat image=imread("/home/qingyuanxingsi/workspace/C++/contents/angelababy.jpg");

       namedWindow("Box Blur" );

       //Box Blur
       Mat boxBlur;
       boxFilter(image, boxBlur, -1,Size(5, 5));
       imshow("Box Blur" ,boxBlur );
       imwrite("boxBlur.jpg",boxBlur);

       namedWindow("Average Blur");
       //Average Blur
       Mat averageBlur;
       blur(image, averageBlur,Size(7, 7));
       imshow("Average Blur" ,averageBlur );
       imwrite("averageBlur.jpg",averageBlur);

       namedWindow("Gaussian Blur" );

       //Gaussian Blur
       Mat gaussianBlur;
       GaussianBlur(image, gaussianBlur, Size( 3, 3 ), 0, 0 );
       imshow("Gaussian Blur" ,gaussianBlur);
       imwrite("gaussianBlur.jpg",gaussianBlur);

       waitKey(0);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;采用三种滤波方式得到的图像依次如下所示(方框滤波,均值滤波,高斯滤波):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Box Blur" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/boxBlur_zpsc775ff39.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Average Blur" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/averageBlur_zps5004133d.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gaussian Blur" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gaussianBlur_zps90a2a616.jpg" /&gt;&lt;/p&gt;
&lt;h1 id="_9"&gt;非线性滤波&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id="_10"&gt;相关理论与概念讲解&lt;/h2&gt;
&lt;h3 id="_11"&gt;非线性滤波概述&lt;/h3&gt;
&lt;p&gt;之前的那篇文章里，我们所考虑的滤波器都是线性的，即两个信号之和的响应和他们各自响应之和相等。换句话说，每个像素的输出值是一些输入像素的加权和，线性滤波器易于构造，并且易于从频率响应角度来进行分析。&lt;/p&gt;
&lt;p&gt;其实在很多情况下，使用邻域像素的非线性滤波也许会得到更好的效果。比如在噪声是散粒噪声而不是高斯噪声，即图像偶尔会出现很大的值的时候。在这种情况下，用高斯滤波器对图像进行模糊的话，噪声像素是不会被去除的，它们只是转换为更为柔和但仍然可见的散粒。&lt;/p&gt;
&lt;p&gt;这就到了中值滤波登场的时候了。&lt;/p&gt;
&lt;h3 id="_12"&gt;中值滤波&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;中值滤波(Median filter)&lt;/strong&gt;是一种典型的非线性滤波技术，基本思想是用像素点邻域灰度值的中值来代替该像素点的灰度值，该方法在去除脉冲噪声、椒盐噪声的同时又能保留图像边缘细节.&lt;/p&gt;
&lt;p&gt;中值滤波是基于排序统计理论的一种能有效抑制噪声的非线性信号处理技术，其基本原理是把数字图像或数字序列中一点的值用该点的一个邻域中各点值的中值代替，让周围的像素值接近的真实值，从而消除孤立的噪声点，对于斑点噪声（speckle noise）和椒盐噪声（salt-and-pepper noise）来说尤其有用，因为它不依赖于邻域内那些与典型值差别很大的值。中值滤波器在处理连续图像窗函数时与线性滤波器的工作方式类似，但滤波过程却不再是加权运算。&lt;/p&gt;
&lt;p&gt;中值滤波在一定的条件下可以克服常见线性滤波器如最小均方滤波、方框滤波器、均值滤波等带来的图像细节模糊，而且对滤除脉冲干扰及图像扫描噪声非常有效，也常用于保护边缘信息,保存边缘的特性使它在不希望出现边缘模糊的场合也很有用，是非常经典的平滑噪声处理方法。&lt;/p&gt;
&lt;h4 id="_13"&gt;中值滤波与均值滤波器比较&lt;/h4&gt;
&lt;p&gt;中值滤波器与均值滤波器比较的优势：在均值滤波器中，由于噪声成分被放入平均计算中，所以输出受到了噪声的影响，但是在中值滤波器中，由于噪声成分很难选上，所以几乎不会影响到输出。因此同样用3x3区域进行处理，中值滤波消除的噪声能力更胜一筹。中值滤波无论是在消除噪声还是保存边缘方面都是一个不错的方法。&lt;/p&gt;
&lt;p&gt;中值滤波器与均值滤波器比较的劣势：中值滤波花费的时间是均值滤波的5倍以上。&lt;/p&gt;
&lt;p&gt;顾名思义，中值滤波选择每个像素的邻域像素中的中值作为输出，或者说中值滤波将每一像素点的灰度值设置为该点某邻域窗口内的所有像素点灰度值的中值。&lt;/p&gt;
&lt;p&gt;例如，取3 x 3的函数窗，计算以点$[i,j]$为中心的函数窗像素中值步骤如下:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;按强度值大小排列像素点．&lt;/li&gt;
&lt;li&gt;选择排序像素集的中间值作为点$[i,j]$的新值．&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这一过程如图下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Median Filter" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/median_filter_1_zpsb49d3057.png" /&gt;&lt;/p&gt;
&lt;p&gt;一般采用奇数点的邻域来计算中值，但如果像素点数为偶数时，中值就取排序像素中间两点的平均值．&lt;/p&gt;
&lt;p&gt;中值滤波在一定条件下，可以克服线性滤波器（如均值滤波等）所带来的图像细节模糊，而且对滤除脉冲干扰即&lt;strong&gt;图像扫描噪声&lt;/strong&gt;最为有效。在实际运算过程中并不需要图像的统计特性，也给计算带来不少方便。但是对一些细节多，特别是线、尖顶等细节多的图像不宜采用中值滤波。&lt;/p&gt;
&lt;h3 id="_14"&gt;双边滤波&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;双边滤波(Bilateral filter)&lt;/strong&gt;是一种非线性的滤波方法，是结合图像的空间邻近度和像素值相似度的一种折衷处理，同时考虑空域信息和灰度相似性，达到保边去噪的目的。具有简单、非迭代、局部的特点。&lt;/p&gt;
&lt;p&gt;双边滤波器的好处是可以做边缘保存（edge preserving），一般过去用的维纳滤波或者高斯滤波去降噪，都会较明显地模糊边缘，对于高频细节的保护效果并不明显。双边滤波器顾名思义比高斯滤波多了一个高斯方差$\sigma_d$，它是基于空间分布的高斯滤波函数，所以在边缘附近，离的较远的像素不会太多影响到边缘上的像素值，这样就保证了边缘附近像素值的保存。但是由于保存了过多的高频信息，对于彩色图像里的高频噪声，双边滤波器不能够干净的滤掉，只能够对于低频信息进行较好的滤波。&lt;/p&gt;
&lt;p&gt;在双边滤波器中，输出像素的值依赖于邻域像素值的加权值组合：&lt;/p&gt;
&lt;p&gt;\begin{equation}
 g(i,j) = \frac{\sum_{k,l} f(k,l)w(i,j,k,l)}{\sum_{k,l} w(i,j,k,l)}
 \end{equation}&lt;/p&gt;
&lt;p&gt;而加权系数$w(i,j,k,l)$取决于定义域核和值域核的乘积。&lt;/p&gt;
&lt;p&gt;其中定义域核表示如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
d(i,j,k,l) = exp(-\frac{(i-k)^2+(j-l)^2}{2\sigma_d^2})
\end{equation}&lt;/p&gt;
&lt;p&gt;定义域滤波对应图示：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Bilateral Filter I" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bilateral_filter_1_zpsdf39e81d.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;值域核表示为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
r(i,j,k,l) = exp(-\frac{||f(i,j)-f(k,l)||^2}{2\sigma_r^2})
\end{equation}&lt;/p&gt;
&lt;p&gt;值域滤波：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Bilateral Filter II" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bilateral_filter_2_zpsfb1c3bd4.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;两者相乘后，就会产生依赖于数据的双边滤波权重函数：&lt;/p&gt;
&lt;p&gt;\begin{equation}
w(i,j,k,l) = exp(-\frac{(i-k)^2+(j-l)^2}{2\sigma_d^2}-\frac{||f(i,j)-f(k,l)||^2}{2\sigma_r^2})
\end{equation}&lt;/p&gt;
&lt;p&gt;好了,关于中值滤波和双边滤波就介绍到这里啦。以下给出源码示例,图片还是采用椒盐化后的&lt;strong&gt;LENA&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt="LENA PEPPER" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lena_pepper_zps03b26e7f.jpg" /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include "opencv2/core/core.hpp"
#include "opencv2/highgui/highgui.hpp"
#include "opencv2/imgproc/imgproc.hpp"

using namespace cv;

int main( )
{
       Mat image=imread("/home/qingyuanxingsi/workspace/C++/contents/lena_pepper.jpg");

       namedWindow("Median Blur" );

       //Median Blur
       Mat out;
       medianBlur(image, out, 7);
       imshow("Median Blur" ,out );
       imwrite("medianBlur.jpg",out);

       namedWindow("Bilateral Blur");
       //Bilateral Blur
       Mat bilateralBlur;
       bilateralFilter( image, bilateralBlur, 25, 25*2, 25/2 );
       imshow("Bilateral Blur" ,bilateralBlur );
       imwrite("bilateralBlur.jpg",bilateralBlur);

       waitKey(0);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;经过中值滤波处理后得到的图像如下所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Median Filter" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/medianBlur_zpse9e58cde.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;双边滤波后则得到如下图像:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Bilateral Filter" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bilateralBlur_zps500576ef.jpg" /&gt;&lt;/p&gt;
&lt;h1 id="_15"&gt;形态学图像处理&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id="_16"&gt;相关理论与概念&lt;/h2&gt;
&lt;h3 id="_17"&gt;形态学概述&lt;/h3&gt;
&lt;p&gt;形态学（morphology）一词通常表示生物学的一个分支，该分支主要研究动植物的形态和结构。而我们图像处理中指的形态学，往往表示的是数学形态学。下面一起来了解数学形态学的概念。&lt;/p&gt;
&lt;p&gt;数学形态学（Mathematical morphology)是一门建立在格论和拓扑学基础之上的图像分析学科，是数学形态学图像处理的基本理论。其基本的运算包括：二值腐蚀和膨胀、二值开闭运算、骨架抽取、极限腐蚀、击中击不中变换、形态学梯度、Top-hat变换、颗粒分析、流域变换、灰值腐蚀和膨胀、灰值开闭运算、灰值形态学梯度等。&lt;/p&gt;
&lt;p&gt;简单来讲，形态学操作就是基于形状的一系列图像处理操作。其中,最基本的形态学操作有二种------膨胀与腐蚀(Dilation与Erosion)。&lt;/p&gt;
&lt;p&gt;膨胀与腐蚀能实现多种多样的功能，主要如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;消除噪声&lt;/li&gt;
&lt;li&gt;分割(isolate)出独立的图像元素，在图像中连接(join)相邻的元素。&lt;/li&gt;
&lt;li&gt;寻找图像中的明显的极大值区域或极小值区域&lt;/li&gt;
&lt;li&gt;求出图像的梯度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们在这里给出下文会用到的，用于对比膨胀与腐蚀运算的“青原行思”字样毛笔字原图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="qingyuanxingsi" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/qingyuanxingsi_zpseffb8780.png" /&gt;&lt;/p&gt;
&lt;p&gt;在进行腐蚀和膨胀的讲解之前，首先需要注意，&lt;strong&gt;腐蚀和膨胀是对白色部分（高亮部分）而言的，不是黑色部分。膨胀就是图像中的高亮部分进行膨胀，“领域扩张”，效果图拥有比原图更大的高亮区域。腐蚀就是原图中的高亮部分被腐蚀，“领域被蚕食”，效果图拥有比原图更小的高亮区域&lt;/strong&gt;。&lt;/p&gt;
&lt;h3 id="_18"&gt;膨胀&lt;/h3&gt;
&lt;p&gt;其实，膨胀就是求&lt;strong&gt;局部最大值&lt;/strong&gt;的操作。&lt;/p&gt;
&lt;p&gt;按数学方面来说，膨胀或者腐蚀操作就是将图像（或图像的一部分区域，我们称之为$A$）与核（我们称之为$B$）进行卷积。
核可以是任何的形状和大小，它拥有一个单独定义出来的参考点，我们称其为锚点（anchorpoint）。多数情况下，核是一个小的中间带有参考点和实心正方形或者圆盘，其实，我们可以把核视为模板或者掩码。&lt;/p&gt;
&lt;p&gt;而膨胀就是求局部最大值的操作，核$B$与图形卷积，即计算核$B$覆盖的区域的像素点的最大值，并把这个最大值赋值给参考点指定的像素。这样就会使图像中的高亮区域逐渐增长。如下图所示，这就是膨胀操作的初衷。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Dilation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/dilation_zps98aef8c3.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;膨胀的数学表达式：&lt;/p&gt;
&lt;p&gt;\begin{equation}
dst(x,y) = max_{(x\prime,y\prime):element(x\prime,y\prime) \neq 0} src(x+x\prime,y+y\prime)
\end{equation}&lt;/p&gt;
&lt;p&gt;膨胀效果图（毛笔字）：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Dilation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/dilateImage_zps0dd1d7a2.png" /&gt;&lt;/p&gt;
&lt;h3 id="_19"&gt;腐蚀&lt;/h3&gt;
&lt;p&gt;再来看一下腐蚀，大家应该知道，膨胀和腐蚀是一对好基友，是相反的一对操作，所以腐蚀就是求局部最小值的操作。
我们一般都会把腐蚀和膨胀对应起来理解和学习。下文就可以看到，两者的函数原型也是基本上一样的。&lt;/p&gt;
&lt;p&gt;原理图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Erosion" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/erosion_zpsed6af489.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;腐蚀的数学表达式：&lt;/p&gt;
&lt;p&gt;\begin{equation}
dst(x,y) = min_{(x\prime,y\prime):element(x\prime,y\prime) \neq 0} src(x+x\prime,y+y\prime)
\end{equation}&lt;/p&gt;
&lt;p&gt;腐蚀效果图（毛笔字）：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Erosion" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/erodeImage_zpsd284dea2.png" /&gt;&lt;/p&gt;
&lt;p&gt;代码示例如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#include &amp;lt;opencv2/core/core.hpp&amp;gt;
#include&amp;lt;opencv2/highgui/highgui.hpp&amp;gt;
#include&amp;lt;opencv2/imgproc/imgproc.hpp&amp;gt;
#include &amp;lt;iostream&amp;gt;

using namespace std;
using namespace cv;

int main(  )
{
       //Load original Image
       Mat image = imread("/home/qingyuanxingsi/workspace/C++/contents/qingyuanxingsi.png");

       //Create the window
       namedWindow("Dilation");
       namedWindow("Erosion");

        //Get Self-defined kernel
       Mat element = getStructuringElement(MORPH_RECT, Size(5, 5));
       Mat dilateImage;
       dilate(image,dilateImage, element);

       imshow("Dilation", dilateImage);
       imwrite("dilateImage.png",dilateImage);

       Mat erodeImage;
       erode(image,erodeImage, element);

       imshow("Erosion", erodeImage);
       imwrite("erodeImage.png",erodeImage);

       waitKey(0);

       return 0;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;生成的效果图上面已给出。&lt;/p&gt;
&lt;h1 id="_20"&gt;形态学处理进阶&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;首先呢，要知道形态学的高级形态，往往都是建立在腐蚀和膨胀这两个基本操作之上的。对膨胀和腐蚀心中有数了，接下来的高级形态学操作，应该就不难理解。&lt;/p&gt;
&lt;h2 id="opening-operation"&gt;开运算（Opening Operation）&lt;/h2&gt;
&lt;p&gt;开运算（Opening Operation），其实就是先腐蚀后膨胀的过程。其数学表达式如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
dst = open(src,element) = dilate(erode(src,element))
\end{equation}&lt;/p&gt;
&lt;p&gt;开运算可以用来消除小物体、在纤细点处分离物体、平滑较大物体的边界的同时并不明显改变其面积。效果图是这样的：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Open" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/openImage_zps8b0d1f5c.png" /&gt;&lt;/p&gt;
&lt;h2 id="closing-operation"&gt;闭运算(Closing Operation)&lt;/h2&gt;
&lt;p&gt;先膨胀后腐蚀的过程称为闭运算(Closing Operation)，其数学表达式如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
dst = close(src,element) = erode(dilate(src,element))
\end{equation}&lt;/p&gt;
&lt;p&gt;闭运算能够排除小型黑洞(黑色区域)。效果图如下所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Close" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/closeImage_zpse027027f.png" /&gt;&lt;/p&gt;
&lt;h2 id="morphologicalgradient"&gt;形态学梯度（MorphologicalGradient）&lt;/h2&gt;
&lt;p&gt;形态学梯度（Morphological Gradient）为膨胀图与腐蚀图之差，数学表达式如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
dst &amp;amp;= gradient(src,element) \\
&amp;amp;= dilate(src-element) - erode(src,element)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;对二值图像进行这一操作可以将团块（blob）的边缘突出出来。我们可以用形态学梯度来保留物体的边缘轮廓，如下所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gradient" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gradientImage_zpsf671d62e.png" /&gt;&lt;/p&gt;
&lt;h2 id="top-hat"&gt;顶帽（Top Hat）&lt;/h2&gt;
&lt;p&gt;顶帽运算（Top Hat）又常常被译为”礼帽“运算。为原图像与上文刚刚介绍的“开运算“的结果图之差，数学表达式如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
dst = tophat(src,element) = src - open(src,element)
\end{equation}&lt;/p&gt;
&lt;p&gt;因为开运算带来的结果是放大了裂缝或者局部低亮度的区域，因此，从原图中减去开运算后的图，得到的效果图突出了比原图轮廓周围的区域更明亮的区域，且这一操作和选择的核的大小相关。&lt;/p&gt;
&lt;p&gt;顶帽运算往往用来分离比邻近点亮一些的斑块。当一幅图像具有大幅的背景的时候，而微小物品比较有规律的情况下，可以使用顶帽运算进行背景提取。如下所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Tophat" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/tophatImage_zps1073d1fb.png" /&gt;&lt;/p&gt;
&lt;h2 id="black-hat"&gt;黑帽（Black Hat）&lt;/h2&gt;
&lt;p&gt;黑帽（Black Hat）运算为”闭运算“的结果图与原图像之差。数学表达式为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
dst = blackhat(src,element) = close(src,element)-src
\end{equation}&lt;/p&gt;
&lt;p&gt;黑帽运算后的效果图突出了比原图轮廓周围的区域更暗的区域，且这一操作和选择的核的大小相关。所以，黑帽运算用来分离比邻近点暗一些的斑块。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Black Hat" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/blackhatImage_zps354ea31c.png" /&gt;&lt;/p&gt;
&lt;p&gt;还是使用我们之前的毛笔字图片,增加的部分代码如下:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Mat openImage;
namedWindow("Open OP");
//Open operation
morphologyEx(image,openImage, MORPH_OPEN, element);
imshow("Open OP", openImage);
imwrite("openImage.png",openImage);

Mat closeImage;
namedWindow("Close OP");
//Close operation
morphologyEx(image,closeImage, MORPH_CLOSE, element);
imshow("Close OP", closeImage);
imwrite("closeImage.png",closeImage);

Mat gradientImage;
namedWindow("Gradient OP");
//gradient operation
morphologyEx(image,gradientImage, MORPH_GRADIENT, element);
imshow("Gradient OP", gradientImage);
imwrite("gradientImage.png",gradientImage);

Mat tophatImage;
namedWindow("Tophat OP");
//Tophat operation
morphologyEx(image,tophatImage, MORPH_TOPHAT, element);
imshow("Tophat OP", tophatImage);
imwrite("tophatImage.png",tophatImage);

Mat blackhatImage;
namedWindow("Blackhat OP");
//Blackhat operation
morphologyEx(image,blackhatImage, MORPH_BLACKHAT, element);
imshow("Blackhat OP", blackhatImage);
imwrite("blackhatImage.png",blackhatImage);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;效果图以上已给出。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;TODO BOARD:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;密集光流算法&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h1 id="_21"&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/honpey/article/details/8770919"&gt;直方图均衡化原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/sundy_2004/article/details/7259614"&gt;OpenCV直方图均衡化(cvEqualizeHist)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Histogram_equalization"&gt;Histogram equalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/poem_qianmo/article/details/22745559#"&gt;【OpenCV入门教程之八】线性邻域滤波专场：方框滤波、均值滤波与高斯滤波&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/poem_qianmo/article/details/23184547"&gt;【OpenCV入门教程之九】 非线性滤波专场：中值滤波、双边滤波&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/poem_qianmo/article/details/23710721"&gt;【OpenCV入门教程之十】 形态学图像处理（一）：膨胀与腐蚀&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/poem_qianmo/article/details/23710721"&gt;【OpenCV入门教程之十一】 形态学图像处理（二）：开运算、闭运算、形态学梯度、顶帽、黑帽合辑&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="计算机视觉"></category><category term="图像处理"></category></entry><entry><title>机器学习实战(I):手写数字识别</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-shi-zhan-ishou-xie-shu-zi-shi-bie.html" rel="alternate"></link><updated>2014-05-13T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-05-13:ji-qi-xue-xi-shi-zhan-ishou-xie-shu-zi-shi-bie.html</id><summary type="html">&lt;p&gt;从大三到现在一直在学习机器学习相关的理论,可是隐约间觉得自己对于机器学习算法的理解还只是停留在理论层面,理解还是很浅薄的,并不深刻。此外,最近听到了一些关于机器学习理论与实践关系的说法,觉得挺有道理的,摘录如下:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;只有真正将机器学习算法应用到某个具体问题上的时候,你的学习才刚刚开始。&lt;/li&gt;
&lt;li&gt;机器学习的过程实际上是一个学习理论、实践、学习理论、实践...的过程,只有经过这样一轮又一轮的过滤,你才能慢慢的开始理解机器学习算法。从哲学上来说,其实这就是一个理论联系实际的过程。&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Talk is cheap,show me your code&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;When I cannot create,I do not understand&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;总而言之,言而总之,婶婶地觉得我该实践实践了,真正针对实际问题,动手去实现一些机器学习算法,并检验它们的实际效果。&lt;/p&gt;
&lt;p&gt;作为机器学习实战系列的第一篇,本博文选取的问题来自&lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt;,我选取了&lt;a href="https://www.kaggle.com/c/digit-recognizer"&gt;Digit Recognizer&lt;/a&gt;这个问题,于是牛刀霍霍,开始解剖这个问题。&lt;/p&gt;
&lt;p&gt;根据该问主页上的提示,我打算实现Random Forest以及Convolutional Network来检验这两个问题针对手写数字识别这个问题的实际应用效果,以下就记录一下整个过程吧。&lt;/p&gt;
&lt;h1 id="_1"&gt;随机森林&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;随机森林算法是在决策树算法的基础上提出,关于决策树算法的相关知识可参考我之前写的一篇&lt;a href="http://www.qingyuanxingsi.com/Decision%20Tree.html"&gt;机器学习系列(I):决策树算法&lt;/a&gt;,它的核心思想是随机选取训练样本以及随机选取特征,从而针对这些训练样本以及随机选取的特征根据决策树算法构建多棵决策树,最后综合多棵决策树的结果得出须预测样本的分类。由于之前实现过决策树算法,于是这里我就不想实现这个算法了,具体采用&lt;strong&gt;scikit&lt;/strong&gt;工具包中提供的随机森林分类器进行训练。&lt;/p&gt;
&lt;p&gt;具体实现时,由于Kaggle提供的测试集是以csv格式提供的,在一大通搜索之后,我最后采用了&lt;a href="http://pandas.pydata.org/"&gt;pandas&lt;/a&gt;库处理文件输入及输出问题,根据其官网上的说明:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Pandas&lt;/strong&gt; is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;可见&lt;strong&gt;pandas&lt;/strong&gt;是针对python开发的一个高性能且易用的数据结构以及数据分析工具库。在处理Kaggle数据的时候用了一下,个人感觉还是挺好用的。&lt;/p&gt;
&lt;p&gt;另外需要说明的一点是,面对MNIST数据集的时候,最先想到的一个问题是如何提取特征以用于手写数字识别的问题中,大概查了一下,一种比较简单的方法是将整个图像分成很多不重叠的2*2的小区域,然后统计每个小区域中不为0的像素值的总数(另,可针对不同的区域设置不同的权重)。在查找的过程中也发现了很多其他方法,有兴趣的朋友可以自行Google之。因为Kaggle提供的训练集每张图片都是一个长度为784的长向量,个人觉得计算2*2小区域包含不为0像素的总数不是很方便(现在看来还是挺简单的昂),所以直接一股脑地把整个784位的向量作为特征扔给随机森林分类器了。(&lt;strong&gt;唉,现在觉得自己太不负责任了,怎么能够这样呢?&lt;/strong&gt;).以下给出采用随机森林进行训练的源代码。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#-*- coding:utf-8 -*-
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

'''
A simple random forest program for minist digit recognition
@author:qingyuanxingsi
@date:2014/05/11
'''

csvPath = '/home/qingyuanxingsi/data/digit_recognizer/train.csv'
testPath = '/home/qingyuanxingsi/data/digit_recognizer/test.csv'
destPath = '/home/qingyuanxingsi/data/digit_recognizer/result.csv'
n_samples = 42000

#Loading the csv data
digitArray = pd.read_csv(csvPath).as_matrix()

#Split the data into two parts:data and tag
dataArray = digitArray[:,1:]
tagArray = digitArray[:,0:1].reshape(n_samples,)

#Feed the data and tag into the random forest classifier of the scikit toolkit

randomForestClassifier = RandomForestClassifier(n_estimators = 10)
randomForestClassifier.fit(dataArray,tagArray)

#Read the test dat in
testDat = pd.read_csv(testPath).as_matrix()

tags = []
for i in range(testDat.shape[0]):
    tags.append(randomForestClassifier.predict(testDat[i]))

pd.Series(tags).to_csv(destPath,index=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;得到分析结果并提交到Kaggle之后,最后的准确率没我想象的那么惨不忍睹,Kaggle选取部分测试集进行测试之后给的SCORE是94%左右,效果也不是灰常灰常差啦。&lt;/p&gt;
&lt;p&gt;当然,这个准确率个人实在不是很满意,所以打算拿Convolutional Neural Network试试。&lt;/p&gt;
&lt;h1 id="_2"&gt;卷积神经网络&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id="_3"&gt;概述&lt;/h2&gt;
&lt;p&gt;卷积神经网络是一种特殊的深层的神经网络模型，它的特殊性体现在两个方面，一方面它的神经元间的连接是&lt;strong&gt;非全连接&lt;/strong&gt;的，另一方面同一层中某些神经元之间的连接的&lt;strong&gt;权重是共享的&lt;/strong&gt;（即相同的）。它的非全连接和权值共享的网络结构使之更类似于生物神经网络，降低了网络模型的复杂度（对于很难学习的深层结构来说，这是非常重要的），减少了权值的数量。&lt;/p&gt;
&lt;p&gt;卷积网络最初是受视觉神经机制的启发而设计的，是为识别二维形状而设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者其他形式的变形具有高度不变性。1962年Hubel和Wiesel通过对猫视觉皮层细胞的研究，提出了&lt;strong&gt;感受野(receptive field)&lt;/strong&gt;的概念，1984年日本学者Fukushima基于感受野概念提出的神经认知机(neocognitron)模型，它可以看作是卷积神经网络的第一个实现网络，也是感受野概念在人工神经网络领域的首次应用。&lt;/p&gt;
&lt;p&gt;神经认知机将一个视觉模式分解成许多子模式(特征)，然后进入分层递阶式相连的特征平面进行处理，它试图将视觉系统模型化，使其能够在即使物体有位移或轻微变形的时候，也能完成识别。神经认知机能够利用位移恒定能力从激励模式中学习，并且可识别这些模式的变化形。在其后的应用研究中，Fukushima将神经认知机主要用于手写数字的识别。随后，国内外的研究人员提出多种卷积神经网络形式，在邮政编码识别（Y. LeCun etc）、车牌识别和人脸识别等方面得到了广泛的应用。&lt;/p&gt;
&lt;h2 id="cnn"&gt;CNN的结构&lt;/h2&gt;
&lt;p&gt;卷积网络是为识别二维形状而特殊设计的一个多层感知器，这种网络结构对平移、比例缩放、倾斜或者共他形式的变形具有高度不变性。 这些良好的性能是网络在有监督方式下学会的，网络的结构主要有稀疏连接和权值共享两个特点，包括如下形式的约束：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;特征提取。每一个神经元从上一层的局部接受域得到突触输人，因而迫使它提取局部特征。一旦一个特征被提取出来，只要它相对于其他特征的位置被近似地保留下来，它的精确位置就变得没有那么重要了。&lt;/li&gt;
&lt;li&gt;特征映射。网络的每一个计算层都是由多个特征映射组成的，每个特征映射都是平面形式的。平面中单独的神经元在约束下共享相同的突触权值集，这种结构形式具有如下的有益效果：a.平移不变性。b.自由参数数量的缩减(通过权值共享实现)。&lt;/li&gt;
&lt;li&gt;子抽样。每个卷积层跟着一个实现局部平均和子抽样的计算层，由此特征映射的分辨率降低。这种操作具有使特征映射的输出对平移和其他形式的变形的敏感度下降的作用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="sparse-connectivity"&gt;稀疏连接(Sparse Connectivity)&lt;/h3&gt;
&lt;p&gt;卷积网络通过在相邻两层之间强制使用局部连接模式来利用图像的空间局部特性，在第$m$层的隐层单元只与第$m-1$层的输入单元的局部区域有连接，第$m-1$层的这些局部区域被称为空间连续的接受域。我们可以将这种结构描述如下：设第$m-1$层为视网膜输入层，第$m$层的接受域的宽度为3，也就是说该层的每个单元与且仅与输入层的3个相邻的神经元相连，第$m$层与第$m+1$层具有类似的链接规则，如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="CNN_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cnn_1_zpsa4a2fbd1.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;可以看到$m+1$层的神经元相对于第$m$层的接受域的宽度也为3，但相对于输入层的接受域为5，这种结构将学习到的过滤器（对应于输入信号中被最大激活的单元)限制在局部空间模式（因为每个单元对它接受域外的variation不做反应）。从上图也可以看出，多个这样的层堆叠起来后，会使得过滤器（不再是线性的）逐渐成为全局的（也就是覆盖到了更大的视觉区域）。例如上图中第$m+1$层的神经元可以对宽度为5的输入进行一个非线性的特征编码。&lt;/p&gt;
&lt;h3 id="shared-weights"&gt;权值共享(Shared Weights)&lt;/h3&gt;
&lt;p&gt;在卷积网络中，每个稀疏过滤器$h_i$通过共享权值都会覆盖整个可视域，这些共享权值的单元构成一个特征映射，如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="CNN_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cnn_2_zps12bba757.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;在图中，有3个隐层单元，他们属于同一个特征映射。同种颜色的链接的权值是相同的，我们仍然可以使用梯度下降的方法来学习这些权值，只需要对原始算法做一些小的改动，这里共享权值的梯度是所有共享参数的梯度的总和。我们不禁会问为什么要权重共享呢？一方面，重复单元能够对特征进行识别，而不考虑它在可视域中的位置。另一方面，权值共享使得我们能更有效的进行特征抽取，因为它极大的减少了需要学习的自由变量的个数。通过控制模型的规模，卷积网络对视觉问题可以具有很好的泛化能力。&lt;/p&gt;
&lt;h3 id="the-full-model"&gt;The Full Model&lt;/h3&gt;
&lt;p&gt;卷积神经网络是一个多层的神经网络，每层由多个二维平面组成，而每个平面由多个独立神经元组成。网络中包含一些简单元和复杂元，分别记为S-元和C-元。S-元聚合在一起组成S-面，S-面聚合在一起组成S-层，用Us表示。C-元、C-面和C-层($U_s$)之间存在类似的关系。网络的任一中间级由S-层与C-层串接而成，而输入级只含一层，它直接接受二维视觉模式，样本特征提取步骤已嵌入到卷积神经网络模型的互联结构中。&lt;/p&gt;
&lt;p&gt;一般地，$U_s$为特征提取层，每个神经元的输入与前一层的局部感受野相连，并提取该局部的特征，一旦该局部特征被提取后，它与其他特征间的位置关系 也随之确定下来；$U_c$是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射为一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性(这一句表示没看懂，那位如果看懂了，请给我讲解一下)。此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数，降低了网络参数选择的复杂度。卷积神经网络中的每一个特征提取层(S-层)都紧跟着一个用来求局部平均与二次提取的计算层(C-层)，这种特有的两次特征提取结构使网络在识别时对输入样本有较高的畸变容忍能力。&lt;/p&gt;
&lt;p&gt;下图是一个卷积网络的实例:&lt;/p&gt;
&lt;p&gt;&lt;img alt="CNN_3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cnn_3_zps8de51bda.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;图中的卷积网络工作流程如下，输入层由32×32个感知节点组成，接收原始图像。然后，计算流程在卷积和子抽样之间交替进行，如下所 述：第一隐藏层进行卷积，它由8个特征映射组成，每个特征映射由28×28个神经元组成，每个神经元指定一个5×5的接受域；第二隐藏层实现子 抽样和局部平均，它同样由8个特征映射组成，但其每个特征映射由14×14个神经元组成。每个神经元具有一个 2×2 的接受域，一个可训练系数，一个可训练偏置和一个sigmoid激活函数。可训练系数和偏置控制神经元的操作点。第三隐藏层进行第二次卷积，它由20个特征映射组成每个特征映射由10×10个神经元组成。该隐藏层中的每个神经元可能具有和下一个隐藏层几个特征映射相连的突触连接，它以与第一个卷积层相似的方式操作。第四个隐藏层进行第二次子抽样和局部平均汁算。它由20个特征映射组成，但每个特征映射由5×5个神经元组成，它以与第一次抽样相似的方式操作。第五个隐藏层实现卷积的最后阶段，它由 120 个神经元组成，每个神经元指定一个5×5的接受域。最后是个全连接层，得到输出向量。相继的计算层在卷积和抽样之间的连续交替，我们得到一个“双尖塔”的效果，也就是在每个卷积或抽样层，随着空间分辨率下降，与相应的前一层相比特征映射的数量增加。卷积之后进行子抽样的思想是受到动物视觉系统中的“简单的”细胞后面跟着“复杂的”细胞的想法的启发而产生的。图中所示的多层感知器包含近似100000个突触连接，但只有大约2600个自由参数。自由参数在数量上显著地减少是通过权值共享获得 的，学习机器的能力（以VC维的形式度量）因而下降，这又提高它的泛化能力。而且它对自由参数的调整通过反向传播学习的随机形式来实 现。另一个显著的特点是使用权值共享使得以并行形式实现卷积网络变得可能。这是卷积网络对全连接的多层感知器而言的另一个优点。&lt;/p&gt;
&lt;p&gt;以上我们简要介绍了一下卷积神经网络的基本概念和基本架构,如何对每个部分进行简单的代码实现可参考&lt;a href="http://www.deeplearning.net/tutorial/lenet.html"&gt;Convolutional Neural Networks (LeNet)&lt;/a&gt;,里面对如何对卷积以及Pooling进行代码实现做了粗略的介绍。实际上,我构建的用于识别手写数字的程序也是在以上代码的基础上理解之后稍加修改后形成的。以下给出具体代码(以下代码中&lt;a href="http://deeplearning.net/tutorial/code/mlp.py"&gt;mlp&lt;/a&gt;以及&lt;a href="http://deeplearning.net/tutorial/code/logistic_sgd.py"&gt;logistic_sgd&lt;/a&gt;模块可从文字上给出的链接处下载):&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import cPickle
import gzip
import os
import sys
import time

import numpy

import theano
import theano.tensor as T
from theano.tensor.signal import downsample
from theano.tensor.nnet import conv

from logistic_sgd import LogisticRegression, load_data
from mlp import HiddenLayer
import pandas as pd


class LeNetConvPoolLayer(object):
    """Pool Layer of a convolutional network """

    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2)):
        """
        Allocate a LeNetConvPoolLayer with shared variable internal parameters.

        :type rng: numpy.random.RandomState
        :param rng: a random number generator used to initialize weights

        :type input: theano.tensor.dtensor4
        :param input: symbolic image tensor, of shape image_shape

        :type filter_shape: tuple or list of length 4
        :param filter_shape: (number of filters, num input feature maps,
                              filter height,filter width)

        :type image_shape: tuple or list of length 4
        :param image_shape: (batch size, num input feature maps,
                             image height, image width)

        :type poolsize: tuple or list of length 2
        :param poolsize: the downsampling (pooling) factor (#rows,#cols)
        """

        assert image_shape[1] == filter_shape[1]
        self.input = input

        # there are "num input feature maps * filter height * filter width"
        # inputs to each hidden unit
        fan_in = numpy.prod(filter_shape[1:])
        # each unit in the lower layer receives a gradient from:
        # "num output feature maps * filter height * filter width" /
        #   pooling size
        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) /
                   numpy.prod(poolsize))
        # initialize weights with random weights
        W_bound = numpy.sqrt(6. / (fan_in + fan_out))
        self.W = theano.shared(numpy.asarray(
            rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),
            dtype=theano.config.floatX),
                               borrow=True)

        # the bias is a 1D tensor -- one bias per output feature map
        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX)
        self.b = theano.shared(value=b_values, borrow=True)

        # convolve input feature maps with filters
        conv_out = conv.conv2d(input=input, filters=self.W,
                filter_shape=filter_shape, image_shape=image_shape)

        # downsample each feature map individually, using maxpooling
        pooled_out = downsample.max_pool_2d(input=conv_out,
                                            ds=poolsize, ignore_border=True)

        # add the bias term. Since the bias is a vector (1D array), we first
        # reshape it to a tensor of shape (1,n_filters,1,1). Each bias will
        # thus be broadcasted across mini-batches and feature map
        # width &amp;amp; height
        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))

        # store parameters of this layer
        self.params = [self.W, self.b]


def evaluate_lenet5(learning_rate=0.1, n_epochs=200,
                    dataset='/home/qingyuanxingsi/data/digit_recognizer/mnist.pkl.gz',
                    nkerns=[20, 50], batch_size=500):
    """ Demonstrates lenet on MNIST dataset

    :type learning_rate: float
    :param learning_rate: learning rate used (factor for the stochastic
                          gradient)

    :type n_epochs: int
    :param n_epochs: maximal number of epochs to run the optimizer

    :type dataset: string
    :param dataset: path to the dataset used for training /testing (MNIST here)

    :type nkerns: list of ints
    :param nkerns: number of kernels on each layer
    """

    rng = numpy.random.RandomState(23455)

    datasets = load_data(dataset)

    train_set_x, train_set_y = datasets[0]
    valid_set_x, valid_set_y = datasets[1]
    test_set_x, test_set_y = datasets[2]

    #import my own dataset
    predict_set = pd.read_csv('/home/qingyuanxingsi/data/digit_recognizer/test.csv').as_matrix()
    predict_set = predict_set/256.0
    predict_set_x = theano.shared(numpy.asarray(predict_set,
                                               dtype=theano.config.floatX),
                                 borrow=True)
    predict_set_tag = numpy.zeros([predict_set.shape[0],])

    # compute number of minibatches for training, validation and testing
    n_train_batches = train_set_x.get_value(borrow=True).shape[0]
    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]
    n_test_batches = test_set_x.get_value(borrow=True).shape[0]
    n_predict_batches = predict_set_x.get_value(borrow=True).shape[0]
    n_train_batches /= batch_size
    n_valid_batches /= batch_size
    n_test_batches /= batch_size
    n_predict_batches /= batch_size

    # allocate symbolic variables for the data
    index = T.lscalar()  # index to a [mini]batch
    x = T.matrix('x')   # the data is presented as rasterized images
    y = T.ivector('y')  # the labels are presented as 1D vector of
                        # [int] labels

    ishape = (28, 28)  # this is the size of MNIST images

    ######################
    # BUILD ACTUAL MODEL #
    ######################
    print '... building the model'

    # Reshape matrix of rasterized images of shape (batch_size,28*28)
    # to a 4D tensor, compatible with our LeNetConvPoolLayer
    layer0_input = x.reshape((batch_size, 1, 28, 28))

    # Construct the first convolutional pooling layer:
    # filtering reduces the image size to (28-5+1,28-5+1)=(24,24)
    # maxpooling reduces this further to (24/2,24/2) = (12,12)
    # 4D output tensor is thus of shape (batch_size,nkerns[0],12,12)
    layer0 = LeNetConvPoolLayer(rng, input=layer0_input,
            image_shape=(batch_size, 1, 28, 28),
            filter_shape=(nkerns[0], 1, 5, 5), poolsize=(2, 2))

    # Construct the second convolutional pooling layer
    # filtering reduces the image size to (12-5+1,12-5+1)=(8,8)
    # maxpooling reduces this further to (8/2,8/2) = (4,4)
    # 4D output tensor is thus of shape (nkerns[0],nkerns[1],4,4)
    layer1 = LeNetConvPoolLayer(rng, input=layer0.output,
            image_shape=(batch_size, nkerns[0], 12, 12),
            filter_shape=(nkerns[1], nkerns[0], 5, 5), poolsize=(2, 2))

    # the HiddenLayer being fully-connected, it operates on 2D matrices of
    # shape (batch_size,num_pixels) (i.e matrix of rasterized images).
    # This will generate a matrix of shape (20,32*4*4) = (20,512)
    layer2_input = layer1.output.flatten(2)

    # construct a fully-connected sigmoidal layer
    layer2 = HiddenLayer(rng, input=layer2_input, n_in=nkerns[1] * 4 * 4,
                         n_out=500, activation=T.tanh)

    # classify the values of the fully-connected sigmoidal layer
    layer3 = LogisticRegression(input=layer2.output, n_in=500, n_out=10)

    # the cost we minimize during training is the NLL of the model
    cost = layer3.negative_log_likelihood(y)

    # create a function to compute the mistakes that are made by the model
    test_model = theano.function([index], layer3.errors(y),
             givens={
                x: test_set_x[index * batch_size: (index + 1) * batch_size],
                y: test_set_y[index * batch_size: (index + 1) * batch_size]})

    validate_model = theano.function([index], layer3.errors(y),
            givens={
                x: valid_set_x[index * batch_size: (index + 1) * batch_size],
                y: valid_set_y[index * batch_size: (index + 1) * batch_size]})

    # create a list of all model parameters to be fit by gradient descent
    params = layer3.params + layer2.params + layer1.params + layer0.params

    # create a list of gradients for all model parameters
    grads = T.grad(cost, params)

    # train_model is a function that updates the model parameters by
    # SGD Since this model has many parameters, it would be tedious to
    # manually create an update rule for each model parameter. We thus
    # create the updates list by automatically looping over all
    # (params[i],grads[i]) pairs.
    updates = []
    for param_i, grad_i in zip(params, grads):
        updates.append((param_i, param_i - learning_rate * grad_i))

    train_model = theano.function([index], cost, updates=updates,
          givens={
            x: train_set_x[index * batch_size: (index + 1) * batch_size],
            y: train_set_y[index * batch_size: (index + 1) * batch_size]})

    ###############
    # TRAIN MODEL #
    ###############
    print '... training'
    # early-stopping parameters
    patience = 10000  # look as this many examples regardless
    patience_increase = 2  # wait this much longer when a new best is
                           # found
    improvement_threshold = 0.995  # a relative improvement of this much is
                                   # considered significant
    validation_frequency = min(n_train_batches, patience / 2)
                                  # go through this many
                                  # minibatche before checking the network
                                  # on the validation set; in this case we
                                  # check every epoch

    best_params = None
    best_validation_loss = numpy.inf
    best_iter = 0
    test_score = 0.
    start_time = time.clock()

    epoch = 0
    done_looping = False

    while (epoch &amp;lt; n_epochs) and (not done_looping):
        epoch = epoch + 1
        for minibatch_index in xrange(n_train_batches):

            iter = (epoch - 1) * n_train_batches + minibatch_index

            if iter % 100 == 0:
                print 'training @ iter = ', iter
            cost_ij = train_model(minibatch_index)

            if (iter + 1) % validation_frequency == 0:

                # compute zero-one loss on validation set
                validation_losses = [validate_model(i) for i
                                     in xrange(n_valid_batches)]
                this_validation_loss = numpy.mean(validation_losses)
                print('epoch %i, minibatch %i/%i, validation error %f %%' % \
                      (epoch, minibatch_index + 1, n_train_batches, \
                       this_validation_loss * 100.))

                # if we got the best validation score until now
                if this_validation_loss &amp;lt; best_validation_loss:

                    #improve patience if loss improvement is good enough
                    if this_validation_loss &amp;lt; best_validation_loss *  \
                       improvement_threshold:
                        patience = max(patience, iter * patience_increase)

                    # save best validation score and iteration number
                    best_validation_loss = this_validation_loss
                    best_iter = iter

                    # test it on the test set
                    test_losses = [test_model(i) for i in xrange(n_test_batches)]
                    test_score = numpy.mean(test_losses)
                    print(('     epoch %i, minibatch %i/%i, test error of best '
                           'model %f %%') %
                          (epoch, minibatch_index + 1, n_train_batches,
                           test_score * 100.))

            if patience &amp;lt;= iter:
                done_looping = True
                break

    end_time = time.clock()
    print('Optimization complete.')
    print('Best validation score of %f %% obtained at iteration %i,'\
          'with test performance %f %%' %
          (best_validation_loss * 100., best_iter + 1, test_score * 100.))
    print &amp;gt;&amp;gt; sys.stderr, ('The code for file ' +
                          os.path.split(__file__)[1] +
                          ' ran for %.2fm' % ((end_time - start_time) / 60.))

    ###############
    # PREDICT MODEL #
    ###############
    print '... predicting'
    predict_model = theano.function([index], layer3.y_pred,
            givens={
                x: predict_set_x[index * batch_size: (index + 1) * batch_size]})
    predict_set_y = [predict_model(i) for i
                                     in xrange(n_predict_batches)]
    pd.DataFrame(predict_set_y).to_csv('/home/qingyuanxingsi/data/digit_recognizer/result_cnn.csv')


if __name__ == '__main__':
    evaluate_lenet5()


def experiment(state, channel):
    evaluate_lenet5(state.learning_rate, dataset=state.dataset)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;由于昨天晚上将n_epoches设置成200的时候,程序迭代到一定次数我的渣机就直接崩掉了,所以今天把它直接设置成60了,跑了大概4-5个小时后,最后终于拿到了结果,截图如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt="CNN_PERFORMANCE" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cnn_performance_zps05d5f2b6.png" /&gt;&lt;/p&gt;
&lt;p&gt;把预测结果提交给Kaggle之后,Score攀升至99.6%。由此可以看出,卷积神经网络相比随机森林而言还是强太多啊(好吧,另外一个因素是对于随机森林我根本没有做特征提取的工作)。此外,如果我的渣机强大一点的话,我们就能做更多次迭代,由此得到的准确率也可能更高吧。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果大家对于手写数字识别这个TASK有更好的模型或者有一些其他的想法,欢迎交流昂!&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id="_4"&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://ibillxia.github.io/blog/2013/04/06/Convolutional-Neural-Networks/"&gt;卷积神经网络（CNN）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;卷积神经网络基本概念可参考&lt;a href="http://blog.csdn.net/zouxy09/article/details/8781543"&gt;Deep Learning（深度学习）学习笔记整理系列之（七）&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.deeplearning.net/tutorial/lenet.html"&gt;Convolutional Neural Networks (LeNet)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="手写数字识别"></category></entry><entry><title>机器学习系列(VII):Kernels</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-viikernels.html" rel="alternate"></link><updated>2014-04-29T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-29:ji-qi-xue-xi-xi-lie-viikernels.html</id><summary type="html">&lt;p&gt;这一篇作为机器学习系列的第七篇,主要介绍核方法,关于&lt;strong&gt;SVM&lt;/strong&gt;的部分Pluskid已经写的很通俗易懂了,具体请参考&lt;a href="http://www.qingyuanxingsi.com/category/pearls.html"&gt;小小收藏夹[持续更新中]&lt;/a&gt;中SVM部分的链接,本文主要补充关于核方法的一些其他知识吧。&lt;/p&gt;
&lt;p&gt;我们之前介绍的所有方法均假定Object可被恰当地表示为一定长的特征向量$x_i \in R^D$.然而,对于某些情况而言,我们也许并不知道如何将它们表示成定长的特征向量。如,我们如何表示可变长的文本文档或者蛋白质序列?如何表示具有复杂3D结构的分子结构?如何表示具有可变大小和形状的进化树?&lt;/p&gt;
&lt;p&gt;解决以上问题的方法之一是为数据定义一生成模型，并使用推断得到的隐含表示作为特征输入到标准的方法中,如&lt;em&gt;Deep Learning&lt;/em&gt;.另一方法则是我们假定我们有某种方法能够衡量Objects之间的相似度,因此我们并不需要将数据预处理为特征向量的形式。如当我们比较字符串时,我们可以计算它们之间的编辑距离。令$k(x,x\prime) \geq 0$作为衡量$x$和$x\prime$之间差异性的某种标准($k$被称为核函数&lt;strong&gt;Kernel Function&lt;/strong&gt;).以下我们介绍几种常见的核函数。&lt;/p&gt;
&lt;h1 id="_1"&gt;核函数&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;我们将核函数定义为:$k(x,x\prime) \in R$,即将两参数$x,x\prime \in X$($X$为某抽象空间)映射为一实数值。一般而言,它具有对称性,即$k(x,x\prime) = k(x\prime,x)$和非负性,即$k(x,x\prime) \geq 0$.以下我们给出几个例子:&lt;/p&gt;
&lt;h2 id="rbf-kernels"&gt;RBF Kernels&lt;/h2&gt;
&lt;p&gt;Squared exponential kernel(SE kernel）或高斯核(Gaussian kernel)定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = exp(-{1 \over 2}(x-x\prime)^T\Sigma^{-1}(x-x\prime))
\end{equation}&lt;/p&gt;
&lt;p&gt;若$\Sigma$是对角阵,则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = exp(-{1 \over 2}\sum_{j=1}^D \frac{1}{\sigma_j^2}(x_j-x_j\prime)^2)
\end{equation}&lt;/p&gt;
&lt;p&gt;我们可将$\sigma_j$理解为对于维度$j$的&lt;em&gt;Characteristic length scale&lt;/em&gt;.若$\sigma_j = \infty$,则相应的维度可被忽略掉,此时则称其为&lt;em&gt;ARD kernel&lt;/em&gt;.若$\Sigma$呈球面分布,我们则得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = exp(-\frac{||x-x\prime||^2}{2\sigma^2})
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\sigma^2$被称为带宽(&lt;strong&gt;Bandwith&lt;/strong&gt;).上式为径向基核的一个例子(它只是$||x-x\prime||$的函数)。&lt;/p&gt;
&lt;h2 id="_2"&gt;文档核&lt;/h2&gt;
&lt;p&gt;当我们想要进行文档分类时,如果有一种方法能够比较文档$x_i$和$x_i\prime$想必是极好的。如果我们采用bag-of-words表示且令$x_{ij}$表示词$j$在文档$i$中出现的次数,我们可以采用余弦距离,定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x_i,x_i\prime) = \frac{x_i^T x_i \prime}{||x_i||_2 ||x_i \prime||_2}
\end{equation}&lt;/p&gt;
&lt;p&gt;然而这种方法并不好,主要缺点有如下两点:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文档中可能包含一些&lt;strong&gt;停用词&lt;/strong&gt;(无实际意义的词,如&lt;code&gt;the&lt;/code&gt;,&lt;code&gt;and&lt;/code&gt;等),它们本身不能用于判断两个文档的相似度,而在上述计算中被包含在内了;&lt;/li&gt;
&lt;li&gt;如果一个词能被用于判断两个文档之间的相似度,那么如果它在一个文档中出现多次，则我们人为地提高了两个文档的相似度.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;幸运的是,我们通过简单的预处理就能大幅提升性能---我们将词频统计向量替换为&lt;strong&gt;词频-倒排文档频率TF-IDF&lt;/strong&gt;.我们首先作如下定义:&lt;/p&gt;
&lt;p&gt;我们将词频定义为(减少由于一个词在文档中出现多次造成的影响):&lt;/p&gt;
&lt;p&gt;\begin{equation}
tf(x_{ij}) \triangleq log(1+x_{ij})
\end{equation}&lt;/p&gt;
&lt;p&gt;我们将倒排文档频率定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
idf(j) \triangleq log \frac{N}{1+\sum_{i=1}^N 1_{x_{ij} &amp;gt; 0}}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$N$表示文档总数,分母则计算包含$j$的文档总数。&lt;/p&gt;
&lt;p&gt;最后,我们定义:&lt;/p&gt;
&lt;p&gt;\begin{equation}
tf-idf(x_i) \triangleq [tf(x_{ij} \times idf(j)]_{j=1}^V
\end{equation}&lt;/p&gt;
&lt;p&gt;我们将上式带入余弦距离即可。于是我们得到Kernel:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x_i,x_i\prime) = \frac{\phi(x_i)^T\phi(x_i\prime)}{||\phi(x_i)||_2||\phi(x_i\prime)||_2}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\phi(x) = tf-idf(x)$.&lt;/p&gt;
&lt;h2 id="mercerpositive-definite-kernels"&gt;Mercer(positive definite) kernels&lt;/h2&gt;
&lt;p&gt;某些方法要求&lt;em&gt;Gram matrix&lt;/em&gt;对于任意输入集合${x_i}_{i=1}^N$均是正定的,这样的核称为Mercer kercel或正定核。我们1可以证明以上提到的高斯核以及文档核均是Mercel kernel.&lt;em&gt;Gram matrix&lt;/em&gt;定义为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
K = \left(
\begin{array}{cc}
k(x_1,x_1) &amp;amp; \ldots &amp;amp; k(x_1,x_N) \\
           &amp;amp; \vdots &amp;amp;           \\
k(x_N,x_1) &amp;amp; \ldots &amp;amp; k(x_N,x_N)
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Mercer's Theorem&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;若&lt;em&gt;Gram&lt;/em&gt;矩阵是正定的，则我们可作如下特征值分解:&lt;/p&gt;
&lt;p&gt;\begin{equation}
K = U^T\Lambda U
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\Lambda$是特征值均大于0的对角阵。现我们考虑$K$中的一个元素,有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k_{ij} = (\Lambda^{1 \over 2}U_{:,i})^T(\Lambda^{1 \over 2}U_{:,j})
\end{equation}&lt;/p&gt;
&lt;p&gt;令$\phi(x_i) = \Lambda^{1 \over 2}U_{:,i}$,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k_{ij} = \phi(x_i)^T \phi(x_j)
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们可以看到核矩阵的每一项可通过对特征向量作内积得到,而特征向量可由矩阵分解得到的特征向量$U$定义。一般地,如果kernel是Mercer Kernel,那么就存在一个从$x \in X$到$R^D$的映射$\phi$,使得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = \phi(x)^T \phi(x\prime)
\end{equation}&lt;/p&gt;
&lt;p&gt;比如,我们考察多项式核$k(x,x\prime) = (\gamma x^Tx\prime+r)^M$,其中$r&amp;gt;0$.我们可以证明$\phi(x)$将包含到$M$的所有项。例如,若我们取$M=2,\gamma = r = 1$且$x,x\prime \in R^2$,我们则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
(1+x^Tx\prime)^2 &amp;amp;= (1+x_1x_1\prime+x_2x_2\prime)^2 \\
&amp;amp;= 1+2x_1x_1\prime+2x_2x_2\prime+(x_1x_1\prime)^2+(x_2x_2\prime)^2+2x_1x_1\prime x_2x_2\prime
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;此时我们有：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\phi(x) = [1,\sqrt{2}x_1,\sqrt{2}x_2,x_1^2,x_2^2,\sqrt{2}x_1x_2]^T
\end{equation}&lt;/p&gt;
&lt;p&gt;因此采用该kernel等同于在6维空间内进行计算。对于Gaussian核而言，映射后则是在无穷维空间内。在这种情形下，显式表示特征向量显然是不可行的。&lt;/p&gt;
&lt;h2 id="linear-kernels"&gt;Linear Kernels&lt;/h2&gt;
&lt;p&gt;根据Kernel推导出特征向量是非常困难的一件事(仅当Kernel为Ｍercer时才可行),然而有特征向量推导出Kernel则容易的多。&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = \phi(x)^T \phi(x\prime) = &amp;lt;\phi(x),\phi(x\prime)&amp;gt;
\end{equation}&lt;/p&gt;
&lt;p&gt;如果$\phi(x)=x$,我们得到线性核,定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = x^Tx\prime
\end{equation}&lt;/p&gt;
&lt;p&gt;其实也就是在原空间内进行计算。&lt;/p&gt;
&lt;h2 id="matern-kernels"&gt;Matern kernels&lt;/h2&gt;
&lt;p&gt;Matern Kernels通常被用于高斯过程回归,并被定位为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(r) = \frac{2^{1-\nu}}{\Gamma(\nu)}(\frac{\sqrt{2\nu}r}{\ell})^{\nu}K_{\nu}(\frac{\sqrt{2\nu}r}{\ell})
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$r=||x-x\prime||,\nu&amp;gt;0,\ell&amp;gt;0$且$K_{\nu}$为修改后的Bessel函数。当$\nu \to \infty$时,它趋近于高斯核。若$\nu ={1 \over 2}$,它简化为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(r) = exp(-r/\ell)
\end{equation}&lt;/p&gt;
&lt;p&gt;若$D=1$,我们可以使用该核定义一高斯过程,我们得到&lt;em&gt;Ornstein-Uhlenbeck process&lt;/em&gt;,它刻画了一个正在做布朗运动的例子的速度.&lt;/p&gt;
&lt;h2 id="_3"&gt;字符串核&lt;/h2&gt;
&lt;p&gt;当输入项为结构化数据时,核方法才能真正发挥其威力。以下我们举一个例子简要说明一下多项式核。考察两个字符串长度分别为$D$和$D\prime的定义在符号表${A,R,N,D,C,E,Q,G,H,I,L,K,M,F,P,S,T,W,Y,V}$上的字符串$$x$和$x\prime$.令$x$为一长度为110的字符串,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="String_Data_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/string_data_1_zps0cae832f.png" /&gt;&lt;/p&gt;
&lt;p&gt;$x\prime$为长度为153的字符串。&lt;/p&gt;
&lt;p&gt;&lt;img alt="String_Data_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/string_data_2_zps284c6529.png" /&gt;&lt;/p&gt;
&lt;p&gt;两个字符串均包含$LQE$,如上图琐事。我们可以将两个字符串的相似度定义为它们共同包含的字串的数量。正式地，若$x = usv$,则我们可以说$s$是$x$的字串.令$\phi_s(x)$表示字串$s$在字符串$x$中出现的次数。因此字符串核可定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = \sum_{s \in A*} w_s \phi_s(x) \phi_s(x\prime)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$w_s \geq 0$且$A*$表示由字符表$A$所能生成的所有字符串。它是Mercer Kernel且可在$O(|x|+|x\prime|)$时间内计算得到。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:Why $O(|x|+|x\prime|)$,求大神解答。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;有了上式之后,我们就能"为所欲为"了。若当$|s|&amp;gt;1$时我们令$w_s = 0$,那么我们得到一bag-of-characters核,此时$\phi(x)$表示$A$中每个字符在$x$中出现的次数;如果我们采用空格分隔$s$,我们得到一bag-of-words核,其中$\phi(x)$表示每个词在$x$中出现的次数。&lt;/p&gt;
&lt;h2 id="kernels-derived-from-probabilistic-models"&gt;Kernels derived from probabilistic models&lt;/h2&gt;
&lt;p&gt;假定我们有一生成概率模型$p(x|\theta)$.那么我们有多种方法可以通过它构造核函数,以使模型更适用于判别类人物。以下我们简要介绍两种方法:&lt;/p&gt;
&lt;h3 id="probability-product-models"&gt;Probability product models&lt;/h3&gt;
&lt;p&gt;方法之一是定义如下Kernel:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x_i,x_j) = \int p(x|x_i)^{\rho}p(x|x_j)^{\rho} dx
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\rho&amp;gt;0$,且$p(x|x_i)$一般通过计算$p(x|\hat{\theta}(x_i))$来近似,其中$\hat{\theta}(x_i)$是通过计算单一数据向量得到的对于参数的估计值。上式被称为&lt;em&gt;Probability product kernel&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;尽管使用单一数据点估计参数显得很诡异，然后我们需要注意的是它仅被用于衡量两个Objects之间的相似度。特别地,如果我们采用$x_i$进行估计而得到的模型认为$x_j$是很有可能得到的,则证明$x_i$和$x_j$是相似的。如我们假定$p(x|\theta) = N(\mu,\sigma^2I)$,其中$\sigma^2$是定值。若$\rho = 1$,且我们令$\hat{\mu}(x_i) = x_i$和$\hat{\mu}(x_j) = x_j$,我们得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x_i,x_j) = \frac{1}{(4\pi\sigma^2)^{D/2}}exp(-\frac{1}{4\sigma^2}||x_i-x_j||^2)
\end{equation}&lt;/p&gt;
&lt;p&gt;即RBF Kernel.&lt;/p&gt;
&lt;h3 id="fisher-kernels"&gt;Fisher kernels&lt;/h3&gt;
&lt;p&gt;另一更为有效的通过生成模型定义Kernel的方法是采用Fisher kernel.定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
k(x,x\prime) = g(x)^T F^{-1} g(x\prime)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$g$是对数似然函数梯度或score vector在极大似然估计处的取值。&lt;/p&gt;
&lt;p&gt;\begin{equation}
g(x)  = \bigtriangledown_{\theta} log p(x|\theta)|{\hat{\theta}}
\end{equation}&lt;/p&gt;
&lt;p&gt;$F$为Fisher information matrix,即Hessian.&lt;/p&gt;
&lt;p&gt;\begin{equation}
F = \bigtriangledown \bigtriangledown log p(x|\theta)|{\hat{\theta}}
\end{equation}&lt;/p&gt;
&lt;p&gt;注意$\hat{\theta}$为所有数据的函数,因此$x$和$x\prime$之间的相似度的计算也是将所有数据考虑在内的,所以我们只需要Fit一次Model.&lt;/p&gt;
&lt;p&gt;该模型背后的Intuition是如果它们的方向梯度是相似的话,则两个向量也是相似的。(&lt;code&gt;好吧,其实这个也不是弄得灰常清楚啦,TAT&lt;/code&gt;)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;后缀树Suffix Tree&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="SVM"></category><category term="Kernels"></category></entry><entry><title>机器学习系列(VI):Latent Linear Models</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-vilatent-linear-models.html" rel="alternate"></link><updated>2014-04-27T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-27:ji-qi-xue-xi-xi-lie-vilatent-linear-models.html</id><summary type="html">&lt;h1 id="_1"&gt;因子分析&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id="_2"&gt;问题&lt;/h2&gt;
&lt;p&gt;当样本个数$m$远远大于其特征个数$n$时，这样不管是进行回归、聚类等都没有太大的问题。然而当训练样例个数$m$太小，甚至$m \ll n$的时候，使用梯度下降法进行回归时，如果初值不同，得到的参数结果会有很大偏差（因为方程数小于参数个数）。另外，如果使用多元高斯分布(Multivariate Gaussian distribution)对数据进行拟合时，也会有问题。让我们来演算一下，看看会有什么问题：&lt;/p&gt;
&lt;p&gt;多元高斯分布的参数估计公式如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\mu &amp;amp;= {1 \over m}\sum_{i=1}^m x^{(i)} \\
\Sigma &amp;amp;= {1 \over m}\sum_{i=1}^m (x^{(i)}-\mu)(x^{(i)}-\mu)^T
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$x^{(i)}$表示样例，共有$m$个，每个样例$n$个特征，因此$\mu$是$n$维向量，$\Sigma$是$n*n$协方差矩阵。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;当$m \ll n$时，我们会发现$\Sigma$是奇异阵($|\Sigma|=0$)&lt;/strong&gt;，也就是说$\Sigma^{-1}$不存在，没办法拟合出多元高斯分布了，确切的说是我们估计不出来$\Sigma$。&lt;/p&gt;
&lt;p&gt;如果我们仍然想用多元高斯分布来估计样本，那怎么办呢？&lt;/p&gt;
&lt;h3 id="_3"&gt;限制协方差矩阵&lt;/h3&gt;
&lt;p&gt;当没有足够的数据去估计$\Sigma$时，那么只能对模型参数进行一定假设，之前我们想估计出完全的$\Sigma$(矩阵中的全部元素），现在我们假设$\Sigma$就是对角阵（各特征间相互独立），那么我们只需要计算每个特征的方差即可，最后的$\Sigma$只有对角线上的元素不为0.&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Sigma_{jj} = {1 \over m}\sum_{i=1}^m (x_j^{(i)}-\mu_j)^2
\end{equation}&lt;/p&gt;
&lt;p&gt;回想我们之前在&lt;strong&gt;Gaussian Models&lt;/strong&gt;一文中讨论过的二维多元高斯分布的几何特性，在平面上的投影是个椭圆，中心点由$\mu$决定，椭圆的形状由$\Sigma$决定。$\Sigma$如果变成对角阵，就意味着椭圆的两个轴都和坐标轴平行了。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cov Matrix" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cov_matrix_zps81794a47.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;如果我们想对$\Sigma$进一步限制的话，可以假设对角线上的元素都是等值的。&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Sigma = \sigma^2 I
\end{equation}&lt;/p&gt;
&lt;p&gt;其中:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sigma^2 = \frac{1}{mn}\sum_{j=1}^n \sum_{i=1}^m (x_j^{(i)}-\mu_j)^2
\end{equation}&lt;/p&gt;
&lt;p&gt;也就是上一步对角线上元素的均值，反映到二维高斯分布图上就是椭圆变成圆。&lt;/p&gt;
&lt;p&gt;当我们要估计出完整的$\Sigma$时，我们需要$m&amp;gt;=n+1$才能保证在最大似然估计下得出的$\Sigma$是非奇异的。然而在上面的任何一种假设限定条件下，只要$m&amp;gt;=2$都可以估计出限定的$\Sigma$。&lt;/p&gt;
&lt;p&gt;这样做的缺点也是显然易见的，我们认为特征间独立，这个假设太强。接下来，我们给出一种称为因子分析的方法，使用更多的参数来分析特征间的关系，并且不需要计算一个完整的$\Sigma$。&lt;/p&gt;
&lt;h2 id="_4"&gt;因子分析&lt;/h2&gt;
&lt;p&gt;下面通过一个简单例子，来引出因子分析背后的思想。&lt;/p&gt;
&lt;p&gt;因子分析的实质是认为$m$个$n$维特征的训练样例$x^{(i)}$的产生过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先在一个$k$维的空间中按照多元高斯分布生成$m$个$z^{(i)}$（$k$维向量),即&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
z^{(i)} \sim\ N(0,I)
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;然后存在一个变换矩阵$\Lambda \in R^{n*k}$，将$z^{(i)}$映射到$n$维空间中，即&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\Lambda z^{(i)}
\end{equation}&lt;/p&gt;
&lt;p&gt;因为$z^{(i)}$的均值是0，映射后仍然是0。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;然后将$\Lambda z^{(i)}$加上一个均值$\mu$（$n$维），即&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\mu + \Lambda z^{(i)}
\end{equation}&lt;/p&gt;
&lt;p&gt;对应的意义是将变换后的$\Lambda z^{(i)}$（$n$维向量）移动到样本$x^{(i)}$的中心点$\mu$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;由于真实样例$x^{(i)}$与上述模型生成的有误差，因此我们继续加上误差$\epsilon$($n$维向量),而且$\epsilon$符合多元高斯分布，即&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\epsilon \sim\ N(0,\mathbf \Psi) \\
\mu + \Lambda z^{(i)} + \epsilon 
\end{split}
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最后的结果认为是真实的训练样例$x^{(i)}$的生成公式&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
x^{(i)} = \mu + \Lambda z^{(i)} + \epsilon 
\end{equation}&lt;/p&gt;
&lt;p&gt;让我们使用一种直观方法来解释上述过程：&lt;/p&gt;
&lt;p&gt;假设我们有$m=5$个2维的样本点$x^{(i)}$（两个特征），如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Original Data" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/original_data_zps9626b246.png" /&gt;&lt;/p&gt;
&lt;p&gt;那么按照因子分析的理解，样本点的生成过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;我们首先认为在1维空间（这里$k=1$），存在着按正态分布生成的$m$个点$z^{(i)}$，如下:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Demo_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_1_zps3e189697.png" /&gt;&lt;/p&gt;
&lt;p&gt;均值为0，方差为1。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;然后使用某个$\Lambda=(a,b)^T$将一维的$z$映射到2维，图形表示如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Demo_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_2_zps166db3d8.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;之后加上$\mu\ (\mu_1,\mu_2)^T$，即将所有点的横坐标移动$\mu_1$，纵坐标移动$\mu_2$，将直线移到一个位置，使得直线过点$\mu$，原始左边轴的原点现在为$\mu$(红色点)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Demo_3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_3_zps178e1a6a.png" /&gt;&lt;/p&gt;
&lt;p&gt;然而，样本点不可能这么规则，在模型上会有一定偏差，因此我们需要将上步生成的点做一些扰动（误差），扰动$\epsilon \sim\ N(0,\mathbf \Psi)$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;加入扰动后，我们得到黑色样本$x^{(i)}$如下：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="Demo_4" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_4_zps0cc40309.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中由于$z$和$\epsilon$的均值都为0，因此$\mu$也是原始样本点（黑色点）的均值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由以上的直观分析，我们知道了&lt;strong&gt;因子分析其实就是认为高维样本点实际上是由低维样本点经过高斯分布、线性变换、误差扰动生成的，因此高维数据可以使用低维来表示&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id="_5"&gt;因子分析模型&lt;/h2&gt;
&lt;p&gt;上面的过程是从隐含随机变量$z$经过变换和误差扰动来得到观测到的样本点。其中$z$被称为因子，是低维的。&lt;/p&gt;
&lt;p&gt;我们将式子再列一遍如下：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
z &amp;amp;\sim\ N(0,I)   \\
\epsilon &amp;amp;\sim\ N(0,\mathbf \Psi) \\
x &amp;amp;= \mu + \Lambda z + \epsilon
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中误差$\epsilon$和$z$是独立的。&lt;/p&gt;
&lt;p&gt;下面使用的因子分析表示方法是矩阵表示法，在参考资料中给出了一些其他的表示方法，如果不明白矩阵表示法，可以参考其他资料。&lt;/p&gt;
&lt;p&gt;矩阵表示法认为$z$和$x$联合符合多元高斯分布，如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left[
\begin{array}{cc}
z \\
x
\end{array}
\right]\sim\ N(\mu_{zx},\Sigma)
\end{equation}&lt;/p&gt;
&lt;p&gt;求$\mu_{zx}$之前需要求$E[x]$&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E[x] &amp;amp;= E[\mu + \Lambda z + \epsilon] \\
&amp;amp;= \mu + \Lambda E[z] + E[\epsilon] \\
&amp;amp;= \mu
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们已知$E[z]=0$，因此:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mu_{zx} = \left[
\begin{array}{cc}
0 \\
\mu
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;下一步是计算$\Sigma$，其中$\Sigma_{zz} = Cov(z) = I$.接着求$\Sigma_{zx}$&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E[(z-E[z])(x-E[x])^T] &amp;amp;= E[z(\mu + \Lambda z + \epsilon - \mu)^T]   \\
&amp;amp;= E[zz^T]\Lambda^T + E[z\epsilon^T] \\
&amp;amp;= \Lambda^T
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;这个过程中利用了$z$和$\epsilon$独立假设($E[z\epsilon^T] = E[z]E[\epsilon^T]=0$)。并将$\Lambda$看作已知变量。&lt;/p&gt;
&lt;p&gt;接着求$\Sigma_{xx}$&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E[(x-E(x))(x-E[x])^T] &amp;amp;= E[(\mu+\Lambda z+\epsilon -\mu)(\mu+\Lambda z+\epsilon -\mu)^T]  \\
&amp;amp;= E[\Lambda zz^T \Lambda^T+ \epsilon z^T \Lambda^T+\Lambda z \epsilon^T+\epsilon \epsilon^T] \\
&amp;amp;= \Lambda E[zz^T] \Lambda^T + E[\epsilon \epsilon^T]  \\
&amp;amp;= \Lambda \Lambda^T + \mathbf \Psi
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;然后得出联合分布的最终形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\left[
\begin{array}{cc}
z \\
x 
\end{array}
\right] \sim\
N\left(
\left[
\begin{array}{cc}
0 \\
\mu
\end{array}
\right],
\left[
\begin{array}{cc}
I &amp;amp; \Lambda^T \\
\Lambda &amp;amp; \Lambda \Lambda^T + \mathbf \Psi
\end{array}
\right]
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;从上式中可以看出$x$的边缘分布为$x \sim\ N(\mu,\Lambda \Lambda^T + \mathbf \Psi)$.&lt;/p&gt;
&lt;p&gt;那么对样本${x^{(i);i=1,...,m}}$进行最大似然估计&lt;/p&gt;
&lt;p&gt;\begin{equation}
\ell(\mu,\Lambda,\mathbf \Psi) = log \prod_{i=1}^m \frac{1}{(2\pi)^{n/2} |\Lambda \Lambda^T+\mathbf \Psi|}exp(-{1 \over 2}(x^{(i)}-\mu)^T(\Lambda \Lambda^T + \mathbf \Psi)^{-1} (x^{(i)}-\mu))
\end{equation}&lt;/p&gt;
&lt;p&gt;然后对各个参数求偏导数不就得到各个参数的值了么？可惜我们得不到closed-form。想想也是，如果能得到，还干嘛将$z$和$x$放在一起求联合分布呢。根据之前对参数估计的理解，在有隐含变量$z$时，我们可以考虑使用EM来进行估计。&lt;/p&gt;
&lt;h2 id="em"&gt;因子分析的EM估计&lt;/h2&gt;
&lt;p&gt;我们先来明确一下各个参数,$z$为隐含变量,$\mu,\Lambda,\mathbf \Psi$为待估参数。&lt;/p&gt;
&lt;p&gt;回想EM算法两个步骤:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;循环重复直至收敛{ &lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;E Step&lt;/strong&gt;)对于每一个$i$,计算:
\begin{equation}
Q_i(z^{(i)}) := p(z^{(i)}|x^{(i)};\theta)
\end{equation}
(&lt;strong&gt;M Step&lt;/strong&gt;)计算:
\begin{equation}
\theta := arg max_{\theta} \sum_{i} \sum_{z^{(i)}} Q_i(z^{(i)}) \ log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
\end{equation}
}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们套用一下:&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;E Step&lt;/strong&gt;):&lt;/p&gt;
&lt;p&gt;\begin{equation}
Q_i(z^{(i)}) := p(z^{(i)}|\mu,\Lambda,\mathbf \Psi)
\end{equation}&lt;/p&gt;
&lt;p&gt;根据之前在&lt;em&gt;Gaussian Models&lt;/em&gt;中提到过的&lt;strong&gt;Posterior Conditional&lt;/strong&gt;的相关知识,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\mu_{z^{(i)}|x^{i}} &amp;amp;= \Lambda^T (\Lambda \Lambda^T+\mathbf \Psi)^{-1}(x^{(i)}-\mu)  \\
\Sigma_{z^{(i)}|x^{i}} &amp;amp;= I - \Lambda^T (\Lambda \Lambda^T+\mathbf \Psi)^{-1} \Lambda
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;那么根据多元高斯分布公式，得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
Q_i(z^{(i)}) = \frac{1}{(2\pi)^{k/2}|\Sigma_{z^{(i)}|x^{(i)}}|^{1/2}}exp(-{1 \over 2}(z^{(i)}-\mu_{z^{(i)}|x^{(i)}})^T \Sigma_{z^{(i)}|x^{(i)}}^{-1} (z^{(i)}-\mu_{z^{(i)}|x^{(i)}}))
\end{equation}&lt;/p&gt;
&lt;p&gt;(&lt;strong&gt;M Step&lt;/strong&gt;):&lt;/p&gt;
&lt;p&gt;直接写要最大化的目标是:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{i=1}^m \int_{z^{(i)}} Q_i(z^{(i)}) \ log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} dz^{(i)}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中待估参数是$\mu,\Lambda,\mathbf \Psi$.下面我们重点求$\Lambda$的估计公式&lt;/p&gt;
&lt;p&gt;首先将上式简化为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\sum_{i=1}^m &amp;amp;\int_{z^{(i)}} Q_i(z^{(i)}) \ [log p(x^{(i)}|z^{(i)};\mu,\Lambda,\mathbf \Psi) +p(z^{(i)})-log\ Q_i(z^{(i)})]dz^{(i)} \\
&amp;amp;= \sum_{i=1}^m E_{z^{(i)} \sim\ Q_i} [log p(x^{(i)}|z^{(i)};\mu,\Lambda,\mathbf \Psi) +p(z^{(i)})-log\ Q_i(z^{(i)})]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;这里$z^{(i)} \sim\ Q_i$表示$z^{(i)}$服从$Q_i$分布。然后去掉与$\Lambda$不相关的项（后两项），得&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\sum_{i=1}^m &amp;amp; E[log p(x^{(i)}|z^{(I)};\mu,\Lambda,\Psi)] \\
&amp;amp;= \sum_{i=1}^m E[log \frac{1}{(2\pi)^{n/2}|\mathbf \Psi|^{1/2}}exp(-{1 \over 2}(x^{(i)}-\mu-\Lambda z^{(i)})^T |\mathbf \Psi|^{-1} (x^{(i)}-\mu-\Lambda z^{(i)})]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;去掉不相关的前两项后，对$\Lambda$求偏导得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\bigtriangledown_{\Lambda} &amp;amp; \sum_{i=1}^N -E[{1 \over 2}(x^{(i)}-\mu-\Lambda z^{(i)})^T |\mathbf \Psi|^{-1} (x^{(i)}-\mu-\Lambda z^{(i)})] \\
&amp;amp;= \sum_{i=1}^m \bigtriangledown_{\Lambda} E[- tr({1 \over 2}{z^{(i)}}^T\Lambda^T \Psi^{-1} z^{(i)}) + tr({z^{(i)}}^T\Lambda^T \Psi^{-1} (x^{(i)}-\mu))] \\
&amp;amp;= \sum_{i=1}^m \bigtriangledown_{\Lambda} E[- tr({1 \over 2}\Lambda^T \Psi^{-1} z^{(i)}{z^{(i)}}^T) + tr(\Lambda^T \Psi^{-1} (x^{(i)}-\mu){z^{(i)}}^T)]  \\
&amp;amp;= \sum_{i=1}^m E[-\Psi^{-1} z^{(i)}{z^{(i)}}^T) + \Psi^{-1} (x^{(i)}-\mu){z^{(i)}}^T]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;最后让其值为0，并且化简得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Lambda = (\sum_{i=1}^m (x^{(i)}-\mu) E_{z^{(i)} \sim\ Q_i}{z^{(i)}}^T)(\sum_{i=1}^m E_{z^{(i)} \sim\ Q_i}[{z^{(i)}z^{(i)}}^T])^{-1}
\end{equation}&lt;/p&gt;
&lt;p&gt;到这里我们发现，这个公式有点眼熟，与之前回归中的最小二乘法矩阵形式类似:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\theta^T = (y^TX)(X^TX)^{-1}
\end{equation}&lt;/p&gt;
&lt;p&gt;这里解释一下两者的相似性，我们这里的$x$是$z$的线性函数（包含了一定的噪声）。在E步得到$z$的估计后，我们找寻的$\Lambda$实际上是$x$和$z$的线性关系。而最小二乘法也是去找特征和结果直接的线性关系。&lt;/p&gt;
&lt;p&gt;到这还没完，我们需要求得括号里面的值.&lt;/p&gt;
&lt;p&gt;根据我们之前对$z|x$的定义，我们知道:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E_{z^{(i)} \sim\ Q_i}{z^{(i)}}^T &amp;amp;= \mu_{z^{(i)}|x^{(i)}}^T \\
E_{z^{(i)} \sim\ Q_i}[{z^{(i)}z^{(i)}}^T] &amp;amp;= \mu_{z^{(i)}|x^{(i)}} \mu_{z^{(i)}|x^{(i)}}^T + \Sigma_{z^{(i)}|x^{(i)}}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;将上式带入即可得到$\Lambda$的EM估计值。其他参数也可以通过类似的方法获得。&lt;/p&gt;
&lt;h2 id="_6"&gt;总结&lt;/h2&gt;
&lt;p&gt;根据上面的EM的过程，要对样本$X$进行因子分析，只需知道要分解的因子数($z$的维度)即可。通过EM，我们能够得到转换矩阵$\Lambda$和误差协方差$\Psi$。&lt;/p&gt;
&lt;p&gt;因子分析实际上是降维，在得到各个参数后，可以求得$z$。但是$z$的各个参数含义需要自己去琢磨。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;因子分析(&lt;strong&gt;Factor Analysis&lt;/strong&gt;)是一种数据简化的技术。它通过研究众多变量之间的内部依赖关系，探求观测数据中的基本结构，并用少数几个假想变量来表示其基本的数据结构。这几个假想变量能够反映原来众多变量的主要信息。原始的变量是可观测的显在变量，而假想变量是不可观测的潜在变量，称为因子。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id="pca"&gt;PCA&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;PCA的思想是将$n$维特征映射到$k$维上($k&amp;lt;n$），这$k$维是全新的正交特征。这$k$维特征称为主元，是重新构造出来的$k$维特征，而不是简单地从$n$维特征中去除其余$n-k$维特征。&lt;/p&gt;
&lt;h2 id="pca_1"&gt;PCA计算过程&lt;/h2&gt;
&lt;p&gt;首先介绍PCA的计算过程,假设我们得到的2维数据如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt="PCA_DATA" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/pca_data_zps6051a959.png" /&gt;&lt;/p&gt;
&lt;p&gt;行代表了样例，列代表特征，这里有10个样例，每个样例两个特征。可以这样认为，有10篇文档，$x$是10篇文档中“learn”出现的TF-IDF，$y$是10篇文档中“study”出现的TF-IDF。也可以认为有10辆汽车，$x$是千米/小时的速度，$y$是英里/小时的速度，等等。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一步&lt;/strong&gt;:分别求$x$和$y$的平均值，然后对于所有的样例，都减去对应的均值。这里$x$的均值是1.81，$y$的均值是1.91，那么第一个样例减去均值后即为（0.69,0.49），得到&lt;/p&gt;
&lt;p&gt;&lt;img alt="Data_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/data_1_zpsa7474565.png" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二步&lt;/strong&gt;，求特征协方差矩阵，如果数据是3维，那么协方差矩阵是&lt;/p&gt;
&lt;p&gt;\begin{equation}
C=\left(
\begin{array}{cc}
cov(x,x) &amp;amp; cov(x,y) &amp;amp; cov(x,z) \\
cov(y,x) &amp;amp; cov(y,y) &amp;amp; cov(y,z) \\
cov(z,x) &amp;amp; cov(z,y) &amp;amp; cov(z,z)
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;这里只有$x$和$y$，求解得&lt;/p&gt;
&lt;p&gt;\begin{equation}
cov=\left(
\begin{array}{cc}
.616555566 &amp;amp; .615444444 \\
.615444444 &amp;amp; .716555556
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;对角线上分别是$x$和$y$的方差，非对角线上是协方差。协方差大于0表示$x$和$y$若有一个增，另一个也增；小于0表示一个增，一个减；协方差为0时，两者独立。协方差绝对值越大，两者对彼此的影响越大，反之越小。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第三步&lt;/strong&gt;，求协方差的特征值和特征向量，得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
eigenvalues=\left(
\begin{array}{cc}
.0490833989 \\
1.28402771
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
eigenvectors=\left(
\begin{array}{cc}
-.735178656 &amp;amp; -.677873399 \\
.677873399 &amp;amp; -.735178656
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;上面是两个特征值，下面是对应的特征向量，特征值0.0490833989对应特征向量为$(-0.735178656,0.677873399)^T$，这里的特征向量都归一化为单位向量。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第四步&lt;/strong&gt;，将特征值按照从大到小的顺序排序，选择其中最大的$k$个，然后将其对应的$k$个特征向量分别作为列向量组成特征向量矩阵。&lt;/p&gt;
&lt;p&gt;这里特征值只有两个，我们选择其中最大的那个，这里是1.28402771，对应的特征向量是$(-0.677873399,-0.735178656)^T$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第五步&lt;/strong&gt;，将样本点投影到选取的特征向量上。假设样例数为$m$，特征数为$n$，减去均值后的样本矩阵为$DataAdjust(m \times n)$，协方差矩阵是$n \times n$，选取的$k$个特征向量组成的矩阵为$EigenVectors(n \times k)$。那么投影后的数据FinalData为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
FinalData(m \times k) = DataAdjust(m \times n) EigenVectors(n \times k)
\end{equation}&lt;/p&gt;
&lt;p&gt;于是我们得到结果:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Data_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/data_2_zps92a790d2.png" /&gt;&lt;/p&gt;
&lt;p&gt;这样，就将原始样例的$n$维特征变成了$k$维，这$k$维就是原始特征在$k$维上的投影。上面的数据可以认为是learn和study特征融合为一个新的特征叫做LS特征，该特征基本上代表了这两个特征。这样PCA的过程基本结束。在第一步减均值之后，其实应该还有一步对特征做方差归一化。比如一个特征是汽车速度（0到100），一个是汽车的座位数（2到6），显然第二个的方差比第一个小。因此，如果样本特征中存在这种情况，那么在第一步之后，求每个特征的标准差$\sigma$，然后对每个样例在该特征下的数据除以$\sigma$。&lt;/p&gt;
&lt;p&gt;归纳一下，使用我们之前熟悉的表示方法，在求协方差之前的步骤是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Let $\mu = {1 \over m}\sum_{i=1}^m x^{(i)}$;&lt;/li&gt;
&lt;li&gt;Replace each $x^{(i)}$ with $x^{(i)}-\mu$;&lt;/li&gt;
&lt;li&gt;Let $\sigma_j^2 = {1 \over m}\sum_i (x_j^{(i)})^2$;&lt;/li&gt;
&lt;li&gt;Replace each $x_j^{i}$ with $x_j^{i}/\sigma_j$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中$x^{(i)}$是样例，共$m$个，每个样例$n$个特征，也就是说$x^{(i)}$是$n$维向量。$x_j^{(i)}$是第$i$个样例的第$j$个特征。$\mu$是样例均值。$\sigma_j$是第$j$个特征的标准差。&lt;/p&gt;
&lt;p&gt;整个PCA过程貌似及其简单，就是求协方差的特征值和特征向量，然后做数据转换。但是有没有觉得很神奇，为什么求协方差的特征向量就是最理想的$k$维向量？其背后隐藏的意义是什么？整个PCA的意义是什么？&lt;/p&gt;
&lt;h2 id="pca_2"&gt;PCA理论基础&lt;/h2&gt;
&lt;p&gt;要解释为什么协方差矩阵的特征向量就是$k$维理想特征，我看到的有三个理论：分别是最大方差理论、最小错误理论和坐标轴相关度理论。这里简单探讨前两种，最后一种在讨论PCA意义时简单概述。&lt;/p&gt;
&lt;h3 id="_7"&gt;最大方差理论&lt;/h3&gt;
&lt;p&gt;在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。因此我们认为，最好的$k$维特征是将$n$维样本点转换为$k$维后，每一维上的样本方差都很大。&lt;/p&gt;
&lt;p&gt;比如下图有5个样本点：（已经做过预处理，均值为0，特征方差归一）&lt;/p&gt;
&lt;p&gt;&lt;img alt="Max_var_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/max_var_1_zpsb6611a20.png" /&gt;&lt;/p&gt;
&lt;p&gt;下面将样本投影到某一维上，这里用一条过原点的直线表示（预处理的过程实质是将原点移到样本点的中心点）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Max_var_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/max_var_2_zps05e7cd62.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;假设我们选择两条不同的直线做投影，那么左右两条中哪个好呢？根据我们之前的方差最大化理论，左边的好，因为投影后的样本点之间方差最大。&lt;/p&gt;
&lt;p&gt;这里先解释一下投影的概念：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Max_var_3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/max_var_3_zps46d5b364.png" /&gt;&lt;/p&gt;
&lt;p&gt;红色点表示样例$x^{(i)}$，蓝色点表示$x^{(i)}$在$u$上的投影，$u$是直线的斜率也是直线的方向向量，而且是单位向量。蓝色点是示$x^{(i)}$在$u$上的投影点，离原点的距离是${x^{(i)}}^Tu$或$u^Tx^{(i)}$.由于这些样本点（样例）的每一维特征均值都为0，因此投影到$u$上的样本点（只有一个到原点的距离值）的均值仍然是0。&lt;/p&gt;
&lt;p&gt;回到上面左右图中的左图，我们要求的是最佳的$u$，使得投影后的样本点方差最大。&lt;/p&gt;
&lt;p&gt;由于投影后均值为0，因此方差为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
{1 \over m}\sum_{i=1}^m ({x^{(i)}}^Tu)^2 &amp;amp;= {1 \over m}\sum_{i=1}^m u^Tx^{(i)}{x^{(i)}}^Tu  \\
&amp;amp;= u^T ({1 \over m}\sum_{i=1}^m x^{(i)}{x^{(i)}}^T)u.
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;中间那部分很熟悉啊，不就是样本特征的协方差矩阵么（$x^{(i)}$的均值为0，一般协方差矩阵都除以$m-1$，这里用$m$）。&lt;/p&gt;
&lt;p&gt;用$\lambda$来表示${1 \over m}\sum_{i=1}^m ({x^{(i)}}^Tu)^2$，$\Sigma$表示${1 \over m}\sum_{i=1}^m x^{(i)}{x^{(i)}}^T$，那么上式可写作:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\lambda = u^T \Sigma u
\end{equation}&lt;/p&gt;
&lt;p&gt;由于$u$是单位向量，即$u^Tu=1$，上式两边都左乘$u$得，$u\lambda = \lambda u = uu^T \Sigma u$&lt;/p&gt;
&lt;p&gt;即$\Sigma u = \lambda u$&lt;/p&gt;
&lt;p&gt;We got it！$\lambda$就是$\Sigma$的特征值，$u$是特征向量。最佳的投影直线是特征值$\lambda$最大时对应的特征向量，其次是$\lambda$第二大对应的特征向量，依次类推。&lt;/p&gt;
&lt;p&gt;因此，我们只需要对协方差矩阵进行特征值分解，得到的前$k$大特征值对应的特征向量就是最佳的$k$维新特征，而且这$k$维新特征是正交的。得到前$k$个$u$以后，样例$x^{(i)}$通过以下变换可以得到新的样本。&lt;/p&gt;
&lt;p&gt;\begin{equation}
y^{(i)} = \left[
\begin{array}{cc}
u_1^Tx^{(i)} \\
u_2^Tx^{(i)} \\
...  \\
u_k^Tx^{(i)}
\end{array}
\right] \in R^k
\end{equation}&lt;/p&gt;
&lt;p&gt;其中的第$j$维就是$x^{(i)}$在$u_j$上的投影。通过选取最大的$k$个$u$，使得方差较小的特征（如噪声）被丢弃。&lt;/p&gt;
&lt;h3 id="_8"&gt;最小平方误差理论&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Least Squard Error" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/lqe_zps9c291e8a.png" /&gt;&lt;/p&gt;
&lt;p&gt;假设有这样的二维样本点（红色点），回顾我们前面探讨的是求一条直线，使得样本点投影到直线上的点的方差最大。本质是求直线，那么度量直线求的好不好，不仅仅只有方差最大化的方法。再回想我们学过的线性回归等，目的也是求一个线性函数使得直线能够最佳拟合样本点，那么我们能不能认为最佳的直线就是回归后的直线呢？回归时我们的最小二乘法度量的是样本点到直线的坐标轴距离。比如这个问题中，特征是$x$，类标签是$y$。回归时最小二乘法度量的是距离$d$。如果使用回归方法来度量最佳直线，那么就是直接在原始样本上做回归了，跟特征选择就没什么关系了。&lt;/p&gt;
&lt;p&gt;因此，我们打算选用另外一种评价直线好坏的方法，使用点到直线的距离$d\prime$来度量。&lt;/p&gt;
&lt;p&gt;现在有$n$个样本点$(x_1,x_2,\dots,x_n)$，每个样本点为$m$维（这节内容中使用的符号与上面的不太一致，需要重新理解符号的意义）。将样本点$x_k$在直线上的投影记为$x_k \prime$，那么我们就是要最小化&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{k=1}^n ||x_k \prime - x_k||^2
\end{equation}&lt;/p&gt;
&lt;p&gt;这个公式称作最小平方误差（Least Squared Error）。&lt;/p&gt;
&lt;p&gt;而确定一条直线，一般只需要确定一个点，并且确定方向即可。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第一步确定点&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;假设要在空间中找一点$x_0$来代表这$n$个样本点，“代表”这个词不是量化的，因此要量化的话，我们就是要找一个$m$维的点$x_0$，使得&lt;/p&gt;
&lt;p&gt;\begin{equation}
J_0(x_0) = \sum_{k=1}^n ||x_0-x_k||^2
\end{equation}&lt;/p&gt;
&lt;p&gt;最小。其中$J_0(x_0)$是平方错误评价函数（squared-error criterion function).对$x_0$求偏导易知:&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_0 = \sum_{k=1}^n x_k
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;第二步确定方向&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;我们从$x_0$拉出要求的直线，假设直线的方向是单位向量$e$。那么直线上任意一点，比如$x_k \prime$就可以用点$x_0$和$e$来表示&lt;/p&gt;
&lt;p&gt;\begin{equation}
x_k \prime = x_0 + a_k e
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$a_k$是$x_k \prime$到点$x_0$的距离。&lt;/p&gt;
&lt;p&gt;我们重新定义最小平方误差：&lt;/p&gt;
&lt;p&gt;\begin{equation}
J_1(a_1,\dots,a_n,e) = \sum_{k=1}^n ||x_k \prime - x_k||^2 = \sum_{k=1}^n ||(x_0+a_ke)-x_k)||^2
\end{equation}&lt;/p&gt;
&lt;p&gt;这里的$k$只是相当于$i$。$J_1$就是最小平方误差函数，其中的未知参数是$a_1,\dots,a_n$和$e$。&lt;/p&gt;
&lt;p&gt;实际上是求$J_1$的最小值。首先将上式展开：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
J_1(a_1,\dots,a_n,e) &amp;amp;= \sum_{k=1}^n ||(x_0+a_ke)-x_k)||^2 = \sum_{k=1}^n ||a_ke-(x_k-x_0)||^2 \\
&amp;amp;= \sum_{k=1}^n a_k^2||e||^2 - 2\sum_{k=1}^n a_ke^T(x_k-x_0)+ \sum_{k=1}^n ||x_k-x_0||^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们首先固定$e$，将其看做是常量,$||e||^2 = 1$，然后对$a_k$进行求导，得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
a_k = e^T(x_k-x_0)
\end{equation}&lt;/p&gt;
&lt;p&gt;这个结果意思是说，如果知道了$e$，那么将$x_k-x_0$与$e$做内积，就可以知道了$x_k$在$e$上的投影离$x_0$的长度距离，不过这个结果不用求都知道。&lt;/p&gt;
&lt;p&gt;然后是固定$a_k$，对$e$求偏导数，我们先将上式代入$J_1$，得 &lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
J_1(e) &amp;amp;= \sum_{k=1}^n a_k^2||e||^2 - 2\sum_{k=1}^n a_k^2 + \sum_{k=1}^n ||x_k -x_0||^2 \\
&amp;amp;= -\sum_{k=1}^n [e^T(x_k-x_0)]^2 + \sum_{k=1}^n ||x_k - x_0 ||^2  \\
&amp;amp;= -\sum_{k=1}^n e^T (x_k-x_0)(x_k-x_0)^T e + \sum_{k=1}^n ||x_k - x_0 ||^2 \\
&amp;amp;= -e^TSe+\sum_{k=1}^n ||x_k-x_0||^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$S=\sum_{k=1}^n (x_k-x_0)(x_k-x_0)^T$与协方差矩阵类似，只是缺少个分母$n-1$，我们称之为散列矩阵（scatter matrix）。&lt;/p&gt;
&lt;p&gt;然后可以对$e$求偏导数，但是$e$需要首先满足$e^Te=1$，引入拉格朗日乘子$\lambda$，来使$e^TSe$最大（$J_1$最小），令:&lt;/p&gt;
&lt;p&gt;\begin{equation}
u = e^TSe - \lambda(e^Te-1)
\end{equation}&lt;/p&gt;
&lt;p&gt;求偏导:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial u}{\partial e} = 2Se - 2\lambda e = 0 
\end{equation}&lt;/p&gt;
&lt;p&gt;于是我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
Se = \lambda e
\end{equation}&lt;/p&gt;
&lt;p&gt;两边除以$n-1$就变成了，对协方差矩阵求特征值向量了。&lt;/p&gt;
&lt;p&gt;从不同的思路出发，最后得到同一个结果，对协方差矩阵求特征向量，求得后特征向量上就成为了新的坐标，如下图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="PCA_Transform" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/pca_transform_zps137a3ecb.png" /&gt;&lt;/p&gt;
&lt;p&gt;这时候点都聚集在新的坐标轴周围，因为我们使用的最小平方误差的意义就在此。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:&lt;/p&gt;
&lt;p&gt;PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="svd"&gt;SVD分解&lt;/h2&gt;
&lt;p&gt;以上我们介绍了获得PCA solution的两种方法，现我们提供基于奇异值分解(&lt;strong&gt;Singular Value Decomposition&lt;/strong&gt;)另一种方法.任一$N \times D$矩阵$X$均可分解为如下形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
X = USV^T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$U$是一$N \times N$矩阵,其列向量均正交(即$U^TU = I_N$),$V$是一$D \times D$矩阵且行列向量均正交（即$V^TV = VV^T = I_D$,$S$为一$N \times D$矩阵，其主对角线上包含$r = min(N,D)$奇异值$\sigma_i \geq 0$(&lt;code&gt;Why?&lt;/code&gt;),其余则由0填充。如下图中(a)所示：&lt;/p&gt;
&lt;p&gt;&lt;img alt="SVD" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/svd_zpsfec54ad5.png" /&gt;&lt;/p&gt;
&lt;p&gt;假定$N&amp;gt;D$,则最多有$D$个奇异值；$U$右边的$N-D$行由于与$0$相乘，所以是无关项，于是我们得到的&lt;em&gt;Economy sized SVD&lt;/em&gt;或者&lt;em&gt;Thin SVD&lt;/em&gt;避免了对于这些无关元素的计算，可被表示为如下形式：若$N&amp;gt;D$,我们有：&lt;/p&gt;
&lt;p&gt;\begin{equation}
X = \hat{U} \hat{S} \hat{V}^T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\hat{S}$为$D \times D$矩阵，$\hat{V}$为$D \times D$矩阵。&lt;/p&gt;
&lt;p&gt;若$N&amp;lt;D$,此时$\hat{U}$为$N \times N$矩阵，$\hat{S}$为$N \times N$矩阵，$\hat{V}$为$N \times D$矩阵。&lt;/p&gt;
&lt;p&gt;以下正式开始我们对PCA solution的推导：&lt;/p&gt;
&lt;p&gt;若$X = USV^T$,则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
X^TX = VS^TU^TUSV^T = V(S^TS)V^T = VDV^T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$D=S^2$是包含奇异值平方的对角阵。于是有：$(X^TX)V = VD$&lt;/p&gt;
&lt;p&gt;故$V$为$X^TX$的特征向量集合，$D$为$X^TX$的特征值集合(奇异值的平方)。对$XX^T$也能得到类似的结果。&lt;/p&gt;
&lt;p&gt;实际上,PCA的过程实际上是取$S$中最大的若干特征值(为了降维，我们并不是取所有的特征值）然后重新构造原矩阵的过程，即&lt;strong&gt;PCA只是对于原矩阵的低秩近似&lt;/strong&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kernel-PCA&lt;/li&gt;
&lt;li&gt;Probablistic PCA&lt;/li&gt;
&lt;li&gt;如何确定隐含变量空间的维度&lt;/li&gt;
&lt;li&gt;ICA&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h1 id="_9"&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/jerrylead/archive/2011/05/11/2043317.html"&gt;因子分析（Factor Analysis)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"&gt;Expectation–maximization algorithm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html"&gt;主成分分析（Principal components analysis)-最大方差解释&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020216.html"&gt;主成分分析（Principal components analysis）-最小平方误差解释&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Machine Learning:A Probablistic Perspective Chapter 12&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/jerrylead/archive/2011/04/19/2021071.html"&gt;独立成分分析（Independent Component Analysis)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="Latent Linear Models"></category><category term="Factor Analysis"></category><category term="EM"></category><category term="PCA"></category></entry><entry><title>机器学习系列(V): Generalized linear models and the exponential family</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-v-generalized-linear-models-and-the-exponential-family.html" rel="alternate"></link><updated>2014-04-23T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-23:ji-qi-xue-xi-xi-lie-v-generalized-linear-models-and-the-exponential-family.html</id><summary type="html">&lt;h1 id="_1"&gt;指数分布族&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;一件特别神奇的事是实际上我们学过或者用到的很多分布函数均可以写成一种一致的形式,事实上，属于这个家族的分布函数还是很多的，例如高斯分布、Bernoulli分布、二项分布、多项分布、指数分布、泊松分布、Dirichlet分布等,作为一个庞大家族的一员，这些分布函数具有一个它们引以为豪的共同的名字------&lt;strong&gt;指数分布族&lt;/strong&gt;。以下我们就介绍一下指数分布族的基础知识吧。&lt;/p&gt;
&lt;h2 id="_2"&gt;定义&lt;/h2&gt;
&lt;p&gt;传说中的这个神奇的家族中的分一个分布函数均可写成如下形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x) = h(x)e^{\theta^T T(x)-A(\theta)}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\theta$为参数向量,$T(x)$为"Sufficient statistics"向量,$A(\theta)$为cumulate generating function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;上式中我们不难注意到$\theta$和$x$仅在$\theta^T T(x)$一项中&lt;code&gt;耦合&lt;/code&gt;在一起。另,指数分布族函数之积仍是指数分布族函数,只是可能不再具有良好的参数形式。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;为了得到一个归一化的分布，我们有:对于任一$\theta$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\int p(x)dx = e^{-A(\theta)}\int h(x) e^{\theta^T T(x)} dx = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
e^{A(\theta)} = \int h(x) e^{\theta^T T(x)} dx
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;当$T(x)=x$时,$A(\theta)$是对于$h(x)$做&lt;strong&gt;拉普拉斯变换&lt;/strong&gt;之后的$log$值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;以下我们举几个实例以使我们对其有一个更为清晰的认识:&lt;/p&gt;
&lt;h3 id="bernoulli-distribution"&gt;Bernoulli Distribution&lt;/h3&gt;
&lt;p&gt;对于Bernoulli分布,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(x) &amp;amp;= \alpha^x (1-\alpha)^{1-x}   \\
     &amp;amp;= exp[log(\alpha^x (1-\alpha)^{1-x})]  \\
     &amp;amp;= exp[xlog \alpha + (1-x) log(1-\alpha)] \\
     &amp;amp;= exp[xlog \frac{\alpha}{1-\alpha}+log(1-\alpha)]  \\
     &amp;amp;= exp[x\theta - log(1+e^{\theta})]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;于是我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
T(x) &amp;amp;= x \\
\theta &amp;amp;= log \frac{\alpha}{1-\alpha} \\
A(\theta) &amp;amp;= log(1+e^{\theta})
\end{split}
\end{equation}&lt;/p&gt;
&lt;h3 id="univariate-gaussian"&gt;Univariate Gaussian&lt;/h3&gt;
&lt;p&gt;对于单变量高斯分布,我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Univariate Gaussian" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/univariate_gaussian_zps8407f3a0.png" /&gt;&lt;/p&gt;
&lt;p&gt;其中:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Univariate Gaussian Params" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/univariate_gaussian_params_zps58c5d9d3.png" /&gt;&lt;/p&gt;
&lt;h3 id="multivariate-gaussian"&gt;Multivariate Gaussian&lt;/h3&gt;
&lt;p&gt;对于形如下式的多变量高斯分布:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} e^{(x-\mu)^T \Sigma^{-1} (x-\mu)/2}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们有:(下式我并未真正推导)&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
h(x) &amp;amp;= (2\pi)^{-D/2} \\
T(x) &amp;amp;= \left(
\begin{array}{cc}
x \\
xx^T
\end{array}
\right) \\
\theta &amp;amp;= \left(
\begin{array}{cc}
\Sigma^{-1}\mu \\
-{1 \over 2}\Sigma^{-1}
\end{array}
\right)
\end{split}
\end{equation}&lt;/p&gt;
&lt;h2 id="_3"&gt;一阶导数&lt;/h2&gt;
&lt;p&gt;&lt;img alt="First Derivative" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/first_derivative_zps6152bd77.png" /&gt;&lt;/p&gt;
&lt;h2 id="_4"&gt;二阶导数&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Second Derivative" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/second_deriative_zps8d444606.png" /&gt;&lt;/p&gt;
&lt;p&gt;即$A(\theta)$是凸的($\succeq$表示正定positive definite)&lt;/p&gt;
&lt;h2 id="maximum-likelihood"&gt;Maximum Likelihood&lt;/h2&gt;
&lt;p&gt;根据$p(x)$的定义,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l(\theta) = \sum_{i=1}^{N} log p(x_i|\theta) = \sum_{i=1}^{N} [log h(x_i) + \theta^T T(x_i) - A(\theta)]
\end{equation}&lt;/p&gt;
&lt;p&gt;为了求得极大似然解,我们对$\theta$求偏导有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l \prime(\theta) = [\sum_{i=1}^N T(x_i)] - NA\prime(\theta) = 0
\end{equation}&lt;/p&gt;
&lt;p&gt;于是我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
A\prime(\hat\theta_{ML})=\frac{1}{N} \sum_{i=1}^N T(x_i)
\end{equation}&lt;/p&gt;
&lt;h2 id="conjugate-priors-in-bayesian-statistics"&gt;Conjugate Priors in Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;根据Bayes Rule,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{\int p(x|\theta)p(\theta) d\theta}
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:分母项只是一归一化项,其值与$\theta$无关。于是我们有$p(\theta|x) \propto p(x|\theta)p(\theta)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;正如我们之前提到的那样，为了简化计算,我们最好使得先验分布$p(\theta)$与Marginal Likelihood $p(x|\theta)$具有相似的形式(此时它们成为共轭分布)。如当先验分布是Dirichlet分布时,当我们取Marginal Likelihood为多项式分布时,后验分布为Dirichlet分布。由于Dirichlet分布与多项式分布具有相似的形式,在一定程度上可以简化我们的计算。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:关于Dirichlet与Multinomial之间的关系我们会在之后的&lt;em&gt;Dirichlet Process&lt;/em&gt;一篇中详细展开,敬请期待,此处不再赘述。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;常见的共轭分布如下表所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Conjugate Prior" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/conjugate_priors_zps0d694b87.png" /&gt;&lt;/p&gt;
&lt;h1 id="_5"&gt;广义线性模型&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;从软件工程的视角来看,如果我们把广义线性模型(Generalized Linear Models,GLM)看作一个类,线性回归与Logistic回归只不过是该类的两个实例而已,由此可见GLM是一个较为高大上的东东啊!以下对其进行一个较为简单的介绍:&lt;/p&gt;
&lt;h2 id="_6"&gt;基础知识&lt;/h2&gt;
&lt;p&gt;为了理解GLM,我们首先引入:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y_i|\theta,\sigma^2) = exp[\frac{y_i\theta-A(\theta)}{\sigma^2}+c(y_i,\sigma^2)]
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\sigma^2$为&lt;em&gt;dispersion parameter&lt;/em&gt;(一般设置为1),$\theta$为自然参数,$A$为partition function,$c$为归一化常数.如对于Logistic Regression而言,$\theta$为log-odds ratio,$\theta = log(\frac{\mu}{1-\mu})$,其中$\mu = E[y] = p(y=1)$为mean.为了将mean转化为自然参数,我们引入函数$\psi$,有:$\theta = \mathbf \Psi(\mu)$.实际上,如果存在逆向映射,我们则有:$\mu = \mathbf \Psi^{-1}(\theta)$.此外,根据我们以上推导的$A(\theta)$的一阶导数知:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mu = \Psi^{-1}(\theta) = A\prime(\theta)
\end{equation}&lt;/p&gt;
&lt;p&gt;若我们令:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\eta_i = w^T x_i
\end{equation}&lt;/p&gt;
&lt;p&gt;我们可以定义如下mean function,记为$g^{-1}$,使得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mu_i = g^{-1}(\eta_i) = g^{-1}(w^T x_i)
\end{equation}&lt;/p&gt;
&lt;p&gt;Mean function的逆函数,即$g()$,则称为&lt;em&gt;Link function&lt;/em&gt;.我们可以选取任意函数作为$g()$,只要它是可逆的,且使得逆函数具有合适的取值范围。如针对Logistic Regression而言,我们有:$\mu_i = g^{-1}(\eta_i) = sigm(\eta_i)$&lt;/p&gt;
&lt;p&gt;一种最为简单的Link function的函数是取$g=\psi$,称为Canonical Link Function.上面我们定义$\theta_i = \eta_i = w^Tx_i$,于是模型变为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y_i|x_i,w,\sigma^2) = exp[\frac{y_iw^Tx_i-A(w^Tx_i)}{\sigma^2}+c(y_i,\sigma^2)]
\end{equation}&lt;/p&gt;
&lt;p&gt;下表中,我们给出一些常见分布函数及其对应的Canonical Link Functions.我们可以看到对于Bernoulli和二项分布而言,Canonical Link Function为logit函数,$g(\mu)=log(\frac{\eta}{1-\eta})$,其逆函数为logistic函数,$\mu = sigm(\eta)$.&lt;/p&gt;
&lt;p&gt;根据我们第一部分对一阶导数以及二阶导数的推导我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
E[y|x_i,w,\sigma^2] &amp;amp;= \mu_i = A\prime(\theta_i) \\
var[y|x_i,w,\sigma^2] &amp;amp;= \sigma_i^2 = A\prime\prime(\theta_i)\sigma^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;为了更便于理解,我们举几个具体的例子:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于线性回归而言,我们有:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
log p(y_i|x_i,w,\sigma^2) = \frac{y_i\mu_i-\frac{\mu_i^2}{2}}{\sigma^2} - {1 \over 2}(\frac{y_i^2}{\sigma_i^2}+log(2\pi \sigma^2))
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$y_i \in R$,且$\theta_i = \mu_i = w^T x_i$,这里我们取$A(\theta) = \theta^2/2$,因此$E[y_i] = \mu_i$,且$var[y_i]=\sigma^2$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于二项分布而言,我们有:(&lt;strong&gt;后面这两个未证明结论的正确性&lt;/strong&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
log p(y_i|x_i,w) = y_i log(\frac{\pi_i}{1-\pi_i}) +N_ilog(1-\pi_i)+log 
\left(
\begin{array}{cc}
N_i \\
y_i
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$y_i \in {0,1,...,N_i}$,$\pi_i = sigm(w^Tx_i)$,$\theta_i = log(\pi_i/(1-\pi_i))=w^Tx_i$，且$\sigma^2=1$.这里我们取$A(\theta) = N_ilog(1+e^{\theta})$.于是我们有$E[y_i] = N_i \pi_i=\mu_i$,$var[y_i] = N_i \pi_i (1-\pi_i)$.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于Poisson Regression而言,我们有:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
log p(y_i|x_i,w) = y_i log \mu_i - \mu_i - log(y_i!)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$y_i \in {0,1,2,...}$,$\mu_i = exp(w^Tx_i)$,$\theta_i = log(\mu_i) = w^Tx_i$且$\sigma^2=1$.这里我们取$A(\theta) = e^{\theta}$,于是我们有$E[y_i] = var[y_i] = \mu_i$.&lt;/p&gt;
&lt;h2 id="mle-estimation"&gt;MLE Estimation&lt;/h2&gt;
&lt;p&gt;其极大似然函数具有如下形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\ell(w) = log p(D|w) &amp;amp;= \frac{1}{\sigma^2}\sum_{i=1}^N \ell_i \\
\ell_i &amp;amp;\triangleq \theta_iy_i-A(\theta_i)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们可通过下式计算其梯度向量:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\frac{\ell_i}{w_j} &amp;amp;= \frac{\ell_i}{\theta_i}\frac{\theta_i}{\mu_i}\frac{\mu_i}{\eta_i}\frac{\eta_i}{w_j} \\
&amp;amp;= (y_i - A\prime(\theta_i))\frac{\theta_i}{\mu_i}\frac{\mu_i}{\eta_i}x_{ij} \\
&amp;amp;= (y_i-\mu_i)\frac{\theta_i}{\mu_i}\frac{\mu_i}{\eta_i}x_{ij}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;如果我们采用Canonical Link Function,$\theta_i = \eta_i$,上式可简化为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\bigtriangledown_w \ell(w) = \frac{1}{\sigma^2}[\sum_{i=1}^N (y_i-\mu_i)x_i]
\end{equation}&lt;/p&gt;
&lt;p&gt;利用该结果执行梯度下降即可得到ML估计值。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;TODO Board&lt;/em&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Probit Regression&lt;/li&gt;
&lt;li&gt;Multi-task Learning(&lt;em&gt;Transfer Learning&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Generalized Linear Mixture Models&lt;/li&gt;
&lt;li&gt;Learning to rank&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h1 id="_7"&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.cs.columbia.edu/~jebara/4771/tutorials/lecture12.pdf"&gt;Exponential Family&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="Exponential Family"></category><category term="Generalized Linear Models"></category></entry><entry><title>机器学习外传之Deep Learning(I):Sparse Autoencoder</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-wai-chuan-zhi-deep-learningisparse-autoencoder.html" rel="alternate"></link><updated>2014-04-13T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-13:ji-qi-xue-xi-wai-chuan-zhi-deep-learningisparse-autoencoder.html</id><summary type="html">&lt;p&gt;其实本来没准备看Deep Learning的,之前这个高端大气上档次的内容一直都不再我的学习计划之内。基于如下几个原因吧,最后决定还是稍微看一下吧:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep Learning是Machine Learning:A Probabilistic Perspective的最后一章,反正终归是要看的。(之前觉得自己可能看不懂,但是好歹试试水吧);&lt;/li&gt;
&lt;li&gt;一个小伙伴毕业设计就在做Deep Learning;&lt;/li&gt;
&lt;li&gt;实验室老大好像对Deep Learning很感兴趣;&lt;/li&gt;
&lt;li&gt;好奇心害死人啊！！！！！！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;言归正传,作为Deep Learning系列的第一篇,我们首先还是说明一下两件事:Deep Learning是什么?Deep Learning是用来干什么的?&lt;/p&gt;
&lt;h1 id="a-brief-introduction-to-deep-learning"&gt;A Brief Introduction to Deep Learning&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id="_1"&gt;背景&lt;/h2&gt;
&lt;p&gt;机器学习（Machine Learning)是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。机器能否像人类一样能具有学习能力呢？1959年美国的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。这个程序向人们展示了机器学习的能力，提出了许多令人深思的社会问题与哲学问题（呵呵，人工智能正常的轨道没有很大的发展，这些什么哲学伦理啊倒发展的挺快。什么未来机器越来越像人，人越来越像机器啊。什么机器会反人类啊，ATM是开第一枪的啊等等。人类的思维无穷啊）。&lt;/p&gt;
&lt;p&gt;机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Problems to be solved" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/problems_to_be_solved_zps19ee52eb.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等。目前我们通过机器学习去解决这些问题的思路都是这样的（以视觉感知为例子）：&lt;/p&gt;
&lt;p&gt;&lt;img alt="ML Process" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/ml_process_zpsd8b105ec.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;从开始的通过传感器（例如CMOS）来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。最后一个部分，也就是机器学习的部分，绝大部分的工作是在这方面做的，也存在很多的paper和研究。&lt;/p&gt;
&lt;p&gt;而中间的三部分，概括起来就是特征表达。&lt;strong&gt;良好的特征表达，对最终算法的准确性起了非常关键的作用&lt;/strong&gt;，而且系统主要的计算和测试工作都耗在这一大部分。但，这块实际中一般都是人工完成的。靠人工提取特征。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Feature Representation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/feature_representation_zps744a0f4f.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;截止现在，也出现了不少NB的特征（好的特征应具有不变性（大小、尺度和旋转等）和可区分性）：例如Sift的出现，是局部图像特征描述子研究领域一项里程碑式的工作。由于SIFT对尺度、旋转以及一定视角和光照变化等图像变化都具有不变性，并且SIFT具有很强的可区分性，的确让很多问题的解决变为可能。但它也不是万能的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="SIFT" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sift_zpsb99f6fe8.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;然而，手工地选取特征是一件非常费力、启发式（需要专业知识）的方法，能不能选取好很大程度上靠经验和运气，而且它的调节需要大量的时间。既然手工选取特征不太好，那么能不能自动地学习一些特征呢？答案是能！Deep Learning就是用来干这个事情的，看它的一个别名&lt;code&gt;Unsupervised Feature Learning&lt;/code&gt;,就可以顾名思义了，Unsupervised的意思就是不要人参与特征的选取过程。&lt;/p&gt;
&lt;p&gt;那它是怎么学习的呢？怎么知道哪些特征好哪些不好呢？我们说机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为的学科。好，那我们人的视觉系统是怎么工作的呢？为什么在茫茫人海，芸芸众生，滚滚红尘中我们都可以找到另一个她（因为，你存在我深深的脑海里，我的梦里 我的心里 我的歌声里……）。人脑那么NB，我们能不能参考人脑，模拟人脑呢？（好像和人脑扯上点关系的特征啊，算法啊，都不错，但不知道是不是人为强加的，为了使自己的作品变得神圣和高雅。）&lt;/p&gt;
&lt;p&gt;近几十年以来，认知神经科学、生物学等等学科的发展，让我们对自己这个神秘的而又神奇的大脑不再那么的陌生。也给人工智能的发展推波助澜。&lt;/p&gt;
&lt;h2 id="_2"&gt;人脑视觉机理&lt;/h2&gt;
&lt;p&gt;1981 年的诺贝尔医学奖，颁发给了David Hubel（出生于加拿大的美国神经生物学家)和TorstenWiesel，以及Roger Sperry。前两位的主要贡献，是发现了视觉系统的信息处理过程是分级的：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Brain Activity" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/brain_activity_zps40cf5595.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;我们看看他们做了什么。1958 年，DavidHubel 和Torsten Wiesel在JohnHopkins University，研究瞳孔区域与大脑皮层神经元的对应关系。他们在猫的后脑头骨上，开了一个3毫米的小洞，向洞里插入电极，测量神经元的活跃程度。&lt;/p&gt;
&lt;p&gt;然后，他们在小猫的眼前，展现各种形状、各种亮度的物体。并且，在展现每一件物体时，还改变物体放置的位置和角度。他们期望通过这个办法，让小猫瞳孔感受不同类型、不同强弱的刺激。&lt;/p&gt;
&lt;p&gt;之所以做这个试验，目的是去证明一个猜测。位于后脑皮层的不同视觉神经元，与瞳孔所受刺激之间，存在某种对应关系。一旦瞳孔受到某一种刺激，后脑皮层的某一部分神经元就会活跃。经历了很多天反复的枯燥的试验，同时牺牲了若干只可怜的小猫，David Hubel 和Torsten Wiesel发现了一种被称为“方向选择性细胞（Orientation Selective Cell）”的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。&lt;/p&gt;
&lt;p&gt;这个发现激发了人们对于神经系统的进一步思考。神经-中枢-大脑的工作过程，或许是一个不断迭代、不断抽象的过程。&lt;/p&gt;
&lt;p&gt;这里的关键词有两个，一个是&lt;code&gt;抽象&lt;/code&gt;，一个是&lt;code&gt;迭代&lt;/code&gt;。从原始信号，做低级抽象，逐渐向高级抽象迭代。人类的逻辑思维，经常使用高度抽象的概念。&lt;/p&gt;
&lt;p&gt;例如，从原始信号摄入开始（瞳孔摄入像素Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Layers" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/layers_zps17c2e46a.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;这个生理学的发现，促成了计算机人工智能，在四十年后的突破性发展。&lt;/p&gt;
&lt;p&gt;总的来说，人的视觉系统的信息处理是分级的。从低级的V1区提取边缘特征，再到V2区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。也就是说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，就越利于分类。例如，单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。&lt;/p&gt;
&lt;p&gt;敏感的人注意到关键词了：&lt;strong&gt;分层&lt;/strong&gt;。而Deep learning的Deep是不是就表示存在多少层，也就是多深呢？没错。那Deep Learning是如何借鉴这个过程的呢？毕竟是归于计算机来处理，面对的一个问题就是怎么对这个过程建模？&lt;/p&gt;
&lt;p&gt;因为我们要学习的是特征的表达，那么关于特征，或者说关于这个层级特征，我们需要了解地更深入点。所以在说Deep Learning之前，我们有必要再啰嗦下特征。&lt;/p&gt;
&lt;h2 id="_3"&gt;特征&lt;/h2&gt;
&lt;p&gt;特征是机器学习系统的原材料，对最终模型的影响是毋庸置疑的。如果数据被很好的表达成了特征，通常线性模型就能达到满意的精度。那对于特征，我们需要考虑什么呢？&lt;/p&gt;
&lt;h3 id="_4"&gt;特征表示的粒度&lt;/h3&gt;
&lt;p&gt;学习算法在一个什么粒度上的特征表示，才有能发挥作用？就一个图片来说，像素级的特征根本没有价值。例如下面的摩托车，从像素级别，根本得不到任何信息，其无法进行摩托车和非摩托车的区分。而如果特征是一个具有结构性（或者说有含义）的时候，比如是否具有车把手（handle），是否具有车轮（wheel），就很容易把摩托车和非摩托车区分，学习算法才能发挥作用。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Feature Motor" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/feature_motor_zps687d7adf.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_5"&gt;初级(浅层)特征表示&lt;/h3&gt;
&lt;p&gt;既然像素级的特征表示方法没有作用，那怎样的表示才有用呢？&lt;/p&gt;
&lt;p&gt;1995 年前后，Bruno Olshausen和 David Field 两位学者任职Cornell University，他们试图同时用生理学和计算机的手段，双管齐下，研究视觉问题。&lt;/p&gt;
&lt;p&gt;他们收集了很多黑白风景照片，从这些照片中，提取出400个小碎片，每个照片碎片的尺寸均为16x16像素，不妨把这400个碎片标记为$S[i]$,$i= 0,.. 399$。接下来，再从这些黑白风景照片中，随机提取另一个碎片，尺寸也是 16x16 像素，不妨把这个碎片标记为$T$。&lt;/p&gt;
&lt;p&gt;他们提出的问题是，如何从这400个碎片中，选取一组碎片$S[k]$,通过叠加的办法，合成出一个新的碎片，而这个新的碎片，应当与随机选择的目标碎片$T$尽可能相似，同时$S[k]$的数量尽可能少。用数学的语言来描述，就是：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_k (a[k] * S[k]) \rightarrow T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$a[k]$是在叠加碎片$S[k]$时的权重系数。&lt;/p&gt;
&lt;p&gt;为解决这个问题，Bruno Olshausen和 David Field发明了一个算法：稀疏编码(&lt;strong&gt;Sparse Coding&lt;/strong&gt;)。&lt;/p&gt;
&lt;p&gt;稀疏编码是一个重复迭代的过程，每次迭代分两步:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择一组$S[k]$，然后调整$a[k]$，使得$\sum_k (a[k]*S[k])$最接近$T$;&lt;/li&gt;
&lt;li&gt;固定住$a[k]$,在400个碎片中,选择其它更合适的碎片$S\prime[k]$，替代原先的$S[k]$,使得$\sum_k (a[k]*S\prime[k])$最接近$T$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;经过几次迭代后，最佳的$S[k]$组合，被遴选出来了。令人惊奇的是，被选中的$S[k]$,基本上都是照片上不同物体的边缘线，这些线段形状相似，区别在于方向。&lt;/p&gt;
&lt;p&gt;Bruno Olshausen和 David Field的算法结果，与David Hubel 和Torsten Wiesel的生理发现，不谋而合！&lt;/p&gt;
&lt;p&gt;也就是说，复杂图形，往往由一些基本结构组成。比如下图：一个图可以通过用64种正交的edges（可以理解成正交的基本结构）来线性表示。比如样例$x$可以用1-64个edges中的三个按照0.8,0.3,0.5的权重调和而成。而其他基本edge没有贡献，因此均为0 。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sparse Coding" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sparse_coding_zps5aa4b3d3.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;另外，大牛们还发现，不仅图像存在这个规律，声音也存在。他们从未标注的声音中发现了20种基本的声音结构，其余的声音可以由这20种基本结构合成。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sound Sparse Structure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sound_sparse_coding_zpsaddf2fb2.jpg" /&gt;&lt;/p&gt;
&lt;h3 id="_6"&gt;结构性特征表示&lt;/h3&gt;
&lt;p&gt;小块的图形可以由基本edge构成，更结构化，更复杂的，具有概念性的图形如何表示呢？这就需要更高层次的特征表示，比如V2，V4。因此V1看像素级是像素级。V2看V1是像素级，这个是层次递进的，高层表达由底层表达的组合而成。专业点说就是基basis。V1提取出的basis是边缘，然后V2层是V1层这些basis的组合，这时候V2区得到的又是高一层的basis。即上一层的basis组合的结果，上上层又是上一层的组合basis(所以有大牛说Deep learning就是“搞基”，因为难听，所以美其名曰Deep learning或者Unsupervised Feature Learning）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="NN Structure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/nn_structure_zps5884685c.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;直观上说，就是找到make sense的小patch再将其进行combine，就得到了上一层的feature，递归地向上learning feature。&lt;/p&gt;
&lt;p&gt;在不同object上做training所得的edge basis是非常相似的，但object parts和models就会completely different了（那咱们分辨car或者face是不是容易多了):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Object Classification" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/object_classification_zps81877999.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;从文本来说，一个doc表示什么意思？我们描述一件事情，用什么来表示比较合适？用一个一个字嘛，我看不是，字就是像素级别了，起码应该是term，换句话说每个doc都由term构成，但这样表示概念的能力就够了嘛，可能也不够，需要再上一步，达到topic级，有了topic，再到doc就合理。但每个层次的数量差距很大，比如doc表示的概念-&amp;gt;topic（千-万量级)-&amp;gt;term（10万量级)-&amp;gt;word（百万量级）。&lt;/p&gt;
&lt;p&gt;一个人在看一个doc的时候，眼睛看到的是word，由这些word在大脑里自动切词形成term，在按照概念组织的方式，先验的学习，得到topic，然后再进行高层次的learning。&lt;/p&gt;
&lt;h3 id="_7"&gt;需要有多少个特征？&lt;/h3&gt;
&lt;p&gt;我们知道需要层次的特征构建，由浅入深，但每一层该有多少个特征呢？&lt;/p&gt;
&lt;p&gt;任何一种方法，特征越多，给出的参考信息就越多，准确性会得到提升。但特征多意味着计算复杂，探索的空间大，可以用来训练的数据在每个特征上就会稀疏，都会带来各种问题，并不一定特征越多越好。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Feature Number" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/feature_number_zps90f6b4a2.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;好了，到了这一步，终于可以聊到Deep Learning了。上面我们聊到为什么会有Deep Learning（让机器自动学习良好的特征，而免去人工选取过程。还有参考人的分层视觉处理系统），我们得到一个结论就是Deep Learning需要多层来获得更抽象的特征表达。那么多少层才合适呢？用什么架构来建模呢？怎么进行非监督训练呢？以下我们给出Deep Learning的基本思想。&lt;/p&gt;
&lt;h2 id="deep-learning"&gt;Deep Learning的基本思想&lt;/h2&gt;
&lt;p&gt;假设我们有一个系统$S$，它有$n$层$(S_1,…S_n)$，它的输入是$I$，输出是$O$，形象地表示为:$I \rightarrow S_1 \rightarrow S_2 \rightarrow ….\rightarrow S_n \rightarrow O$，如果输出$O$等于输入$I$，即输入$I$经过这个系统变化之后没有任何的信息损失(呵呵，大牛说，这是不可能的。信息论中有个“信息逐层丢失”的说法（信息处理不等式):设处理$a$信息得到$b$，再对$b$处理得到$c$，那么可以证明：$a$和$c$的互信息不会超过$a$和$b$的互信息。这表明信息处理不会增加信息，大部分处理会丢失信息。当然了，如果丢掉的是没用的信息那多好啊），保持了不变，这意味着输入$I$经过每一层$S_i$都没有任何的信息损失，即在任何一层$S_i$，它都是原有信息（即输入$I$）的另外一种表示。现在回到我们的主题Deep Learning，我们需要自动地学习特征，假设我们有一堆输入$I$(如一堆图像或者文本),假设我们设计了一个系统$S$（有$n$层），我们通过调整系统中参数，使得它的输出仍然是输入$I$，那么我们就可以自动地获取得到输入$I$的一系列层次特征，即$S_1，…, S_n$。&lt;/p&gt;
&lt;p&gt;对于深度学习来说，其思想就是对堆叠多个层，也就是说这一层的输出作为下一层的输入。通过这种方式，就可以实现对输入信息进行分级表达了。&lt;/p&gt;
&lt;p&gt;另外，前面是假设输出严格地等于输入，这个限制太严格，我们可以略微地放松这个限制，例如我们只要使得输入与输出的差别尽可能地小即可，这个放松会导致另外一类不同的Deep Learning方法。上述就是Deep Learning的基本思想。&lt;/p&gt;
&lt;h1 id="bp-neural-network"&gt;BP Neural Network&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;上述我们主要意在建立关于Deep Learning一点直观的印象,下面我们就开始介绍Deep Learning所涉及的模型了。首先第一个要介绍的就是Sparse AutoEncoder(稀疏自编码器)。为了更好的理解Sparse AutoEncoder,我们首先介绍一下Back Propagation Algorithm,而采用Back Propagation算法的一个典型的代表就是BP神经网络(&lt;code&gt;想了解神经网络基础知识的童鞋请自行Google之&lt;/code&gt;)。以下我们便主要介绍BP神经网络。&lt;/p&gt;
&lt;h2 id="_8"&gt;概述&lt;/h2&gt;
&lt;p&gt;BP(Back Propagation)神经网络是1986年由Rumelhart和McCelland为首的科研小组提出，参见他们发表在Nature上的论文&lt;a href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf"&gt;Learning representations by back-propagating errors&lt;/a&gt;.值得一提的是，该文的第三作者Geoffrey E. Hinton就是在深度学习邻域率先取得突破的神犇。&lt;/p&gt;
&lt;p&gt;BP神经网络是一种&lt;strong&gt;按误差逆向传播算法训练的多层前馈网络&lt;/strong&gt;，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的 输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断 调整网络的权值和阈值，使网络的误差平方和最小。&lt;/p&gt;
&lt;p&gt;其中,&lt;strong&gt;前馈&lt;/strong&gt;是指输入从输入层到隐含层到输出层前向传播,而误差则由输出层反向经隐含层传到输入层,而在误差反向传播的过程中动态更新神经网络连接权值,以使得网络的误差平方和最小。&lt;/p&gt;
&lt;h2 id="bp"&gt;BP网络模型&lt;/h2&gt;
&lt;p&gt;一个典型的BP神经网络模型如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="BP Neural Network" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bp_nn_zps5c17c098.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;BP神经网络与其他神经网络模型类似，不同的是，BP神经元的传输函数为非线性函数(而在感知机中为阶跃函数，在线性神经网络中为线性函数)，最常用的是log-sigmoid函数或tan-sigmoid函数。BP神经网络(BPNN)一般为多层神经网络，上图中所示的BP神经网络的隐层的传输函数即为非线性函数，隐层可以有多层，而输出层的传输函数为线性函数，当然也可以是非线性函数，只不过线性函数的输出结果取值范围较大，而非线性函数则限制在较小范围（如logsig函数输出 取值在(0,1)区间）。上图所示的神经网络的输入输出关系如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入层与隐层的关系:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{h} = \mathit{f_{1}} (\boldsymbol{W^{(1)}x}+\boldsymbol{b^{(1)}})
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$x$为$m$维特征向量(列向量)，$\boldsymbol{W^{(1)}}$为$n×m$维权值矩阵，$\boldsymbol{b^{(1)}}$为$n$维的偏置(bias)向量(列向量)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隐层与输出层的关系:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{y} = \mathit{f_{2}} (\boldsymbol{W^{(2)}h}+\boldsymbol{b^{(2)}})
\end{equation}&lt;/p&gt;
&lt;h2 id="bp_1"&gt;BP网络的学习方法&lt;/h2&gt;
&lt;p&gt;神经网络的关键之一是权值的确定，也即神经网络的学习，下面主要讨论一下BP神经网络的学习方法，它是一种监督学习的方法。&lt;/p&gt;
&lt;p&gt;假定我们有$q$个带label的样本(即输入)$p_1,p_2,…,p_q$，对应的label(即期望输出Target)为$T_1,T_2,…,T_q$，神经网络的实际输出为$a2_1,a2_2,…,a2_q$，隐层的输出为$a1[.]$.那么可以定义误差函数：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{E(W,B)} = \frac{1}{2}\sum_{k=1}^{n}(t_{k} - a2_{k})^{2}
\end{equation}&lt;/p&gt;
&lt;p&gt;BP算法的目标是使得实际输出approximate期望输出，即使得训练误差最小化。BP算法利用梯度下降(Gradient Descent)法来求权值的变化及误差的反向传播。对于上图中的BP神经网络，我们首先计算输出层的权值的变化量，从第$i$个输入到第$k$个输出的权值改变为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\Delta w2_{ki} &amp;amp;= - \eta \frac{\partial E}{\partial w2_{ki}} \\
&amp;amp;= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial w2_{ki}} \\
&amp;amp;= \eta (t_{k}-a2_{k})f_{2}’a1_{i} \\
&amp;amp;= \eta \delta_{ki}a1_{i}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\eta$为学习速率。同理可得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\Delta b2_{ki} &amp;amp;= - \eta \frac{\partial E}{\partial b2_{ki}} \\
&amp;amp;= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial b2_{ki}} \\
&amp;amp;= \eta (t_{k}-a2_{k})f_{2}’ \\
&amp;amp;= \eta \delta_{ki}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;而隐层的权值变化为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\Delta w1_{ij} &amp;amp;= - \eta \frac{\partial E}{\partial w1_{ij}} \\
&amp;amp;= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial a1_{i}} \frac{\partial a1_{i}}{\partial w1_{ij}} \\
&amp;amp;= \eta \sum_{k=1}^{n}(t_{k}-a2_{k})f_{2}’w2_{ki}f_{1}’p_{j} \\
&amp;amp;= \eta \delta_{ij}p_{j}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\delta_{ij} = e_{i}f_{1}’, e_{i} = \sum_{k=1}^{n}\delta_{ki}w2_{ki}$。同理可得，$\Delta b1_{i} = \eta \delta_{ij}$。&lt;/p&gt;
&lt;p&gt;这里我们注意到，输出层的误差为$e_j,j=1..n$，隐层的误差为$e_i,i=1..m$，其中$e_i$可以认为是$e_j$的加权组合，由于作用函数的 存在，$e_j$的等效作用为$\delta_{ji} = e_{j}f’()$。&lt;/p&gt;
&lt;h2 id="bp_2"&gt;BP网络的设计&lt;/h2&gt;
&lt;p&gt;在进行BP网络的设计是，一般应从网络的层数、每层中的神经元个数和激活函数、初始值以及学习速率等几个方面来进行考虑，下面是一些选取的原则。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网络的层数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;理论已经证明，具有偏差和至少一个S型隐层加上一个线性输出层的网络，能够逼近任何有理函数，增加层数可以进一步降低误差，提高精度，但同时也是网络 复杂化。另外不能用仅具有非线性激活函数的单层网络来解决问题，因为能用单层网络解决的问题，用自适应线性网络也一定能解决，而且自适应线性网络的 运算速度更快，而对于只能用非线性函数解决的问题，单层精度又不够高，也只有增加层数才能达到期望的结果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隐层神经元的个数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;网络训练精度的提高，可以通过采用一个隐含层，而增加其神经元个数的方法来获得，这在结构实现上要比增加网络层数简单得多。一般而言，我们用精度和 训练网络的时间来恒量一个神经网络设计的好坏：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;神经元数太少时，网络不能很好的学习，训练迭代的次数也比较多，训练精度也不高。&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;神经元数太多时，网络的功能越强大，精确度也更高，训练迭代的次数也大，可能会出现过拟合(over fitting)现象。
    由此，我们得到神经网络隐层神经元个数的选取原则是：在能够解决问题的前提下，再加上一两个神经元，以加快误差下降速度即可。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;初始权值的选取
一般初始权值是取值在$(−1,1)$之间的随机数。另外威得罗等人在分析了两层网络是如何对一个函数进行训练后，提出选择初始权值量级为$\sqrt[r]{s}$的策略， 其中$r$为输入个数，$s$为第一层神经元个数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学习速率
学习速率一般选取为0.01−0.8，大的学习速率可能导致系统的不稳定，但小的学习速率导致收敛太慢，需要较长的训练时间。对于较复杂的网络， 在误差曲面的不同位置可能需要不同的学习速率，为了减少寻找学习速率的训练次数及时间，比较合适的方法是采用变化的自适应学习速率，使网络在 不同的阶段设置不同大小的学习速率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;期望误差的选取
在设计网络的过程中，期望误差值也应当通过对比训练后确定一个合适的值，这个合适的值是相对于所需要的隐层节点数来确定的。一般情况下，可以同时对两个不同 的期望误差值的网络进行训练，最后通过综合因素来确定其中一个网络。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="bp_3"&gt;BP网络的局限性&lt;/h2&gt;
&lt;p&gt;BP网络具有以下的几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;需要较长的训练时间：这主要是由于学习速率太小所造成的，可采用变化的或自适应的学习速率来加以改进。&lt;/li&gt;
&lt;li&gt;完全不能训练：这主要表现在网络的麻痹上，通常为了避免这种情况的产生，一是选取较小的初始权值，而是采用较小的学习速率。&lt;/li&gt;
&lt;li&gt;局部最小值：这里采用的梯度下降法可能收敛到局部最小值，采用多层网络或较多的神经元，有可能得到更好的结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="bp_4"&gt;BP网络的改进&lt;/h2&gt;
&lt;p&gt;BP算法改进的主要目标是加快训练速度，避免陷入局部极小值等，常见的改进方法有带动量因子算法、自适应学习速率、变化的学习速率以及作用函数后缩法等。 动量因子法的基本思想是在反向传播的基础上，在每一个权值的变化上加上一项正比于前次权值变化的值，并根据反向传播法来产生新的权值变化。而自适应学习速率的方法则是针对一些特定的问题的。改变学习速率的方法的原则是，若连续几次迭代中，若目标函数对某个权倒数的符号相同，则这个权的学习速率增加，反之若符号相反则减小它的学习速率。而作用函数后缩法则是将作用函数进行平移，即加上一个常数。&lt;/p&gt;
&lt;h1 id="sparse-autoencoder"&gt;Sparse AutoEncoder&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;有了以上基础知识,我们以下介绍一下Sparse AutoEncoder(稀疏自编码器)。&lt;/p&gt;
&lt;p&gt;目前为止，我们已经介绍了BP神经网络,而它采用的是有监督学习(使用带标签数据)。现在假设我们只有一个没有带类别标签的训练样本集合 ${x^{(1)}, x^{(2)}, x^{(3)}, \ldots}$，其中$x^{(i)} \in \Re^{n}$.自编码神经网络是一种无监督学习算法，它使用了反向传播算法，并让目标值等于输入值，比如 $y^{(i)} = x^{(i)}$ 。下图是一个自编码神经网络的示例。&lt;/p&gt;
&lt;p&gt;&lt;img alt="AutoEncoder NN" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/Autoencoder_zps061072cf.png" /&gt;&lt;/p&gt;
&lt;p&gt;自编码神经网络尝试学习一个$h_{W,b}(x) \approx x$的函数。换句话说，它尝试逼近一个恒等函数，从而使得输出$\hat{x}$接近于输入 $x$ 。恒等函数虽然看上去不太有学习的意义，但是当我们为自编码神经网络加入某些限制，比如限定隐藏神经元的数量，我们就可以从输入数据中发现一些有趣的结构。举例来说，假设某个自编码神经网络的输入$x$是一张$10\times10$图像（共100个像素）的像素灰度值，于是 $n=100$,其隐藏层$L_2$中有50个隐藏神经元。注意，输出也是100维的$y \in \Re^{100}$由于只有50个隐藏神经元，我们迫使自编码神经网络去学习输入数据的压缩表示，也就是说，它必须从50维的隐藏神经元激活度向量$a^{(2)} \in \Re^{50}$中重构出100维的像素灰度值输入$x$。如果网络的输入数据是完全随机的，比如每一个输入$x_i$都是一个跟其它特征完全无关的独立同分布高斯随机变量，那么这一压缩表示将会非常难学习。但是如果输入数据中隐含着一些特定的结构，比如某些输入特征是彼此相关的，那么这一算法就可以发现输入数据中的这些相关性。事实上，这一简单的自编码神经网络通常可以学习出一个跟主元分析（PCA）结果非常相似的输入数据的低维表示。&lt;/p&gt;
&lt;p&gt;我们刚才的论述是基于隐藏神经元数量较小的假设。但是即使隐藏神经元的数量较大（可能比输入像素的个数还要多），我们仍然通过给自编码神经网络施加一些其他的限制条件来发现输入数据中的结构。具体来说，如果我们给隐藏神经元加入稀疏性限制，那么自编码神经网络即使在隐藏神经元数量较多的情况下仍然可以发现输入数据中一些有趣的结构。&lt;/p&gt;
&lt;p&gt;稀疏性可以被简单地解释如下。如果当神经元的输出接近于1的时候我们认为它被激活，而输出接近于0的时候认为它被抑制，那么使得神经元大部分的时间都是被抑制的限制则被称作稀疏性限制。这里我们假设的神经元的激活函数是sigmoid函数。如果你使用tanh作为激活函数的话，当神经元输出为-1的时候，我们认为神经元是被抑制的。&lt;/p&gt;
&lt;p&gt;注意到$a^{(2)}_j$表示隐藏神经元$j$的激活度，但是这一表示方法中并未明确指出哪一个输入$x$带来了这一激活度。所以我们将使用 $a^{(2)}_j(x)$来表示在给定输入为$x$情况下，自编码神经网络隐藏神经元$j$的激活度。 进一步，让&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat\rho_j = \frac{1}{m} \sum_{i=1}^m \left[ a^{(2)}_j(x^{(i)}) \right]
\end{equation}&lt;/p&gt;
&lt;p&gt;表示隐藏神经元$j$的平均活跃度（在训练集上取平均）。我们可以近似的加入一条限制&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat\rho_j = \rho,
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\rho$是稀疏性参数，通常是一个接近于0的较小的值（比如$\rho=0.05$）。换句话说，我们想要让隐藏神经元$j$的平均活跃度接近0.05。为了满足这一条件，隐藏神经元的活跃度必须接近于0。&lt;/p&gt;
&lt;p&gt;为了实现这一限制，我们将会在我们的优化目标函数中加入一个额外的惩罚因子，而这一惩罚因子将惩罚那些$\hat\rho_j$和$\rho$有显著不同的情况从而使得隐藏神经元的平均活跃度保持在较小范围内。惩罚因子的具体形式有很多种合理的选择，我们将会选择以下这一种：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{j=1}^{s_2} \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j}.
\end{equation}&lt;/p&gt;
&lt;p&gt;这里，$s_2$是隐藏层中隐藏神经元的数量，而索引$j$依次代表隐藏层中的每一个神经元。如果你对相对熵(KL divergence）比较熟悉，这一惩罚因子实际上是基于它的。于是惩罚因子也可以被表示为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{j=1}^{s_2} {\rm KL}(\rho || \hat\rho_j),
\end{equation}&lt;/p&gt;
&lt;p&gt;其中 ${\rm KL}(\rho || \hat\rho_j)= \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j}$是一个以 $\rho$为均值和一个以$\hat\rho_j$为均值的两个伯努利随机变量之间的相对熵。相对熵是一种标准的用来测量两个分布之间差异的方法。&lt;/p&gt;
&lt;p&gt;这一惩罚因子有如下性质，当$\hat\rho_j = \rho$时,$\textstyle {\rm KL}(\rho || \hat\rho_j) = 0$，并且随着$\hat\rho_j$与$\rho$ 之间的差异增大而单调递增。举例来说，在下图中，我们设定$\rho = 0.2$并且画出了相对熵值${\rm KL}(\rho || \hat\rho_j)$随着 $\hat\rho_j$变化的变化。&lt;/p&gt;
&lt;p&gt;&lt;img alt="KL Penalty" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/KL_zpsb42db2ff.png" /&gt;&lt;/p&gt;
&lt;p&gt;我们可以看出，相对熵在$\hat\rho_j=\rho$时达到它的最小值0，而当$\hat\rho_j$靠近0或者1的时候，相对熵则变得非常大（其实是趋向于$\infty$）。所以，最小化这一惩罚因子具有使得$\hat\rho_j$靠近$\rho$的效果。现在，我们的总体代价函数可以表示为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
J_{\rm sparse}(W,b) = J(W,b) + \beta \sum_{j=1}^{s_2} {\rm KL}(\rho || \hat\rho_j),
\end{equation}
其中$J(W,b)$如之前所定义，而$\beta$控制稀疏性惩罚因子的权重。$\hat\rho_j$项则也（间接地）取决于$W,b$,因为它是隐藏神经元$j$的平均激活度，而隐藏层神经元的激活度取决于$W,b$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总而言之,通过控制隐含层神经元的数量或对它们施加稀疏性限制,我们就能得到关于原始数据的一种有效的特征表示&lt;/strong&gt;。具体而言,当我们采用我们上面提到的2D图像作为输入时,不同的隐藏单元学会了在图像的不同位置和方向进行边缘检测。显而易见，这些特征对物体识别等计算机视觉任务是十分有用的。若将其用于其他输入域（如音频），该算法也可学到对这些输入域有用的表示或特征。&lt;/p&gt;
&lt;h1 id="_9"&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8775360"&gt;Deep Learning（深度学习）学习笔记整理系列之（一)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8775488"&gt;Deep Learning（深度学习）学习笔记整理系列之（二)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8775518"&gt;Deep Learning（深度学习）学习笔记整理系列之（三)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ibillxia.github.io/blog/2013/03/30/back-propagation-neural-networks/"&gt;反向传播(BP)神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/wentingtu/archive/2012/06/05/2536425.html"&gt;BP神经网络模型与学习算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.codeproject.com/Articles/13582/Back-propagation-Neural-Net"&gt;BP神经网络C++实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://deeplearning.stanford.edu/wiki/index.php/%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7"&gt;自稀疏算法与稀疏性&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%AF%E8%A7%86%E5%8C%96%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C"&gt;可视化自编码器训练结果&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Deep Learning"></category><category term="Machine Learning"></category><category term="Back Propagation"></category><category term="BP Neural Network"></category><category term="Sparse Autoencoder"></category></entry><entry><title>机器学习系列(IV):聚类大观园</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-ivju-lei-da-guan-yuan.html" rel="alternate"></link><updated>2014-04-11T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-11:ji-qi-xue-xi-xi-lie-ivju-lei-da-guan-yuan.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Clustering&lt;/em&gt;和&lt;em&gt;Classification&lt;/em&gt;无疑是机器学习领域两个重量级的TASK,而且这两个概念作为初学者是比较容易混淆的。以下简要说明一下这两个概念之间的区别。Classification和Clustering都是要把一堆Objects分到不同的Group,但是两者还是有很明显的差异的。具体而言,Classification属于Supervised Learning,即训练样本必须是已标注样本，而聚类是Unsupervised Learning,我们要从未标注样本中进行学习，然后把Objects分到不同的Group中去。&lt;/p&gt;
&lt;p&gt;言归正传,Clustering,一言以蔽之,其核心思想就是"物以类聚,人以群分"。但是其核心前提在于&lt;strong&gt;物以何聚,人以何分&lt;/strong&gt;,即使用什么用来度量Objects之间的差异性。一种比较自然的想法就是使用Objects属性之间的差异用以度量Objects之间的差异性。&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta(x_i,x_{x\prime}） = \sum_{j=1}^D \Delta_j(x_{ij},x_{x\prime j})
\end{equation}&lt;/p&gt;
&lt;p&gt;常见的用来Capture属性之间差异性的函数则有如下几种:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高斯距离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大家最熟悉的无疑就是高斯距离了,当然，其只能应用于实数值。&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta_j (x_{ij},x_{x\prime j}) = (x_{ij}-x{i\prime j})^2
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;街区距离($l_1$距离)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;高斯距离由于采用二次形式,因此较大的差异容易被放大化,因此在很多场景下Gaussian距离对于Outliers很敏感。为了解决这个问题,人们引入了街区距离(city block distance),直观上理解就是在一个街区中我们从一点到另外一点要经过的街区数。定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta_j (x_{ij},x_{x\prime j}) = |x_{ij}-x_{i\prime j}|
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;海明距离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于类别类型的变量,如&lt;em&gt;{red,green,blue}&lt;/em&gt;，我们通常采用的办法则是当特征不同时,赋值为1,否则赋值为0;将所有的类别型变量相加有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta(x_i,x_{i\prime}) = \sum_{j=1}^D 1_{x_{ij} \neq x_{x\prime j}}
\end{equation}&lt;/p&gt;
&lt;p&gt;有了衡量Objects之间差异性的尺度,我们就能采用合适的算法将Objects分到不同的Group当中去。在正式介绍相关算法之间,我们还是简要说明一下如何衡量不同的聚类方法的好坏。&lt;/p&gt;
&lt;h1 id="_1"&gt;聚类方法评价&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;总体而言,聚类算法的核心目的即在于将相似的Objects分到同一类中去,并保证不同类之间的Objects之间的差异性。目前针对聚类算法广泛使用的评价指标有如下三种:&lt;/p&gt;
&lt;h2 id="purity"&gt;Purity&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Cluster Objects" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cluster_objects_zps871f5eb1.png" /&gt;&lt;/p&gt;
&lt;p&gt;如上图所示,每一个圆圈代表一个Cluster,每个圆圈内的物体均已被标注为&lt;em&gt;{A,B,C}&lt;/em&gt;中的一种。令$N_{ij}$表示Cluster i中label为j的物体的数目,$N_i$表示Cluster i中物体的总数。另一个cluster的纯度(purity)被定义为$p_i \triangleq max_j p_{ij}$,因此整个聚类结果的纯度为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
purity \triangleq \sum_{i} \frac{N_i}{N}p_i
\end{equation}&lt;/p&gt;
&lt;p&gt;根据上述定义,上图中聚类结果的纯度为:&lt;/p&gt;
&lt;p&gt;${6 \over 17}{5 \over 6}+{6 \over 17}{4 \over 6}+{5 \over 17}{3 \over 5} = 0.71$ &lt;/p&gt;
&lt;p&gt;不难看出,聚类的纯度越高，则表明该聚类算法越好。&lt;/p&gt;
&lt;h2 id="rand-index"&gt;Rand index&lt;/h2&gt;
&lt;p&gt;令$U={u_1,...,u_R}$和$V={v_1,...,v_C}$分别表示对于$N$个数据点的两种不同的划分方式。例如,$U$可能是我们估计的聚类结果，而$V$则是根据物体的Label得到的参考聚类结果。现我们定义如下四个值:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TP(True Positives):同时属于$U$和$V$中同一集合的物体对的数目,即不管对于$U$还是$V$而言,这些物体对均被分配到同一集合中;&lt;/li&gt;
&lt;li&gt;TN(True Negatives):对于$U$和$V$而言,被分配到不同集合的物体对的数目,即在参考聚类中,它们被分到不同的集合,在我们估计的聚类结果中,它们也被划分到不同的集合。&lt;/li&gt;
&lt;li&gt;FP(False Positives):在$V$中被分配到不同集合，而在$U$中被分配到相同集合的物体对的数目。&lt;/li&gt;
&lt;li&gt;FN(False Negatives):在$V$中被分配到相同集合，而在$U$中被分配到不同集合的物体对的数目。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而Rand index被定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
R \triangleq \frac{TP+TN}{TP+FP+FN+TN}
\end{equation}&lt;/p&gt;
&lt;p&gt;即我们估计的结果中被分配到正确的类中的物体对所占的比例。显然,$0 \leq R \leq 1$.&lt;/p&gt;
&lt;p&gt;还是举上图中的例子为例,说明其计算过程:&lt;/p&gt;
&lt;p&gt;上图中三个Cluster中点的个数分别是6,6,5.因此其中“positives”的数目为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
TP+FP = C_6^2+C_6^2+C_5^2 = 40
\end{equation}&lt;/p&gt;
&lt;p&gt;其中True Positives的数目为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
TP = C_5^2+C_4^2+C_2^2+C_3^2 = 20
\end{equation}&lt;/p&gt;
&lt;p&gt;同理我们可以得到$FN=24,TN=72$.因此我们有$R = \frac{20+72}{40+24+72} = 0.68$.&lt;/p&gt;
&lt;h2 id="mutual-information"&gt;Mutual Information&lt;/h2&gt;
&lt;p&gt;另外一种衡量聚类质量的方法是计算$U$和$V$之间的互信息。关于互信息的内容请参考&lt;a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html"&gt;机器学习系列(II):Generative models for discrete data&lt;/a&gt;,此处不再赘述。&lt;/p&gt;
&lt;h1 id="_2"&gt;基本聚类算法&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;本部分会着重介绍三种聚类算法,包括K-means,Spectral Clustering以及Hierarchical Clustering,且容我一一道来。&lt;/p&gt;
&lt;h2 id="k-means-k-medoids"&gt;K-Means &amp;amp; K-Medoids&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Cluster Points" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cluster_01_zps3c7151b4.png" /&gt;&lt;/p&gt;
&lt;p&gt;如上图所示,对于二维空间上的若干个点,我们要将它们分成若干类。我们直观上来看,上图中数据点大致可以分为3类,如果我们将每一类用不同的颜色标注，则可得到下图:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cluster Color" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cluster_color_zps7de900bf.png" /&gt;&lt;/p&gt;
&lt;p&gt;那么计算机要如何来完成这个任务呢？当然，计算机还没有高级到能够“通过形状大致看出来”，不过，对于这样的$N$维欧氏空间中的点进行聚类，有一个非常简单的经典算法，也就是本节我们要介绍的K-Means。在介绍K-Means的具体步骤之前，让我们先来看看它对于需要进行聚类的数据的一个基本假设吧：对于每一个cluster，我们可以选出一个中心点(center)，使得该cluster中的所有的点到该中心点的距离小于到其他cluster的中心的距离。虽然实际情况中得到的数据并不能保证总是满足这样的约束，但这通常已经是我们所能达到的最好的结果，而那些误差通常是固有存在的或者问题本身的不可分性造成的。例如下图所示的两个高斯分布，从两个分布中随机地抽取一些数据点出来，混杂到一起，现在要让你将这些混杂在一起的数据点按照它们被生成的那个分布分开来：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gaussian" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gaussian_zpsb3e5d8e5.png" /&gt;&lt;/p&gt;
&lt;p&gt;由于这两个分布本身有很大一部分重叠在一起了，例如，对于数据点2.5来说，它由两个分布产生的概率都是相等的，你所做的只能是一个猜测；稍微好一点的情况是2，通常我们会将它归类为左边的那个分布，因为概率大一些，然而此时它由右边的分布生成的概率仍然是比较大的，我们仍然有不小的几率会猜错。而整个阴影部分是我们所能达到的最小的猜错的概率，这来自于问题本身的不可分性，无法避免。因此，我们将K-Means所依赖的这个假设看作是合理的。&lt;/p&gt;
&lt;p&gt;基于这样一个假设,我们再来导出K-Means所要优化的目标函数:设我们一共有$N$个数据点需要分为$K$个cluster,K-Means要做的就是最小化:&lt;/p&gt;
&lt;p&gt;\begin{equation}
J = \sum_{n=1}^N\sum_{k=1}^K r_{nk} \|x_n-\mu_k\|^2
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$r_{nk}$在数据点$n$被归类到cluster k的时候为1,否则为0。直接寻找$r_{nk}$和$\mu_k$来最小化$J$并不容易，不过我们可以采取迭代的办法：先固定$\mu_k$,选择最优的$r_{nk}$，很容易看出，只要将数据点归类到离他最近的那个中心就能保证J最小。下一步则固定$r_{nk}$，再求最优的$\mu_k$。将$J$对$\mu_k$ 求导并令导数等于零，很容易得到$J$最小的时候$\mu_k$应该满足：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mu_k=\frac{\sum_n r_{nk}x_n}{\sum_n r_{nk}}
\end{equation}
亦即$\mu_k$的值应当是所有cluster k中的数据点的平均值。由于每一次迭代都是取到$J$的最小值，因此$J$只会不断地减小（或者不变），而不会增加，这保证了K-Means最终会到达一个极小值。虽然K-Means并不能保证总是能得到全局最优解，但是对于这样的问题，像K-Means这种复杂度的算法，这样的结果已经是很不错的了。&lt;/p&gt;
&lt;p&gt;下面我们来总结一下K-Means算法的具体步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选定$K$个中心$\mu_k$的初值。这个过程通常是针对具体的问题有一些启发式的选取方法，或者大多数情况下采用随机选取的办法。因为前面说过K-Means并不能保证全局最优，而是否能收敛到全局最优解其实和初值的选取有很大的关系，所以有时候我们会多次选取初值跑K-Means,并取其中最好的一次结果。&lt;/li&gt;
&lt;li&gt;将每个数据点归类到离它最近的那个中心点所代表的cluster中。&lt;/li&gt;
&lt;li&gt;用公式$\mu_k = \frac{1}{N_k}\sum_{j\in\text{cluster}_k}x_j$计算出每个cluster的新的中心点。&lt;/li&gt;
&lt;li&gt;重复第二步，一直到迭代了最大的步数或者前后的$J$的值相差小于一个阈值为止。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:&lt;strong&gt;K-Means&lt;/strong&gt;并不能保证全局最优,而且该算法得到的结果的好坏直接依赖于初始值的选取,当初始值选取不当时,最后得到的结果可能并不好,所以一般采用的方法是多次随机选取初始值,并选择结果最好的一次就行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;K-Means方法有一个很明显的局限就是它的距离衡量标准是高斯函数,所以只适用于特征是实数值的情形,而不能适用于当数据包含类型数据的情形。因此人们引入了K-Medoids算法。&lt;/p&gt;
&lt;p&gt;K-Medoids算法在K-Means算法的基础上做出了如下两个改变:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将原来的目标函数$J$中的欧氏距离改为一个任意的dissimilarity measure函数$\mathcal{V}$：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\tilde{J} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\mathcal{V}(x_n,\mu_k)
\end{equation}&lt;/p&gt;
&lt;p&gt;最常见的方式是构造一个dissimilarity matrix $\mathbf{D}$来代表$\mathcal{V}$，其中的元素$\mathbf{D}_{ij}$表示Object $i$和Object $j$之间的差异程度。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中心点的选取不再取同一Cluster数据的均值,&lt;strong&gt;而是从在已有的数据点里面选取的&lt;/strong&gt;,具体而言,选一个到该Cluster中其他点距离之和最小的点。这使得K-Medoids算法不容易受到那些由于误差之类的原因产生的Outlier的影响，更加robust一些。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="spectral-clustering"&gt;Spectral Clustering&lt;/h2&gt;
&lt;p&gt;Spectral Clustering(谱聚类)是一种基于图论的聚类方法，它能够识别任意形状的样本空间且收敛于全局最优解，其基本思想是利用样本数据的相似矩阵进行特征分解后得到的特征向量进行聚类，可见，它与样本feature无关而只与样本个数有关。&lt;/p&gt;
&lt;h3 id="_3"&gt;图的划分&lt;/h3&gt;
&lt;p&gt;图划分的目的是将有权无向图划分为两个或以上子图，使得子图规模差不多而割边权重之和最小。图的划分可以看做是有约束的最优化问题，它的目的是看怎么把每个点划分到某个子图中，比较不幸的是当你选择各种目标函数后发现该优化问题往往是NP-hard的。&lt;/p&gt;
&lt;p&gt;怎么解决这个问题呢？松弛方法往往是一种利器(比如SVM中的松弛变量),对于图的划分可以认为能够将某个点的一部分划分在子图1中，另一部分划分在子图2中,而不是非此即彼,使用松弛方法的目的是将组合优化问题转化为数值优化问题，从而可以在多项式时间内解决之，最后在还原划分时可以通过阈值来还原，或者使用类似K-Means这样的方法，之后会有相关说明。&lt;/p&gt;
&lt;h3 id="_4"&gt;相关定义&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;用$G=(V,E)$表示无向图，其中$V$和$E$分别为其顶点集和边集；&lt;/li&gt;
&lt;li&gt;说某条边属于某个子图是指该边的两个顶点都包含在子图中；&lt;/li&gt;
&lt;li&gt;假设边$e$的两个不同端点为$i$和$j$，则该边的权重用$\omega_{i,j}$表示，对于无向无环图有$\omega_{i,j} = \omega_{j,i}$且$\omega_{i,i}=0$,为方便以下的“图”都指无向无环图；&lt;/li&gt;
&lt;li&gt;对于图的某种划分方案$Cut$的定义为：所有两端点不在同一子图中的边的权重之和，它可以被看成该划分方案的&lt;strong&gt;损失函数&lt;/strong&gt;,希望这种损失越小越好,本文以二分无向图为例，假设原无向图$G$被划分为$G_1$和$G_2$，那么有:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}
\end{equation}&lt;/p&gt;
&lt;h3 id="laplacian"&gt;Laplacian矩阵&lt;/h3&gt;
&lt;p&gt;假设无向图$G$被划分为$G_1$和$G_2$两个子图，该图的顶点数为:n=|V|，用$q$表示维指示向量，表明该划分方案，每个分量定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
c_1 &amp;amp; i \in G_1 \\
c_2 &amp;amp; i \in G_2 
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;于是有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \sum_{i \in G_1,j \in G_2} \omega_{i,j} = \frac{\sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i-q_j)^2}{2(c_1-c_2)^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;又因为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i-q_j)^2 &amp;amp;= \sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i^2-2q_iq_j+q_j^2) \\
&amp;amp;= \sum_{i=1}^n \sum_{j=1}^n -2\omega_{i,j}q_iq_j + \sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i^2+q_j^2) \\
&amp;amp;= \sum_{i=1}^n \sum_{j=1}^n -2\omega_{i,j}q_iq_j + \sum_{i=1}^n 2q_i^2(\sum_{j=1}^n \omega_{i,j}) \\
&amp;amp;=2q^T(D-W)q
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$D$为对角矩阵，对角线元素为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
D_{i,i} = \sum_{j=1}^n \omega_{i,j}
\end{equation}&lt;/p&gt;
&lt;p&gt;$W$为权重矩阵：$W_{i,j} = \omega_{i,j}$且$W_{i,i}=0$。&lt;/p&gt;
&lt;p&gt;重新定义一个对称矩阵$L$，它便是Laplacian矩阵：&lt;/p&gt;
&lt;p&gt;\begin{equation}
L=D-W
\end{equation}&lt;/p&gt;
&lt;p&gt;矩阵元素为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
L_{i,j} = 
\left \lbrace
\begin{array}{cc}
\sum_{j=1}^n \omega_{i,j} &amp;amp; i = j \\
-\omega_{i,j} &amp;amp; i \neq j
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;进一步观察：&lt;/p&gt;
&lt;p&gt;\begin{equation}
q^TLq = {1 \over 2} \sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i-q_j)^2
\end{equation}&lt;/p&gt;
&lt;p&gt;如果所有权重值都为非负，那么就有，这说明Laplacian矩阵是半正定矩阵；而当无向图为连通图时有特征值0且对应特征向量为$[1,1,1...1]^T$，这反映了，如果将无向图划分成两个子图，一个为其本身，另一个为空时，为0(当然，这种划分是没有意义的)。&lt;/p&gt;
&lt;p&gt;其实上面推导的目的在于得到下面的关系：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \frac{q^TLq}{(c_1-c_2)^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;这个等式的核心价值在于：将最小化划分的问题转变为最小化二次函数；从另一个角度看，实际上是把原来求离散值松弛为求连续实数值。&lt;/p&gt;
&lt;p&gt;观察下图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Graph" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/graph_zps39bd50fe.png" /&gt;&lt;/p&gt;
&lt;p&gt;根据上图我们可以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
W = 
\left[
\begin{array}{cc}
0.0  &amp;amp; 0.8 &amp;amp; 0.6 &amp;amp; 0.0 &amp;amp; 0.1 &amp;amp; 0.0 \\
0.8  &amp;amp; 0.0 &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.6  &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.2 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.2 &amp;amp; 0.0 &amp;amp; 0.8 &amp;amp; 0.7 \\
0.1  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.8 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.7 &amp;amp; 0.8 &amp;amp; 0.0
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
D = 
\left[
\begin{array}{cc}
1.5  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 1.6 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 1.6 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.7 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.7 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.5
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;Laplacian矩阵为：
\begin{equation}
L = 
\left[
\begin{array}{cc}
1.5  &amp;amp; -0.8 &amp;amp; -0.6 &amp;amp; 0.0 &amp;amp; -0.1 &amp;amp; 0.0 \\
-0.8  &amp;amp; 1.6 &amp;amp; -0.8 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
-0.6  &amp;amp; -0.8 &amp;amp; 1.6 &amp;amp; -0.2 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; -0.2 &amp;amp; 1.7 &amp;amp; -0.8 &amp;amp; -0.7 \\
-0.1  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; -0.8 &amp;amp; 1.7 &amp;amp; -0.8 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; -0.7 &amp;amp; -0.8 &amp;amp; 1.5
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;Laplacian矩阵是一种有效表示图的方式，任何一个Laplacian矩阵都对应一个权重非负地无向有权图,而满足以下条件的就是Laplacian矩阵:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为对称半正定矩阵，保证所有特征值都大于等于0;&lt;/li&gt;
&lt;li&gt;矩阵有唯一的0特征值，其对应的特征向量为$[1,1,1...1]^T$，它反映了图的一种划分方式：一个子图包含原图所有端点，另一个子图为空。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="_5"&gt;划分方法&lt;/h3&gt;
&lt;h4 id="minimum-cut"&gt;Minimum Cut方法&lt;/h4&gt;
&lt;p&gt;考虑最简单情况，另$C_1=1=-C_2$,无向图划分指示向量定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
1 &amp;amp; i \in G_1 \\
-1 &amp;amp; i \in G_2 
\end{array}
\right.
\end{equation}                          &lt;/p&gt;
&lt;p&gt;要优化的目标为$Cut(G_1,G_2)$，由之前的推导可以将该问题松弛为以下问题：&lt;/p&gt;
&lt;p&gt;\begin{equation}
min \ q^TLq \\
s.t. [1,1,...1]^Tq=0 \\
q^Tq = n
\end{equation}&lt;/p&gt;
&lt;p&gt;从这个问题的形式可以联想到&lt;strong&gt;Rayleigh quotient&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\begin{equation}
R(L,q) = \frac{q^TLq}{q^Tq}
\end{equation}&lt;/p&gt;
&lt;p&gt;原问题的最优解就是该Rayleigh quotient的最优解，而由Rayleigh quotient的性质可知：它的最小值，第二小值，......，最大值分别对应矩阵$L$的最小特征值，第二小特征值，......，最大特征值，且极值$q$在相应的特征向量处取得，即需要求解下特征系统的特征值和特征向量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Lq = \lambda q
\end{equation}&lt;/p&gt;
&lt;p&gt;这里需要注意约束条件$[1,1,...1]^Tq=0$，显然的最小特征值为0,此时对应特征向量为[1,1,...1]^T,不满足这个约束条件(剔除了无意义划分)，于是最优解应该在第二小特征值对应的特征向量处取得。&lt;/p&gt;
&lt;p&gt;当然，求得特征向量后还要考虑如何恢复划分，比如可以这样：特征向量分量值为正所对应的端点划分为$G_1$，反之划分为$G_2$；也可以这样：将特征向量分量值从小到大排列，以中位数为界划分$G_1$和$G_2$；还可以用K-Means算法聚类。&lt;/p&gt;
&lt;h4 id="ratio-cut"&gt;Ratio Cut方法&lt;/h4&gt;
&lt;p&gt;实际当中，划分图时除了要考虑最小化$Cut$外还要考虑划分的平衡性，为缓解出现类似一个子图包含非常多端点而另一个只包含很少端点的情况，出现了Ratio Cut，它衡量子图大小的标准是子图包含的端点个数。&lt;/p&gt;
&lt;p&gt;定义$n_1$为子图1包含端点数，$n_2$为子图2包含端点数，$n_2=n-n_1$，则优化目标函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
obj = Cut(G_1,G_2)(\frac{1}{n_1}+\frac{1}{n_2})
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}
\end{equation}&lt;/p&gt;
&lt;p&gt;做一个简单变换：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
obj &amp;amp;= Cut(G_1,G_2)(\frac{1}{n_1}+\frac{1}{n_2}) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}(\frac{1}{n_1}+\frac{1}{n_2}) \\
&amp;amp;= \sum_{i \in G_1,j \in G_2} \omega_{i,j}(\frac{n_1+n_2}{n_1n_2})=\sum_{i \in G_1,j \in G_2} \omega_{i,j}(\frac{(n_1+n_2)^2}{n_1n_2n}) \\
&amp;amp;= \sum_{i \in G_1,j \in G_2} \omega_{i,j}(\sqrt{\frac{n_1}{n_2n}}+\sqrt{\frac{n_2}{n_1n}})^2 \\
&amp;amp;= \sum_{i \in G_1,j \in G_2} \omega_{i,j}(q_i-q_j)^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
\sqrt{\frac{n_1}{n_2n}} &amp;amp; i \in G_1 \\
\sqrt{\frac{n_2}{n_1n}} &amp;amp; i \in G_2
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;看吧，这形式多给力，原问题就松弛为下面这个问题了：&lt;/p&gt;
&lt;p&gt;\begin{equation}
min \ q^TLq \\
s.t. q^Te=0 \\
q^Tq = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;依然用&lt;strong&gt;Rayleigh quotient&lt;/strong&gt;求解其第二小特征值及其特征向量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:关于原问题为什么能松弛为上述形式,没看懂,望高人指点(Normalized Cut这部分也没完全懂)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id="normalized-cut"&gt;Normalized Cut方法&lt;/h4&gt;
&lt;p&gt;与Ratio Cut类似，不同之处是，它衡量子图大小的标准是:子图各个端点的Degree之和。
定义:$d_1$为子图1Degree之和:&lt;/p&gt;
&lt;p&gt;\begin{equation}
d_1 = \sum_{i \in G_1} d_i
\end{equation}&lt;/p&gt;
&lt;p&gt;$d_2$为子图2Degree之和:&lt;/p&gt;
&lt;p&gt;\begin{equation}
d_2 = \sum_{i \in G_2} d_i
\end{equation}&lt;/p&gt;
&lt;p&gt;则优化目标函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
obj = Cut(G_1,G_2)(\frac{1}{d_1}+\frac{1}{d_2}) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}(q_i-q_j)^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
\sqrt{\frac{d_1}{d_2d}} &amp;amp; i \in G_1 \\
\sqrt{\frac{d_2}{d_1d}} &amp;amp; i \in G_2
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;原问题就松弛为下面这个问题了：&lt;/p&gt;
&lt;p&gt;\begin{equation}
min \ q^TLq \\
s.t. q^De=0 \\
q^Dq = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;用泛化的Rayleigh quotient表示为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
R(L,q) = \frac{q^TLq}{q^TDq}
\end{equation}&lt;/p&gt;
&lt;p&gt;那问题就变成求解下特征系统的特征值和特征向量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
Lq &amp;amp;= \lambda Dq \\
&amp;amp;\Leftrightarrow Lq = \lambda D^{1 \over 2}D^{1 \over 2}q \\
&amp;amp;\Leftrightarrow D^{-\frac{1}{2}}LD^{-\frac{1}{2}}D^{1 \over 2}q = \lambda D^{1 \over 2}q \\
&amp;amp;\Leftrightarrow L\prime q\prime = \lambda q\prime
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$L\prime = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$,$q\prime=D^{1 \over 2}q$
                                                                                                显然，上式中最上面式子和最下面式子有相同的特征值，但是对应特征值的特征向量关系为:$q\prime=D^{1 \over 2}q$，因此我们可以先求最下面式子的特征值及其特征向量，然后为每个特征向量乘以$D^{-\frac{1}{2}}$就得到最上面式子的特征向量。&lt;/p&gt;
&lt;p&gt;哦,对了,矩阵$L\prime = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$叫做Normalized Laplacian，因为它对角线元素值都为1。&lt;/p&gt;
&lt;h3 id="spectral-clustering_1"&gt;Spectral Clustering&lt;/h3&gt;
&lt;p&gt;上边说了这么多，其实就是想将图的划分应用于聚类，而且这种聚类只需要样本的相似矩阵即可，把每个样本看成图中的一个顶点，样本之间的相似度看成由这两点组成的边的权重值，那么相似矩阵就是一幅有权无向图。对照图的划分方法，有下列两类Spectral Clustering算法，他们的区别在于Laplacian矩阵是否是规范化的:&lt;/p&gt;
&lt;p&gt;&lt;img alt="SC_Classify" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sc_classify_zpse5f47df7.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unnormalized Spectral Clustering算法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;算法输入：样本相似矩阵$S$和要聚类的类别数$K$。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据矩阵$S$建立权重矩阵$W$、三角矩阵$D$；&lt;/li&gt;
&lt;li&gt;建立Laplacian矩阵$L$；&lt;/li&gt;
&lt;li&gt;求矩阵$L$的前$K$小个特征值及其对应的特征向量，注意最小的那个特征值一定是0且对应的特征向量为[1,1,...1]^T；&lt;/li&gt;
&lt;li&gt;以这K组特征向量组成新的矩阵，其行数为样本数，列数为$K$，这里就是做了降维操作，从$N$维降到$K$维，(实际上除去那个全为1的向量维度降为了$K-1$)；&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用K-Means算法进行聚类，得到$K$个Cluster。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Normalized Spectral Clustering算法&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;算法输入：样本相似矩阵$S$和要聚类的类别数$K$。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据矩阵$S$建立权重矩阵$W$、三角矩阵$D$；&lt;/li&gt;
&lt;li&gt;建立Laplacian矩阵$L$以及$L\prime = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$；&lt;/li&gt;
&lt;li&gt;求矩阵$L\prime$的前$K$小个特征值及其对应的特征向量，注意最小的那个特征值一定是0且对应的特征向量为$[1,1...1]^T$；&lt;/li&gt;
&lt;li&gt;利用$q\prime=D^{1 \over 2}q$求得矩阵$L$前$K$小个特征向量；&lt;/li&gt;
&lt;li&gt;以这$K$组特征向量组成新的矩阵，其行数为样本数$N$，列数为$K$；&lt;/li&gt;
&lt;li&gt;使用K-Means算法进行聚类，得到$K$个Cluster。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="_6"&gt;源码实现&lt;/h3&gt;
&lt;p&gt;以下我们给出Unnormalized Spectral Clustering的Python源码实现:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import sys

import numpy as np
import matplotlib.pyplot as plt

from scipy.linalg import eig
from scipy.cluster.vq import kmeans2
from scipy.sparse.linalg import eigen
from scipy.spatial.kdtree import KDTree

def get_noise(stddev=0.25, numpoints=150):
    # 2d gaussian random noise
    x = np.random.normal(0, stddev, numpoints)
    y = np.random.normal(0, stddev, numpoints)
    return np.column_stack((x, y))

def get_circle(center=(0.0, 0.0), r=1.0, numpoints=150):
    # use polar coordinates to get uniformly distributed points
    step = np.pi * 2.0 / numpoints
    t = np.arange(0, np.pi * 2.0, step)
    x = center[0] + r * np.cos(t)
    y = center[1] + r * np.sin(t)
    return np.column_stack((x, y))

def radial_kernel(c=1.5):
    def inner(a, b):
        d = a - b
        return np.exp((-1 * (np.sqrt(np.dot(d, d.conj()))**2)) / c)
    return inner

def circle_samples():
    "Generate noisy circle points"
    circles = []
    for radius in (1.0, 2.8, 5.0):
        circles.append(get_circle(r=radius) + get_noise())
    return np.vstack(circles)

def mutual_knn(points, n=10, distance=radial_kernel()):
    knn = {}
    kt = KDTree(points)
    for i, point in enumerate(points):
        # cannot use euclidean distance directly
        for neighbour in kt.query(point, n + 1)[1]:
            if i != neighbour:
                knn.setdefault(i, []).append(
                    (distance(point, points[neighbour]), neighbour))
    return knn

def get_distance_matrix(knn):
    n = len(knn)
    W = np.zeros((n, n))
    for point, nearest_neighbours in knn.iteritems():
        for distance, neighbour in nearest_neighbours:
            W[point][neighbour] = distance
    return W

def rename_clusters(idx):
    # so that first cluster has index 0
    num = -1
    seen = {}
    newidx = []
    for id in idx:
        if id not in seen:
            num += 1
            seen[id] = num
        newidx.append(seen[id])
    return np.array(newidx)

def cluster_points(L):
    # sparse eigen is a little bit faster than eig
    #evals, evcts = eigen(L, k=15, which="SM")
    evals, evcts = eig(L)
    evals, evcts = evals.real, evcts.real
    edict = dict(zip(evals, evcts.transpose()))
    evals = sorted(edict.keys())
    # second and third smallest eigenvalue + vector
    Y = np.array([edict[k] for k in evals[1:3]]).transpose()
    res, idx = kmeans2(Y, 3, minit='random')
    return evals[:15], Y, rename_clusters(idx)

def change_tick_fontsize(ax, size):
    for tl in ax.get_xticklabels():
        tl.set_fontsize(size)
    for tl in ax.get_yticklabels():
        tl.set_fontsize(size)

def get_colormap():
    # map cluster label to color (0, 1, 2) -&amp;gt; (orange, blue, green)
    from matplotlib.colors import ListedColormap
    orange = (0.918, 0.545, 0.0)
    blue = (0.169, 0.651, 0.914)
    green = (0.0, 0.58, 0.365)
    return ListedColormap([orange, blue, green])

def plot_circles(ax, points, idx, colormap):
    plt.scatter(points[:,0], points[:,1], s=10, c=idx, cmap=colormap,
        alpha=0.9, facecolors="none")
    plt.xlabel("x1", fontsize=8)
    plt.ylabel("x2", fontsize=8)
    change_tick_fontsize(ax, 8 )
    plt.ylim(-6, 6)
    plt.xlim(-6, 6)

def plot_eigenvalues(ax, evals):
    plt.scatter(np.arange(0, len(evals)), evals,
        c=(0.0, 0.58, 0.365), linewidth=0)
    plt.xlabel("Number", fontsize=8)
    plt.ylabel("Eigenvalue", fontsize=8)
    plt.axhline(0, ls="--", c="k")
    change_tick_fontsize(ax, 8 )

def plot_eigenvectors(ax, Y, idx, colormap):
    from matplotlib.ticker import MaxNLocator
    from mpl_toolkits.axes_grid import make_axes_locatable
    divider = make_axes_locatable(ax)
    ax2 = divider.new_vertical(size="100%", pad=0.05)
    fig1 = ax.get_figure()
    fig1.add_axes(ax2)
    ax2.set_title("Eigenvectors", fontsize=10)
    ax2.scatter(np.arange(0, len(Y)), Y[:,0], s=10, c=idx, cmap=colormap,
        alpha=0.9, facecolors="none")
    ax2.axhline(0, ls="--", c="k")
    ax2.yaxis.set_major_locator(MaxNLocator(4))
    ax.yaxis.set_major_locator(MaxNLocator(4))
    ax.axhline(0, ls="--", c="k")
    ax.scatter(np.arange(0, len(Y)), Y[:,1], s=10, c=idx, cmap=colormap,
        alpha=0.9, facecolors="none")
    ax.set_xlabel("index", fontsize=8)
    ax2.set_ylabel("2nd Smallest", fontsize=8)
    ax.set_ylabel("3nd Smallest", fontsize=8)
    change_tick_fontsize(ax, 8 )
    change_tick_fontsize(ax2, 8 )
    for tl in ax2.get_xticklabels():
        tl.set_visible(False)

def plot_spec_clustering(ax, Y, idx, colormap):
    plt.title("Spectral Clustering", fontsize=10)
    plt.scatter(Y[:,0], Y[:,1], c=idx, cmap=colormap, s=10, alpha=0.9,
        facecolors="none")
    plt.xlabel("Second Smallest Eigenvector", fontsize=8)
    plt.ylabel("Third Smallest Eigenvector", fontsize=8)
    change_tick_fontsize(ax, 8 )

def plot_figure(points, evals, Y, idx):
    colormap = get_colormap()
    fig = plt.figure(figsize=(6, 5.5))

    fig.subplots_adjust(wspace=0.4, hspace=0.3)
    ax = fig.add_subplot(2, 2, 1)
    plot_circles(ax, points, idx, colormap)

    ax = fig.add_subplot(2, 2, 2)
    plot_eigenvalues(ax, evals)

    ax = fig.add_subplot(2, 2, 3)
    plot_eigenvectors(ax, Y, idx, colormap)

    ax = fig.add_subplot(2, 2, 4)
    plot_spec_clustering(ax, Y, idx, colormap)

    plt.show()

def main(args):
    points = circle_samples()
    knn_points = mutual_knn(points)
    W = get_distance_matrix(knn_points)
    G = np.diag([sum(Wi) for Wi in W])

    # unnormalized graph Laplacian
    L = G - W
    evals, Y, idx = cluster_points(L)

    plot_figure(points, evals, Y, idx)

if __name__ == "__main__":
    main(sys.argv)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行源码得到下图:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spectral Clustering" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/SpectralClustering_zpsb5f59a0e.png" /&gt;&lt;/p&gt;
&lt;h3 id="_7"&gt;总结&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;图划分问题中的关键点在于选择合适的指示向量并将其进行松弛化处理，从而将最小化划分的问题转变为最小化二次函数，进而转化为求Rayleigh quotient极值的问题;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spectral Clustering的各个阶段为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择合适的相似性函数计算相似度矩阵；&lt;/li&gt;
&lt;li&gt;计算矩阵L的特征值及其特征向量，比如可以用&lt;code&gt;Lanczos&lt;/code&gt;迭代算法；&lt;/li&gt;
&lt;li&gt;如何选择K，可以采用启发式方法，比如，发现第1到m的特征值都挺小的，到了m+1突然变成较大的数，那么就可以选择K=m；&lt;/li&gt;
&lt;li&gt;用K-Means算法聚类，当然它不是唯一选择；&lt;/li&gt;
&lt;li&gt;Normalized Spectral Clustering在让Cluster间相似度最小而Cluster内部相似度最大方面表现要更好，所以首选这类方法。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="hierarchical-clustering"&gt;Hierarchical Clustering&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Hierarchical_Clustering" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hierarchical_clustering_zps8134ae3b.png" /&gt;&lt;/p&gt;
&lt;p&gt;Hierarchical Clustering正如它字面上的意思那样，是层次化的聚类，得出来的结构是一棵树，如上图所示。在前面我们介绍过不少聚类方法，但是都是“平坦”型的聚类，然而他们还有一个更大的共同点，或者说是弱点，就是难以确定类别数。我们这里要说的Hierarchical Clustering则从某种意义上来说也算是解决了这个问题，因为在做Clustering的时候并不需要知道类别数，而得到的结果是一棵树，事后可以在任意的地方横切一刀，得到指定数目的cluster,按需取即可。&lt;/p&gt;
&lt;p&gt;听上去很诱人，不过其实Hierarchical Clustering的想法很简单，主要分为两大类：Agglomerative(自底向上)和Divisive(自顶向下)。首先说前者，自底向上，一开始，每个数据点各自为一个类别，然后每一次迭代选取距离最近的两个类别，把他们合并，直到最后只剩下一个类别为止，至此一棵树构造完成。&lt;/p&gt;
&lt;p&gt;看起来很简单吧？其实确实也是比较简单的，不过还是有两个问题需要先说清除才行：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如何计算两个点的距离？这个通常是problem dependent的，一般情况下可以直接用一些比较通用的距离就可以了，比如欧氏距离等。&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如何计算两个类别之间的距离？一开始所有的类别都是一个点，计算距离只是计算两个点之间的距离，但是经过后续合并之后，一个类别里就不止一个点了，那距离又要怎样算呢？到这里又有三个变种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single Linkage：又叫做nearest-neighbor,就是取两个集合中距离最近的两个点的距离作为这两个集合的距离，容易造成一种叫做Chaining的效果，两个cluster明明从“大局”上离得比较远，但是由于其中个别的点距离比较近就被合并了,并且这样合并之后Chaining效应会进一步扩大,最后会得到比较松散的cluster 。&lt;/li&gt;
&lt;li&gt;Complete Linkage：这个则完全是Single Linkage的反面极端，取两个集合中距离最远的两个点的距离作为两个集合的距离。其效果也是刚好相反的，限制非常大，两个cluster即使已经很接近了，但是只要有不配合的点存在，就顽固到底，老死不相合并，也是不太好的办法。&lt;/li&gt;
&lt;li&gt;Group Average：这种方法看起来相对有道理一些，也就是把两个集合中的点两两的距离全部放在一起求一个平均值,相对也能得到合适一点的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的来说，一般都不太用Single Linkage或者Complete Linkage这两种过于极端的方法。整个agglomerative hierarchical clustering 的算法就是这个样子，描述起来还是相当简单的，不过计算起来复杂度还是比较高的，要找出距离最近的两个点，需要一个双重循环，而且 Group Average计算距离的时候也是一个双重循环。&lt;/p&gt;
&lt;p&gt;另外，需要提一下的是本文一开始的那个树状结构图，它有一个专门的称呼，叫做Dendrogram，其实就是一种二叉树，画的时候让子树的高度和它两个后代合并时相互之间的距离大小成比例，就可以得到一个相对直观的结构概览。&lt;/p&gt;
&lt;p&gt;Agglomerative clustering差不多就这样了，再来看Divisive clustering,也就是自顶向下的层次聚类，这种方法并没有Agglomerative clustering这样受关注，大概因为把一个节点分割为两个并不如把两个节点结合为一个那么简单吧，通常在需要做hierarchical clustering 但总体的cluster数目又不太多的时候可以考虑这种方法，这时可以分割到符合条件为止，而不必一直分割到每个数据点一个cluster 。&lt;/p&gt;
&lt;p&gt;总的来说，Divisive clustering 的每一次分割需要关注两个方面：一是选哪一个cluster来分割；二是如何分割。关于cluster的选取，通常采用一些衡量松散程度的度量值来比较，例如cluster中距离最远的两个数据点之间的距离，或者cluster中所有节点相互距离的平均值等，直接选取最“松散”的一个cluster来进行分割。而分割的方法也有多种，比如，直接采用普通的flat clustering算法（例如K-Means来进行二类聚类，不过这样的方法计算量变得很大，而且像K-Means 这样的和初值选取关系很大的算法，会导致结果不稳定。另一种比较常用的分割方法(&lt;code&gt;简而言之就是一个排除异己的过程&lt;/code&gt;)如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;待分割的cluster记为$G$,在$G$中取出一个到其他点的平均距离最远的点$x$,构成新cluster H；&lt;/li&gt;
&lt;li&gt;在 G 中选取这样的点$x\prime$,$x\prime$到$G$中其他点的平均距离减去$x\prime$到$H$中所有点的平均距离这个差值最大，将其归入$H$中；&lt;/li&gt;
&lt;li&gt;重复上一个步骤，直到差值为负。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;到此为止，关于Hierarchical clustering介绍就结束了。总的来说，Hierarchical clustering算法似乎都是描述起来很简单，计算起来很困难（计算量很大）。并且，不管是Agglomerative还是 Divisive 实际上都是贪心算法了，也并不能保证能得到全局最优的。而得到的结果，虽然说可以从直观上来得到一个比较形象的大局观，但是似乎实际用处并不如众多Flat clustering 算法那么广泛。&lt;/p&gt;
&lt;h1 id="_8"&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Adjusted_rand_index#Adjusted_Rand_index"&gt;Rand Index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=17"&gt;漫谈 Clustering (1): k-means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=40"&gt;漫谈 Clustering (2): k-medoids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/vivounicorn/archive/2012/02/10/2343377.html"&gt;Spectral Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Machine Learning:A Probabilistic Perspective(&lt;em&gt;Chapter 25&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Clustering"></category><category term="Unsupervised Learning"></category><category term="K-Means"></category><category term="Spectral Learning"></category><category term="Hierarchical Clustering"></category></entry><entry><title>机器学习系列(III):Gaussian Models</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iiigaussian-models.html" rel="alternate"></link><updated>2014-03-15T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-03-15:ji-qi-xue-xi-xi-lie-iiigaussian-models.html</id><summary type="html">&lt;p&gt;在&lt;a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html"&gt;上一篇&lt;/a&gt;中我们着重介绍了对于离散数据的生成模型，紧接上一篇，本篇我们介绍对于连续数据的生成模型。好吧,废话我们就不多说了,直接进入正文。&lt;/p&gt;
&lt;h1 id="mle-for-mvn"&gt;MLE for MVN&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id="basics-about-mvn"&gt;Basics about MVN&lt;/h2&gt;
&lt;p&gt;谈到连续分布,我们很自然地就会想到高斯分布,从小学到现在，印象中第一个走入我脑海中的看着比较高端大气上档次的就是Gaussian分布了。这次我们的重点也会完全集中在Gaussian分布之了,在正式讨论之前，我们先介绍一些关于Gaussian分布的基础知识。&lt;/p&gt;
&lt;p&gt;在$D$维空间中,MVN(Multivariate Normal)多变量正态分布的概率分布函数具有如下形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
N(x|\mu,\Sigma) \triangleq \frac{1}{(2\pi)^{D/2}det(\Sigma)^{1/2}} exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中的指数部分是$x$与$\mu$之间的&lt;a href="http://en.wikipedia.org/wiki/Mahalanobis_distance"&gt;Mahalanobis距离&lt;/a&gt;。为了更好地理解这个量,我们对$\Sigma$做特征值分解,即$\Sigma = U \Lambda U^T$,其中$U$是一正交阵,满足$U^TU=I$,而$\Lambda$是特征值矩阵。&lt;/p&gt;
&lt;p&gt;通过特征值分解,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Sigma^{-1} = U^{-T}\Lambda^{-1}U^{-1} = U\Lambda^{-1}U^T = \sum_{i=1}^{D}\frac{1}{\lambda_i}u_iu_i^T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$u_i$是$U$的第$i$列。因此Mahalanobis距离可被改写为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
(x-\mu)^T\Sigma^{-1}(x-\mu) &amp;amp;= (x-\mu)^T (\sum_{i=1}^{D}\frac{1}{\lambda_i}u_iu_i^T) (x-\mu)   \\
                            &amp;amp;= \sum_{i=1}^{D} \frac{1}{\lambda_i}(x-\mu)^T u_iu_i^T (x-\mu)    \\
                            &amp;amp;= \sum_{i=1}^{D} \frac{y_i^2}{x_i}                                \\
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$y_i \triangleq u_i^T(x-\mu)$。另2维空间中的椭圆方程为:&lt;/p&gt;
&lt;p&gt;$$\frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2} = 1$$&lt;/p&gt;
&lt;p&gt;因此我们可知Gaussian概率密度的等高线沿着椭圆分布,特征向量决定椭圆的朝向,而特征值则决定椭圆有多&lt;code&gt;“椭”&lt;/code&gt;。一般来说，如果我们将坐标系移动$\mu$,然后按$U$旋转，此时的欧拉距离即为Mahalanobis距离。&lt;/p&gt;
&lt;h2 id="mle-for-mvn_1"&gt;MLE for MVN&lt;/h2&gt;
&lt;p&gt;以下我们给出MVN参数的MLE(极大似然估计)的证明:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 3.1 若我们获取的$N$个独立同分布的样本$x_i \sim\ N(x|\mu,\Sigma)$,则关于$\mu$以及
$\Sigma$的极大似然分布如下:
&lt;img alt="MLE for Gaussian" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mle_zpscaea1f03.png"&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们不加证明地给出如下公式组:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;\begin{equation}
  \begin{split}
  &amp;amp;\frac{\partial(b^Ta)}{\partial a} = b  \\
  &amp;amp;\frac{\partial(a^TAa)}{\partial a} = (A+A^T)a \\
  &amp;amp;\frac{\partial}{\partial A} tr(BA) = B^T  \\
  &amp;amp;\frac{\partial}{\partial A} log |A| = A^{-T} \\
  &amp;amp;tr(ABC) = tr(CAB) = tr(BCA)
  \end{split}
  \end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最后一个等式称为迹的循环置换性质(cyclic permutation property)。利用这个性质,我们使用&lt;code&gt;trace trick&lt;/code&gt;可以得到下式:&lt;/p&gt;
&lt;p&gt;$$x^TAx = tr(x^TAx) = tr(xx^TA) = tr(Axx^T)$$&lt;/p&gt;
&lt;p&gt;证明:&lt;/p&gt;
&lt;p&gt;对数似然函数为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l(\mu,\Sigma) = log p(D|\mu,\Sigma) = \frac{N}{2} log |\Lambda| - \frac{1}{2} \sum_{i=1}^{N} (x_i-\mu)^T \Lambda (x_i-\mu)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\Lambda = \Sigma^{-1}$为精度矩阵。令$y_i=x_i-\mu$并利用链式法则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial}{\partial \mu}(x_i-\mu)^T \Sigma^{-1} (x_i-\mu) = \frac {\partial}{\partial y_i} y_i^T \Sigma^{-1} y_i \frac{\partial y_i}{\partial \mu}=-(\Sigma^{-T}+\Sigma^{-1})y_i
\end{equation}&lt;/p&gt;
&lt;p&gt;即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial}{\partial \mu} l(\mu,\Sigma) = -\frac{1}{2} \sum_{i=1}^{N} -2\Sigma^{-1}(x_i-\mu) = 0
\end{equation}&lt;/p&gt;
&lt;p&gt;故有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} x_i = \bar{x}
\end{equation}&lt;/p&gt;
&lt;p&gt;即最大似然均值即为经验均值。&lt;/p&gt;
&lt;p&gt;利用trace trick我们重写对数似然函数为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l(\Lambda) = \frac{N}{2} log |\Lambda| - \frac{1}{2} \sum_{i} tr[(x_i-\mu)(x_i-\mu)^T \Lambda]
           = \frac{N}{2} log |\Lambda| - \frac{1}{2} tr[S_u\Lambda]
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$S_u \triangleq \sum_{i=1}^{N} (x_i-\mu)(x_i-\mu)^T$,业界尊称其为分散度矩阵(&lt;code&gt;Scatter Matrix&lt;/code&gt;),以后我们聊LDA的时候会再次碰到。对$\Lambda$求偏导有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial l(\Lambda)}{\partial \Lambda} = \frac{N}{2}\Lambda^{-T} - \frac{1}{2} S_u^{T} = 0
\end{equation}&lt;/p&gt;
&lt;p&gt;得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{\Sigma} = \frac{1}{N} \sum_{i=1}^{N} (x_i-\mu)(x_i-\mu)^T
\end{equation}&lt;/p&gt;
&lt;p&gt;证毕。&lt;/p&gt;
&lt;h1 id="gaussian-discriminant-analysis"&gt;Gaussian Discriminant Analysis&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;我们在上一篇中提到了Naive Bayes方法,其实质无非是估计在每一类下特定的样本出现的概率，进而我们可以把该特定样本分配给概率值最大的那个类。而对于连续数据而言，其实质其实也是一样的，每一个MVN(我们可以看做一类或者一个Component)都可能生成一些数据，我们估计在每一个Component下生成特定样本的概率，然后把该特定样本分配给概率值最大的那个Component即可。即我们可以定义如下的条件分布:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x|y=c,\theta) = N(x|\mu_c,\Sigma_c)
\end{equation}&lt;/p&gt;
&lt;p&gt;上述模型即为高斯判别分析(Gaussian Discriminant Analysis,GDA)(&lt;code&gt;注意,该模型为生成模型，而不是判别模型&lt;/code&gt;)。如果$\Sigma_c$是对角阵，即所有的特征都是独立的时，该模型等同于Naive Bayes.&lt;/p&gt;
&lt;h2 id="qda"&gt;QDA&lt;/h2&gt;
&lt;p&gt;在上式中带入高斯密度函数的定义，则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=c|x,\theta) = \frac{\pi_c |2\pi\Sigma_c|^{-1 \over 2}exp[-{1 \over 2}(x-\mu_c)^T\Sigma_C^{-1}(x-\mu_c)]}{\sum_{c\prime}\pi_{c\prime} |2\pi\Sigma_{c\prime}|^{-1 \over 2}exp[-{1 \over 2}(x-\mu_{c\prime})^T\Sigma_{c\prime}^{-1}(x-\mu_{c\prime})]}
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中$\pi$为各个Component的先验概率分布。根据上式得到的模型则称为Quadratic Discriminant Analysis(QDA).以下给出在2类以及3类情形下可能的决策边界形状,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Decision Boundary" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/decision_boundary_zps289df588.jpeg"&gt;&lt;/p&gt;
&lt;h2 id="linear-discriminant-analysislda"&gt;Linear Discriminant Analysis(LDA)&lt;/h2&gt;
&lt;p&gt;当各个Gaussian Component的协方差矩阵相同时，此时我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(y=c|x,\theta) &amp;amp;\propto \pi_c exp[\mu_c^T\Sigma^{-1}x-{1 \over 2}x^T\Sigma^{-1}x-{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c] \\
&amp;amp;= exp[\mu_c^T\Sigma^{-1}x-{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c]exp[-{1 \over 2}x^T\Sigma^{-1}x]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中$exp[-{1 \over 2}x^T\Sigma^{-1}x]$是独立于$c$的，分子分母相除抵消到此项。&lt;/p&gt;
&lt;p&gt;令:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\gamma_c &amp;amp;= -{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c \\
\beta_c &amp;amp;= \Sigma^{-1}\mu_c
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;于是有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=c|x,\theta) = \frac{e^{\beta_c^Tx+\gamma_c}}{\sum_{c\prime}e^{\beta_{c\prime}^Tx+\gamma_{c\prime}}}=S(\eta)_c
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\eta = [\beta_1^Tx+\gamma_1,...,\beta_C^Tx+\gamma_c]$,$S$为softmax函数(类似于max函数,故得此名),定义如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Softmax Function" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/softmax_zpscdabec6d.png"&gt;&lt;/p&gt;
&lt;p&gt;若将$\eta_c$除以一个常数(temperature),当$T\to 0$时，我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Softmax Function" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/boltzman_distribution_zpsa00cdbf0.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;换句话说，当温度很低时，分布集中在概率最大的那个状态上，而当温度高时，所有的状态呈现均匀分布。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:该术语来自于统计物理学，在统计物理学中，人们更倾向于使用波尔兹曼分布（Boltzmann distribution）一词。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;关于&lt;code&gt;式16&lt;/code&gt;有一个有趣的性质，即对该式取log,我们则会得到一个关于$x$的线性方程。因此对于任意两类之间的决策边界将会是一条直线，据此该模型也被称为线性判别分析(Linear Discriminant Analysis,LDA)。而且对于二分类问题，我们可以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=c|x,\theta) = p(y=c\prime|x,\theta)
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
\beta_c^Tx+\gamma_c = \beta_{c\prime}^Tx+\gamma_{c\prime}
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
x^T(\beta_{c\prime}-\beta_c) = \gamma_{c\prime}-\gamma_c
\end{equation}&lt;/p&gt;
&lt;h2 id="two-class-lda"&gt;Two-class LDA&lt;/h2&gt;
&lt;p&gt;为了加深对以上等式的理解，对于二分类的情况，我们做如下说明:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(y=1|x,\theta) &amp;amp;= \frac{e^{\beta_1^Tx+\gamma_1}}{e^{\beta_1^Tx+\gamma_1}+e^{\beta_0^Tx+\gamma_0}} \\
&amp;amp;= \frac{1}{1+e^{(\beta_0-\beta_1)^Tx+(\gamma_0-\gamma_1)}} \\
&amp;amp;=sigm((\beta_1-\beta_0)^Tx+(\gamma_1-\gamma_0))
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$sigm(\eta)$代表&lt;a href="http://en.wikipedia.org/wiki/Sigmoid_function"&gt;Sigmoid函数&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;现有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\gamma_1-\gamma_0 &amp;amp;= -{1 \over 2}\mu_1^T\Sigma^{-1}\mu_1+{1 \over 2}\mu_0^T\Sigma^{-1}\mu_0+log(\pi_1/\pi_0)  \\
&amp;amp;=-{1 \over 2}(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1+\mu_0)+log(\pi_1/\pi_0)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此若我们另:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\omega &amp;amp;= \beta_1-\beta_0 = \Sigma^{-1}(\mu_1-\mu_0) \\
x_0 &amp;amp;= {1 \over 2}(\mu_1+\mu_0)-(\mu_1-\mu_0)\frac{log(\pi_1/\pi_0)}{(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0)}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;则有$\omega^Tx_0 = -(\gamma_1-\gamma_0)$,即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=1|x,\theta) = sigm(\omega^T(x-x_0))
\end{equation}&lt;/p&gt;
&lt;p&gt;因此最后的决策规则很简单:将$x$平移$x_0$,然后投影到$\omega$上，通过结果是正还是负决定它到底属于哪一类。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Two class LDA" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/2_class_lda_zps98b80132.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;当$\Sigma = \sigma^2I$时，$\omega$与$\mu_1-\mu_0$同向。这时我们只需要判断投影点离$\mu_1$和$\mu_0$中的那个点近。当它们的先验概率$\pi_1 = \pi_0$时，投影点位于其中点；当$\pi_1&amp;gt;\pi_0$时，则$x_0$越趋近于$\mu_0$,直线的更大部分先验地属于类1;反之亦然。&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1" title="LDA算法可能导致overfitting,具体解决方法请参阅Machine Learning:a probabilistic perspective一书4.2.*部分"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h1 id="inference-in-joint-gaussian-distributions"&gt;Inference in joint Gaussian distributions&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;给定联合概率分布$p(x_1,x_2)$,如果我们能够计算边际概率分布$p(x1)$以及条件概率分布$p(x_1|x_2)$想必是极好的而且是及有用的。以下我们仅给出结论,下式表明&lt;strong&gt;如果两变量符合联合高斯分布，则它们的边际分布以及条件分布也都是高斯分布&lt;/strong&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 3.2&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2" title="具体证明请参考ML:APP一书4.3.4.3一节"&gt;2&lt;/a&gt;&lt;/sup&gt; 假定$x=(x_1,x_2)$服从联合高斯分布,且参数如下:
\begin{equation}
\mu = \left(
        \begin{array}{ccc}
        \mu_1 \\
        \mu_2
        \end{array}
      \right)
\end{equation}
\begin{equation}
\Sigma = \left(
        \begin{array}{ccc}
        \Sigma_{11} &amp;amp; \Sigma_{12} \\
        \Sigma_{21} &amp;amp; \Sigma_{22}
        \end{array}
      \right)
\end{equation}
则我们可以得到如下边际概率分布:
\begin{equation}
p(x_1) = N(x_1|\mu_1,\Sigma_{11})  \\
p(x_2) = N(x_2|\mu_2,\Sigma_{22})
\end{equation}
另其后验条件分布为:
\begin{equation}
p(x_1|x_2) = N(x_1|\mu_{1|2},\Sigma_{1|2})
\end{equation}
\begin{equation}
\begin{split}
\mu_{1|2} &amp;amp;= \mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2) \\
&amp;amp;=\mu_1-\Lambda_{11}^{-1}\Lambda_{12}(x_2-\mu_2) \\
&amp;amp;=\Sigma_{1|2}(\Lambda_{11}\mu_1-\Lambda_{12}(x_2-\mu_2)) \\
\end{split}
\end{equation}
\begin{equation}
\Sigma_{1|2} = \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}=\Lambda_{11}^{-1}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id="linear-gaussian-systems"&gt;Linear Gaussian Systems&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;给定两变量，$x$和$y$.令$x \in R^{D_x}$为一隐含变量,$y \in R^{D_y}$为关于$x$的包含噪声的观察值。此外，我们假定存在如下prior和likelihood:
\begin{equation}
\begin{split}
p(x) &amp;amp;= N(x|\mu_x,\Sigma_x) \\
p(y|x) &amp;amp;= N(y|Ax+b,\Sigma_y)
\end{split}
\end{equation}
上式即称为&lt;em&gt;Linear Gaussian System&lt;/em&gt;。此时我们有:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 3.3&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3" title="证明请参考ML:APP一书4.4.3节"&gt;3&lt;/a&gt;&lt;/sup&gt; 给定一Linear Gaussian System.其后验分布$p(x|y)$具有如下形式:
\begin{equation}
\begin{split}
p(x|y) &amp;amp;= N(x|\mu_{x|y},\Sigma_{x|y}) \\
\Sigma_{x|y}^{-1} &amp;amp;= \Sigma_x^{-1}+A^T\Sigma_y^{-1}A  \\
\mu_{x|y} &amp;amp;= \Sigma_{x|y}[A^T\Sigma_y^{-1}(y-b)+\Sigma_x^{-1}\mu_x] 
\end{split}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="inferring-an-unknown-vector-from-noisy-measurements"&gt;Inferring an unknown vector from noisy measurements&lt;/h2&gt;
&lt;p&gt;下面我们举一个简单的例子以进一步说明Linear Gaussian System:
现有$N$个观测向量,$y_i \sim\ N(x,\Sigma_y)$,prior服从高斯分布$x \sim\ N(\mu_0,\Sigma_0)$.令$A=I,b=0$,此外，我们采用$\bar{y}$作为我们的有效估计值,其精度为$N\Sigma_y^{-1},$我们有:
\begin{equation}
\begin{split}
p(x|y_1,...,y_N) &amp;amp;= N(x|\mu_N,\Sigma_N) \\
\Sigma_N^{-1} &amp;amp;= \Sigma_{0}^{-1}+N\Sigma_{y}^{-1} \\
\mu_N &amp;amp;= \Sigma_N(\Sigma_y^{-1}(N\bar{y})+\Sigma_0^{-1}\mu_0)
\end{split}
\end{equation}
为了更直观地解释以上模型,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Radar Blips" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/radar_blips_zpsfd1a04d1.png"&gt;&lt;/p&gt;
&lt;p&gt;我们可将x视为2维空间中一个物体的真实位置(但我们并不知道),例如一枚导弹或者一架飞机,$y_i$则是我们的观测值(含噪声),可以视为雷达上的一些点。当我们得到越来越多的点时，我们就能够更好地进行定位。&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4" title="另外一种方法为Kalman Filter Algorithm"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;现假定我们有多个测量设备，且我们想利用多个设备的观测值进行估计，这种方法称为&lt;code&gt;sensor fusion&lt;/code&gt;.如果我们有具有不同方差的多组观测值，那么posterior将会是它们的加权平均。如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sensor Fusion" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sensor_fusion_zps4ad8202f.png"&gt;&lt;/p&gt;
&lt;p&gt;我们采用不带任何信息的关于$x$的先验分布，即$p(x)=N(\mu_0,\Sigma_0)=N(0,10^10I_2)$,我们得到2个观测值,$y_1 \sim\ N(x,\Sigma_{y,1}$,$y_2 \sim\ N(x,\Sigma_{y,2})$,我们需要计算$p(x|y_1,y_2)$.&lt;/p&gt;
&lt;p&gt;如上图(a),我们设定$\Sigma_{y,1} = \Sigma_{y,2} = 0.01I_2$,因此两个传感器都相当可靠，posterior mean即位于两个观测值中间；如上图(b)，我们设定$\Sigma_{y,1}=0.05I_2$且$\Sigma_{y,2}=0.01I_2$,因此传感器2比传感器1可靠，此时posterior mean更靠近于$y_2$;如上图(c),我们有:
\begin{equation}
\Sigma_{y,1} = 0.01
\left(
\begin{array}{cc}
10 &amp;amp; 1 \\
1  &amp;amp; 1
\end{array}
\right),
\Sigma_{y,2} = 0.01
\left(
\begin{array}{cc}
1 &amp;amp; 1 \\
1  &amp;amp; 10
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;从上式我们不难看出，传感器1在第2个分量上更可靠，传感器2在第1个分量上也更可靠。此时，posterior mean采用传感器1的第二分量以及传感器1的第二分量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:当sensor测量精度未知时，我们则需要它们的测量精度也进行估计。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id="the-wishart-distribution"&gt;The Wishart Distribution&lt;/h1&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;在多变量统计学中,Wishart分布是继高斯分布后最重要且最有用的模型。  ------Press&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;既然Press他老人家都说了Wishart分布很重要，而且我们下一部分会用到它，那么我们就必须得介绍介绍它了。(它主要被用来Model关于协方差矩阵的不确定性)&lt;/p&gt;
&lt;h2 id="wishart-distribution"&gt;Wishart Distribution&lt;/h2&gt;
&lt;p&gt;Wishart概率密度函数具有如下形式:
\begin{equation}
Wi(\Lambda|S,\nu)=\frac{1}{Z_{Wi}}|\Lambda|^{(\nu-D-1)/2}exp(-{1 \over 2}tr(\Lambda S^{-1}))
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\nu$为自由度，$S$为缩放矩阵。其归一项具有如下形式:(&lt;code&gt;很恐怖，对吧!&lt;/code&gt;)
\begin{equation}
Z_{Wi}=2^{\nu D/2}\Gamma_D(\nu/2)|S|^{\nu/2}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\Gamma_D(a)$为多变量Gamma函数:
\begin{equation}
\Gamma_D(x) = \pi^{D(D-1)/4}\prod_{i=1}^{D} \Gamma(x+(1-i)/2)
\end{equation}&lt;/p&gt;
&lt;p&gt;于是有$\Gamma_1(a)=\Gamma(a)$且有:
\begin{equation}
\Gamma_D(\nu_0/2)=\prod_{i=1}^{D} \Gamma(\frac{\nu_0+1-i}{2})
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;仅当$\nu&amp;gt;D-1$时归一项存在&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;其实Wishart分布和Gaussian分布是有联系的。具体而言，令$x_i \sim\ N(0,\Sigma)$,则离散度矩阵$S=\sum_{i=1}^{N}x_ix_i^T$服从Wishart分布,且$S \sim\ Wi(\Sigma,1)$。于是有$E(S)=N\Sigma$.&lt;/p&gt;
&lt;p&gt;更一般地，我们可以证明$Wi(S,\nu)$的mean和mode具有如下形式:
\begin{equation}
mean=\nu S,mode=(\nu-D-1)S
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;仅当$\nu&amp;gt;D+1$时mode存在&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;当$D=1$时，Wishart分布退化为Gamma分布，且有:
\begin{equation}
Wi(\lambda|s^{-1},v) = Ga(\lambda|{\nu \over 2},{s \over 2})
\end{equation}&lt;/p&gt;
&lt;h2 id="inverse-wishart-distribution"&gt;Inverse Wishart Distribution&lt;/h2&gt;
&lt;p&gt;若$\Sigma^{-1} \sim\ Wi(S,\nu)$,则$\Sigma \sim\ IW(S^{-1},\nu+D+1)$,其中$IW$为逆Wishart分布。当$\nu&amp;gt;D-1$且$S \succ 0$时,我们有:
\begin{equation}
\begin{split}
IW(\Sigma|S,\nu) &amp;amp;= \frac{1}{Z_{IW}}|\Sigma|^{-(\nu+D+1)/2}exp(-{1 \over 2}tr(S^{-1}\Sigma^{-1})) \\
Z_{IW} &amp;amp;= |S|^{-\nu/2}2^{\nu D/2}\Gamma_D(\nu/2)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;此外我们可以证明逆Wishart分布具有如下性质:
\begin{equation}
mean=\frac{S^{-1}}{\nu-D-1},mode=\frac{S^{-1}}{\nu+D+1}
\end{equation}&lt;/p&gt;
&lt;p&gt;当$D=1$时,它们退化为逆Gamma分布:
\begin{equation}
IW(\sigma^2|S^{-1},\nu)=IG(\sigma^2|\nu/2,S/2)
\end{equation}&lt;/p&gt;
&lt;h1 id="inferring-the-parameters-of-an-mvnrefregularized-gaussian-covariance-estimationref"&gt;Inferring the parameters of an MVN&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5" title="本部分部分参考Regularized Gaussian Covariance Estimation"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;到目前为止，我们已经讨论了在$\theta=(\mu,\Sigma)$已知的条件下如何inference，现我们讨论一下如何对参数本身进行估计。假定$x_i \sim\ N(\mu,\Sigma)$ for $i=1:N$.本节主要分为两个部分:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算$p(\mu|D,\Sigma)$;&lt;/li&gt;
&lt;li&gt;计算$p(\Sigma|D,\mu)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="posterior-distribution-of-mu"&gt;Posterior distribution of $\mu$&lt;/h2&gt;
&lt;p&gt;我们已经就如何计算$\mu$的极大似然估计值进行了讨论,现我们讨论如何计算其posterior.&lt;/p&gt;
&lt;p&gt;其likelihood具有如下形式:
\begin{equation}
p(D|\mu) = N(\bar{x}|\mu,{1 \over N}\Sigma)
\end{equation}&lt;/p&gt;
&lt;p&gt;为了简便起见，我们采用共轭先验分布，即高斯。特别地，若$p(\mu)=N(\mu|m_0,V_0)$.此时我们可以根据之前Linear Gaussian System的结论得到(和我们之前提到的雷达的例子雷同):
\begin{equation}
\begin{split}
p(\mu|D,\Sigma) &amp;amp;= N(\mu|m_N,V_N) \\
V_N^{-1} &amp;amp;= V_0^{-1}+N\Sigma^{-1} \\
m_N &amp;amp;= V_N(\Sigma^{-1}(N\bar{x})+V_0^{-1}m_0)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们可以通过设定$V_0 = \infty I$提供一个不带任何信息的先验，此时我们有$p(\mu|D,\Sigma)=N(\bar{x},{1 \over N}\Sigma)$,即和MLE得到的结果相同。&lt;/p&gt;
&lt;h2 id="posterior-distribution-of-sigma"&gt;Posterior distribution of $\Sigma$&lt;/h2&gt;
&lt;p&gt;现我们讨论如何计算$p(\Sigma|D,\mu)$,其likelihood具有如下形式:&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6" title="参见MLE for Gaussian部分"&gt;6&lt;/a&gt;&lt;/sup&gt;
\begin{equation}
p(D|\mu,\Sigma) \propto |\Sigma|^{-{N \over 2}}exp(-{1 \over 2}tr(S_{\mu}\Sigma^{-1}))
\end{equation}&lt;/p&gt;
&lt;p&gt;之前我们提到过如果采用共轭先验能够减少计算的复杂度，而此likelihood的共轭先验就是我们之前提到的非常恐怖的逆Wishart分布，即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
IW(\Sigma|S_0^{-1},\nu_0) \propto |\Sigma|^{-(\nu_0+D+1)/2}exp(-{1 \over 2}tr(S_0\Sigma^{-1}))
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中$N_0=\nu+D+1$控制着先验的强度,和likelihood中的$N$的作用基本相同。&lt;/p&gt;
&lt;p&gt;将先验和likelihood相乘我们得到如下posterior:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(\Sigma|D,\mu) &amp;amp;\propto |\Sigma|^{-{N \over 2}}exp(-{1 \over 2}tr(S_{\mu}\Sigma^{-1}))|\Sigma|^{-(\nu_0+D+1)/2}exp(-{1 \over 2}tr(S_0\Sigma^{-1})) \\
&amp;amp;=|\Sigma|^{-\frac{N+(\nu_0+D+1)}{2}}exp(-{1 \over 2}tr(\Sigma^{-1}(S_{\mu}+S_0))) \\
&amp;amp;=IW(\Sigma|S_N,\nu_N) \\
\nu_N &amp;amp;= \nu_0+N    \\
S_N^{-1} &amp;amp;= S_0+S_{\mu}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;总而言之，从上式我们可以看到，posterior $v_N$的强度为$\nu_0$加$N$;posterior离散度矩阵是先验离散度矩阵$S_0$和数据离散度矩阵$S_{\mu}$之和。&lt;/p&gt;
&lt;h3 id="map-estimation"&gt;MAP Estimation&lt;/h3&gt;
&lt;p&gt;根据我们之前得到的关于$\Sigma$的极大似然估计值,即:&lt;/p&gt;
&lt;p&gt;&lt;img alt="MLE for Covariance" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mle_covariance_zpsb7265e9d.png"&gt;&lt;/p&gt;
&lt;p&gt;从上式我们可以看出矩阵的rank为$min(N,D)$。若$N$小于$D$,该矩阵不是full rank的，因此不可逆。另尽管$N$可能大于$D$,$\hat{\Sigma}$也可能是ill-conditioned(接近奇异)。&lt;/p&gt;
&lt;p&gt;为了解决上述问题，我们可以采用posterior mean或mode.我们可以证明$\Sigma$的MAP估计值如下:(使用我们推导MLE时所用的技巧，其实并不难，亲证下式正确):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat\Sigma_{MAP} = \frac{S_N}{\nu_N+D+1} = \frac{S_0+S_{\mu}}{N_0+N_{\mu}}
\end{equation}&lt;/p&gt;
&lt;p&gt;当我们采用improper uniform prior，即$S_0=0,N_0=0$时，我们即得MLE估计值。&lt;/p&gt;
&lt;p&gt;当$D/N$较大时，选择一个包含信息的合适的prior就相当必要了。令$\mu=\bar{x}$,故有$S_{\mu}=S_{\bar{x}}$,此时MAP估计值可被重写为prior mode和MLE的convex combination.令$\Sigma_0 \triangleq  \frac{S_0}{N_0}$为prior mode,则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\hat\Sigma_{MAP} = \frac{S_0+S_{\bar{x}}}{N_0+N} &amp;amp;= \frac{N_0}{N_0+N}\frac{S_0}{N_0}+\frac{N}{N_0+N}\frac{S}{N} \\
&amp;amp;=\lambda\Sigma_0+(1-\lambda)\hat{\Sigma}_{mle}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\lambda=\frac{N_0}{N_0+N}$,控制着向prior &lt;code&gt;shrinkage&lt;/code&gt;的程度。对于$\lambda$而言，我们可以通过交叉验证设置其值。&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7-back"&gt;&lt;a class="simple-footnote" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7" title="其他方法见ML:APP一书4.6.2.1节"&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;而对于先验的协方差矩阵$S_0$,一般采用如下prior:$S_0=diag(\hat{\Sigma}_{mle})$.因此，我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="S_0" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/S0_zps47da8ac0.png"&gt;&lt;/p&gt;
&lt;p&gt;由上式我们可以看出，对角线的元素和极大似然估计值相等，而非对角线则趋近于0.因此该技巧也被称为&lt;em&gt;shrinkage estimation,or regularized estimation&lt;/em&gt;.&lt;/p&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1"&gt;LDA算法可能导致overfitting,具体解决方法请参阅Machine Learning:a probabilistic perspective一书4.2.*部分 &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2"&gt;具体证明请参考ML:APP一书4.3.4.3一节 &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3"&gt;证明请参考ML:APP一书4.4.3节 &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4"&gt;另外一种方法为Kalman Filter Algorithm &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5"&gt;本部分部分参考&lt;a href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/"&gt;Regularized Gaussian Covariance Estimation&lt;/a&gt; &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6"&gt;参见MLE for Gaussian部分 &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7"&gt;其他方法见ML:APP一书4.6.2.1节 &lt;a class="simple-footnote-back" href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="Machine Learning"></category><category term="Gaussian Models"></category><category term="Generative Models"></category></entry><entry><title>机器学习系列(II):Generative models for discrete data</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html" rel="alternate"></link><updated>2014-03-04T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-03-04:ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html</id><summary type="html">&lt;h1 id="_1"&gt;博客若干事&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id="_2"&gt;博客更新&lt;/h2&gt;
&lt;p&gt;根据目前的学习进度、自己的空闲时间以及时间的充裕度，现将博客的更新时间定于周三，更新周期为每一周或者两周更新一次。另由于目前自己对于Latex公式还不是特别熟，所以博文中的公式可能会出现部分错误，请大家谅解。此外，博客刚刚创建，很多东西都在完善当中，包括博客的插件，博文的排版等等，这些方面之后会慢慢完善，目前已开放的功能仅基本支持博文的显示以及评论。&lt;/p&gt;
&lt;p&gt;由于机器学习领域问题一般涉及公式较多，目前采取的渲染方式是通过相应的JS插件，导致的直接后果是页面的载入速度较慢，这方面以后可能将公式转换为图片然后输出。&lt;/p&gt;
&lt;p&gt;好吧，博客方面要说的就这么多吧。&lt;/p&gt;
&lt;h2 id="_3"&gt;机器学习浅谈&lt;/h2&gt;
&lt;p&gt;机器学习要研究的问题无非有四:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为什么要学习?&lt;/li&gt;
&lt;li&gt;学习什么？&lt;/li&gt;
&lt;li&gt;怎么学习？&lt;/li&gt;
&lt;li&gt;怎么更好地学习？&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;也许所有的理论，所有的事无非要解决的就是这四件事吧，为什么、做什么、怎么做、怎么做好。(作者注)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;大概所有的思想、理论、模型大致是围绕这四个方向进行的，而且这四个问题都得到了较好的解决。以上这些理论比较繁杂，而且我也没完全弄懂，所以咱们慢慢啃吧。&lt;/p&gt;
&lt;p&gt;今天我们要谈的是主要是生成模型，与之对应的则是判别模型。生成模型和判别模型的区别在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成模型首先计算联合概率分布$p(x,y)$,然后据此计算$p(y|x)$;而判别模型往往直接计算$p(y|x)$;&lt;/li&gt;
&lt;li&gt;生成模型关注数据是怎么生成的，然后进行预测；判别模型不关注数据的具体生成过程，直接预测。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文主要介绍针对离散数据的生成模型，限于篇幅，本文仅对其中其中的两个模型进行介绍------Dirichlet-Multinomial Model以及朴素贝叶斯分类器，Dirichlet-Multinomial Model被广泛使用在各种语言模型中，而朴素贝叶斯分类器最为人所知的应用大概就是垃圾邮件过滤(Spam Filtering)了吧。&lt;/p&gt;
&lt;p&gt;以下我们正式开始介绍这两个模型。&lt;/p&gt;
&lt;h1 id="dirichlet-multinomial-model"&gt;Dirichlet-multinomial Model&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;我们现在要Model的问题很简单，假设现有一个$K$面的骰子，我们需要推断它出现第$k$面的概率。&lt;/p&gt;
&lt;h2 id="likelihood"&gt;Likelihood&lt;/h2&gt;
&lt;p&gt;假设我们掷骰子$N$次，$D={x_1,...,x_N}$,其中，$x_i\in {1,...,K}$.我们假设数据是独立同分布的(iid),Likelihood则有如下形式：
\begin{equation}
p(D|\theta) = \prod_{k=1}^{K}\theta_k^{N_k}
\end{equation}
其中，$N_k = \sum_{i=1}^{N}1_{y_i=k}$是第$k$面出现的次数。(1为指示函数，下同)&lt;/p&gt;
&lt;h2 id="_4"&gt;先验分布&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Machine Learning:A probabilistic perspective&lt;/code&gt;一书告诉我们如果先验分布和Likelihood的形式相同(共轭先验分布，conjugate prior)时，能够很好的简化我们的计算过程。基于此理，我们选择Dirichlet分布作为我们的先验分布，Dirichlet分布具有如下形式:
\begin{equation}
Dir(\theta|\alpha) = \frac{1}{B(\alpha)}\prod_{k=1}^{K} \theta_k^{\alpha_k-1}1_{x \in S_k}
\end{equation}&lt;/p&gt;
&lt;h2 id="_5"&gt;后验分布&lt;/h2&gt;
&lt;p&gt;将Likelihood乘以Prior，后验分布的形式告诉我们后验分布也服从Dirichlet Distribution:
\begin{equation}
p(\theta|D)     \propto p(D|\theta)p(\theta)    \
                \propto \prod_{k=1}^{K} \theta_k^{N_k}\theta_{k}^{\alpha_k-1}=\prod_{k=1}^{K}\theta_k^{\alpha_k+N_k-1} \
                =Dir(\theta|\alpha_1+N_1,...,\alpha_K+N_K)
\end{equation}
现在我们计算关于参数$\theta$的极大后验估计(MAP),其中,$\sum_{k}\theta_k=1$.
引入Lagrange乘子之后我们需要优化的目标函数为:
\begin{equation}
l(\theta,\lambda) = \sum_{k} N_klog\theta_k+\sum_{k}(\alpha_k-1)log\theta_k+\lambda(1-\sum_{k}\theta_k)
\end{equation}
为简便起见，记$N\prime_k\triangleq N_k+\alpha_k-1$.对$\lambda$求导得：
\begin{equation}
\frac{\partial l}{\partial \lambda}=(1-\sum_{k}\theta_k) = 0
\end{equation}
对$\theta_k$求导得，
\begin{equation}
\frac{\partial l}{\partial \theta_k}=\frac{N\prime_k}{\theta_k}-\lambda=0 \
N\prime_k = \lambda\theta_k
\end{equation}
由上两式得：
\begin{equation}
\sum_{k} N\prime_k = \lambda\sum_{k}\theta_k  \
N+\alpha_0-K=\lambda
\end{equation}
其中，$\alpha_0\triangleq \sum_{k=1}^{K}\alpha_k$是先验的有效样本大小。因此我们可以得出极大后验估计值为：
\begin{equation}
\hat{\theta}_k=\frac{N_k+\alpha_k-1}{N+\alpha_0-K}
\end{equation}
如果采用uniform prior $\alpha_k=1$，这时得到的最大后验估计值即与经验值相同。
\begin{equation}
\hat{\theta_k} = \frac{N_k}{N}
\end{equation}&lt;/p&gt;
&lt;h2 id="posterior-predicative"&gt;Posterior predicative&lt;/h2&gt;
&lt;p&gt;\begin{equation}
p(X=j|D) = \int P(X=j|\theta)p(\theta|D)d\theta \
=\int P(X=j|\theta_j)[\int p(\theta_{-j},\theta_j|D)d\theta{-j}]d\theta_j \
=\int \theta_jp(\theta_j|D)d\theta_j=E[\theta_j|D] = \frac{\alpha_j+N_j}{\sum_{k}\alpha_k+N_k}=\frac{\alpha_j+N_j}{\alpha_0+N}
\end{equation}
其中，$\theta_{-j}$代表$\theta$中除$\theta_j$之外的所有分量。&lt;/p&gt;
&lt;h2 id="example"&gt;Example&lt;/h2&gt;
&lt;p&gt;上述模型的一个很重要的应用场景是语言模型，即预测一个序列中下一个可能出现的词。以下我们举一个非常简单的例子，我们假定每一个词$X_i \in {1,...,K}$都是通过$Cat(\theta)$独立取样得到的，该模型被称为bag of words model.给定一已知的样本序列，我们需要预测下一个最可能出现的是什么词?&lt;/p&gt;
&lt;p&gt;如，假设我们取样得到如下样本:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Mary had a little lamb,little lamb,little lamb,
Mary had a little lamb, its fleece as white as snow
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;另外，我们假设我们的字典中有如下词:&lt;/p&gt;
&lt;p&gt;&lt;img alt="words" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/words_zps55d01c8c.png" /&gt;&lt;/p&gt;
&lt;p&gt;这里unk代表unknown，表示未在样本中出现过的所有词。为了给上述样本中的每一行进行编码，我们先从采样样本中去掉标点符号以及&lt;code&gt;停用词&lt;/code&gt;(即没有实际意义的词，一般只是各种助词等),如，a,as,the等。此外我们还需要对所有的词进行处理仅得到其词根，如saw处理为see，running处理为run等。最后，我们对每一行进行索引编号,得到如下结果:&lt;/p&gt;
&lt;p&gt;&lt;img alt="index" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/index_zpsba0a3fb0.png?t=1393987077" /&gt;&lt;/p&gt;
&lt;p&gt;这里我们不考虑词序，仅考虑每个词在样本中出现的次数。统计得到如下结果:&lt;/p&gt;
&lt;p&gt;&lt;img alt="token_count" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/token_count_zps1876920b.png?t=1393987078" /&gt;&lt;/p&gt;
&lt;p&gt;将以上每个计数值记为$N_j$,如果对于$\theta$我们采用Dirichlet先验分布，则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(\bar{X}_j|D)=E[\theta_j|D]=\frac{\alpha_j+N_j}{\sum_t \alpha_t+N_t}=\frac{1+N_j}{10+17}
\end{equation}&lt;/p&gt;
&lt;p&gt;通过代入每一个计数值，我们便能得出每个词出现的概率。至此，我们得到了该语言模型的所有参数，进而可以进行各种预测。&lt;/p&gt;
&lt;h1 id="naive-bayes-classifier"&gt;Naive Bayes Classifier&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id="_6"&gt;引子&lt;/h2&gt;
&lt;p&gt;朴素贝叶斯分类是一种十分简单的分类算法，它的思想很naive,但是其实际应用效果还是不错的。所以一个模型的好坏并非在于其复杂度，而在于我们是否将它用到了正确的地方，就算一个非常Naive的模型，如果用在了恰当的地方，也能产生很好的效果。具体就机器学习算法而言，只有真正对一个算法的特性、适用条件、优缺点有非常深刻的理解，才能真正把机器学习算法或模型用好。朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。&lt;/p&gt;
&lt;p&gt;引用&lt;a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html"&gt;CodingLabs&lt;/a&gt;提到的一个例子来抛出我们的问题，并以此为基础介绍我们的模型:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;对于SNS社区来说，不真实账号（使用虚假身份或用户的小号）是一个普遍存在的问题，作为SNS社区的运营商，希望可以检测出这些不真实账号，从而在一些运营分析报告中避免这些账号的干扰，亦可以加强对SNS社区的了解与监管。
如果通过纯人工检测，需要耗费大量的人力，效率也十分低下，如能引入自动检测机制，必将大大提升工作效率。这个问题说白了，就是要将社区中所有账号在真实账号和不真实账号两个类别上进行分类。
首先设C=0表示真实账号，C=1表示不真实账号。
假设我们目前确定能作为评判用户帐号是否真实的特征属性有如下几个:(实际上在机器学习领域确定特征属性是一项特别重要且复杂的工作，我们这里为了简化，直接给出本问题的特征属性)
F1：日志数量/注册天数；F2：好友数量/注册天数；F3：是否使用真实头像。在SNS社区中这三项都是可以直接从数据库里得到或计算出来的。(对这些属性进行区间划分保证这些属性取离散值)
接下来的工作是我们从数据库得到了一些新的记录，给出了如上三个特征，我们需要预测这些用户是否真实的用户。
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="introduction-to-naive-bayes-classifier"&gt;Introduction to Naive Bayes Classifier&lt;/h2&gt;
&lt;p&gt;Naive Bayes Classifier要解决的问题是对于具有D个特征属性，每个属性可以取${1,...,K}$中任意一个值的样本进行分类，即$x \in {1,...,K}^D$。朴素贝叶斯分类器是一个生成模型，我们需要计算关于类别的条件概率$p(x|y=c)$.朴素贝叶斯假定给定类别c的条件下，各特征属性之间是相互独立的。于是我们有:
\begin{equation}
p(x|y=c,\theta) = \prod_{j=1}^{D} p(x_j|y=c,\theta{jc})
\end{equation}
我们得到的模型即为Naive Bayes Classifier(NBC).在上面的SNS真实用户检测的例子中，C=2,D=3。&lt;/p&gt;
&lt;p&gt;Naive Bayes Classifier的基本算法流程如下所示:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Algorithm 2.2 Naive Bayes Classifier算法框架
1.  根据得到的样本数据计算在每一可能的类别下各属性取值的条件概率，即计算p(x|y=c);
2.  根据计算得到的条件概率计算新样本属于各个类别的概率，即计算p(y|x*);
3.  比较计算得到的新样本属于不同类别的概率值，选择值最大的那个类别作为新样本的类别。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这里不给出针对具体数据的计算过程，想了解具体每一步怎么算的亲们请参考&lt;a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html"&gt;算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id="mutual-information"&gt;Mutual Information&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;NBC需要计算关于很多特征的联合概率分布，可能会导致过拟合；此外，算法的运行时间是$O(CD)$,对于有些应用来说可能计算量太大了。一个解决上述问题的普遍被采用的方案是进行特征选取，去掉和分类无关的无用属性。最简单的方法是考察每个特征与分类属性之间的相关性，并权衡复杂性以及准确度选取K个最相关的属性用于训练。该方法被称为&lt;code&gt;variable ranking,filtering,or screening&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;衡量相关性的一种方式是通过互信息，如下:
\begin{equation}
I(X,Y)=\sum_{x_j}\sum_{Y}P(x_j,y)log\frac{p(x_j,Y)}{p(x_j)p(y)}
\end{equation}
互信息可被理解为当我们观察到特征$x_j$时对于分类属性造成的熵减。对每个特征属性分别计算互信息后，选取较大的若干个用于训练即可。&lt;/p&gt;
&lt;h1 id="appendix-imutual-information"&gt;Appendix I:Mutual Information&lt;/h1&gt;
&lt;h2 id="kl-divergence"&gt;KL divergence&lt;/h2&gt;
&lt;p&gt;衡量两个概率分布$p$和$q$差异性的一种方法是KL距离(Kullback-Leibler divergence or relative entropy).定义如下:
\begin{equation}
KL(p||q)\triangleq \sum_{k=1}^{K} p_klog\frac{p_k}{q_k}
\end{equation}
上式可以改写为:
\begin{equation}
KL(p||q) \triangleq \sum_{k}p_klogp_k-\sum_{k}p_klogq_k = -H(p)+H(p,q)
\end{equation}
其中，$H(p,q)$称为联合熵，定义为:
\begin{equation}
H(p,q)\triangleq -\sum_{k}p_klogq_k
\end{equation}
其实，联合熵可被理解为用分布$q$编码来自分布$p$的数据时所需要的最小位数，$H(p)$即是用本身分布编码本身信息所需要的最小比特位数，因此KL距离的含义即是使用$q$编码来自$p$的信息相对于分布$p$本身而言多需要的位数。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 2.1 $KL(p,q) \ge 0$,且当且仅当$p=q$时等号成立；&lt;/p&gt;
&lt;p&gt;为证明上式，我们引入琴生不等式，即任意凸函数$f$,有:
\begin{equation}
f(\sum_{i=1}^{n}\lambda_ix_i) \le \sum_{i=1}^{n}\lambda_if(x_i)
\end{equation}
其中$\lambda_i\ge 0,\sum_{i=1}^{n}\lambda_i=1$&lt;/p&gt;
&lt;p&gt;Proof:
\begin{equation}
-KL(p||q)=-\sum_{x \in A}p(x)log\frac{p(x)}{q(x)}=-\sum_{x \in A}p(x)log\frac{q(x)}{p(x)} \
\le log \sum_{x \in A}p(x)log\frac{q(x)}{p(x)}=log \sum_{x \in A} q(x) \
\le log \sum_{x \in X}q(x)=log 1=0
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;另外一个重要的推论是离散分布中一致分布的熵最大，即$H(X) \le log |X|$.&lt;/p&gt;
&lt;p&gt;\begin{equation}
0 \le KL(q||u) = \sum_{x} p(x)log \frac{p(x)}{u(x)} \
= \sum_{x}p(x)logp(x)-\sum_{x}p(x)logu(x) = -H(X)+log|X|
\end{equation}
该式是Laplace不充分理由原则的公式表示，它的含义是当没有其他理由证明其他分布好于一致分布时，应当采用一致分布。&lt;/p&gt;
&lt;h2 id="mutual-information_1"&gt;Mutual Information&lt;/h2&gt;
&lt;p&gt;考察两个随机变量，$X$和$Y$。假如我们想知道一个变量包含关于另一变量的多少信息，我们可以计算相关系数，但那只针对实数随机变量而言。一个更通用的办法是衡量联合分布和分布乘积的相关性，即MI.定义如下：
\begin{equation}
I(X;Y) \triangleq KL((p(X,Y)||p(X)p(Y)) = \sum_{x}\sum_{y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}
\end{equation}
$I(X;Y) \ge 0 $成立且当且仅当$p(X,Y=P(X)P(Y)$时取等。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;\begin{equation}
I(X;Y) = H(X)-H(X|Y) = H(Y)-H(Y|X)
\end{equation}
其中，减式的后半部分称为条件熵，证明此处从略。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id="_7"&gt;参考文献&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html"&gt;算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="Classfication"></category><category term="Generative Models"></category><category term="Mutual Information"></category></entry><entry><title>机器学习系列(I):决策树算法</title><link href="http://www.qingyuanxingsi.com/Decision%20Tree.html" rel="alternate"></link><updated>2014-03-03T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-03-03:Decision Tree.html</id><summary type="html">&lt;h1 id="_1"&gt;写在前面&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;好吧，今天我的博客在线下默默地开张了，花了好长时间才把中文显示的问题解决。言归正传，之所以开通这个博客，原因有二：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对已经学过的知识进行梳理，保证学习过程的稳步前进；&lt;/li&gt;
&lt;li&gt;敦促自己每周有一定的学习目标,以更好地推进自己的学习.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关于这个博客其他的我就不说了，如果你觉得这个博客有点用，你愿意花点时间看看，我会灰常感激滴。如果你觉得这个博客没什么用，直接忽略就好。此外，这篇博客所有内容均host在Github上，本着分享，协作的精神，如果你愿意而且有时间欢迎投稿至qingyuanxingsi@163.com,I would be much glad to receive your mails.&lt;/p&gt;
&lt;h1 id="_2"&gt;简介&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id="_3"&gt;二三闲话&lt;/h2&gt;
&lt;p&gt;这是本博客的第一篇博文，也是第一篇关于机器学习方面的博文，因此我想扯些闲话。就我而言，我觉得所有的机器学习算法并不只是模型本身那么简单，背后其实还有一些别的东西，从某种角度来说，它们也是模型的创立者认识世界的方式。&lt;/p&gt;
&lt;p&gt;举贝叶斯为例，从他的模型中可能能推断出他也许认为万物皆有联系，所有的事物都不是孤立的，都是相互联系，相互影响的。一个事物的改变会引起其他事物的相应变化，世界是一个相互联系的整体。另，我经常听到人们抱怨这个世界不公平，这个世界并不是他们想要的那种模样；或者说自从多年前姚晨和凌潇肃离婚之后，好多人都不再相信爱情了(just a joke）。虽然说这是生活中再平常不过的桥段，从这两个例子中，也许我们能看到另外一些东西，我们很久很久以前都对这个世界有一些先入为主的认识(&lt;strong&gt;prior&lt;/strong&gt;),我们愿意相信这个世界是公平的，爱情是非常美好的一件事。后来，慢慢的我们发现这个世界其实有很多不公平的事，我们发现这个世界里的爱情没我们想象的那么美好，我们看到了一些真实世界实实在在存在的事情(&lt;strong&gt;data&lt;/strong&gt;),于是我们对于这个世界的认识发生了改变，我们开始相信一些原来不相信的事情，对我们之前深信不疑的事情也不再那么确信。(&lt;strong&gt;posterior&lt;/strong&gt;)(关于这个模型我们下一次说吧).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;曾经相信过爱情，后来知道，原来爱情必须转化为亲情才可能持久，但是转化为亲情的爱情，犹如化入杯水中的冰块──它还是冰块吗？                    &lt;br /&gt;
曾经相信过海枯石烂作为永恒不灭的表征，后来知道，原来海其实很容易枯，石，原来很容易烂。雨水，很可能不再来，沧海，不会再成桑田。原来，自己脚下所踩的地球，很容易被毁灭。海枯石烂的永恒，原来不存在。                   &lt;br /&gt;
...                     &lt;br /&gt;
相信与不相信之间，彷佛还有令人沉吟的深度。(龙应台《相信，不相信》）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;举上面例子的目的意在说明其实机器学习算法也许并非就是些模型，就是些数学而已，它也许能给我们提供看待世界的另一种角度，也许能带给我们一些有益的思考。关于闲话就说到这儿，以后我们有时间慢慢扯。&lt;/p&gt;
&lt;h2 id="introduction-to-decision-trees"&gt;Introduction to Decision Trees&lt;/h2&gt;
&lt;p&gt;所谓决策树，顾名思义，是一种树，一种依托于策略抉择而建立起来的树。决策树中非叶节点(非根节点)均是决策节点，决策节点的取值决定了决策树具体下一步跳到那个节点，每个决策节点的分支则分别代表了决策属性可能的取值；每一个叶节点代表了一个分类属性，即决策过程的完成。从根节点到叶节点的每一条路径代表了一个可能的决策过程。&lt;/p&gt;
&lt;p&gt;举个例子，也许大家能对决策树到底是什么有一个更为清楚直观的认识:&lt;/p&gt;
&lt;p&gt;一个非常经典的例子是一个女生找对象的过程，在女孩决定是否相亲的过程中可能产生如下对话:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;女儿：多大年纪了？
母亲：26。
女儿：长的帅不帅？
母亲：挺帅的。
女儿：收入高不？
母亲：不算很高，中等情况。
女儿：是公务员不？
母亲：是，在税务局上班呢。
女儿：那好，我去见见。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个女孩的决策过程就是典型的分类树决策。相当于通过年龄、长相、收入和是否公务员对将男人分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么这个可以用下图表示女孩的决策逻辑：&lt;/p&gt;
&lt;p&gt;&lt;img alt="girl" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/girl_zpsd5a3cfed.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;根据奥卡姆剃刀原则(&lt;code&gt;Simpler is better&lt;/code&gt;),我们尽可能想构造得到的决策书尽可能的小。因此，如何选择上图中决策属性是所有决策树算法的核心所在。我们尽量在每一步要有限选取最有分辨能力的属性作为决策属性，以保证树尽可能的小。针对决策树，我们主要介绍两种比较典型的算法ID3以及C4.5,另外CART(Classification and Regression Tree)是另外使用的比较多的算法，商用的版本则有C5.0,它主要针对C4.5算法做了很多性能上的优化。具体针对CART以及C5.0的介绍本文将不再涉及。&lt;/p&gt;
&lt;h1 id="id3"&gt;ID3&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2 id="id3_1"&gt;ID3算法基本框架&lt;/h2&gt;
&lt;p&gt;ID3算法是一个由Ross Quinlan发明的用于决策树的算法。它是一个启发式算法，具体算法框架可参见《机器学习》一书中的描述，如下所示:
                       &lt;img alt="ID3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/id3_zpsaa2fe321.jpg" /&gt;&lt;/p&gt;
&lt;h2 id="_4"&gt;分裂属性的选取&lt;/h2&gt;
&lt;p&gt;如上图算法框架所示，判断测试某个属性为最佳的分类属性是ID3的核心问题，以下介绍两个比较重要的概念：信息熵和信息增益。&lt;/p&gt;
&lt;h3 id="_5"&gt;信息熵&lt;/h3&gt;
&lt;p&gt;为了精确地定义信息增益，我们先定义信息论中广泛使用的一个度量标准，称为熵(entropy),它刻画了任意样例集的纯度，另一种理解则是用来编码信息所需的最少比特位数。
\begin{equation}
Entropy(S) = -\sum_{i=1}^{c} p_ilog(p_i)
\end{equation}                                    &lt;br /&gt;
其中，$p_i$是属性S属于类别i的概率。&lt;/p&gt;
&lt;h3 id="_6"&gt;信息增益&lt;/h3&gt;
&lt;p&gt;已经有了熵作为衡量训练样例集合纯度的标准，现在可以定义属性分类训练数据的效力的度量标准。这个标准被称为&lt;strong&gt;“信息增益（information gain）”&lt;/strong&gt;。简单的说，一个属性的信息增益就是由于使用这个属性分割样例而导致的期望熵降低(或者说，样本按照某属性划分时造成熵减少的期望,个人结合前面理解，总结为用来衡量给定的属性区分训练样例的能力)。更精确地讲，一个属性A相对样例集合S的信息增益$Gain(S,A)$被定义为：
\begin{equation}
Gain(S,A)=Entropy(S) - \sum_{v \in S_v} \frac{|S_v|}{|S|}Entropy(S_v)
\end{equation}                 &lt;br /&gt;
其中：
    $V(A)$是属性A的值域；
    $S$是样本集合；
    $S_v$是S在属性A上取值等于v的样本集合。&lt;/p&gt;
&lt;p&gt;对于上述算法框架中迭代的每一步，针对样本集合S,我们分别算出针对每个可能的属性的信息增益值，并选择值最大的那个对应的属性作为我们该步的分裂属性即可。依次迭代，便能构造我们想要的决策树。&lt;/p&gt;
&lt;h3 id="python"&gt;Python代码实现&lt;/h3&gt;
&lt;p&gt;实践出真知，磨刀霍霍，我们小小地实现一下。对于以上提到的ID3算法，基于Python我们给出了相应的源码实现，如下:(本博客中所有源码仅是算法思想的一个比较粗略的实现，很多方面还不成熟，特此说明，以后不再提及)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import numpy as np
import math
import operator

class DTree_ID3:
    def runDT(self, dataset, features):
        classList = [sample[-1] for sample in dataset]
        if classList.count(classList[0]) == len(classList):
            return classList[0]
        if len(dataset[0]) == 1:
            return self.classify(classList)

        max_index = self.Max_InfoGain(dataset) ##index
        max_fea = features[max_index]
        myTree = {max_fea:{}}
        fea_val = [sample[max_index] for sample in dataset]
        unique = set(fea_val);    
        del (features[max_index]) 
        for values in unique:          
            sub_dataset = self.splitDataSet(dataset, max_index, values)         
            myTree[max_fea][values] = self.runDT(sub_dataset,features) 
        features.insert(max_index,max_fea)  
        return myTree

    def classify(self,classList):
        classCount = {}
        for vote in classList:
            if vote not in classCount.keys():
                classCount[vote] = 0
            classCount[vote] += 1
        sortedClassCount = sorted(classCount.iteritems(), key = operator.itemgetter(1), revese = True)
        return sortedClassCount[0][0]

    def Max_InfoGain(self, data_set):
        #compute all features InfoGain, return the maximal one
        Num_Fea = len(data_set[1,:])
        #Num_Tup = len(data_set)
        max_IG = 1
        max_Fea = -1
        for i in range(Num_Fea-1):
            InfoGain = self.Info(data_set[:,[i,-1]])
            if (max_IG &amp;gt; InfoGain):
                max_IG = InfoGain
                max_Fea = i
        return max_Fea

    def Info(self, data):
        dic = {}
        for tup in data:
            if tup[0] not in dic.keys():
                dic[tup[0]] = {}
                dic[tup[0]][tup[1]] = 1
            elif tup[1] not in dic[tup[0]]:
                dic[tup[0]][tup[1]] = 1
            else:
                dic[tup[0]][tup[1]] += 1

        S_total = 0.0
        for key in dic:
            s = 0.0
            for label in dic[key]:
                s += dic[key][label]
            S_each = 0.0
            for label in dic[key]:
                prob = float(dic[key][label]/s)
                if prob !=0 :
                    S_each -= prob*math.log(prob,2)
            S_total += s/len(data[:,0])*S_each
        return S_total

    def splitDataSet(self,dataSet,featureIndex,value):
        subDataSet = []
        dataSet = dataSet.tolist()
        for sample in dataSet:
            if sample[featureIndex] == value:
                reducedSample = sample[:featureIndex]  
                reducedSample.extend(sample[featureIndex+1:])  
                subDataSet.append(reducedSample)
        return np.asarray(subDataSet)

if __name__ == "__main__":
    dataSet = np.array([["Cool","High","Yes","Yes"],["Ugly","High","No","No"],
               ["Cool","Low","No","No"],["Cool","Low","Yes","Yes"],
               ["Cool","Medium","No","Yes"],["Ugly","Medium","Yes","No"]])
    featureSet = ["Appearance","Salary","Office Guy"]
    dTree = DTree_ID3()
    tree = dTree.runDT(dataSet,featureSet)
    print(tree)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id="c45"&gt;C4.5&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;C4.5决策树在ID3决策树的基础之上稍作改进，并克服了其两大缺点:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用信息增益选择属性偏向于选择分枝比较多的属性，即取值多的属性;&lt;/li&gt;
&lt;li&gt;不能处理连续属性.                &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于这两个问题，C4.5都给出了具体的解决方案，以下做一个简要的阐述。&lt;/p&gt;
&lt;h2 id="_7"&gt;信息增益率&lt;/h2&gt;
&lt;p&gt;C4.5选取了信息增益率作为选择决策属性的依据，克服了用信息增益来选择属性时偏向选择值多的属性的不足。信息增益率定义为： 
\begin{equation}
GainRatio(S,A)=\frac{Gain(S,A)}{SplitInfo(S,A)}
\end{equation}
其中$Gain(S,A)$和ID3算法中的信息增益计算相同，而$SplitInfo(S,A)$代表了按照属性A分裂样本集合S的广度和均匀性。
\begin{equation}
SplitInfo(S,A)=-\sum_{i=1}^{c} \frac{|S_i|}{|S|}log\frac{|S_i|}{|S|}
\end{equation}
其中$S_i$表示根据属性A分割S而成的样本子集;&lt;/p&gt;
&lt;h2 id="_8"&gt;处理连续属性&lt;/h2&gt;
&lt;p&gt;对于离散值，C4.5和ID3的处理方法相同，对于某个属性的值连续时，假设这这个节点上的数据集合样本为total，C4.5算法进行如下处理：   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将样本数据该属性A上的具体数值按照升序排列，得到属性序列值：${A_1,A_2,A_3,...,A{total}}$&lt;/li&gt;
&lt;li&gt;在上一步生成的序列值中生成total-1个分割点。第i个分割点的取值为$A_i$和$A_{i+1}$的均值，每个分割点都将属性序列划分为两个子集;&lt;/li&gt;
&lt;li&gt;计算每个分割点的信息增益(Information Gain),得到total-1个信息增益。}&lt;/li&gt;
&lt;li&gt;对分裂点的信息增益进行修正：减去log2(N-1)/|D|，其中N为可能的分裂点个数，D为数据集合大小。&lt;/li&gt;
&lt;li&gt;选择修正后的信息增益值最大的分类点作为该属性的最佳分类点&lt;/li&gt;
&lt;li&gt;计算最佳分裂点的信息增益率(Gain Ratio)作为该属性的Gain Ratio&lt;/li&gt;
&lt;li&gt;选择Gain Ratio最大的属性作为分类属性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id="_9"&gt;总结&lt;/h1&gt;
&lt;p&gt;决策树方法是机器学习算法中比较重要且较易于理解的一种分类算法，本文介绍了两种决策树算法，ID3和C4.5.决策树算法的核心在于分裂属性的选取上，对此，ID3采用了信息增益作为评估指标，但是ID3也有不能处理连续属性值和易于选取取值较多的属性，C4.5对这两个问题都给出了相应的解决方案。&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category></entry></feed>