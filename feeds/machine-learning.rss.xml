<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>苹果的味道</title><link href="http://www.qingyuanxingsi.com/" rel="alternate"></link><link href="http://www.qingyuanxingsi.com/feeds/machine-learning.rss.xml" rel="self"></link><id>http://www.qingyuanxingsi.com/</id><updated>2014-04-23T00:00:00+08:00</updated><entry><title>机器学习系列(V): Generalized linear models and the exponential family</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-v-generalized-linear-models-and-the-exponential-family.html" rel="alternate"></link><updated>2014-04-23T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-23:ji-qi-xue-xi-xi-lie-v-generalized-linear-models-and-the-exponential-family.html</id><summary type="html">&lt;h1&gt;指数分布族&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;一件特别神奇的事是实际上我们学过或者用到的很多分布函数均可以写成一种一致的形式,事实上，属于这个家族的分布函数还是很多的，例如高斯分布、Bernoulli分布、二项分布、多项分布、指数分布、泊松分布、Dirichlet分布等,作为一个庞大家族的一员，这些分布函数具有一个它们引以为豪的共同的名字------&lt;strong&gt;指数分布族&lt;/strong&gt;。以下我们就介绍一下指数分布族的基础知识吧。&lt;/p&gt;
&lt;h2&gt;定义&lt;/h2&gt;
&lt;p&gt;传说中的这个神奇的家族中的分一个分布函数均可写成如下形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x) = h(x)e^{\theta^T T(x)-A(\theta)}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\theta$为参数向量,$T(x)$为"Sufficient statistics"向量,$A(\theta)$为cumulate generating function.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;上式中我们不难注意到$\theta$和$x$仅在$\theta^T T(x)$一项中&lt;code&gt;耦合&lt;/code&gt;在一起。另,指数分布族函数之积仍是指数分布族函数,只是可能不再具有良好的参数形式。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;为了得到一个归一化的分布，我们有:对于任一$\theta$,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\int p(x)dx = e^{-A(\theta)}\int h(x) e^{\theta^T T(x)} dx = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
e^{A(\theta)} = \int h(x) e^{\theta^T T(x)} dx
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;当$T(x)=x$时,$A(\theta)$是对于$h(x)$做&lt;strong&gt;拉普拉斯变换&lt;/strong&gt;之后的$log$值。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;以下我们举几个实例以使我们对其有一个更为清晰的认识:&lt;/p&gt;
&lt;h3&gt;Bernoulli Distribution&lt;/h3&gt;
&lt;p&gt;对于Bernoulli分布,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(x) &amp;amp;= \alpha^x (1-\alpha)^{1-x}   \\
     &amp;amp;= exp[log(\alpha^x (1-\alpha)^{1-x})]  \\
     &amp;amp;= exp[xlog \alpha + (1-x) log(1-\alpha)] \\
     &amp;amp;= exp[xlog \frac{\alpha}{1-\alpha}+log(1-\alpha)]  \\
     &amp;amp;= exp[x\theta - log(1+e^{\theta})]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;于是我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
T(x) &amp;amp;= x \\
\theta &amp;amp;= log \frac{\alpha}{1-\alpha} \\
A(\theta) &amp;amp;= log(1+e^{\theta})
\end{split}
\end{equation}&lt;/p&gt;
&lt;h3&gt;Univariate Gaussian&lt;/h3&gt;
&lt;p&gt;对于单变量高斯分布,我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Univariate Gaussian" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/univariate_gaussian_zps8407f3a0.png" /&gt;&lt;/p&gt;
&lt;p&gt;其中:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Univariate Gaussian Params" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/univariate_gaussian_params_zps58c5d9d3.png" /&gt;&lt;/p&gt;
&lt;h3&gt;Multivariate Gaussian&lt;/h3&gt;
&lt;p&gt;对于形如下式的多变量高斯分布:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x) = \frac{1}{(2\pi)^{D/2}|\Sigma|^{1/2}} e^{(x-\mu)^T \Sigma^{-1} (x-\mu)/2}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们有:(下式我并未真正推导)&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
h(x) &amp;amp;= (2\pi)^{-D/2} \\
T(x) &amp;amp;= \left(
\begin{array}{cc}
x \\
xx^T
\end{array}
\right) \\
\theta &amp;amp;= \left(
\begin{array}{cc}
\Sigma^{-1}\mu \\
-{1 \over 2}\Sigma^{-1}
\end{array}
\right)
\end{split}
\end{equation}&lt;/p&gt;
&lt;h2&gt;一阶导数&lt;/h2&gt;
&lt;p&gt;&lt;img alt="First Derivative" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/first_derivative_zps6152bd77.png" /&gt;&lt;/p&gt;
&lt;h2&gt;二阶导数&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Second Derivative" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/second_deriative_zps8d444606.png" /&gt;&lt;/p&gt;
&lt;p&gt;即$A(\theta)$是凸的($\succeq$表示正定positive definite)&lt;/p&gt;
&lt;h2&gt;Maximum Likelihood&lt;/h2&gt;
&lt;p&gt;根据$p(x)$的定义,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l(\theta) = \sum_{i=1}^{N} log p(x_i|\theta) = \sum_{i=1}^{N} [log h(x_i) + \theta^T T(x_i) - A(\theta)]
\end{equation}&lt;/p&gt;
&lt;p&gt;为了求得极大似然解,我们对$\theta$求偏导有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l \prime(\theta) = [\sum_{i=1}^N T(x_i)] - NA\prime(\theta) = 0
\end{equation}&lt;/p&gt;
&lt;p&gt;于是我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
A\prime(\hat\theta_{ML})=\frac{1}{N} \sum_{i=1}^N T(x_i)
\end{equation}&lt;/p&gt;
&lt;h2&gt;Conjugate Priors in Bayesian Statistics&lt;/h2&gt;
&lt;p&gt;根据Bayes Rule,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{\int p(x|\theta)p(\theta) d\theta}
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:分母项只是一归一化项,其值与$\theta$无关。于是我们有$p(\theta|x) \propto p(x|\theta)p(\theta)$.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;正如我们之前提到的那样，为了简化计算,我们最好使得先验分布$p(\theta)$与Marginal Likelihood $p(x|\theta)$具有相似的形式(此时它们成为共轭分布)。如当先验分布是Dirichlet分布时,当我们取Marginal Likelihood为多项式分布时,后验分布为Dirichlet分布。由于Dirichlet分布与多项式分布具有相似的形式,在一定程度上可以简化我们的计算。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:关于Dirichlet与Multinomial之间的关系我们会在之后的&lt;em&gt;Dirichlet Process&lt;/em&gt;一篇中详细展开,敬请期待,此处不再赘述。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;常见的共轭分布如下表所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Conjugate Prior" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/conjugate_priors_zps0d694b87.png" /&gt;&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.cs.columbia.edu/~jebara/4771/tutorials/lecture12.pdf"&gt;Exponential Family&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="Exponential Family"></category><category term="Generalized Linear Models"></category></entry><entry><title>机器学习外传之Deep Learning(I):Sparse Autoencoder</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-wai-chuan-zhi-deep-learningisparse-autoencoder.html" rel="alternate"></link><updated>2014-04-13T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-13:ji-qi-xue-xi-wai-chuan-zhi-deep-learningisparse-autoencoder.html</id><summary type="html">&lt;p&gt;其实本来没准备看Deep Learning的,之前这个高端大气上档次的内容一直都不再我的学习计划之内。基于如下几个原因吧,最后决定还是稍微看一下吧:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep Learning是Machine Learning:A Probabilistic Perspective的最后一章,反正终归是要看的。(之前觉得自己可能看不懂,但是好歹试试水吧);&lt;/li&gt;
&lt;li&gt;一个小伙伴毕业设计就在做Deep Learning;&lt;/li&gt;
&lt;li&gt;实验室老大好像对Deep Learning很感兴趣;&lt;/li&gt;
&lt;li&gt;好奇心害死人啊！！！！！！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;言归正传,作为Deep Learning系列的第一篇,我们首先还是说明一下两件事:Deep Learning是什么?Deep Learning是用来干什么的?&lt;/p&gt;
&lt;h1&gt;A Brief Introduction to Deep Learning&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;背景&lt;/h2&gt;
&lt;p&gt;机器学习（Machine Learning)是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能的学科。机器能否像人类一样能具有学习能力呢？1959年美国的塞缪尔(Samuel)设计了一个下棋程序，这个程序具有学习能力，它可以在不断的对弈中改善自己的棋艺。4年后，这个程序战胜了设计者本人。又过了3年，这个程序战胜了美国一个保持8年之久的常胜不败的冠军。这个程序向人们展示了机器学习的能力，提出了许多令人深思的社会问题与哲学问题（呵呵，人工智能正常的轨道没有很大的发展，这些什么哲学伦理啊倒发展的挺快。什么未来机器越来越像人，人越来越像机器啊。什么机器会反人类啊，ATM是开第一枪的啊等等。人类的思维无穷啊）。&lt;/p&gt;
&lt;p&gt;机器学习虽然发展了几十年，但还是存在很多没有良好解决的问题：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Problems to be solved" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/problems_to_be_solved_zps19ee52eb.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;例如图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等等。目前我们通过机器学习去解决这些问题的思路都是这样的（以视觉感知为例子）：&lt;/p&gt;
&lt;p&gt;&lt;img alt="ML Process" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/ml_process_zpsd8b105ec.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;从开始的通过传感器（例如CMOS）来获得数据。然后经过预处理、特征提取、特征选择，再到推理、预测或者识别。最后一个部分，也就是机器学习的部分，绝大部分的工作是在这方面做的，也存在很多的paper和研究。&lt;/p&gt;
&lt;p&gt;而中间的三部分，概括起来就是特征表达。&lt;strong&gt;良好的特征表达，对最终算法的准确性起了非常关键的作用&lt;/strong&gt;，而且系统主要的计算和测试工作都耗在这一大部分。但，这块实际中一般都是人工完成的。靠人工提取特征。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Feature Representation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/feature_representation_zps744a0f4f.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;截止现在，也出现了不少NB的特征（好的特征应具有不变性（大小、尺度和旋转等）和可区分性）：例如Sift的出现，是局部图像特征描述子研究领域一项里程碑式的工作。由于SIFT对尺度、旋转以及一定视角和光照变化等图像变化都具有不变性，并且SIFT具有很强的可区分性，的确让很多问题的解决变为可能。但它也不是万能的。&lt;/p&gt;
&lt;p&gt;&lt;img alt="SIFT" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sift_zpsb99f6fe8.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;然而，手工地选取特征是一件非常费力、启发式（需要专业知识）的方法，能不能选取好很大程度上靠经验和运气，而且它的调节需要大量的时间。既然手工选取特征不太好，那么能不能自动地学习一些特征呢？答案是能！Deep Learning就是用来干这个事情的，看它的一个别名&lt;code&gt;Unsupervised Feature Learning&lt;/code&gt;,就可以顾名思义了，Unsupervised的意思就是不要人参与特征的选取过程。&lt;/p&gt;
&lt;p&gt;那它是怎么学习的呢？怎么知道哪些特征好哪些不好呢？我们说机器学习是一门专门研究计算机怎样模拟或实现人类的学习行为的学科。好，那我们人的视觉系统是怎么工作的呢？为什么在茫茫人海，芸芸众生，滚滚红尘中我们都可以找到另一个她（因为，你存在我深深的脑海里，我的梦里 我的心里 我的歌声里……）。人脑那么NB，我们能不能参考人脑，模拟人脑呢？（好像和人脑扯上点关系的特征啊，算法啊，都不错，但不知道是不是人为强加的，为了使自己的作品变得神圣和高雅。）&lt;/p&gt;
&lt;p&gt;近几十年以来，认知神经科学、生物学等等学科的发展，让我们对自己这个神秘的而又神奇的大脑不再那么的陌生。也给人工智能的发展推波助澜。&lt;/p&gt;
&lt;h2&gt;人脑视觉机理&lt;/h2&gt;
&lt;p&gt;1981 年的诺贝尔医学奖，颁发给了David Hubel（出生于加拿大的美国神经生物学家)和TorstenWiesel，以及Roger Sperry。前两位的主要贡献，是发现了视觉系统的信息处理过程是分级的：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Brain Activity" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/brain_activity_zps40cf5595.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;我们看看他们做了什么。1958 年，DavidHubel 和Torsten Wiesel在JohnHopkins University，研究瞳孔区域与大脑皮层神经元的对应关系。他们在猫的后脑头骨上，开了一个3毫米的小洞，向洞里插入电极，测量神经元的活跃程度。&lt;/p&gt;
&lt;p&gt;然后，他们在小猫的眼前，展现各种形状、各种亮度的物体。并且，在展现每一件物体时，还改变物体放置的位置和角度。他们期望通过这个办法，让小猫瞳孔感受不同类型、不同强弱的刺激。&lt;/p&gt;
&lt;p&gt;之所以做这个试验，目的是去证明一个猜测。位于后脑皮层的不同视觉神经元，与瞳孔所受刺激之间，存在某种对应关系。一旦瞳孔受到某一种刺激，后脑皮层的某一部分神经元就会活跃。经历了很多天反复的枯燥的试验，同时牺牲了若干只可怜的小猫，David Hubel 和Torsten Wiesel发现了一种被称为“方向选择性细胞（Orientation Selective Cell）”的神经元细胞。当瞳孔发现了眼前的物体的边缘，而且这个边缘指向某个方向时，这种神经元细胞就会活跃。&lt;/p&gt;
&lt;p&gt;这个发现激发了人们对于神经系统的进一步思考。神经-中枢-大脑的工作过程，或许是一个不断迭代、不断抽象的过程。&lt;/p&gt;
&lt;p&gt;这里的关键词有两个，一个是&lt;code&gt;抽象&lt;/code&gt;，一个是&lt;code&gt;迭代&lt;/code&gt;。从原始信号，做低级抽象，逐渐向高级抽象迭代。人类的逻辑思维，经常使用高度抽象的概念。&lt;/p&gt;
&lt;p&gt;例如，从原始信号摄入开始（瞳孔摄入像素Pixels），接着做初步处理（大脑皮层某些细胞发现边缘和方向），然后抽象（大脑判定，眼前的物体的形状，是圆形的），然后进一步抽象（大脑进一步判定该物体是只气球）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Layers" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/layers_zps17c2e46a.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;这个生理学的发现，促成了计算机人工智能，在四十年后的突破性发展。&lt;/p&gt;
&lt;p&gt;总的来说，人的视觉系统的信息处理是分级的。从低级的V1区提取边缘特征，再到V2区的形状或者目标的部分等，再到更高层，整个目标、目标的行为等。也就是说高层的特征是低层特征的组合，从低层到高层的特征表示越来越抽象，越来越能表现语义或者意图。而抽象层面越高，存在的可能猜测就越少，就越利于分类。例如，单词集合和句子的对应是多对一的，句子和语义的对应又是多对一的，语义和意图的对应还是多对一的，这是个层级体系。&lt;/p&gt;
&lt;p&gt;敏感的人注意到关键词了：&lt;strong&gt;分层&lt;/strong&gt;。而Deep learning的Deep是不是就表示存在多少层，也就是多深呢？没错。那Deep Learning是如何借鉴这个过程的呢？毕竟是归于计算机来处理，面对的一个问题就是怎么对这个过程建模？&lt;/p&gt;
&lt;p&gt;因为我们要学习的是特征的表达，那么关于特征，或者说关于这个层级特征，我们需要了解地更深入点。所以在说Deep Learning之前，我们有必要再啰嗦下特征。&lt;/p&gt;
&lt;h2&gt;特征&lt;/h2&gt;
&lt;p&gt;特征是机器学习系统的原材料，对最终模型的影响是毋庸置疑的。如果数据被很好的表达成了特征，通常线性模型就能达到满意的精度。那对于特征，我们需要考虑什么呢？&lt;/p&gt;
&lt;h3&gt;特征表示的粒度&lt;/h3&gt;
&lt;p&gt;学习算法在一个什么粒度上的特征表示，才有能发挥作用？就一个图片来说，像素级的特征根本没有价值。例如下面的摩托车，从像素级别，根本得不到任何信息，其无法进行摩托车和非摩托车的区分。而如果特征是一个具有结构性（或者说有含义）的时候，比如是否具有车把手（handle），是否具有车轮（wheel），就很容易把摩托车和非摩托车区分，学习算法才能发挥作用。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Feature Motor" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/feature_motor_zps687d7adf.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;初级(浅层)特征表示&lt;/h3&gt;
&lt;p&gt;既然像素级的特征表示方法没有作用，那怎样的表示才有用呢？&lt;/p&gt;
&lt;p&gt;1995 年前后，Bruno Olshausen和 David Field 两位学者任职Cornell University，他们试图同时用生理学和计算机的手段，双管齐下，研究视觉问题。&lt;/p&gt;
&lt;p&gt;他们收集了很多黑白风景照片，从这些照片中，提取出400个小碎片，每个照片碎片的尺寸均为16x16像素，不妨把这400个碎片标记为$S[i]$,$i= 0,.. 399$。接下来，再从这些黑白风景照片中，随机提取另一个碎片，尺寸也是 16x16 像素，不妨把这个碎片标记为$T$。&lt;/p&gt;
&lt;p&gt;他们提出的问题是，如何从这400个碎片中，选取一组碎片$S[k]$,通过叠加的办法，合成出一个新的碎片，而这个新的碎片，应当与随机选择的目标碎片$T$尽可能相似，同时$S[k]$的数量尽可能少。用数学的语言来描述，就是：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_k (a[k] * S[k]) \rightarrow T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$a[k]$是在叠加碎片$S[k]$时的权重系数。&lt;/p&gt;
&lt;p&gt;为解决这个问题，Bruno Olshausen和 David Field发明了一个算法：稀疏编码(&lt;strong&gt;Sparse Coding&lt;/strong&gt;)。&lt;/p&gt;
&lt;p&gt;稀疏编码是一个重复迭代的过程，每次迭代分两步:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择一组$S[k]$，然后调整$a[k]$，使得$\sum_k (a[k]*S[k])$最接近$T$;&lt;/li&gt;
&lt;li&gt;固定住$a[k]$,在400个碎片中,选择其它更合适的碎片$S\prime[k]$，替代原先的$S[k]$,使得$\sum_k (a[k]*S\prime[k])$最接近$T$。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;经过几次迭代后，最佳的$S[k]$组合，被遴选出来了。令人惊奇的是，被选中的$S[k]$,基本上都是照片上不同物体的边缘线，这些线段形状相似，区别在于方向。&lt;/p&gt;
&lt;p&gt;Bruno Olshausen和 David Field的算法结果，与David Hubel 和Torsten Wiesel的生理发现，不谋而合！&lt;/p&gt;
&lt;p&gt;也就是说，复杂图形，往往由一些基本结构组成。比如下图：一个图可以通过用64种正交的edges（可以理解成正交的基本结构）来线性表示。比如样例$x$可以用1-64个edges中的三个按照0.8,0.3,0.5的权重调和而成。而其他基本edge没有贡献，因此均为0 。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sparse Coding" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sparse_coding_zps5aa4b3d3.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;另外，大牛们还发现，不仅图像存在这个规律，声音也存在。他们从未标注的声音中发现了20种基本的声音结构，其余的声音可以由这20种基本结构合成。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sound Sparse Structure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sound_sparse_coding_zpsaddf2fb2.jpg" /&gt;&lt;/p&gt;
&lt;h3&gt;结构性特征表示&lt;/h3&gt;
&lt;p&gt;小块的图形可以由基本edge构成，更结构化，更复杂的，具有概念性的图形如何表示呢？这就需要更高层次的特征表示，比如V2，V4。因此V1看像素级是像素级。V2看V1是像素级，这个是层次递进的，高层表达由底层表达的组合而成。专业点说就是基basis。V1提取出的basis是边缘，然后V2层是V1层这些basis的组合，这时候V2区得到的又是高一层的basis。即上一层的basis组合的结果，上上层又是上一层的组合basis(所以有大牛说Deep learning就是“搞基”，因为难听，所以美其名曰Deep learning或者Unsupervised Feature Learning）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="NN Structure" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/nn_structure_zps5884685c.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;直观上说，就是找到make sense的小patch再将其进行combine，就得到了上一层的feature，递归地向上learning feature。&lt;/p&gt;
&lt;p&gt;在不同object上做training所得的edge basis是非常相似的，但object parts和models就会completely different了（那咱们分辨car或者face是不是容易多了):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Object Classification" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/object_classification_zps81877999.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;从文本来说，一个doc表示什么意思？我们描述一件事情，用什么来表示比较合适？用一个一个字嘛，我看不是，字就是像素级别了，起码应该是term，换句话说每个doc都由term构成，但这样表示概念的能力就够了嘛，可能也不够，需要再上一步，达到topic级，有了topic，再到doc就合理。但每个层次的数量差距很大，比如doc表示的概念-&amp;gt;topic（千-万量级)-&amp;gt;term（10万量级)-&amp;gt;word（百万量级）。&lt;/p&gt;
&lt;p&gt;一个人在看一个doc的时候，眼睛看到的是word，由这些word在大脑里自动切词形成term，在按照概念组织的方式，先验的学习，得到topic，然后再进行高层次的learning。&lt;/p&gt;
&lt;h3&gt;需要有多少个特征？&lt;/h3&gt;
&lt;p&gt;我们知道需要层次的特征构建，由浅入深，但每一层该有多少个特征呢？&lt;/p&gt;
&lt;p&gt;任何一种方法，特征越多，给出的参考信息就越多，准确性会得到提升。但特征多意味着计算复杂，探索的空间大，可以用来训练的数据在每个特征上就会稀疏，都会带来各种问题，并不一定特征越多越好。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Feature Number" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/feature_number_zps90f6b4a2.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;好了，到了这一步，终于可以聊到Deep Learning了。上面我们聊到为什么会有Deep Learning（让机器自动学习良好的特征，而免去人工选取过程。还有参考人的分层视觉处理系统），我们得到一个结论就是Deep Learning需要多层来获得更抽象的特征表达。那么多少层才合适呢？用什么架构来建模呢？怎么进行非监督训练呢？以下我们给出Deep Learning的基本思想。&lt;/p&gt;
&lt;h2&gt;Deep Learning的基本思想&lt;/h2&gt;
&lt;p&gt;假设我们有一个系统$S$，它有$n$层$(S_1,…S_n)$，它的输入是$I$，输出是$O$，形象地表示为:$I \rightarrow S_1 \rightarrow S_2 \rightarrow ….\rightarrow S_n \rightarrow O$，如果输出$O$等于输入$I$，即输入$I$经过这个系统变化之后没有任何的信息损失(呵呵，大牛说，这是不可能的。信息论中有个“信息逐层丢失”的说法（信息处理不等式):设处理$a$信息得到$b$，再对$b$处理得到$c$，那么可以证明：$a$和$c$的互信息不会超过$a$和$b$的互信息。这表明信息处理不会增加信息，大部分处理会丢失信息。当然了，如果丢掉的是没用的信息那多好啊），保持了不变，这意味着输入$I$经过每一层$S_i$都没有任何的信息损失，即在任何一层$S_i$，它都是原有信息（即输入$I$）的另外一种表示。现在回到我们的主题Deep Learning，我们需要自动地学习特征，假设我们有一堆输入$I$(如一堆图像或者文本),假设我们设计了一个系统$S$（有$n$层），我们通过调整系统中参数，使得它的输出仍然是输入$I$，那么我们就可以自动地获取得到输入$I$的一系列层次特征，即$S_1，…, S_n$。&lt;/p&gt;
&lt;p&gt;对于深度学习来说，其思想就是对堆叠多个层，也就是说这一层的输出作为下一层的输入。通过这种方式，就可以实现对输入信息进行分级表达了。&lt;/p&gt;
&lt;p&gt;另外，前面是假设输出严格地等于输入，这个限制太严格，我们可以略微地放松这个限制，例如我们只要使得输入与输出的差别尽可能地小即可，这个放松会导致另外一类不同的Deep Learning方法。上述就是Deep Learning的基本思想。&lt;/p&gt;
&lt;h1&gt;BP Neural Network&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;上述我们主要意在建立关于Deep Learning一点直观的印象,下面我们就开始介绍Deep Learning所涉及的模型了。首先第一个要介绍的就是Sparse AutoEncoder(稀疏自编码器)。为了更好的理解Sparse AutoEncoder,我们首先介绍一下Back Propagation Algorithm,而采用Back Propagation算法的一个典型的代表就是BP神经网络(&lt;code&gt;想了解神经网络基础知识的童鞋请自行Google之&lt;/code&gt;)。以下我们便主要介绍BP神经网络。&lt;/p&gt;
&lt;h2&gt;概述&lt;/h2&gt;
&lt;p&gt;BP(Back Propagation)神经网络是1986年由Rumelhart和McCelland为首的科研小组提出，参见他们发表在Nature上的论文&lt;a href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf"&gt;Learning representations by back-propagating errors&lt;/a&gt;.值得一提的是，该文的第三作者Geoffrey E. Hinton就是在深度学习邻域率先取得突破的神犇。&lt;/p&gt;
&lt;p&gt;BP神经网络是一种&lt;strong&gt;按误差逆向传播算法训练的多层前馈网络&lt;/strong&gt;，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的 输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断 调整网络的权值和阈值，使网络的误差平方和最小。&lt;/p&gt;
&lt;p&gt;其中,&lt;strong&gt;前馈&lt;/strong&gt;是指输入从输入层到隐含层到输出层前向传播,而误差则由输出层反向经隐含层传到输入层,而在误差反向传播的过程中动态更新神经网络连接权值,以使得网络的误差平方和最小。&lt;/p&gt;
&lt;h2&gt;BP网络模型&lt;/h2&gt;
&lt;p&gt;一个典型的BP神经网络模型如下图所示。&lt;/p&gt;
&lt;p&gt;&lt;img alt="BP Neural Network" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/bp_nn_zps5c17c098.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;BP神经网络与其他神经网络模型类似，不同的是，BP神经元的传输函数为非线性函数(而在感知机中为阶跃函数，在线性神经网络中为线性函数)，最常用的是log-sigmoid函数或tan-sigmoid函数。BP神经网络(BPNN)一般为多层神经网络，上图中所示的BP神经网络的隐层的传输函数即为非线性函数，隐层可以有多层，而输出层的传输函数为线性函数，当然也可以是非线性函数，只不过线性函数的输出结果取值范围较大，而非线性函数则限制在较小范围（如logsig函数输出 取值在(0,1)区间）。上图所示的神经网络的输入输出关系如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入层与隐层的关系:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{h} = \mathit{f_{1}} (\boldsymbol{W^{(1)}x}+\boldsymbol{b^{(1)}})
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$x$为$m$维特征向量(列向量)，$\boldsymbol{W^{(1)}}$为$n×m$维权值矩阵，$\boldsymbol{b^{(1)}}$为$n$维的偏置(bias)向量(列向量)。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隐层与输出层的关系:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{y} = \mathit{f_{2}} (\boldsymbol{W^{(2)}h}+\boldsymbol{b^{(2)}})
\end{equation}&lt;/p&gt;
&lt;h2&gt;BP网络的学习方法&lt;/h2&gt;
&lt;p&gt;神经网络的关键之一是权值的确定，也即神经网络的学习，下面主要讨论一下BP神经网络的学习方法，它是一种监督学习的方法。&lt;/p&gt;
&lt;p&gt;假定我们有$q$个带label的样本(即输入)$p_1,p_2,…,p_q$，对应的label(即期望输出Target)为$T_1,T_2,…,T_q$，神经网络的实际输出为$a2_1,a2_2,…,a2_q$，隐层的输出为$a1[.]$.那么可以定义误差函数：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\boldsymbol{E(W,B)} = \frac{1}{2}\sum_{k=1}^{n}(t_{k} - a2_{k})^{2}
\end{equation}&lt;/p&gt;
&lt;p&gt;BP算法的目标是使得实际输出approximate期望输出，即使得训练误差最小化。BP算法利用梯度下降(Gradient Descent)法来求权值的变化及误差的反向传播。对于上图中的BP神经网络，我们首先计算输出层的权值的变化量，从第$i$个输入到第$k$个输出的权值改变为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\Delta w2_{ki} &amp;amp;= - \eta \frac{\partial E}{\partial w2_{ki}} \\
&amp;amp;= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial w2_{ki}} \\
&amp;amp;= \eta (t_{k}-a2_{k})f_{2}’a1_{i} \\
&amp;amp;= \eta \delta_{ki}a1_{i}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\eta$为学习速率。同理可得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\Delta b2_{ki} &amp;amp;= - \eta \frac{\partial E}{\partial b2_{ki}} \\
&amp;amp;= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial b2_{ki}} \\
&amp;amp;= \eta (t_{k}-a2_{k})f_{2}’ \\
&amp;amp;= \eta \delta_{ki}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;而隐层的权值变化为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\Delta w1_{ij} &amp;amp;= - \eta \frac{\partial E}{\partial w1_{ij}} \\
&amp;amp;= - \eta \frac{\partial E}{\partial a2_{k}} \frac{\partial a2_{k}}{\partial a1_{i}} \frac{\partial a1_{i}}{\partial w1_{ij}} \\
&amp;amp;= \eta \sum_{k=1}^{n}(t_{k}-a2_{k})f_{2}’w2_{ki}f_{1}’p_{j} \\
&amp;amp;= \eta \delta_{ij}p_{j}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\delta_{ij} = e_{i}f_{1}’, e_{i} = \sum_{k=1}^{n}\delta_{ki}w2_{ki}$。同理可得，$\Delta b1_{i} = \eta \delta_{ij}$。&lt;/p&gt;
&lt;p&gt;这里我们注意到，输出层的误差为$e_j,j=1..n$，隐层的误差为$e_i,i=1..m$，其中$e_i$可以认为是$e_j$的加权组合，由于作用函数的 存在，$e_j$的等效作用为$\delta_{ji} = e_{j}f’()$。&lt;/p&gt;
&lt;h2&gt;BP网络的设计&lt;/h2&gt;
&lt;p&gt;在进行BP网络的设计是，一般应从网络的层数、每层中的神经元个数和激活函数、初始值以及学习速率等几个方面来进行考虑，下面是一些选取的原则。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网络的层数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;理论已经证明，具有偏差和至少一个S型隐层加上一个线性输出层的网络，能够逼近任何有理函数，增加层数可以进一步降低误差，提高精度，但同时也是网络 复杂化。另外不能用仅具有非线性激活函数的单层网络来解决问题，因为能用单层网络解决的问题，用自适应线性网络也一定能解决，而且自适应线性网络的 运算速度更快，而对于只能用非线性函数解决的问题，单层精度又不够高，也只有增加层数才能达到期望的结果。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隐层神经元的个数&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;网络训练精度的提高，可以通过采用一个隐含层，而增加其神经元个数的方法来获得，这在结构实现上要比增加网络层数简单得多。一般而言，我们用精度和 训练网络的时间来恒量一个神经网络设计的好坏：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;神经元数太少时，网络不能很好的学习，训练迭代的次数也比较多，训练精度也不高。&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;神经元数太多时，网络的功能越强大，精确度也更高，训练迭代的次数也大，可能会出现过拟合(over fitting)现象。
    由此，我们得到神经网络隐层神经元个数的选取原则是：在能够解决问题的前提下，再加上一两个神经元，以加快误差下降速度即可。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;初始权值的选取
一般初始权值是取值在$(−1,1)$之间的随机数。另外威得罗等人在分析了两层网络是如何对一个函数进行训练后，提出选择初始权值量级为$\sqrt[r]{s}$的策略， 其中$r$为输入个数，$s$为第一层神经元个数。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学习速率
学习速率一般选取为0.01−0.8，大的学习速率可能导致系统的不稳定，但小的学习速率导致收敛太慢，需要较长的训练时间。对于较复杂的网络， 在误差曲面的不同位置可能需要不同的学习速率，为了减少寻找学习速率的训练次数及时间，比较合适的方法是采用变化的自适应学习速率，使网络在 不同的阶段设置不同大小的学习速率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;期望误差的选取
在设计网络的过程中，期望误差值也应当通过对比训练后确定一个合适的值，这个合适的值是相对于所需要的隐层节点数来确定的。一般情况下，可以同时对两个不同 的期望误差值的网络进行训练，最后通过综合因素来确定其中一个网络。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;BP网络的局限性&lt;/h2&gt;
&lt;p&gt;BP网络具有以下的几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;需要较长的训练时间：这主要是由于学习速率太小所造成的，可采用变化的或自适应的学习速率来加以改进。&lt;/li&gt;
&lt;li&gt;完全不能训练：这主要表现在网络的麻痹上，通常为了避免这种情况的产生，一是选取较小的初始权值，而是采用较小的学习速率。&lt;/li&gt;
&lt;li&gt;局部最小值：这里采用的梯度下降法可能收敛到局部最小值，采用多层网络或较多的神经元，有可能得到更好的结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;BP网络的改进&lt;/h2&gt;
&lt;p&gt;BP算法改进的主要目标是加快训练速度，避免陷入局部极小值等，常见的改进方法有带动量因子算法、自适应学习速率、变化的学习速率以及作用函数后缩法等。 动量因子法的基本思想是在反向传播的基础上，在每一个权值的变化上加上一项正比于前次权值变化的值，并根据反向传播法来产生新的权值变化。而自适应学习速率的方法则是针对一些特定的问题的。改变学习速率的方法的原则是，若连续几次迭代中，若目标函数对某个权倒数的符号相同，则这个权的学习速率增加，反之若符号相反则减小它的学习速率。而作用函数后缩法则是将作用函数进行平移，即加上一个常数。&lt;/p&gt;
&lt;h1&gt;Sparse AutoEncoder&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;有了以上基础知识,我们以下介绍一下Sparse AutoEncoder(稀疏自编码器)。&lt;/p&gt;
&lt;p&gt;目前为止，我们已经介绍了BP神经网络,而它采用的是有监督学习(使用带标签数据)。现在假设我们只有一个没有带类别标签的训练样本集合 ${x^{(1)}, x^{(2)}, x^{(3)}, \ldots}$，其中$x^{(i)} \in \Re^{n}$.自编码神经网络是一种无监督学习算法，它使用了反向传播算法，并让目标值等于输入值，比如 $y^{(i)} = x^{(i)}$ 。下图是一个自编码神经网络的示例。&lt;/p&gt;
&lt;p&gt;&lt;img alt="AutoEncoder NN" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/Autoencoder_zps061072cf.png" /&gt;&lt;/p&gt;
&lt;p&gt;自编码神经网络尝试学习一个$h_{W,b}(x) \approx x$的函数。换句话说，它尝试逼近一个恒等函数，从而使得输出$\hat{x}$接近于输入 $x$ 。恒等函数虽然看上去不太有学习的意义，但是当我们为自编码神经网络加入某些限制，比如限定隐藏神经元的数量，我们就可以从输入数据中发现一些有趣的结构。举例来说，假设某个自编码神经网络的输入$x$是一张$10\times10$图像（共100个像素）的像素灰度值，于是 $n=100$,其隐藏层$L_2$中有50个隐藏神经元。注意，输出也是100维的$y \in \Re^{100}$由于只有50个隐藏神经元，我们迫使自编码神经网络去学习输入数据的压缩表示，也就是说，它必须从50维的隐藏神经元激活度向量$a^{(2)} \in \Re^{50}$中重构出100维的像素灰度值输入$x$。如果网络的输入数据是完全随机的，比如每一个输入$x_i$都是一个跟其它特征完全无关的独立同分布高斯随机变量，那么这一压缩表示将会非常难学习。但是如果输入数据中隐含着一些特定的结构，比如某些输入特征是彼此相关的，那么这一算法就可以发现输入数据中的这些相关性。事实上，这一简单的自编码神经网络通常可以学习出一个跟主元分析（PCA）结果非常相似的输入数据的低维表示。&lt;/p&gt;
&lt;p&gt;我们刚才的论述是基于隐藏神经元数量较小的假设。但是即使隐藏神经元的数量较大（可能比输入像素的个数还要多），我们仍然通过给自编码神经网络施加一些其他的限制条件来发现输入数据中的结构。具体来说，如果我们给隐藏神经元加入稀疏性限制，那么自编码神经网络即使在隐藏神经元数量较多的情况下仍然可以发现输入数据中一些有趣的结构。&lt;/p&gt;
&lt;p&gt;稀疏性可以被简单地解释如下。如果当神经元的输出接近于1的时候我们认为它被激活，而输出接近于0的时候认为它被抑制，那么使得神经元大部分的时间都是被抑制的限制则被称作稀疏性限制。这里我们假设的神经元的激活函数是sigmoid函数。如果你使用tanh作为激活函数的话，当神经元输出为-1的时候，我们认为神经元是被抑制的。&lt;/p&gt;
&lt;p&gt;注意到$a^{(2)}_j$表示隐藏神经元$j$的激活度，但是这一表示方法中并未明确指出哪一个输入$x$带来了这一激活度。所以我们将使用 $a^{(2)}_j(x)$来表示在给定输入为$x$情况下，自编码神经网络隐藏神经元$j$的激活度。 进一步，让&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat\rho_j = \frac{1}{m} \sum_{i=1}^m \left[ a^{(2)}_j(x^{(i)}) \right]
\end{equation}&lt;/p&gt;
&lt;p&gt;表示隐藏神经元$j$的平均活跃度（在训练集上取平均）。我们可以近似的加入一条限制&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat\rho_j = \rho,
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\rho$是稀疏性参数，通常是一个接近于0的较小的值（比如$\rho=0.05$）。换句话说，我们想要让隐藏神经元$j$的平均活跃度接近0.05。为了满足这一条件，隐藏神经元的活跃度必须接近于0。&lt;/p&gt;
&lt;p&gt;为了实现这一限制，我们将会在我们的优化目标函数中加入一个额外的惩罚因子，而这一惩罚因子将惩罚那些$\hat\rho_j$和$\rho$有显著不同的情况从而使得隐藏神经元的平均活跃度保持在较小范围内。惩罚因子的具体形式有很多种合理的选择，我们将会选择以下这一种：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{j=1}^{s_2} \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j}.
\end{equation}&lt;/p&gt;
&lt;p&gt;这里，$s_2$是隐藏层中隐藏神经元的数量，而索引$j$依次代表隐藏层中的每一个神经元。如果你对相对熵(KL divergence）比较熟悉，这一惩罚因子实际上是基于它的。于是惩罚因子也可以被表示为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\sum_{j=1}^{s_2} {\rm KL}(\rho || \hat\rho_j),
\end{equation}&lt;/p&gt;
&lt;p&gt;其中 ${\rm KL}(\rho || \hat\rho_j)= \rho \log \frac{\rho}{\hat\rho_j} + (1-\rho) \log \frac{1-\rho}{1-\hat\rho_j}$是一个以 $\rho$为均值和一个以$\hat\rho_j$为均值的两个伯努利随机变量之间的相对熵。相对熵是一种标准的用来测量两个分布之间差异的方法。&lt;/p&gt;
&lt;p&gt;这一惩罚因子有如下性质，当$\hat\rho_j = \rho$时,$\textstyle {\rm KL}(\rho || \hat\rho_j) = 0$，并且随着$\hat\rho_j$与$\rho$ 之间的差异增大而单调递增。举例来说，在下图中，我们设定$\rho = 0.2$并且画出了相对熵值${\rm KL}(\rho || \hat\rho_j)$随着 $\hat\rho_j$变化的变化。&lt;/p&gt;
&lt;p&gt;&lt;img alt="KL Penalty" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/KL_zpsb42db2ff.png" /&gt;&lt;/p&gt;
&lt;p&gt;我们可以看出，相对熵在$\hat\rho_j=\rho$时达到它的最小值0，而当$\hat\rho_j$靠近0或者1的时候，相对熵则变得非常大（其实是趋向于$\infty$）。所以，最小化这一惩罚因子具有使得$\hat\rho_j$靠近$\rho$的效果。现在，我们的总体代价函数可以表示为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
J_{\rm sparse}(W,b) = J(W,b) + \beta \sum_{j=1}^{s_2} {\rm KL}(\rho || \hat\rho_j),
\end{equation}
其中$J(W,b)$如之前所定义，而$\beta$控制稀疏性惩罚因子的权重。$\hat\rho_j$项则也（间接地）取决于$W,b$,因为它是隐藏神经元$j$的平均激活度，而隐藏层神经元的激活度取决于$W,b$。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总而言之,通过控制隐含层神经元的数量或对它们施加稀疏性限制,我们就能得到关于原始数据的一种有效的特征表示&lt;/strong&gt;。具体而言,当我们采用我们上面提到的2D图像作为输入时,不同的隐藏单元学会了在图像的不同位置和方向进行边缘检测。显而易见，这些特征对物体识别等计算机视觉任务是十分有用的。若将其用于其他输入域（如音频），该算法也可学到对这些输入域有用的表示或特征。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8775360"&gt;Deep Learning（深度学习）学习笔记整理系列之（一)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8775488"&gt;Deep Learning（深度学习）学习笔记整理系列之（二)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.csdn.net/zouxy09/article/details/8775518"&gt;Deep Learning（深度学习）学习笔记整理系列之（三)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ibillxia.github.io/blog/2013/03/30/back-propagation-neural-networks/"&gt;反向传播(BP)神经网络&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/wentingtu/archive/2012/06/05/2536425.html"&gt;BP神经网络模型与学习算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.codeproject.com/Articles/13582/Back-propagation-Neural-Net"&gt;BP神经网络C++实现&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://deeplearning.stanford.edu/wiki/index.php/%E8%87%AA%E7%BC%96%E7%A0%81%E7%AE%97%E6%B3%95%E4%B8%8E%E7%A8%80%E7%96%8F%E6%80%A7"&gt;自稀疏算法与稀疏性&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%AF%E8%A7%86%E5%8C%96%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8%E8%AE%AD%E7%BB%83%E7%BB%93%E6%9E%9C"&gt;可视化自编码器训练结果&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Deep Learning"></category><category term="Machine Learning"></category><category term="Back Propagation"></category><category term="BP Neural Network"></category><category term="Sparse Autoencoder"></category></entry><entry><title>机器学习系列(IV):聚类大观园</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-ivju-lei-da-guan-yuan.html" rel="alternate"></link><updated>2014-04-11T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-04-11:ji-qi-xue-xi-xi-lie-ivju-lei-da-guan-yuan.html</id><summary type="html">&lt;p&gt;&lt;em&gt;Clustering&lt;/em&gt;和&lt;em&gt;Classification&lt;/em&gt;无疑是机器学习领域两个重量级的TASK,而且这两个概念作为初学者是比较容易混淆的。以下简要说明一下这两个概念之间的区别。Classification和Clustering都是要把一堆Objects分到不同的Group,但是两者还是有很明显的差异的。具体而言,Classification属于Supervised Learning,即训练样本必须是已标注样本，而聚类是Unsupervised Learning,我们要从未标注样本中进行学习，然后把Objects分到不同的Group中去。&lt;/p&gt;
&lt;p&gt;言归正传,Clustering,一言以蔽之,其核心思想就是"物以类聚,人以群分"。但是其核心前提在于&lt;strong&gt;物以何聚,人以何分&lt;/strong&gt;,即使用什么用来度量Objects之间的差异性。一种比较自然的想法就是使用Objects属性之间的差异用以度量Objects之间的差异性。&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta(x_i,x_{x\prime}） = \sum_{j=1}^D \Delta_j(x_{ij},x_{x\prime j})
\end{equation}&lt;/p&gt;
&lt;p&gt;常见的用来Capture属性之间差异性的函数则有如下几种:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高斯距离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大家最熟悉的无疑就是高斯距离了,当然，其只能应用于实数值。&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta_j (x_{ij},x_{x\prime j}) = (x_{ij}-x{i\prime j})^2
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;街区距离($l_1$距离)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;高斯距离由于采用二次形式,因此较大的差异容易被放大化,因此在很多场景下Gaussian距离对于Outliers很敏感。为了解决这个问题,人们引入了街区距离(city block distance),直观上理解就是在一个街区中我们从一点到另外一点要经过的街区数。定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta_j (x_{ij},x_{x\prime j}) = |x_{ij}-x_{i\prime j}|
\end{equation}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;海明距离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于类别类型的变量,如&lt;em&gt;{red,green,blue}&lt;/em&gt;，我们通常采用的办法则是当特征不同时,赋值为1,否则赋值为0;将所有的类别型变量相加有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Delta(x_i,x_{i\prime}) = \sum_{j=1}^D 1_{x_{ij} \neq x_{x\prime j}}
\end{equation}&lt;/p&gt;
&lt;p&gt;有了衡量Objects之间差异性的尺度,我们就能采用合适的算法将Objects分到不同的Group当中去。在正式介绍相关算法之间,我们还是简要说明一下如何衡量不同的聚类方法的好坏。&lt;/p&gt;
&lt;h1&gt;聚类方法评价&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;总体而言,聚类算法的核心目的即在于将相似的Objects分到同一类中去,并保证不同类之间的Objects之间的差异性。目前针对聚类算法广泛使用的评价指标有如下三种:&lt;/p&gt;
&lt;h2&gt;Purity&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Cluster Objects" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cluster_objects_zps871f5eb1.png" /&gt;&lt;/p&gt;
&lt;p&gt;如上图所示,每一个圆圈代表一个Cluster,每个圆圈内的物体均已被标注为&lt;em&gt;{A,B,C}&lt;/em&gt;中的一种。令$N_{ij}$表示Cluster i中label为j的物体的数目,$N_i$表示Cluster i中物体的总数。另一个cluster的纯度(purity)被定义为$p_i \triangleq max_j p_{ij}$,因此整个聚类结果的纯度为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
purity \triangleq \sum_{i} \frac{N_i}{N}p_i
\end{equation}&lt;/p&gt;
&lt;p&gt;根据上述定义,上图中聚类结果的纯度为:&lt;/p&gt;
&lt;p&gt;${6 \over 17}{5 \over 6}+{6 \over 17}{4 \over 6}+{5 \over 17}{3 \over 5} = 0.71$ &lt;/p&gt;
&lt;p&gt;不难看出,聚类的纯度越高，则表明该聚类算法越好。&lt;/p&gt;
&lt;h2&gt;Rand index&lt;/h2&gt;
&lt;p&gt;令$U={u_1,...,u_R}$和$V={v_1,...,v_C}$分别表示对于$N$个数据点的两种不同的划分方式。例如,$U$可能是我们估计的聚类结果，而$V$则是根据物体的Label得到的参考聚类结果。现我们定义如下四个值:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TP(True Positives):同时属于$U$和$V$中同一集合的物体对的数目,即不管对于$U$还是$V$而言,这些物体对均被分配到同一集合中;&lt;/li&gt;
&lt;li&gt;TN(True Negatives):对于$U$和$V$而言,被分配到不同集合的物体对的数目,即在参考聚类中,它们被分到不同的集合,在我们估计的聚类结果中,它们也被划分到不同的集合。&lt;/li&gt;
&lt;li&gt;FP(False Positives):在$V$中被分配到不同集合，而在$U$中被分配到相同集合的物体对的数目。&lt;/li&gt;
&lt;li&gt;FN(False Negatives):在$V$中被分配到相同集合，而在$U$中被分配到不同集合的物体对的数目。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而Rand index被定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
R \triangleq \frac{TP+TN}{TP+FP+FN+TN}
\end{equation}&lt;/p&gt;
&lt;p&gt;即我们估计的结果中被分配到正确的类中的物体对所占的比例。显然,$0 \leq R \leq 1$.&lt;/p&gt;
&lt;p&gt;还是举上图中的例子为例,说明其计算过程:&lt;/p&gt;
&lt;p&gt;上图中三个Cluster中点的个数分别是6,6,5.因此其中“positives”的数目为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
TP+FP = C_6^2+C_6^2+C_5^2 = 40
\end{equation}&lt;/p&gt;
&lt;p&gt;其中True Positives的数目为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
TP = C_5^2+C_4^2+C_2^2+C_3^2 = 20
\end{equation}&lt;/p&gt;
&lt;p&gt;同理我们可以得到$FN=24,TN=72$.因此我们有$R = \frac{20+72}{40+24+72} = 0.68$.&lt;/p&gt;
&lt;h2&gt;Mutual Information&lt;/h2&gt;
&lt;p&gt;另外一种衡量聚类质量的方法是计算$U$和$V$之间的互信息。关于互信息的内容请参考&lt;a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html"&gt;机器学习系列(II):Generative models for discrete data&lt;/a&gt;,此处不再赘述。&lt;/p&gt;
&lt;h1&gt;基本聚类算法&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;本部分会着重介绍三种聚类算法,包括K-means,Spectral Clustering以及Hierarchical Clustering,且容我一一道来。&lt;/p&gt;
&lt;h2&gt;K-Means &amp;amp; K-Medoids&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Cluster Points" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cluster_01_zps3c7151b4.png" /&gt;&lt;/p&gt;
&lt;p&gt;如上图所示,对于二维空间上的若干个点,我们要将它们分成若干类。我们直观上来看,上图中数据点大致可以分为3类,如果我们将每一类用不同的颜色标注，则可得到下图:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Cluster Color" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cluster_color_zps7de900bf.png" /&gt;&lt;/p&gt;
&lt;p&gt;那么计算机要如何来完成这个任务呢？当然，计算机还没有高级到能够“通过形状大致看出来”，不过，对于这样的$N$维欧氏空间中的点进行聚类，有一个非常简单的经典算法，也就是本节我们要介绍的K-Means。在介绍K-Means的具体步骤之前，让我们先来看看它对于需要进行聚类的数据的一个基本假设吧：对于每一个cluster，我们可以选出一个中心点(center)，使得该cluster中的所有的点到该中心点的距离小于到其他cluster的中心的距离。虽然实际情况中得到的数据并不能保证总是满足这样的约束，但这通常已经是我们所能达到的最好的结果，而那些误差通常是固有存在的或者问题本身的不可分性造成的。例如下图所示的两个高斯分布，从两个分布中随机地抽取一些数据点出来，混杂到一起，现在要让你将这些混杂在一起的数据点按照它们被生成的那个分布分开来：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gaussian" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/gaussian_zpsb3e5d8e5.png" /&gt;&lt;/p&gt;
&lt;p&gt;由于这两个分布本身有很大一部分重叠在一起了，例如，对于数据点2.5来说，它由两个分布产生的概率都是相等的，你所做的只能是一个猜测；稍微好一点的情况是2，通常我们会将它归类为左边的那个分布，因为概率大一些，然而此时它由右边的分布生成的概率仍然是比较大的，我们仍然有不小的几率会猜错。而整个阴影部分是我们所能达到的最小的猜错的概率，这来自于问题本身的不可分性，无法避免。因此，我们将K-Means所依赖的这个假设看作是合理的。&lt;/p&gt;
&lt;p&gt;基于这样一个假设,我们再来导出K-Means所要优化的目标函数:设我们一共有$N$个数据点需要分为$K$个cluster,K-Means要做的就是最小化:&lt;/p&gt;
&lt;p&gt;\begin{equation}
J = \sum_{n=1}^N\sum_{k=1}^K r_{nk} \|x_n-\mu_k\|^2
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$r_{nk}$在数据点$n$被归类到cluster k的时候为1,否则为0。直接寻找$r_{nk}$和$\mu_k$来最小化$J$并不容易，不过我们可以采取迭代的办法：先固定$\mu_k$,选择最优的$r_{nk}$，很容易看出，只要将数据点归类到离他最近的那个中心就能保证J最小。下一步则固定$r_{nk}$，再求最优的$\mu_k$。将$J$对$\mu_k$ 求导并令导数等于零，很容易得到$J$最小的时候$\mu_k$应该满足：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\mu_k=\frac{\sum_n r_{nk}x_n}{\sum_n r_{nk}}
\end{equation}
亦即$\mu_k$的值应当是所有cluster k中的数据点的平均值。由于每一次迭代都是取到$J$的最小值，因此$J$只会不断地减小（或者不变），而不会增加，这保证了K-Means最终会到达一个极小值。虽然K-Means并不能保证总是能得到全局最优解，但是对于这样的问题，像K-Means这种复杂度的算法，这样的结果已经是很不错的了。&lt;/p&gt;
&lt;p&gt;下面我们来总结一下K-Means算法的具体步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选定$K$个中心$\mu_k$的初值。这个过程通常是针对具体的问题有一些启发式的选取方法，或者大多数情况下采用随机选取的办法。因为前面说过K-Means并不能保证全局最优，而是否能收敛到全局最优解其实和初值的选取有很大的关系，所以有时候我们会多次选取初值跑K-Means,并取其中最好的一次结果。&lt;/li&gt;
&lt;li&gt;将每个数据点归类到离它最近的那个中心点所代表的cluster中。&lt;/li&gt;
&lt;li&gt;用公式$\mu_k = \frac{1}{N_k}\sum_{j\in\text{cluster}_k}x_j$计算出每个cluster的新的中心点。&lt;/li&gt;
&lt;li&gt;重复第二步，一直到迭代了最大的步数或者前后的$J$的值相差小于一个阈值为止。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:&lt;strong&gt;K-Means&lt;/strong&gt;并不能保证全局最优,而且该算法得到的结果的好坏直接依赖于初始值的选取,当初始值选取不当时,最后得到的结果可能并不好,所以一般采用的方法是多次随机选取初始值,并选择结果最好的一次就行。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;K-Means方法有一个很明显的局限就是它的距离衡量标准是高斯函数,所以只适用于特征是实数值的情形,而不能适用于当数据包含类型数据的情形。因此人们引入了K-Medoids算法。&lt;/p&gt;
&lt;p&gt;K-Medoids算法在K-Means算法的基础上做出了如下两个改变:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;将原来的目标函数$J$中的欧氏距离改为一个任意的dissimilarity measure函数$\mathcal{V}$：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;\begin{equation}
\tilde{J} = \sum_{n=1}^N\sum_{k=1}^K r_{nk}\mathcal{V}(x_n,\mu_k)
\end{equation}&lt;/p&gt;
&lt;p&gt;最常见的方式是构造一个dissimilarity matrix $\mathbf{D}$来代表$\mathcal{V}$，其中的元素$\mathbf{D}_{ij}$表示Object $i$和Object $j$之间的差异程度。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中心点的选取不再取同一Cluster数据的均值,&lt;strong&gt;而是从在已有的数据点里面选取的&lt;/strong&gt;,具体而言,选一个到该Cluster中其他点距离之和最小的点。这使得K-Medoids算法不容易受到那些由于误差之类的原因产生的Outlier的影响，更加robust一些。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Spectral Clustering&lt;/h2&gt;
&lt;p&gt;Spectral Clustering(谱聚类)是一种基于图论的聚类方法，它能够识别任意形状的样本空间且收敛于全局最优解，其基本思想是利用样本数据的相似矩阵进行特征分解后得到的特征向量进行聚类，可见，它与样本feature无关而只与样本个数有关。&lt;/p&gt;
&lt;h3&gt;图的划分&lt;/h3&gt;
&lt;p&gt;图划分的目的是将有权无向图划分为两个或以上子图，使得子图规模差不多而割边权重之和最小。图的划分可以看做是有约束的最优化问题，它的目的是看怎么把每个点划分到某个子图中，比较不幸的是当你选择各种目标函数后发现该优化问题往往是NP-hard的。&lt;/p&gt;
&lt;p&gt;怎么解决这个问题呢？松弛方法往往是一种利器(比如SVM中的松弛变量),对于图的划分可以认为能够将某个点的一部分划分在子图1中，另一部分划分在子图2中,而不是非此即彼,使用松弛方法的目的是将组合优化问题转化为数值优化问题，从而可以在多项式时间内解决之，最后在还原划分时可以通过阈值来还原，或者使用类似K-Means这样的方法，之后会有相关说明。&lt;/p&gt;
&lt;h3&gt;相关定义&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;用$G=(V,E)$表示无向图，其中$V$和$E$分别为其顶点集和边集；&lt;/li&gt;
&lt;li&gt;说某条边属于某个子图是指该边的两个顶点都包含在子图中；&lt;/li&gt;
&lt;li&gt;假设边$e$的两个不同端点为$i$和$j$，则该边的权重用$\omega_{i,j}$表示，对于无向无环图有$\omega_{i,j} = \omega_{j,i}$且$\omega_{i,i}=0$,为方便以下的“图”都指无向无环图；&lt;/li&gt;
&lt;li&gt;对于图的某种划分方案$Cut$的定义为：所有两端点不在同一子图中的边的权重之和，它可以被看成该划分方案的&lt;strong&gt;损失函数&lt;/strong&gt;,希望这种损失越小越好,本文以二分无向图为例，假设原无向图$G$被划分为$G_1$和$G_2$，那么有:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}
\end{equation}&lt;/p&gt;
&lt;h3&gt;Laplacian矩阵&lt;/h3&gt;
&lt;p&gt;假设无向图$G$被划分为$G_1$和$G_2$两个子图，该图的顶点数为:n=|V|，用$q$表示维指示向量，表明该划分方案，每个分量定义如下:&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
c_1 &amp;amp; i \in G_1 \\
c_2 &amp;amp; i \in G_2 
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;于是有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \sum_{i \in G_1,j \in G_2} \omega_{i,j} = \frac{\sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i-q_j)^2}{2(c_1-c_2)^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;又因为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i-q_j)^2 &amp;amp;= \sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i^2-2q_iq_j+q_j^2) \\
&amp;amp;= \sum_{i=1}^n \sum_{j=1}^n -2\omega_{i,j}q_iq_j + \sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i^2+q_j^2) \\
&amp;amp;= \sum_{i=1}^n \sum_{j=1}^n -2\omega_{i,j}q_iq_j + \sum_{i=1}^n 2q_i^2(\sum_{j=1}^n \omega_{i,j}) \\
&amp;amp;=2q^T(D-W)q
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$D$为对角矩阵，对角线元素为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
D_{i,i} = \sum_{j=1}^n \omega_{i,j}
\end{equation}&lt;/p&gt;
&lt;p&gt;$W$为权重矩阵：$W_{i,j} = \omega_{i,j}$且$W_{i,i}=0$。&lt;/p&gt;
&lt;p&gt;重新定义一个对称矩阵$L$，它便是Laplacian矩阵：&lt;/p&gt;
&lt;p&gt;\begin{equation}
L=D-W
\end{equation}&lt;/p&gt;
&lt;p&gt;矩阵元素为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
L_{i,j} = 
\left \lbrace
\begin{array}{cc}
\sum_{j=1}^n \omega_{i,j} &amp;amp; i = j \\
-\omega_{i,j} &amp;amp; i \neq j
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;进一步观察：&lt;/p&gt;
&lt;p&gt;\begin{equation}
q^TLq = {1 \over 2} \sum_{i=1}^n \sum_{j=1}^n \omega_{i,j}(q_i-q_j)^2
\end{equation}&lt;/p&gt;
&lt;p&gt;如果所有权重值都为非负，那么就有，这说明Laplacian矩阵是半正定矩阵；而当无向图为连通图时有特征值0且对应特征向量为$[1,1,1...1]^T$，这反映了，如果将无向图划分成两个子图，一个为其本身，另一个为空时，为0(当然，这种划分是没有意义的)。&lt;/p&gt;
&lt;p&gt;其实上面推导的目的在于得到下面的关系：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \frac{q^TLq}{(c_1-c_2)^2}
\end{equation}&lt;/p&gt;
&lt;p&gt;这个等式的核心价值在于：将最小化划分的问题转变为最小化二次函数；从另一个角度看，实际上是把原来求离散值松弛为求连续实数值。&lt;/p&gt;
&lt;p&gt;观察下图：&lt;/p&gt;
&lt;p&gt;&lt;img alt="Graph" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/graph_zps39bd50fe.png" /&gt;&lt;/p&gt;
&lt;p&gt;根据上图我们可以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
W = 
\left[
\begin{array}{cc}
0.0  &amp;amp; 0.8 &amp;amp; 0.6 &amp;amp; 0.0 &amp;amp; 0.1 &amp;amp; 0.0 \\
0.8  &amp;amp; 0.0 &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.6  &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.2 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.2 &amp;amp; 0.0 &amp;amp; 0.8 &amp;amp; 0.7 \\
0.1  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.8 &amp;amp; 0.0 &amp;amp; 0.8 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.7 &amp;amp; 0.8 &amp;amp; 0.0
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
D = 
\left[
\begin{array}{cc}
1.5  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 1.6 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 1.6 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.7 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.7 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 1.5
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;Laplacian矩阵为：
\begin{equation}
L = 
\left[
\begin{array}{cc}
1.5  &amp;amp; -0.8 &amp;amp; -0.6 &amp;amp; 0.0 &amp;amp; -0.1 &amp;amp; 0.0 \\
-0.8  &amp;amp; 1.6 &amp;amp; -0.8 &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
-0.6  &amp;amp; -0.8 &amp;amp; 1.6 &amp;amp; -0.2 &amp;amp; 0.0 &amp;amp; 0.0 \\
0.0  &amp;amp; 0.0 &amp;amp; -0.2 &amp;amp; 1.7 &amp;amp; -0.8 &amp;amp; -0.7 \\
-0.1  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; -0.8 &amp;amp; 1.7 &amp;amp; -0.8 \\
0.0  &amp;amp; 0.0 &amp;amp; 0.0 &amp;amp; -0.7 &amp;amp; -0.8 &amp;amp; 1.5
\end{array}
\right]
\end{equation}&lt;/p&gt;
&lt;p&gt;Laplacian矩阵是一种有效表示图的方式，任何一个Laplacian矩阵都对应一个权重非负地无向有权图,而满足以下条件的就是Laplacian矩阵:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为对称半正定矩阵，保证所有特征值都大于等于0;&lt;/li&gt;
&lt;li&gt;矩阵有唯一的0特征值，其对应的特征向量为$[1,1,1...1]^T$，它反映了图的一种划分方式：一个子图包含原图所有端点，另一个子图为空。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;划分方法&lt;/h3&gt;
&lt;h4&gt;Minimum Cut方法&lt;/h4&gt;
&lt;p&gt;考虑最简单情况，另$C_1=1=-C_2$,无向图划分指示向量定义为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
1 &amp;amp; i \in G_1 \\
-1 &amp;amp; i \in G_2 
\end{array}
\right.
\end{equation}                          &lt;/p&gt;
&lt;p&gt;要优化的目标为$Cut(G_1,G_2)$，由之前的推导可以将该问题松弛为以下问题：&lt;/p&gt;
&lt;p&gt;\begin{equation}
min \ q^TLq \\
s.t. [1,1,...1]^Tq=0 \\
q^Tq = n
\end{equation}&lt;/p&gt;
&lt;p&gt;从这个问题的形式可以联想到&lt;strong&gt;Rayleigh quotient&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;\begin{equation}
R(L,q) = \frac{q^TLq}{q^Tq}
\end{equation}&lt;/p&gt;
&lt;p&gt;原问题的最优解就是该Rayleigh quotient的最优解，而由Rayleigh quotient的性质可知：它的最小值，第二小值，......，最大值分别对应矩阵$L$的最小特征值，第二小特征值，......，最大特征值，且极值$q$在相应的特征向量处取得，即需要求解下特征系统的特征值和特征向量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Lq = \lambda q
\end{equation}&lt;/p&gt;
&lt;p&gt;这里需要注意约束条件$[1,1,...1]^Tq=0$，显然的最小特征值为0,此时对应特征向量为[1,1,...1]^T,不满足这个约束条件(剔除了无意义划分)，于是最优解应该在第二小特征值对应的特征向量处取得。&lt;/p&gt;
&lt;p&gt;当然，求得特征向量后还要考虑如何恢复划分，比如可以这样：特征向量分量值为正所对应的端点划分为$G_1$，反之划分为$G_2$；也可以这样：将特征向量分量值从小到大排列，以中位数为界划分$G_1$和$G_2$；还可以用K-Means算法聚类。&lt;/p&gt;
&lt;h4&gt;Ratio Cut方法&lt;/h4&gt;
&lt;p&gt;实际当中，划分图时除了要考虑最小化$Cut$外还要考虑划分的平衡性，为缓解出现类似一个子图包含非常多端点而另一个只包含很少端点的情况，出现了Ratio Cut，它衡量子图大小的标准是子图包含的端点个数。&lt;/p&gt;
&lt;p&gt;定义$n_1$为子图1包含端点数，$n_2$为子图2包含端点数，$n_2=n-n_1$，则优化目标函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
obj = Cut(G_1,G_2)(\frac{1}{n_1}+\frac{1}{n_2})
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
Cut(G_1,G_2) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}
\end{equation}&lt;/p&gt;
&lt;p&gt;做一个简单变换：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
obj &amp;amp;= Cut(G_1,G_2)(\frac{1}{n_1}+\frac{1}{n_2}) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}(\frac{1}{n_1}+\frac{1}{n_2}) \\
&amp;amp;= \sum_{i \in G_1,j \in G_2} \omega_{i,j}(\frac{n_1+n_2}{n_1n_2})=\sum_{i \in G_1,j \in G_2} \omega_{i,j}(\frac{(n_1+n_2)^2}{n_1n_2n}) \\
&amp;amp;= \sum_{i \in G_1,j \in G_2} \omega_{i,j}(\sqrt{\frac{n_1}{n_2n}}+\sqrt{\frac{n_2}{n_1n}})^2 \\
&amp;amp;= \sum_{i \in G_1,j \in G_2} \omega_{i,j}(q_i-q_j)^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
\sqrt{\frac{n_1}{n_2n}} &amp;amp; i \in G_1 \\
\sqrt{\frac{n_2}{n_1n}} &amp;amp; i \in G_2
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;看吧，这形式多给力，原问题就松弛为下面这个问题了：&lt;/p&gt;
&lt;p&gt;\begin{equation}
min \ q^TLq \\
s.t. q^Te=0 \\
q^Tq = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;依然用&lt;strong&gt;Rayleigh quotient&lt;/strong&gt;求解其第二小特征值及其特征向量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:关于原问题为什么能松弛为上述形式,没看懂,望高人指点(Normalized Cut这部分也没完全懂)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;Normalized Cut方法&lt;/h4&gt;
&lt;p&gt;与Ratio Cut类似，不同之处是，它衡量子图大小的标准是:子图各个端点的Degree之和。
定义:$d_1$为子图1Degree之和:&lt;/p&gt;
&lt;p&gt;\begin{equation}
d_1 = \sum_{i \in G_1} d_i
\end{equation}&lt;/p&gt;
&lt;p&gt;$d_2$为子图2Degree之和:&lt;/p&gt;
&lt;p&gt;\begin{equation}
d_2 = \sum_{i \in G_2} d_i
\end{equation}&lt;/p&gt;
&lt;p&gt;则优化目标函数为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
obj = Cut(G_1,G_2)(\frac{1}{d_1}+\frac{1}{d_2}) = \sum_{i \in G_1,j \in G_2} \omega_{i,j}(q_i-q_j)^2
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中：&lt;/p&gt;
&lt;p&gt;\begin{equation}
q_i = 
\left \lbrace
\begin{array}{cc}
\sqrt{\frac{d_1}{d_2d}} &amp;amp; i \in G_1 \\
\sqrt{\frac{d_2}{d_1d}} &amp;amp; i \in G_2
\end{array}
\right.
\end{equation}&lt;/p&gt;
&lt;p&gt;原问题就松弛为下面这个问题了：&lt;/p&gt;
&lt;p&gt;\begin{equation}
min \ q^TLq \\
s.t. q^De=0 \\
q^Dq = 1
\end{equation}&lt;/p&gt;
&lt;p&gt;用泛化的Rayleigh quotient表示为：&lt;/p&gt;
&lt;p&gt;\begin{equation}
R(L,q) = \frac{q^TLq}{q^TDq}
\end{equation}&lt;/p&gt;
&lt;p&gt;那问题就变成求解下特征系统的特征值和特征向量：&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
Lq &amp;amp;= \lambda Dq \\
&amp;amp;\Leftrightarrow Lq = \lambda D^{1 \over 2}D^{1 \over 2}q \\
&amp;amp;\Leftrightarrow D^{-\frac{1}{2}}LD^{-\frac{1}{2}}D^{1 \over 2}q = \lambda D^{1 \over 2}q \\
&amp;amp;\Leftrightarrow L\prime q\prime = \lambda q\prime
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$L\prime = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$,$q\prime=D^{1 \over 2}q$
                                                                                                显然，上式中最上面式子和最下面式子有相同的特征值，但是对应特征值的特征向量关系为:$q\prime=D^{1 \over 2}q$，因此我们可以先求最下面式子的特征值及其特征向量，然后为每个特征向量乘以$D^{-\frac{1}{2}}$就得到最上面式子的特征向量。&lt;/p&gt;
&lt;p&gt;哦,对了,矩阵$L\prime = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$叫做Normalized Laplacian，因为它对角线元素值都为1。&lt;/p&gt;
&lt;h3&gt;Spectral Clustering&lt;/h3&gt;
&lt;p&gt;上边说了这么多，其实就是想将图的划分应用于聚类，而且这种聚类只需要样本的相似矩阵即可，把每个样本看成图中的一个顶点，样本之间的相似度看成由这两点组成的边的权重值，那么相似矩阵就是一幅有权无向图。对照图的划分方法，有下列两类Spectral Clustering算法，他们的区别在于Laplacian矩阵是否是规范化的:&lt;/p&gt;
&lt;p&gt;&lt;img alt="SC_Classify" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sc_classify_zpse5f47df7.png" /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unnormalized Spectral Clustering算法&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;算法输入：样本相似矩阵$S$和要聚类的类别数$K$。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据矩阵$S$建立权重矩阵$W$、三角矩阵$D$；&lt;/li&gt;
&lt;li&gt;建立Laplacian矩阵$L$；&lt;/li&gt;
&lt;li&gt;求矩阵$L$的前$K$小个特征值及其对应的特征向量，注意最小的那个特征值一定是0且对应的特征向量为[1,1,...1]^T；&lt;/li&gt;
&lt;li&gt;以这K组特征向量组成新的矩阵，其行数为样本数，列数为$K$，这里就是做了降维操作，从$N$维降到$K$维，(实际上除去那个全为1的向量维度降为了$K-1$)；&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用K-Means算法进行聚类，得到$K$个Cluster。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Normalized Spectral Clustering算法&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;算法输入：样本相似矩阵$S$和要聚类的类别数$K$。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;根据矩阵$S$建立权重矩阵$W$、三角矩阵$D$；&lt;/li&gt;
&lt;li&gt;建立Laplacian矩阵$L$以及$L\prime = D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$；&lt;/li&gt;
&lt;li&gt;求矩阵$L\prime$的前$K$小个特征值及其对应的特征向量，注意最小的那个特征值一定是0且对应的特征向量为$[1,1...1]^T$；&lt;/li&gt;
&lt;li&gt;利用$q\prime=D^{1 \over 2}q$求得矩阵$L$前$K$小个特征向量；&lt;/li&gt;
&lt;li&gt;以这$K$组特征向量组成新的矩阵，其行数为样本数$N$，列数为$K$；&lt;/li&gt;
&lt;li&gt;使用K-Means算法进行聚类，得到$K$个Cluster。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;源码实现&lt;/h3&gt;
&lt;p&gt;以下我们给出Unnormalized Spectral Clustering的Python源码实现:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;

&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;matplotlib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pyplot&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;

&lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;eig&lt;/span&gt;
&lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cluster&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vq&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;kmeans2&lt;/span&gt;
&lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sparse&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;linalg&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;eigen&lt;/span&gt;
&lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;scipy&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;spatial&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;kdtree&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KDTree&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;get_noise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpoints&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="n"&gt;gaussian&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt; &lt;span class="n"&gt;noise&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpoints&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stddev&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpoints&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column_stack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;get_circle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;numpoints&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;150&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="n"&gt;polar&lt;/span&gt; &lt;span class="n"&gt;coordinates&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;get&lt;/span&gt; &lt;span class="n"&gt;uniformly&lt;/span&gt; &lt;span class="n"&gt;distributed&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;
    &lt;span class="n"&gt;step&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;numpoints&lt;/span&gt;
    &lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cos&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;center&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;column_stack&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;radial_kernel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;inner&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;conj&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;inner&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;circle_samples&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;Generate noisy circle points&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;circles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;radius&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;2.8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;circles&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;get_circle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;radius&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;get_noise&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;vstack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;circles&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;mutual_knn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;radial_kernel&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;knn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;kt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KDTree&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;cannot&lt;/span&gt; &lt;span class="n"&gt;use&lt;/span&gt; &lt;span class="n"&gt;euclidean&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt; &lt;span class="n"&gt;directly&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;neighbour&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;neighbour&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;setdefault&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[]).&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;neighbour&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;neighbour&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;knn&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;get_distance_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nearest_neighbours&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;knn&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;neighbour&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;nearest_neighbours&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;neighbour&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;distance&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;rename_clusters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;so&lt;/span&gt; &lt;span class="n"&gt;that&lt;/span&gt; &lt;span class="n"&gt;first&lt;/span&gt; &lt;span class="n"&gt;cluster&lt;/span&gt; &lt;span class="n"&gt;has&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="n"&gt;seen&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="n"&gt;newidx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;id&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;seen&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;num&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;seen&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;
        &lt;span class="n"&gt;newidx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seen&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;id&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;newidx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;cluster_points&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;sparse&lt;/span&gt; &lt;span class="n"&gt;eigen&lt;/span&gt; &lt;span class="n"&gt;is&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;little&lt;/span&gt; &lt;span class="n"&gt;bit&lt;/span&gt; &lt;span class="n"&gt;faster&lt;/span&gt; &lt;span class="n"&gt;than&lt;/span&gt; &lt;span class="n"&gt;eig&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evcts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eigen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;which&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SM&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evcts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;eig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evcts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evcts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;real&lt;/span&gt;
    &lt;span class="n"&gt;edict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evcts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
    &lt;span class="n"&gt;evals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;edict&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;second&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;third&lt;/span&gt; &lt;span class="n"&gt;smallest&lt;/span&gt; &lt;span class="n"&gt;eigenvalue&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;vector&lt;/span&gt;
    &lt;span class="n"&gt;Y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;edict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]]).&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kmeans2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;minit&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="err"&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;15&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rename_clusters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_xticklabels&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_yticklabels&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;get_colormap&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;map&lt;/span&gt; &lt;span class="n"&gt;cluster&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="n"&gt;to&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;orange&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blue&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;green&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;matplotlib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;colors&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;
    &lt;span class="n"&gt;orange&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.918&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.545&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;blue&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.169&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.651&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.914&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;green&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.58&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.365&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;ListedColormap&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;orange&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;blue&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;green&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;plot_circles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;facecolors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;x1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;x2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;plot_eigenvalues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.58&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.365&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;linewidth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Number&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Eigenvalue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axhline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ls&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;plot_eigenvectors&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;matplotlib&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ticker&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MaxNLocator&lt;/span&gt;
    &lt;span class="n"&gt;from&lt;/span&gt; &lt;span class="n"&gt;mpl_toolkits&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axes_grid&lt;/span&gt; &lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;make_axes_locatable&lt;/span&gt;
    &lt;span class="n"&gt;divider&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_axes_locatable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;divider&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;new_vertical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;100%&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pad&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;fig1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;fig1&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_axes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Eigenvectors&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;facecolors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axhline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ls&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yaxis&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_major_locator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MaxNLocator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;yaxis&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_major_locator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;MaxNLocator&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axhline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ls&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;k&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;facecolors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;index&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;2nd Smallest&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;3nd Smallest&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tl&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ax2&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_xticklabels&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;tl&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_visible&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;plot_spec_clustering&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Spectral Clustering&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;facecolors&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Second Smallest Eigenvector&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Third Smallest Eigenvector&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fontsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;change_tick_fontsize&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;plot_figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;colormap&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_colormap&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;fig&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;5.5&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;subplots_adjust&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;wspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hspace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plot_circles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plot_eigenvalues&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plot_eigenvectors&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;fig&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_subplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plot_spec_clustering&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;colormap&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;circle_samples&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;knn_points&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mutual_knn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;get_distance_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;knn_points&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diag&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Wi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Wi&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="err"&gt;#&lt;/span&gt; &lt;span class="n"&gt;unnormalized&lt;/span&gt; &lt;span class="n"&gt;graph&lt;/span&gt; &lt;span class="n"&gt;Laplacian&lt;/span&gt;
    &lt;span class="n"&gt;L&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;G&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;
    &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cluster_points&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;L&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;plot_figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;points&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;evals&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;运行源码得到下图:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Spectral Clustering" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/SpectralClustering_zpsb5f59a0e.png" /&gt;&lt;/p&gt;
&lt;h3&gt;总结&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;图划分问题中的关键点在于选择合适的指示向量并将其进行松弛化处理，从而将最小化划分的问题转变为最小化二次函数，进而转化为求Rayleigh quotient极值的问题;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spectral Clustering的各个阶段为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择合适的相似性函数计算相似度矩阵；&lt;/li&gt;
&lt;li&gt;计算矩阵L的特征值及其特征向量，比如可以用&lt;code&gt;Lanczos&lt;/code&gt;迭代算法；&lt;/li&gt;
&lt;li&gt;如何选择K，可以采用启发式方法，比如，发现第1到m的特征值都挺小的，到了m+1突然变成较大的数，那么就可以选择K=m；&lt;/li&gt;
&lt;li&gt;用K-Means算法聚类，当然它不是唯一选择；&lt;/li&gt;
&lt;li&gt;Normalized Spectral Clustering在让Cluster间相似度最小而Cluster内部相似度最大方面表现要更好，所以首选这类方法。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Hierarchical Clustering&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Hierarchical_Clustering" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/hierarchical_clustering_zps8134ae3b.png" /&gt;&lt;/p&gt;
&lt;p&gt;Hierarchical Clustering正如它字面上的意思那样，是层次化的聚类，得出来的结构是一棵树，如上图所示。在前面我们介绍过不少聚类方法，但是都是“平坦”型的聚类，然而他们还有一个更大的共同点，或者说是弱点，就是难以确定类别数。我们这里要说的Hierarchical Clustering则从某种意义上来说也算是解决了这个问题，因为在做Clustering的时候并不需要知道类别数，而得到的结果是一棵树，事后可以在任意的地方横切一刀，得到指定数目的cluster,按需取即可。&lt;/p&gt;
&lt;p&gt;听上去很诱人，不过其实Hierarchical Clustering的想法很简单，主要分为两大类：Agglomerative(自底向上)和Divisive(自顶向下)。首先说前者，自底向上，一开始，每个数据点各自为一个类别，然后每一次迭代选取距离最近的两个类别，把他们合并，直到最后只剩下一个类别为止，至此一棵树构造完成。&lt;/p&gt;
&lt;p&gt;看起来很简单吧？其实确实也是比较简单的，不过还是有两个问题需要先说清除才行：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;如何计算两个点的距离？这个通常是problem dependent的，一般情况下可以直接用一些比较通用的距离就可以了，比如欧氏距离等。&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如何计算两个类别之间的距离？一开始所有的类别都是一个点，计算距离只是计算两个点之间的距离，但是经过后续合并之后，一个类别里就不止一个点了，那距离又要怎样算呢？到这里又有三个变种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Single Linkage：又叫做nearest-neighbor,就是取两个集合中距离最近的两个点的距离作为这两个集合的距离，容易造成一种叫做Chaining的效果，两个cluster明明从“大局”上离得比较远，但是由于其中个别的点距离比较近就被合并了,并且这样合并之后Chaining效应会进一步扩大,最后会得到比较松散的cluster 。&lt;/li&gt;
&lt;li&gt;Complete Linkage：这个则完全是Single Linkage的反面极端，取两个集合中距离最远的两个点的距离作为两个集合的距离。其效果也是刚好相反的，限制非常大，两个cluster即使已经很接近了，但是只要有不配合的点存在，就顽固到底，老死不相合并，也是不太好的办法。&lt;/li&gt;
&lt;li&gt;Group Average：这种方法看起来相对有道理一些，也就是把两个集合中的点两两的距离全部放在一起求一个平均值,相对也能得到合适一点的结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的来说，一般都不太用Single Linkage或者Complete Linkage这两种过于极端的方法。整个agglomerative hierarchical clustering 的算法就是这个样子，描述起来还是相当简单的，不过计算起来复杂度还是比较高的，要找出距离最近的两个点，需要一个双重循环，而且 Group Average计算距离的时候也是一个双重循环。&lt;/p&gt;
&lt;p&gt;另外，需要提一下的是本文一开始的那个树状结构图，它有一个专门的称呼，叫做Dendrogram，其实就是一种二叉树，画的时候让子树的高度和它两个后代合并时相互之间的距离大小成比例，就可以得到一个相对直观的结构概览。&lt;/p&gt;
&lt;p&gt;Agglomerative clustering差不多就这样了，再来看Divisive clustering,也就是自顶向下的层次聚类，这种方法并没有Agglomerative clustering这样受关注，大概因为把一个节点分割为两个并不如把两个节点结合为一个那么简单吧，通常在需要做hierarchical clustering 但总体的cluster数目又不太多的时候可以考虑这种方法，这时可以分割到符合条件为止，而不必一直分割到每个数据点一个cluster 。&lt;/p&gt;
&lt;p&gt;总的来说，Divisive clustering 的每一次分割需要关注两个方面：一是选哪一个cluster来分割；二是如何分割。关于cluster的选取，通常采用一些衡量松散程度的度量值来比较，例如cluster中距离最远的两个数据点之间的距离，或者cluster中所有节点相互距离的平均值等，直接选取最“松散”的一个cluster来进行分割。而分割的方法也有多种，比如，直接采用普通的flat clustering算法（例如K-Means来进行二类聚类，不过这样的方法计算量变得很大，而且像K-Means 这样的和初值选取关系很大的算法，会导致结果不稳定。另一种比较常用的分割方法(&lt;code&gt;简而言之就是一个排除异己的过程&lt;/code&gt;)如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;待分割的cluster记为$G$,在$G$中取出一个到其他点的平均距离最远的点$x$,构成新cluster H；&lt;/li&gt;
&lt;li&gt;在 G 中选取这样的点$x\prime$,$x\prime$到$G$中其他点的平均距离减去$x\prime$到$H$中所有点的平均距离这个差值最大，将其归入$H$中；&lt;/li&gt;
&lt;li&gt;重复上一个步骤，直到差值为负。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;到此为止，关于Hierarchical clustering介绍就结束了。总的来说，Hierarchical clustering算法似乎都是描述起来很简单，计算起来很困难（计算量很大）。并且，不管是Agglomerative还是 Divisive 实际上都是贪心算法了，也并不能保证能得到全局最优的。而得到的结果，虽然说可以从直观上来得到一个比较形象的大局观，但是似乎实际用处并不如众多Flat clustering 算法那么广泛。&lt;/p&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://en.wikipedia.org/wiki/Adjusted_rand_index#Adjusted_Rand_index"&gt;Rand Index&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=17"&gt;漫谈 Clustering (1): k-means&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://blog.pluskid.org/?p=40"&gt;漫谈 Clustering (2): k-medoids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/vivounicorn/archive/2012/02/10/2343377.html"&gt;Spectral Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Machine Learning:A Probabilistic Perspective(&lt;em&gt;Chapter 25&lt;/em&gt;)&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Clustering"></category><category term="Unsupervised Learning"></category><category term="K-Means"></category><category term="Spectral Learning"></category><category term="Hierarchical Clustering"></category></entry><entry><title>机器学习系列(III):Gaussian Models</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iiigaussian-models.html" rel="alternate"></link><updated>2014-03-15T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-03-15:ji-qi-xue-xi-xi-lie-iiigaussian-models.html</id><summary type="html">&lt;p&gt;在&lt;a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html"&gt;上一篇&lt;/a&gt;中我们着重介绍了对于离散数据的生成模型，紧接上一篇，本篇我们介绍对于连续数据的生成模型。好吧,废话我们就不多说了,直接进入正文。&lt;/p&gt;
&lt;h1&gt;MLE for MVN&lt;/h1&gt;
&lt;hr&gt;
&lt;h2&gt;Basics about MVN&lt;/h2&gt;
&lt;p&gt;谈到连续分布,我们很自然地就会想到高斯分布,从小学到现在，印象中第一个走入我脑海中的看着比较高端大气上档次的就是Gaussian分布了。这次我们的重点也会完全集中在Gaussian分布之了,在正式讨论之前，我们先介绍一些关于Gaussian分布的基础知识。&lt;/p&gt;
&lt;p&gt;在$D$维空间中,MVN(Multivariate Normal)多变量正态分布的概率分布函数具有如下形式:&lt;/p&gt;
&lt;p&gt;\begin{equation}
N(x|\mu,\Sigma) \triangleq \frac{1}{(2\pi)^{D/2}det(\Sigma)^{1/2}} exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中的指数部分是$x$与$\mu$之间的&lt;a href="http://en.wikipedia.org/wiki/Mahalanobis_distance"&gt;Mahalanobis距离&lt;/a&gt;。为了更好地理解这个量,我们对$\Sigma$做特征值分解,即$\Sigma = U \Lambda U^T$,其中$U$是一正交阵,满足$U^TU=I$,而$\Lambda$是特征值矩阵。&lt;/p&gt;
&lt;p&gt;通过特征值分解,我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\Sigma^{-1} = U^{-T}\Lambda^{-1}U^{-1} = U\Lambda^{-1}U^T = \sum_{i=1}^{D}\frac{1}{\lambda_i}u_iu_i^T
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$u_i$是$U$的第$i$列。因此Mahalanobis距离可被改写为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
(x-\mu)^T\Sigma^{-1}(x-\mu) &amp;amp;= (x-\mu)^T (\sum_{i=1}^{D}\frac{1}{\lambda_i}u_iu_i^T) (x-\mu)   \\
                            &amp;amp;= \sum_{i=1}^{D} \frac{1}{\lambda_i}(x-\mu)^T u_iu_i^T (x-\mu)    \\
                            &amp;amp;= \sum_{i=1}^{D} \frac{y_i^2}{x_i}                                \\
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$y_i \triangleq u_i^T(x-\mu)$。另2维空间中的椭圆方程为:&lt;/p&gt;
&lt;p&gt;$$\frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2} = 1$$&lt;/p&gt;
&lt;p&gt;因此我们可知Gaussian概率密度的等高线沿着椭圆分布,特征向量决定椭圆的朝向,而特征值则决定椭圆有多&lt;code&gt;“椭”&lt;/code&gt;。一般来说，如果我们将坐标系移动$\mu$,然后按$U$旋转，此时的欧拉距离即为Mahalanobis距离。&lt;/p&gt;
&lt;h2&gt;MLE for MVN&lt;/h2&gt;
&lt;p&gt;以下我们给出MVN参数的MLE(极大似然估计)的证明:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 3.1 若我们获取的$N$个独立同分布的样本$x_i \sim\ N(x|\mu,\Sigma)$,则关于$\mu$以及
$\Sigma$的极大似然分布如下:
&lt;img alt="MLE for Gaussian" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mle_zpscaea1f03.png"&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们不加证明地给出如下公式组:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;\begin{equation}
  \begin{split}
  &amp;amp;\frac{\partial(b^Ta)}{\partial a} = b  \\
  &amp;amp;\frac{\partial(a^TAa)}{\partial a} = (A+A^T)a \\
  &amp;amp;\frac{\partial}{\partial A} tr(BA) = B^T  \\
  &amp;amp;\frac{\partial}{\partial A} log |A| = A^{-T} \\
  &amp;amp;tr(ABC) = tr(CAB) = tr(BCA)
  \end{split}
  \end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最后一个等式称为迹的循环置换性质(cyclic permutation property)。利用这个性质,我们使用&lt;code&gt;trace trick&lt;/code&gt;可以得到下式:&lt;/p&gt;
&lt;p&gt;$$x^TAx = tr(x^TAx) = tr(xx^TA) = tr(Axx^T)$$&lt;/p&gt;
&lt;p&gt;证明:&lt;/p&gt;
&lt;p&gt;对数似然函数为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l(\mu,\Sigma) = log p(D|\mu,\Sigma) = \frac{N}{2} log |\Lambda| - \frac{1}{2} \sum_{i=1}^{N} (x_i-\mu)^T \Lambda (x_i-\mu)
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$\Lambda = \Sigma^{-1}$为精度矩阵。令$y_i=x_i-\mu$并利用链式法则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial}{\partial \mu}(x_i-\mu)^T \Sigma^{-1} (x_i-\mu) = \frac {\partial}{\partial y_i} y_i^T \Sigma^{-1} y_i \frac{\partial y_i}{\partial \mu}=-(\Sigma^{-T}+\Sigma^{-1})y_i
\end{equation}&lt;/p&gt;
&lt;p&gt;即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial}{\partial \mu} l(\mu,\Sigma) = -\frac{1}{2} \sum_{i=1}^{N} -2\Sigma^{-1}(x_i-\mu) = 0
\end{equation}&lt;/p&gt;
&lt;p&gt;故有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} x_i = \bar{x}
\end{equation}&lt;/p&gt;
&lt;p&gt;即最大似然均值即为经验均值。&lt;/p&gt;
&lt;p&gt;利用trace trick我们重写对数似然函数为:&lt;/p&gt;
&lt;p&gt;\begin{equation}
l(\Lambda) = \frac{N}{2} log |\Lambda| - \frac{1}{2} \sum_{i} tr[(x_i-\mu)(x_i-\mu)^T \Lambda]
           = \frac{N}{2} log |\Lambda| - \frac{1}{2} tr[S_u\Lambda]
\end{equation}&lt;/p&gt;
&lt;p&gt;其中，$S_u \triangleq \sum_{i=1}^{N} (x_i-\mu)(x_i-\mu)^T$,业界尊称其为分散度矩阵(&lt;code&gt;Scatter Matrix&lt;/code&gt;),以后我们聊LDA的时候会再次碰到。对$\Lambda$求偏导有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\frac{\partial l(\Lambda)}{\partial \Lambda} = \frac{N}{2}\Lambda^{-T} - \frac{1}{2} S_u^{T} = 0
\end{equation}&lt;/p&gt;
&lt;p&gt;得:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat{\Sigma} = \frac{1}{N} \sum_{i=1}^{N} (x_i-\mu)(x_i-\mu)^T
\end{equation}&lt;/p&gt;
&lt;p&gt;证毕。&lt;/p&gt;
&lt;h1&gt;Gaussian Discriminant Analysis&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;我们在上一篇中提到了Naive Bayes方法,其实质无非是估计在每一类下特定的样本出现的概率，进而我们可以把该特定样本分配给概率值最大的那个类。而对于连续数据而言，其实质其实也是一样的，每一个MVN(我们可以看做一类或者一个Component)都可能生成一些数据，我们估计在每一个Component下生成特定样本的概率，然后把该特定样本分配给概率值最大的那个Component即可。即我们可以定义如下的条件分布:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(x|y=c,\theta) = N(x|\mu_c,\Sigma_c)
\end{equation}&lt;/p&gt;
&lt;p&gt;上述模型即为高斯判别分析(Gaussian Discriminant Analysis,GDA)(&lt;code&gt;注意,该模型为生成模型，而不是判别模型&lt;/code&gt;)。如果$\Sigma_c$是对角阵，即所有的特征都是独立的时，该模型等同于Naive Bayes.&lt;/p&gt;
&lt;h2&gt;QDA&lt;/h2&gt;
&lt;p&gt;在上式中带入高斯密度函数的定义，则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=c|x,\theta) = \frac{\pi_c |2\pi\Sigma_c|^{-1 \over 2}exp[-{1 \over 2}(x-\mu_c)^T\Sigma_C^{-1}(x-\mu_c)]}{\sum_{c\prime}\pi_{c\prime} |2\pi\Sigma_{c\prime}|^{-1 \over 2}exp[-{1 \over 2}(x-\mu_{c\prime})^T\Sigma_{c\prime}^{-1}(x-\mu_{c\prime})]}
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中$\pi$为各个Component的先验概率分布。根据上式得到的模型则称为Quadratic Discriminant Analysis(QDA).以下给出在2类以及3类情形下可能的决策边界形状,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Decision Boundary" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/decision_boundary_zps289df588.jpeg"&gt;&lt;/p&gt;
&lt;h2&gt;Linear Discriminant Analysis(LDA)&lt;/h2&gt;
&lt;p&gt;当各个Gaussian Component的协方差矩阵相同时，此时我们有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(y=c|x,\theta) &amp;amp;\propto \pi_c exp[\mu_c^T\Sigma^{-1}x-{1 \over 2}x^T\Sigma^{-1}x-{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c] \\
&amp;amp;= exp[\mu_c^T\Sigma^{-1}x-{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c]exp[-{1 \over 2}x^T\Sigma^{-1}x]
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中$exp[-{1 \over 2}x^T\Sigma^{-1}x]$是独立于$c$的，分子分母相除抵消到此项。&lt;/p&gt;
&lt;p&gt;令:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\gamma_c &amp;amp;= -{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c \\
\beta_c &amp;amp;= \Sigma^{-1}\mu_c
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;于是有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=c|x,\theta) = \frac{e^{\beta_c^Tx+\gamma_c}}{\sum_{c\prime}e^{\beta_{c\prime}^Tx+\gamma_{c\prime}}}=S(\eta)_c
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\eta = [\beta_1^Tx+\gamma_1,...,\beta_C^Tx+\gamma_c]$,$S$为softmax函数(类似于max函数,故得此名),定义如下:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Softmax Function" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/softmax_zpscdabec6d.png"&gt;&lt;/p&gt;
&lt;p&gt;若将$\eta_c$除以一个常数(temperature),当$T\to 0$时，我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Softmax Function" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/boltzman_distribution_zpsa00cdbf0.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;换句话说，当温度很低时，分布集中在概率最大的那个状态上，而当温度高时，所有的状态呈现均匀分布。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:该术语来自于统计物理学，在统计物理学中，人们更倾向于使用波尔兹曼分布（Boltzmann distribution）一词。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;关于&lt;code&gt;式16&lt;/code&gt;有一个有趣的性质，即对该式取log,我们则会得到一个关于$x$的线性方程。因此对于任意两类之间的决策边界将会是一条直线，据此该模型也被称为线性判别分析(Linear Discriminant Analysis,LDA)。而且对于二分类问题，我们可以得到:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=c|x,\theta) = p(y=c\prime|x,\theta)
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
\beta_c^Tx+\gamma_c = \beta_{c\prime}^Tx+\gamma_{c\prime}
\end{equation}&lt;/p&gt;
&lt;p&gt;\begin{equation}
x^T(\beta_{c\prime}-\beta_c) = \gamma_{c\prime}-\gamma_c
\end{equation}&lt;/p&gt;
&lt;h2&gt;Two-class LDA&lt;/h2&gt;
&lt;p&gt;为了加深对以上等式的理解，对于二分类的情况，我们做如下说明:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(y=1|x,\theta) &amp;amp;= \frac{e^{\beta_1^Tx+\gamma_1}}{e^{\beta_1^Tx+\gamma_1}+e^{\beta_0^Tx+\gamma_0}} \\
&amp;amp;= \frac{1}{1+e^{(\beta_0-\beta_1)^Tx+(\gamma_0-\gamma_1)}} \\
&amp;amp;=sigm((\beta_1-\beta_0)^Tx+(\gamma_1-\gamma_0))
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$sigm(\eta)$代表&lt;a href="http://en.wikipedia.org/wiki/Sigmoid_function"&gt;Sigmoid函数&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;现有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\gamma_1-\gamma_0 &amp;amp;= -{1 \over 2}\mu_1^T\Sigma^{-1}\mu_1+{1 \over 2}\mu_0^T\Sigma^{-1}\mu_0+log(\pi_1/\pi_0)  \\
&amp;amp;=-{1 \over 2}(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1+\mu_0)+log(\pi_1/\pi_0)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;因此若我们另:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\omega &amp;amp;= \beta_1-\beta_0 = \Sigma^{-1}(\mu_1-\mu_0) \\
x_0 &amp;amp;= {1 \over 2}(\mu_1+\mu_0)-(\mu_1-\mu_0)\frac{log(\pi_1/\pi_0)}{(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0)}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;则有$\omega^Tx_0 = -(\gamma_1-\gamma_0)$,即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
p(y=1|x,\theta) = sigm(\omega^T(x-x_0))
\end{equation}&lt;/p&gt;
&lt;p&gt;因此最后的决策规则很简单:将$x$平移$x_0$,然后投影到$\omega$上，通过结果是正还是负决定它到底属于哪一类。&lt;/p&gt;
&lt;p&gt;&lt;img alt="Two class LDA" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/2_class_lda_zps98b80132.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;当$\Sigma = \sigma^2I$时，$\omega$与$\mu_1-\mu_0$同向。这时我们只需要判断投影点离$\mu_1$和$\mu_0$中的那个点近。当它们的先验概率$\pi_1 = \pi_0$时，投影点位于其中点；当$\pi_1&amp;gt;\pi_0$时，则$x_0$越趋近于$\mu_0$,直线的更大部分先验地属于类1;反之亦然。&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1-back"&gt;&lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1" class="simple-footnote" title="LDA算法可能导致overfitting,具体解决方法请参阅Machine Learning:a probabilistic perspective一书4.2.*部分"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h1&gt;Inference in joint Gaussian distributions&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;给定联合概率分布$p(x_1,x_2)$,如果我们能够计算边际概率分布$p(x1)$以及条件概率分布$p(x_1|x_2)$想必是极好的而且是及有用的。以下我们仅给出结论,下式表明&lt;strong&gt;如果两变量符合联合高斯分布，则它们的边际分布以及条件分布也都是高斯分布&lt;/strong&gt;。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 3.2&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2-back"&gt;&lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2" class="simple-footnote" title="具体证明请参考ML:APP一书4.3.4.3一节"&gt;2&lt;/a&gt;&lt;/sup&gt; 假定$x=(x_1,x_2)$服从联合高斯分布,且参数如下:
\begin{equation}
\mu = \left(
        \begin{array}{ccc}
        \mu_1 \\
        \mu_2
        \end{array}
      \right)
\end{equation}
\begin{equation}
\Sigma = \left(
        \begin{array}{ccc}
        \Sigma_{11} &amp;amp; \Sigma_{12} \\
        \Sigma_{21} &amp;amp; \Sigma_{22}
        \end{array}
      \right)
\end{equation}
则我们可以得到如下边际概率分布:
\begin{equation}
p(x_1) = N(x_1|\mu_1,\Sigma_{11})  \\
p(x_2) = N(x_2|\mu_2,\Sigma_{22})
\end{equation}
另其后验条件分布为:
\begin{equation}
p(x_1|x_2) = N(x_1|\mu_{1|2},\Sigma_{1|2})
\end{equation}
\begin{equation}
\begin{split}
\mu_{1|2} &amp;amp;= \mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2) \\
&amp;amp;=\mu_1-\Lambda_{11}^{-1}\Lambda_{12}(x_2-\mu_2) \\
&amp;amp;=\Sigma_{1|2}(\Lambda_{11}\mu_1-\Lambda_{12}(x_2-\mu_2)) \\
\end{split}
\end{equation}
\begin{equation}
\Sigma_{1|2} = \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}=\Lambda_{11}^{-1}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;Linear Gaussian Systems&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;给定两变量，$x$和$y$.令$x \in R^{D_x}$为一隐含变量,$y \in R^{D_y}$为关于$x$的包含噪声的观察值。此外，我们假定存在如下prior和likelihood:
\begin{equation}
\begin{split}
p(x) &amp;amp;= N(x|\mu_x,\Sigma_x) \\
p(y|x) &amp;amp;= N(y|Ax+b,\Sigma_y)
\end{split}
\end{equation}
上式即称为&lt;em&gt;Linear Gaussian System&lt;/em&gt;。此时我们有:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 3.3&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3-back"&gt;&lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3" class="simple-footnote" title="证明请参考ML:APP一书4.4.3节"&gt;3&lt;/a&gt;&lt;/sup&gt; 给定一Linear Gaussian System.其后验分布$p(x|y)$具有如下形式:
\begin{equation}
\begin{split}
p(x|y) &amp;amp;= N(x|\mu_{x|y},\Sigma_{x|y}) \\
\Sigma_{x|y}^{-1} &amp;amp;= \Sigma_x^{-1}+A^T\Sigma_y^{-1}A  \\
\mu_{x|y} &amp;amp;= \Sigma_{x|y}[A^T\Sigma_y^{-1}(y-b)+\Sigma_x^{-1}\mu_x] 
\end{split}
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Inferring an unknown vector from noisy measurements&lt;/h2&gt;
&lt;p&gt;下面我们举一个简单的例子以进一步说明Linear Gaussian System:
现有$N$个观测向量,$y_i \sim\ N(x,\Sigma_y)$,prior服从高斯分布$x \sim\ N(\mu_0,\Sigma_0)$.令$A=I,b=0$,此外，我们采用$\bar{y}$作为我们的有效估计值,其精度为$N\Sigma_y^{-1},$我们有:
\begin{equation}
\begin{split}
p(x|y_1,...,y_N) &amp;amp;= N(x|\mu_N,\Sigma_N) \\
\Sigma_N^{-1} &amp;amp;= \Sigma_{0}^{-1}+N\Sigma_{y}^{-1} \\
\mu_N &amp;amp;= \Sigma_N(\Sigma_y^{-1}(N\bar{y})+\Sigma_0^{-1}\mu_0)
\end{split}
\end{equation}
为了更直观地解释以上模型,如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Radar Blips" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/radar_blips_zpsfd1a04d1.png"&gt;&lt;/p&gt;
&lt;p&gt;我们可将x视为2维空间中一个物体的真实位置(但我们并不知道),例如一枚导弹或者一架飞机,$y_i$则是我们的观测值(含噪声),可以视为雷达上的一些点。当我们得到越来越多的点时，我们就能够更好地进行定位。&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4-back"&gt;&lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4" class="simple-footnote" title="另外一种方法为Kalman Filter Algorithm"&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;现假定我们有多个测量设备，且我们想利用多个设备的观测值进行估计，这种方法称为&lt;code&gt;sensor fusion&lt;/code&gt;.如果我们有具有不同方差的多组观测值，那么posterior将会是它们的加权平均。如下图所示:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sensor Fusion" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sensor_fusion_zps4ad8202f.png"&gt;&lt;/p&gt;
&lt;p&gt;我们采用不带任何信息的关于$x$的先验分布，即$p(x)=N(\mu_0,\Sigma_0)=N(0,10^10I_2)$,我们得到2个观测值,$y_1 \sim\ N(x,\Sigma_{y,1}$,$y_2 \sim\ N(x,\Sigma_{y,2})$,我们需要计算$p(x|y_1,y_2)$.&lt;/p&gt;
&lt;p&gt;如上图(a),我们设定$\Sigma_{y,1} = \Sigma_{y,2} = 0.01I_2$,因此两个传感器都相当可靠，posterior mean即位于两个观测值中间；如上图(b)，我们设定$\Sigma_{y,1}=0.05I_2$且$\Sigma_{y,2}=0.01I_2$,因此传感器2比传感器1可靠，此时posterior mean更靠近于$y_2$;如上图(c),我们有:
\begin{equation}
\Sigma_{y,1} = 0.01
\left(
\begin{array}{cc}
10 &amp;amp; 1 \\
1  &amp;amp; 1
\end{array}
\right),
\Sigma_{y,2} = 0.01
\left(
\begin{array}{cc}
1 &amp;amp; 1 \\
1  &amp;amp; 10
\end{array}
\right)
\end{equation}&lt;/p&gt;
&lt;p&gt;从上式我们不难看出，传感器1在第2个分量上更可靠，传感器2在第1个分量上也更可靠。此时，posterior mean采用传感器1的第二分量以及传感器1的第二分量。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;NOTE:当sensor测量精度未知时，我们则需要它们的测量精度也进行估计。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;The Wishart Distribution&lt;/h1&gt;
&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;在多变量统计学中,Wishart分布是继高斯分布后最重要且最有用的模型。  ------Press&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;既然Press他老人家都说了Wishart分布很重要，而且我们下一部分会用到它，那么我们就必须得介绍介绍它了。(它主要被用来Model关于协方差矩阵的不确定性)&lt;/p&gt;
&lt;h2&gt;Wishart Distribution&lt;/h2&gt;
&lt;p&gt;Wishart概率密度函数具有如下形式:
\begin{equation}
Wi(\Lambda|S,\nu)=\frac{1}{Z_{Wi}}|\Lambda|^{(\nu-D-1)/2}exp(-{1 \over 2}tr(\Lambda S^{-1}))
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\nu$为自由度，$S$为缩放矩阵。其归一项具有如下形式:(&lt;code&gt;很恐怖，对吧!&lt;/code&gt;)
\begin{equation}
Z_{Wi}=2^{\nu D/2}\Gamma_D(\nu/2)|S|^{\nu/2}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中,$\Gamma_D(a)$为多变量Gamma函数:
\begin{equation}
\Gamma_D(x) = \pi^{D(D-1)/4}\prod_{i=1}^{D} \Gamma(x+(1-i)/2)
\end{equation}&lt;/p&gt;
&lt;p&gt;于是有$\Gamma_1(a)=\Gamma(a)$且有:
\begin{equation}
\Gamma_D(\nu_0/2)=\prod_{i=1}^{D} \Gamma(\frac{\nu_0+1-i}{2})
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;仅当$\nu&amp;gt;D-1$时归一项存在&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;其实Wishart分布和Gaussian分布是有联系的。具体而言，令$x_i \sim\ N(0,\Sigma)$,则离散度矩阵$S=\sum_{i=1}^{N}x_ix_i^T$服从Wishart分布,且$S \sim\ Wi(\Sigma,1)$。于是有$E(S)=N\Sigma$.&lt;/p&gt;
&lt;p&gt;更一般地，我们可以证明$Wi(S,\nu)$的mean和mode具有如下形式:
\begin{equation}
mean=\nu S,mode=(\nu-D-1)S
\end{equation}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;仅当$\nu&amp;gt;D+1$时mode存在&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;当$D=1$时，Wishart分布退化为Gamma分布，且有:
\begin{equation}
Wi(\lambda|s^{-1},v) = Ga(\lambda|{\nu \over 2},{s \over 2})
\end{equation}&lt;/p&gt;
&lt;h2&gt;Inverse Wishart Distribution&lt;/h2&gt;
&lt;p&gt;若$\Sigma^{-1} \sim\ Wi(S,\nu)$,则$\Sigma \sim\ IW(S^{-1},\nu+D+1)$,其中$IW$为逆Wishart分布。当$\nu&amp;gt;D-1$且$S \succ 0$时,我们有:
\begin{equation}
\begin{split}
IW(\Sigma|S,\nu) &amp;amp;= \frac{1}{Z_{IW}}|\Sigma|^{-(\nu+D+1)/2}exp(-{1 \over 2}tr(S^{-1}\Sigma^{-1})) \\
Z_{IW} &amp;amp;= |S|^{-\nu/2}2^{\nu D/2}\Gamma_D(\nu/2)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;此外我们可以证明逆Wishart分布具有如下性质:
\begin{equation}
mean=\frac{S^{-1}}{\nu-D-1},mode=\frac{S^{-1}}{\nu+D+1}
\end{equation}&lt;/p&gt;
&lt;p&gt;当$D=1$时,它们退化为逆Gamma分布:
\begin{equation}
IW(\sigma^2|S^{-1},\nu)=IG(\sigma^2|\nu/2,S/2)
\end{equation}&lt;/p&gt;
&lt;h1&gt;Inferring the parameters of an MVN&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5-back"&gt;&lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5" class="simple-footnote" title="本部分部分参考Regularized Gaussian Covariance Estimation"&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/h1&gt;
&lt;hr&gt;
&lt;p&gt;到目前为止，我们已经讨论了在$\theta=(\mu,\Sigma)$已知的条件下如何inference，现我们讨论一下如何对参数本身进行估计。假定$x_i \sim\ N(\mu,\Sigma)$ for $i=1:N$.本节主要分为两个部分:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算$p(\mu|D,\Sigma)$;&lt;/li&gt;
&lt;li&gt;计算$p(\Sigma|D,\mu)$.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Posterior distribution of $\mu$&lt;/h2&gt;
&lt;p&gt;我们已经就如何计算$\mu$的极大似然估计值进行了讨论,现我们讨论如何计算其posterior.&lt;/p&gt;
&lt;p&gt;其likelihood具有如下形式:
\begin{equation}
p(D|\mu) = N(\bar{x}|\mu,{1 \over N}\Sigma)
\end{equation}&lt;/p&gt;
&lt;p&gt;为了简便起见，我们采用共轭先验分布，即高斯。特别地，若$p(\mu)=N(\mu|m_0,V_0)$.此时我们可以根据之前Linear Gaussian System的结论得到(和我们之前提到的雷达的例子雷同):
\begin{equation}
\begin{split}
p(\mu|D,\Sigma) &amp;amp;= N(\mu|m_N,V_N) \\
V_N^{-1} &amp;amp;= V_0^{-1}+N\Sigma^{-1} \\
m_N &amp;amp;= V_N(\Sigma^{-1}(N\bar{x})+V_0^{-1}m_0)
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;我们可以通过设定$V_0 = \infty I$提供一个不带任何信息的先验，此时我们有$p(\mu|D,\Sigma)=N(\bar{x},{1 \over N}\Sigma)$,即和MLE得到的结果相同。&lt;/p&gt;
&lt;h2&gt;Posterior distribution of $\Sigma$&lt;/h2&gt;
&lt;p&gt;现我们讨论如何计算$p(\Sigma|D,\mu)$,其likelihood具有如下形式:&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6-back"&gt;&lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6" class="simple-footnote" title="参见MLE for Gaussian部分"&gt;6&lt;/a&gt;&lt;/sup&gt;
\begin{equation}
p(D|\mu,\Sigma) \propto |\Sigma|^{-{N \over 2}}exp(-{1 \over 2}tr(S_{\mu}\Sigma^{-1}))
\end{equation}&lt;/p&gt;
&lt;p&gt;之前我们提到过如果采用共轭先验能够减少计算的复杂度，而此likelihood的共轭先验就是我们之前提到的非常恐怖的逆Wishart分布，即:&lt;/p&gt;
&lt;p&gt;\begin{equation}
IW(\Sigma|S_0^{-1},\nu_0) \propto |\Sigma|^{-(\nu_0+D+1)/2}exp(-{1 \over 2}tr(S_0\Sigma^{-1}))
\end{equation}&lt;/p&gt;
&lt;p&gt;上式中$N_0=\nu+D+1$控制着先验的强度,和likelihood中的$N$的作用基本相同。&lt;/p&gt;
&lt;p&gt;将先验和likelihood相乘我们得到如下posterior:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
p(\Sigma|D,\mu) &amp;amp;\propto |\Sigma|^{-{N \over 2}}exp(-{1 \over 2}tr(S_{\mu}\Sigma^{-1}))|\Sigma|^{-(\nu_0+D+1)/2}exp(-{1 \over 2}tr(S_0\Sigma^{-1})) \\
&amp;amp;=|\Sigma|^{-\frac{N+(\nu_0+D+1)}{2}}exp(-{1 \over 2}tr(\Sigma^{-1}(S_{\mu}+S_0))) \\
&amp;amp;=IW(\Sigma|S_N,\nu_N) \\
\nu_N &amp;amp;= \nu_0+N    \\
S_N^{-1} &amp;amp;= S_0+S_{\mu}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;总而言之，从上式我们可以看到，posterior $v_N$的强度为$\nu_0$加$N$;posterior离散度矩阵是先验离散度矩阵$S_0$和数据离散度矩阵$S_{\mu}$之和。&lt;/p&gt;
&lt;h3&gt;MAP Estimation&lt;/h3&gt;
&lt;p&gt;根据我们之前得到的关于$\Sigma$的极大似然估计值,即:&lt;/p&gt;
&lt;p&gt;&lt;img alt="MLE for Covariance" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/mle_covariance_zpsb7265e9d.png"&gt;&lt;/p&gt;
&lt;p&gt;从上式我们可以看出矩阵的rank为$min(N,D)$。若$N$小于$D$,该矩阵不是full rank的，因此不可逆。另尽管$N$可能大于$D$,$\hat{\Sigma}$也可能是ill-conditioned(接近奇异)。&lt;/p&gt;
&lt;p&gt;为了解决上述问题，我们可以采用posterior mean或mode.我们可以证明$\Sigma$的MAP估计值如下:(使用我们推导MLE时所用的技巧，其实并不难，亲证下式正确):&lt;/p&gt;
&lt;p&gt;\begin{equation}
\hat\Sigma_{MAP} = \frac{S_N}{\nu_N+D+1} = \frac{S_0+S_{\mu}}{N_0+N_{\mu}}
\end{equation}&lt;/p&gt;
&lt;p&gt;当我们采用improper uniform prior，即$S_0=0,N_0=0$时，我们即得MLE估计值。&lt;/p&gt;
&lt;p&gt;当$D/N$较大时，选择一个包含信息的合适的prior就相当必要了。令$\mu=\bar{x}$,故有$S_{\mu}=S_{\bar{x}}$,此时MAP估计值可被重写为prior mode和MLE的convex combination.令$\Sigma_0 \triangleq  \frac{S_0}{N_0}$为prior mode,则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
\begin{split}
\hat\Sigma_{MAP} = \frac{S_0+S_{\bar{x}}}{N_0+N} &amp;amp;= \frac{N_0}{N_0+N}\frac{S_0}{N_0}+\frac{N}{N_0+N}\frac{S}{N} \\
&amp;amp;=\lambda\Sigma_0+(1-\lambda)\hat{\Sigma}_{mle}
\end{split}
\end{equation}&lt;/p&gt;
&lt;p&gt;其中$\lambda=\frac{N_0}{N_0+N}$,控制着向prior &lt;code&gt;shrinkage&lt;/code&gt;的程度。对于$\lambda$而言，我们可以通过交叉验证设置其值。&lt;sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7-back"&gt;&lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7" class="simple-footnote" title="其他方法见ML:APP一书4.6.2.1节"&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;而对于先验的协方差矩阵$S_0$,一般采用如下prior:$S_0=diag(\hat{\Sigma}_{mle})$.因此，我们有:&lt;/p&gt;
&lt;p&gt;&lt;img alt="S_0" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/S0_zps47da8ac0.png"&gt;&lt;/p&gt;
&lt;p&gt;由上式我们可以看出，对角线的元素和极大似然估计值相等，而非对角线则趋近于0.因此该技巧也被称为&lt;em&gt;shrinkage estimation,or regularized estimation&lt;/em&gt;.&lt;/p&gt;&lt;script type="text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
&lt;ol class="simple-footnotes"&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1"&gt;LDA算法可能导致overfitting,具体解决方法请参阅Machine Learning:a probabilistic perspective一书4.2.*部分 &lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1-back" class="simple-footnote-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2"&gt;具体证明请参考ML:APP一书4.3.4.3一节 &lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2-back" class="simple-footnote-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3"&gt;证明请参考ML:APP一书4.4.3节 &lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3-back" class="simple-footnote-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4"&gt;另外一种方法为Kalman Filter Algorithm &lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4-back" class="simple-footnote-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5"&gt;本部分部分参考&lt;a href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/"&gt;Regularized Gaussian Covariance Estimation&lt;/a&gt; &lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5-back" class="simple-footnote-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6"&gt;参见MLE for Gaussian部分 &lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-6-back" class="simple-footnote-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7"&gt;其他方法见ML:APP一书4.6.2.1节 &lt;a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-7-back" class="simple-footnote-back"&gt;↩&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><category term="Machine Learning"></category><category term="Gaussian Models"></category><category term="Generative Models"></category></entry><entry><title>机器学习系列(II):Generative models for discrete data</title><link href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html" rel="alternate"></link><updated>2014-03-04T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-03-04:ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html</id><summary type="html">&lt;h1&gt;博客若干事&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;博客更新&lt;/h2&gt;
&lt;p&gt;根据目前的学习进度、自己的空闲时间以及时间的充裕度，现将博客的更新时间定于周三，更新周期为每一周或者两周更新一次。另由于目前自己对于Latex公式还不是特别熟，所以博文中的公式可能会出现部分错误，请大家谅解。此外，博客刚刚创建，很多东西都在完善当中，包括博客的插件，博文的排版等等，这些方面之后会慢慢完善，目前已开放的功能仅基本支持博文的显示以及评论。&lt;/p&gt;
&lt;p&gt;由于机器学习领域问题一般涉及公式较多，目前采取的渲染方式是通过相应的JS插件，导致的直接后果是页面的载入速度较慢，这方面以后可能将公式转换为图片然后输出。&lt;/p&gt;
&lt;p&gt;好吧，博客方面要说的就这么多吧。&lt;/p&gt;
&lt;h2&gt;机器学习浅谈&lt;/h2&gt;
&lt;p&gt;机器学习要研究的问题无非有四:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为什么要学习?&lt;/li&gt;
&lt;li&gt;学习什么？&lt;/li&gt;
&lt;li&gt;怎么学习？&lt;/li&gt;
&lt;li&gt;怎么更好地学习？&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;也许所有的理论，所有的事无非要解决的就是这四件事吧，为什么、做什么、怎么做、怎么做好。(作者注)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;大概所有的思想、理论、模型大致是围绕这四个方向进行的，而且这四个问题都得到了较好的解决。以上这些理论比较繁杂，而且我也没完全弄懂，所以咱们慢慢啃吧。&lt;/p&gt;
&lt;p&gt;今天我们要谈的是主要是生成模型，与之对应的则是判别模型。生成模型和判别模型的区别在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成模型首先计算联合概率分布$p(x,y)$,然后据此计算$p(y|x)$;而判别模型往往直接计算$p(y|x)$;&lt;/li&gt;
&lt;li&gt;生成模型关注数据是怎么生成的，然后进行预测；判别模型不关注数据的具体生成过程，直接预测。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本文主要介绍针对离散数据的生成模型，限于篇幅，本文仅对其中其中的两个模型进行介绍------Dirichlet-Multinomial Model以及朴素贝叶斯分类器，Dirichlet-Multinomial Model被广泛使用在各种语言模型中，而朴素贝叶斯分类器最为人所知的应用大概就是垃圾邮件过滤(Spam Filtering)了吧。&lt;/p&gt;
&lt;p&gt;以下我们正式开始介绍这两个模型。&lt;/p&gt;
&lt;h1&gt;Dirichlet-multinomial Model&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;我们现在要Model的问题很简单，假设现有一个$K$面的骰子，我们需要推断它出现第$k$面的概率。&lt;/p&gt;
&lt;h2&gt;Likelihood&lt;/h2&gt;
&lt;p&gt;假设我们掷骰子$N$次，$D={x_1,...,x_N}$,其中，$x_i\in {1,...,K}$.我们假设数据是独立同分布的(iid),Likelihood则有如下形式：
\begin{equation}
p(D|\theta) = \prod_{k=1}^{K}\theta_k^{N_k}
\end{equation}
其中，$N_k = \sum_{i=1}^{N}1_{y_i=k}$是第$k$面出现的次数。(1为指示函数，下同)&lt;/p&gt;
&lt;h2&gt;先验分布&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Machine Learning:A probabilistic perspective&lt;/code&gt;一书告诉我们如果先验分布和Likelihood的形式相同(共轭先验分布，conjugate prior)时，能够很好的简化我们的计算过程。基于此理，我们选择Dirichlet分布作为我们的先验分布，Dirichlet分布具有如下形式:
\begin{equation}
Dir(\theta|\alpha) = \frac{1}{B(\alpha)}\prod_{k=1}^{K} \theta_k^{\alpha_k-1}1_{x \in S_k}
\end{equation}&lt;/p&gt;
&lt;h2&gt;后验分布&lt;/h2&gt;
&lt;p&gt;将Likelihood乘以Prior，后验分布的形式告诉我们后验分布也服从Dirichlet Distribution:
\begin{equation}
p(\theta|D)     \propto p(D|\theta)p(\theta)    \
                \propto \prod_{k=1}^{K} \theta_k^{N_k}\theta_{k}^{\alpha_k-1}=\prod_{k=1}^{K}\theta_k^{\alpha_k+N_k-1} \
                =Dir(\theta|\alpha_1+N_1,...,\alpha_K+N_K)
\end{equation}
现在我们计算关于参数$\theta$的极大后验估计(MAP),其中,$\sum_{k}\theta_k=1$.
引入Lagrange乘子之后我们需要优化的目标函数为:
\begin{equation}
l(\theta,\lambda) = \sum_{k} N_klog\theta_k+\sum_{k}(\alpha_k-1)log\theta_k+\lambda(1-\sum_{k}\theta_k)
\end{equation}
为简便起见，记$N\prime_k\triangleq N_k+\alpha_k-1$.对$\lambda$求导得：
\begin{equation}
\frac{\partial l}{\partial \lambda}=(1-\sum_{k}\theta_k) = 0
\end{equation}
对$\theta_k$求导得，
\begin{equation}
\frac{\partial l}{\partial \theta_k}=\frac{N\prime_k}{\theta_k}-\lambda=0 \
N\prime_k = \lambda\theta_k
\end{equation}
由上两式得：
\begin{equation}
\sum_{k} N\prime_k = \lambda\sum_{k}\theta_k  \
N+\alpha_0-K=\lambda
\end{equation}
其中，$\alpha_0\triangleq \sum_{k=1}^{K}\alpha_k$是先验的有效样本大小。因此我们可以得出极大后验估计值为：
\begin{equation}
\hat{\theta}_k=\frac{N_k+\alpha_k-1}{N+\alpha_0-K}
\end{equation}
如果采用uniform prior $\alpha_k=1$，这时得到的最大后验估计值即与经验值相同。
\begin{equation}
\hat{\theta_k} = \frac{N_k}{N}
\end{equation}&lt;/p&gt;
&lt;h2&gt;Posterior predicative&lt;/h2&gt;
&lt;p&gt;\begin{equation}
p(X=j|D) = \int P(X=j|\theta)p(\theta|D)d\theta \
=\int P(X=j|\theta_j)[\int p(\theta_{-j},\theta_j|D)d\theta{-j}]d\theta_j \
=\int \theta_jp(\theta_j|D)d\theta_j=E[\theta_j|D] = \frac{\alpha_j+N_j}{\sum_{k}\alpha_k+N_k}=\frac{\alpha_j+N_j}{\alpha_0+N}
\end{equation}
其中，$\theta_{-j}$代表$\theta$中除$\theta_j$之外的所有分量。&lt;/p&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;上述模型的一个很重要的应用场景是语言模型，即预测一个序列中下一个可能出现的词。以下我们举一个非常简单的例子，我们假定每一个词$X_i \in {1,...,K}$都是通过$Cat(\theta)$独立取样得到的，该模型被称为bag of words model.给定一已知的样本序列，我们需要预测下一个最可能出现的是什么词?&lt;/p&gt;
&lt;p&gt;如，假设我们取样得到如下样本:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Mary&lt;/span&gt; &lt;span class="n"&gt;had&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;little&lt;/span&gt; &lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;little&lt;/span&gt; &lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;little&lt;/span&gt; &lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;Mary&lt;/span&gt; &lt;span class="n"&gt;had&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;little&lt;/span&gt; &lt;span class="n"&gt;lamb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;its&lt;/span&gt; &lt;span class="n"&gt;fleece&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;white&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;snow&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;另外，我们假设我们的字典中有如下词:&lt;/p&gt;
&lt;p&gt;&lt;img alt="words" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/words_zps55d01c8c.png" /&gt;&lt;/p&gt;
&lt;p&gt;这里unk代表unknown，表示未在样本中出现过的所有词。为了给上述样本中的每一行进行编码，我们先从采样样本中去掉标点符号以及&lt;code&gt;停用词&lt;/code&gt;(即没有实际意义的词，一般只是各种助词等),如，a,as,the等。此外我们还需要对所有的词进行处理仅得到其词根，如saw处理为see，running处理为run等。最后，我们对每一行进行索引编号,得到如下结果:&lt;/p&gt;
&lt;p&gt;&lt;img alt="index" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/index_zpsba0a3fb0.png?t=1393987077" /&gt;&lt;/p&gt;
&lt;p&gt;这里我们不考虑词序，仅考虑每个词在样本中出现的次数。统计得到如下结果:&lt;/p&gt;
&lt;p&gt;&lt;img alt="token_count" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/token_count_zps1876920b.png?t=1393987078" /&gt;&lt;/p&gt;
&lt;p&gt;将以上每个计数值记为$N_j$,如果对于$\theta$我们采用Dirichlet先验分布，则有:&lt;/p&gt;
&lt;p&gt;\begin{equation}
P(\bar{X}_j|D)=E[\theta_j|D]=\frac{\alpha_j+N_j}{\sum_t \alpha_t+N_t}=\frac{1+N_j}{10+17}
\end{equation}&lt;/p&gt;
&lt;p&gt;通过代入每一个计数值，我们便能得出每个词出现的概率。至此，我们得到了该语言模型的所有参数，进而可以进行各种预测。&lt;/p&gt;
&lt;h1&gt;Naive Bayes Classifier&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;引子&lt;/h2&gt;
&lt;p&gt;朴素贝叶斯分类是一种十分简单的分类算法，它的思想很naive,但是其实际应用效果还是不错的。所以一个模型的好坏并非在于其复杂度，而在于我们是否将它用到了正确的地方，就算一个非常Naive的模型，如果用在了恰当的地方，也能产生很好的效果。具体就机器学习算法而言，只有真正对一个算法的特性、适用条件、优缺点有非常深刻的理解，才能真正把机器学习算法或模型用好。朴素贝叶斯的思想基础是这样的：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。通俗来说，就好比这么个道理，你在街上看到一个黑人，我问你你猜这哥们哪里来的，你十有八九猜非洲。为什么呢？因为黑人中非洲人的比率最高，当然人家也可能是美洲人或亚洲人，但在没有其它可用信息下，我们会选择条件概率最大的类别，这就是朴素贝叶斯的思想基础。&lt;/p&gt;
&lt;p&gt;引用&lt;a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html"&gt;CodingLabs&lt;/a&gt;提到的一个例子来抛出我们的问题，并以此为基础介绍我们的模型:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;对于&lt;/span&gt;&lt;span class="n"&gt;SNS&lt;/span&gt;&lt;span class="err"&gt;社区来说，不真实账号（使用虚假身份或用户的小号）是一个普遍存在的问题，作为&lt;/span&gt;&lt;span class="n"&gt;SNS&lt;/span&gt;&lt;span class="err"&gt;社区的运营商，希望可以检测出这些不真实账号，从而在一些运营分析报告中避免这些账号的干扰，亦可以加强对&lt;/span&gt;&lt;span class="n"&gt;SNS&lt;/span&gt;&lt;span class="err"&gt;社区的了解与监管。&lt;/span&gt;
&lt;span class="err"&gt;如果通过纯人工检测，需要耗费大量的人力，效率也十分低下，如能引入自动检测机制，必将大大提升工作效率。这个问题说白了，就是要将社区中所有账号在真实账号和不真实账号两个类别上进行分类。&lt;/span&gt;
&lt;span class="err"&gt;首先设&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="err"&gt;表示真实账号，&lt;/span&gt;&lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="err"&gt;表示不真实账号。&lt;/span&gt;
&lt;span class="err"&gt;假设我们目前确定能作为评判用户帐号是否真实的特征属性有如下几个&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;实际上在机器学习领域确定特征属性是一项特别重要且复杂的工作，我们这里为了简化，直接给出本问题的特征属性&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;F1&lt;/span&gt;&lt;span class="err"&gt;：日志数量&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="err"&gt;注册天数；&lt;/span&gt;&lt;span class="n"&gt;F2&lt;/span&gt;&lt;span class="err"&gt;：好友数量&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="err"&gt;注册天数；&lt;/span&gt;&lt;span class="n"&gt;F3&lt;/span&gt;&lt;span class="err"&gt;：是否使用真实头像。在&lt;/span&gt;&lt;span class="n"&gt;SNS&lt;/span&gt;&lt;span class="err"&gt;社区中这三项都是可以直接从数据库里得到或计算出来的。&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="err"&gt;对这些属性进行区间划分保证这些属性取离散值&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="err"&gt;接下来的工作是我们从数据库得到了一些新的记录，给出了如上三个特征，我们需要预测这些用户是否真实的用户。&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Introduction to Naive Bayes Classifier&lt;/h2&gt;
&lt;p&gt;Naive Bayes Classifier要解决的问题是对于具有D个特征属性，每个属性可以取${1,...,K}$中任意一个值的样本进行分类，即$x \in {1,...,K}^D$。朴素贝叶斯分类器是一个生成模型，我们需要计算关于类别的条件概率$p(x|y=c)$.朴素贝叶斯假定给定类别c的条件下，各特征属性之间是相互独立的。于是我们有:
\begin{equation}
p(x|y=c,\theta) = \prod_{j=1}^{D} p(x_j|y=c,\theta{jc})
\end{equation}
我们得到的模型即为Naive Bayes Classifier(NBC).在上面的SNS真实用户检测的例子中，C=2,D=3。&lt;/p&gt;
&lt;p&gt;Naive Bayes Classifier的基本算法流程如下所示:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;Algorithm&lt;/span&gt; &lt;span class="mf"&gt;2.2&lt;/span&gt; &lt;span class="n"&gt;Naive&lt;/span&gt; &lt;span class="n"&gt;Bayes&lt;/span&gt; &lt;span class="n"&gt;Classifier&lt;/span&gt;&lt;span class="err"&gt;算法框架&lt;/span&gt;
&lt;span class="mf"&gt;1.&lt;/span&gt;  &lt;span class="err"&gt;根据得到的样本数据计算在每一可能的类别下各属性取值的条件概率，即计算&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="mf"&gt;2.&lt;/span&gt;  &lt;span class="err"&gt;根据计算得到的条件概率计算新样本属于各个类别的概率，即计算&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;|&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="mf"&gt;3.&lt;/span&gt;  &lt;span class="err"&gt;比较计算得到的新样本属于不同类别的概率值，选择值最大的那个类别作为新样本的类别。&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这里不给出针对具体数据的计算过程，想了解具体每一步怎么算的亲们请参考&lt;a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html"&gt;算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)&lt;/a&gt;。&lt;/p&gt;
&lt;h1&gt;Mutual Information&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;NBC需要计算关于很多特征的联合概率分布，可能会导致过拟合；此外，算法的运行时间是$O(CD)$,对于有些应用来说可能计算量太大了。一个解决上述问题的普遍被采用的方案是进行特征选取，去掉和分类无关的无用属性。最简单的方法是考察每个特征与分类属性之间的相关性，并权衡复杂性以及准确度选取K个最相关的属性用于训练。该方法被称为&lt;code&gt;variable ranking,filtering,or screening&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;衡量相关性的一种方式是通过互信息，如下:
\begin{equation}
I(X,Y)=\sum_{x_j}\sum_{Y}P(x_j,y)log\frac{p(x_j,Y)}{p(x_j)p(y)}
\end{equation}
互信息可被理解为当我们观察到特征$x_j$时对于分类属性造成的熵减。对每个特征属性分别计算互信息后，选取较大的若干个用于训练即可。&lt;/p&gt;
&lt;h1&gt;Appendix I:Mutual Information&lt;/h1&gt;
&lt;h2&gt;KL divergence&lt;/h2&gt;
&lt;p&gt;衡量两个概率分布$p$和$q$差异性的一种方法是KL距离(Kullback-Leibler divergence or relative entropy).定义如下:
\begin{equation}
KL(p||q)\triangleq \sum_{k=1}^{K} p_klog\frac{p_k}{q_k}
\end{equation}
上式可以改写为:
\begin{equation}
KL(p||q) \triangleq \sum_{k}p_klogp_k-\sum_{k}p_klogq_k = -H(p)+H(p,q)
\end{equation}
其中，$H(p,q)$称为联合熵，定义为:
\begin{equation}
H(p,q)\triangleq -\sum_{k}p_klogq_k
\end{equation}
其实，联合熵可被理解为用分布$q$编码来自分布$p$的数据时所需要的最小位数，$H(p)$即是用本身分布编码本身信息所需要的最小比特位数，因此KL距离的含义即是使用$q$编码来自$p$的信息相对于分布$p$本身而言多需要的位数。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Theorem 2.1 $KL(p,q) \ge 0$,且当且仅当$p=q$时等号成立；&lt;/p&gt;
&lt;p&gt;为证明上式，我们引入琴生不等式，即任意凸函数$f$,有:
\begin{equation}
f(\sum_{i=1}^{n}\lambda_ix_i) \le \sum_{i=1}^{n}\lambda_if(x_i)
\end{equation}
其中$\lambda_i\ge 0,\sum_{i=1}^{n}\lambda_i=1$&lt;/p&gt;
&lt;p&gt;Proof:
\begin{equation}
-KL(p||q)=-\sum_{x \in A}p(x)log\frac{p(x)}{q(x)}=-\sum_{x \in A}p(x)log\frac{q(x)}{p(x)} \
\le log \sum_{x \in A}p(x)log\frac{q(x)}{p(x)}=log \sum_{x \in A} q(x) \
\le log \sum_{x \in X}q(x)=log 1=0
\end{equation}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;另外一个重要的推论是离散分布中一致分布的熵最大，即$H(X) \le log |X|$.&lt;/p&gt;
&lt;p&gt;\begin{equation}
0 \le KL(q||u) = \sum_{x} p(x)log \frac{p(x)}{u(x)} \
= \sum_{x}p(x)logp(x)-\sum_{x}p(x)logu(x) = -H(X)+log|X|
\end{equation}
该式是Laplace不充分理由原则的公式表示，它的含义是当没有其他理由证明其他分布好于一致分布时，应当采用一致分布。&lt;/p&gt;
&lt;h2&gt;Mutual Information&lt;/h2&gt;
&lt;p&gt;考察两个随机变量，$X$和$Y$。假如我们想知道一个变量包含关于另一变量的多少信息，我们可以计算相关系数，但那只针对实数随机变量而言。一个更通用的办法是衡量联合分布和分布乘积的相关性，即MI.定义如下：
\begin{equation}
I(X;Y) \triangleq KL((p(X,Y)||p(X)p(Y)) = \sum_{x}\sum_{y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}
\end{equation}
$I(X;Y) \ge 0 $成立且当且仅当$p(X,Y=P(X)P(Y)$时取等。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;\begin{equation}
I(X;Y) = H(X)-H(X|Y) = H(Y)-H(Y|X)
\end{equation}
其中，减式的后半部分称为条件熵，证明此处从略。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;参考文献&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html"&gt;算法杂货铺——分类算法之朴素贝叶斯分类(Naive Bayesian classification)&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category><category term="Classfication"></category><category term="Generative Models"></category><category term="Mutual Information"></category></entry><entry><title>机器学习系列(I):决策树算法</title><link href="http://www.qingyuanxingsi.com/Decision%20Tree.html" rel="alternate"></link><updated>2014-03-03T00:00:00+08:00</updated><author><name>qingyuanxingsi</name></author><id>tag:www.qingyuanxingsi.com,2014-03-03:Decision Tree.html</id><summary type="html">&lt;h1&gt;写在前面&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;好吧，今天我的博客在线下默默地开张了，花了好长时间才把中文显示的问题解决。言归正传，之所以开通这个博客，原因有二：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对已经学过的知识进行梳理，保证学习过程的稳步前进；&lt;/li&gt;
&lt;li&gt;敦促自己每周有一定的学习目标,以更好地推进自己的学习.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;关于这个博客其他的我就不说了，如果你觉得这个博客有点用，你愿意花点时间看看，我会灰常感激滴。如果你觉得这个博客没什么用，直接忽略就好。此外，这篇博客所有内容均host在Github上，本着分享，协作的精神，如果你愿意而且有时间欢迎投稿至qingyuanxingsi@163.com,I would be much glad to receive your mails.&lt;/p&gt;
&lt;h1&gt;简介&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;二三闲话&lt;/h2&gt;
&lt;p&gt;这是本博客的第一篇博文，也是第一篇关于机器学习方面的博文，因此我想扯些闲话。就我而言，我觉得所有的机器学习算法并不只是模型本身那么简单，背后其实还有一些别的东西，从某种角度来说，它们也是模型的创立者认识世界的方式。&lt;/p&gt;
&lt;p&gt;举贝叶斯为例，从他的模型中可能能推断出他也许认为万物皆有联系，所有的事物都不是孤立的，都是相互联系，相互影响的。一个事物的改变会引起其他事物的相应变化，世界是一个相互联系的整体。另，我经常听到人们抱怨这个世界不公平，这个世界并不是他们想要的那种模样；或者说自从多年前姚晨和凌潇肃离婚之后，好多人都不再相信爱情了(just a joke）。虽然说这是生活中再平常不过的桥段，从这两个例子中，也许我们能看到另外一些东西，我们很久很久以前都对这个世界有一些先入为主的认识(&lt;strong&gt;prior&lt;/strong&gt;),我们愿意相信这个世界是公平的，爱情是非常美好的一件事。后来，慢慢的我们发现这个世界其实有很多不公平的事，我们发现这个世界里的爱情没我们想象的那么美好，我们看到了一些真实世界实实在在存在的事情(&lt;strong&gt;data&lt;/strong&gt;),于是我们对于这个世界的认识发生了改变，我们开始相信一些原来不相信的事情，对我们之前深信不疑的事情也不再那么确信。(&lt;strong&gt;posterior&lt;/strong&gt;)(关于这个模型我们下一次说吧).&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;曾经相信过爱情，后来知道，原来爱情必须转化为亲情才可能持久，但是转化为亲情的爱情，犹如化入杯水中的冰块──它还是冰块吗？                    &lt;br /&gt;
曾经相信过海枯石烂作为永恒不灭的表征，后来知道，原来海其实很容易枯，石，原来很容易烂。雨水，很可能不再来，沧海，不会再成桑田。原来，自己脚下所踩的地球，很容易被毁灭。海枯石烂的永恒，原来不存在。                   &lt;br /&gt;
...                     &lt;br /&gt;
相信与不相信之间，彷佛还有令人沉吟的深度。(龙应台《相信，不相信》）&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;举上面例子的目的意在说明其实机器学习算法也许并非就是些模型，就是些数学而已，它也许能给我们提供看待世界的另一种角度，也许能带给我们一些有益的思考。关于闲话就说到这儿，以后我们有时间慢慢扯。&lt;/p&gt;
&lt;h2&gt;Introduction to Decision Trees&lt;/h2&gt;
&lt;p&gt;所谓决策树，顾名思义，是一种树，一种依托于策略抉择而建立起来的树。决策树中非叶节点(非根节点)均是决策节点，决策节点的取值决定了决策树具体下一步跳到那个节点，每个决策节点的分支则分别代表了决策属性可能的取值；每一个叶节点代表了一个分类属性，即决策过程的完成。从根节点到叶节点的每一条路径代表了一个可能的决策过程。&lt;/p&gt;
&lt;p&gt;举个例子，也许大家能对决策树到底是什么有一个更为清楚直观的认识:&lt;/p&gt;
&lt;p&gt;一个非常经典的例子是一个女生找对象的过程，在女孩决定是否相亲的过程中可能产生如下对话:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="err"&gt;女儿：多大年纪了？&lt;/span&gt;
&lt;span class="err"&gt;母亲：&lt;/span&gt;&lt;span class="mi"&gt;26&lt;/span&gt;&lt;span class="err"&gt;。&lt;/span&gt;
&lt;span class="err"&gt;女儿：长的帅不帅？&lt;/span&gt;
&lt;span class="err"&gt;母亲：挺帅的。&lt;/span&gt;
&lt;span class="err"&gt;女儿：收入高不？&lt;/span&gt;
&lt;span class="err"&gt;母亲：不算很高，中等情况。&lt;/span&gt;
&lt;span class="err"&gt;女儿：是公务员不？&lt;/span&gt;
&lt;span class="err"&gt;母亲：是，在税务局上班呢。&lt;/span&gt;
&lt;span class="err"&gt;女儿：那好，我去见见。&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;这个女孩的决策过程就是典型的分类树决策。相当于通过年龄、长相、收入和是否公务员对将男人分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么这个可以用下图表示女孩的决策逻辑：&lt;/p&gt;
&lt;p&gt;&lt;img alt="girl" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/APPLE/Markdown/girl_zpsd5a3cfed.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;根据奥卡姆剃刀原则(&lt;code&gt;Simpler is better&lt;/code&gt;),我们尽可能想构造得到的决策书尽可能的小。因此，如何选择上图中决策属性是所有决策树算法的核心所在。我们尽量在每一步要有限选取最有分辨能力的属性作为决策属性，以保证树尽可能的小。针对决策树，我们主要介绍两种比较典型的算法ID3以及C4.5,另外CART(Classification and Regression Tree)是另外使用的比较多的算法，商用的版本则有C5.0,它主要针对C4.5算法做了很多性能上的优化。具体针对CART以及C5.0的介绍本文将不再涉及。&lt;/p&gt;
&lt;h1&gt;ID3&lt;/h1&gt;
&lt;hr /&gt;
&lt;h2&gt;ID3算法基本框架&lt;/h2&gt;
&lt;p&gt;ID3算法是一个由Ross Quinlan发明的用于决策树的算法。它是一个启发式算法，具体算法框架可参见《机器学习》一书中的描述，如下所示:
                       &lt;img alt="ID3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/id3_zpsaa2fe321.jpg" /&gt;&lt;/p&gt;
&lt;h2&gt;分裂属性的选取&lt;/h2&gt;
&lt;p&gt;如上图算法框架所示，判断测试某个属性为最佳的分类属性是ID3的核心问题，以下介绍两个比较重要的概念：信息熵和信息增益。&lt;/p&gt;
&lt;h3&gt;信息熵&lt;/h3&gt;
&lt;p&gt;为了精确地定义信息增益，我们先定义信息论中广泛使用的一个度量标准，称为熵(entropy),它刻画了任意样例集的纯度，另一种理解则是用来编码信息所需的最少比特位数。
\begin{equation}
Entropy(S) = -\sum_{i=1}^{c} p_ilog(p_i)
\end{equation}                                    &lt;br /&gt;
其中，$p_i$是属性S属于类别i的概率。&lt;/p&gt;
&lt;h3&gt;信息增益&lt;/h3&gt;
&lt;p&gt;已经有了熵作为衡量训练样例集合纯度的标准，现在可以定义属性分类训练数据的效力的度量标准。这个标准被称为&lt;strong&gt;“信息增益（information gain）”&lt;/strong&gt;。简单的说，一个属性的信息增益就是由于使用这个属性分割样例而导致的期望熵降低(或者说，样本按照某属性划分时造成熵减少的期望,个人结合前面理解，总结为用来衡量给定的属性区分训练样例的能力)。更精确地讲，一个属性A相对样例集合S的信息增益$Gain(S,A)$被定义为：
\begin{equation}
Gain(S,A)=Entropy(S) - \sum_{v \in S_v} \frac{|S_v|}{|S|}Entropy(S_v)
\end{equation}                 &lt;br /&gt;
其中：
    $V(A)$是属性A的值域；
    $S$是样本集合；
    $S_v$是S在属性A上取值等于v的样本集合。&lt;/p&gt;
&lt;p&gt;对于上述算法框架中迭代的每一步，针对样本集合S,我们分别算出针对每个可能的属性的信息增益值，并选择值最大的那个对应的属性作为我们该步的分裂属性即可。依次迭代，便能构造我们想要的决策树。&lt;/p&gt;
&lt;h3&gt;Python代码实现&lt;/h3&gt;
&lt;p&gt;实践出真知，磨刀霍霍，我们小小地实现一下。对于以上提到的ID3算法，基于Python我们给出了相应的源码实现，如下:(本博客中所有源码仅是算法思想的一个比较粗略的实现，很多方面还不成熟，特此说明，以后不再提及)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;numpy&lt;/span&gt; &lt;span class="n"&gt;as&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;math&lt;/span&gt;
&lt;span class="n"&gt;import&lt;/span&gt; &lt;span class="n"&gt;operator&lt;/span&gt;

&lt;span class="n"&gt;class&lt;/span&gt; &lt;span class="n"&gt;DTree_ID3&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;runDT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;classList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;classify&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;max_index&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Max_InfoGain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="err"&gt;##&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;
        &lt;span class="n"&gt;max_fea&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;max_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;myTree&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;max_fea&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;{}}&lt;/span&gt;
        &lt;span class="n"&gt;fea_val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;max_index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;unique&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fea_val&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;    
        &lt;span class="n"&gt;del&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;max_index&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;          
            &lt;span class="n"&gt;sub_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;splitDataSet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;         
            &lt;span class="n"&gt;myTree&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;max_fea&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;runDT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sub_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
        &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;insert&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_index&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;max_fea&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;myTree&lt;/span&gt;

    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;classify&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;classCount&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;vote&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;classList&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;vote&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;classCount&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;classCount&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vote&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="n"&gt;classCount&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;vote&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;sortedClassCount&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sorted&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;classCount&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iteritems&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;operator&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;itemgetter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;revese&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;sortedClassCount&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;Max_InfoGain&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data_set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="n"&gt;compute&lt;/span&gt; &lt;span class="n"&gt;all&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="n"&gt;InfoGain&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;the&lt;/span&gt; &lt;span class="n"&gt;maximal&lt;/span&gt; &lt;span class="n"&gt;one&lt;/span&gt;
        &lt;span class="n"&gt;Num_Fea&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_set&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
        &lt;span class="err"&gt;#&lt;/span&gt;&lt;span class="n"&gt;Num_Tup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_set&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;max_IG&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="n"&gt;max_Fea&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Num_Fea&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;InfoGain&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_set&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;max_IG&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;InfoGain&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;max_IG&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;InfoGain&lt;/span&gt;
                &lt;span class="n"&gt;max_Fea&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;max_Fea&lt;/span&gt;

    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;Info&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;dic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tup&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
                &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="n"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;not&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
            &lt;span class="nl"&gt;else:&lt;/span&gt;
                &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]][&lt;/span&gt;&lt;span class="n"&gt;tup&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

        &lt;span class="n"&gt;S_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;S_each&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
            &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kt"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dic&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
                &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;S_each&lt;/span&gt; &lt;span class="o"&gt;-=&lt;/span&gt; &lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;S_total&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;S_each&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;S_total&lt;/span&gt;

    &lt;span class="n"&gt;def&lt;/span&gt; &lt;span class="n"&gt;splitDataSet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;featureIndex&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;subDataSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
        &lt;span class="n"&gt;dataSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt; &lt;span class="n"&gt;in&lt;/span&gt; &lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;featureIndex&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;reducedSample&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;featureIndex&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  
                &lt;span class="n"&gt;reducedSample&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;featureIndex&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;  
                &lt;span class="n"&gt;subDataSet&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reducedSample&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;asarray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subDataSet&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;dataSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Cool&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;High&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Ugly&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;High&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
               &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Cool&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Low&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Cool&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Low&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
               &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Cool&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Medium&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Ugly&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Medium&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Yes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;No&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
    &lt;span class="n"&gt;featureSet&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Appearance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Salary&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Office Guy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;dTree&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DTree_ID3&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;tree&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dTree&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;runDT&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataSet&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;featureSet&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tree&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h1&gt;C4.5&lt;/h1&gt;
&lt;hr /&gt;
&lt;p&gt;C4.5决策树在ID3决策树的基础之上稍作改进，并克服了其两大缺点:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用信息增益选择属性偏向于选择分枝比较多的属性，即取值多的属性;&lt;/li&gt;
&lt;li&gt;不能处理连续属性.                &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于这两个问题，C4.5都给出了具体的解决方案，以下做一个简要的阐述。&lt;/p&gt;
&lt;h2&gt;信息增益率&lt;/h2&gt;
&lt;p&gt;C4.5选取了信息增益率作为选择决策属性的依据，克服了用信息增益来选择属性时偏向选择值多的属性的不足。信息增益率定义为： 
\begin{equation}
GainRatio(S,A)=\frac{Gain(S,A)}{SplitInfo(S,A)}
\end{equation}
其中$Gain(S,A)$和ID3算法中的信息增益计算相同，而$SplitInfo(S,A)$代表了按照属性A分裂样本集合S的广度和均匀性。
\begin{equation}
SplitInfo(S,A)=-\sum_{i=1}^{c} \frac{|S_i|}{|S|}log\frac{|S_i|}{|S|}
\end{equation}
其中$S_i$表示根据属性A分割S而成的样本子集;&lt;/p&gt;
&lt;h2&gt;处理连续属性&lt;/h2&gt;
&lt;p&gt;对于离散值，C4.5和ID3的处理方法相同，对于某个属性的值连续时，假设这这个节点上的数据集合样本为total，C4.5算法进行如下处理：   &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将样本数据该属性A上的具体数值按照升序排列，得到属性序列值：${A_1,A_2,A_3,...,A{total}}$&lt;/li&gt;
&lt;li&gt;在上一步生成的序列值中生成total-1个分割点。第i个分割点的取值为$A_i$和$A_{i+1}$的均值，每个分割点都将属性序列划分为两个子集;&lt;/li&gt;
&lt;li&gt;计算每个分割点的信息增益(Information Gain),得到total-1个信息增益。}&lt;/li&gt;
&lt;li&gt;对分裂点的信息增益进行修正：减去log2(N-1)/|D|，其中N为可能的分裂点个数，D为数据集合大小。&lt;/li&gt;
&lt;li&gt;选择修正后的信息增益值最大的分类点作为该属性的最佳分类点&lt;/li&gt;
&lt;li&gt;计算最佳分裂点的信息增益率(Gain Ratio)作为该属性的Gain Ratio&lt;/li&gt;
&lt;li&gt;选择Gain Ratio最大的属性作为分类属性。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;总结&lt;/h1&gt;
&lt;p&gt;决策树方法是机器学习算法中比较重要且较易于理解的一种分类算法，本文介绍了两种决策树算法，ID3和C4.5.决策树算法的核心在于分裂属性的选取上，对此，ID3采用了信息增益作为评估指标，但是ID3也有不能处理连续属性值和易于选取取值较多的属性，C4.5对这两个问题都给出了相应的解决方案。&lt;/p&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine Learning"></category></entry></feed>