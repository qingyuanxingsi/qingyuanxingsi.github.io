<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>苹果的味道</title>
    <meta name="description" content="">
    <meta name="author" content="qingyuanxingsi">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="./theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.1.1/css/bootstrap.no-icons.min.css" rel="stylesheet">
    <link href="./theme/local.css" rel="stylesheet">
    <link href="./theme/pygments.css" rel="stylesheet">
    <link href="./theme/font-awesome.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Gudea:400,400italic|Alegreya+SC' rel='stylesheet' type='text/css'>
</head>

<body>
<header class="blog-header">
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
	<a href="." class="brand">苹果的味道</a>
      </div>

      <div class="span3" id="blog-nav">
	<ul class="nav nav-pills pull-right">
            <li><a href="./pages/about.html">About</a></li>
	    <li >
	      <a href="./category/distributed-system.html ">Distributed System</a>
	    <li >
	      <a href="./category/life.html ">Life</a>
	    <li >
	      <a href="./category/machine-learning.html ">Machine Learning</a>
	    <li >
	      <a href="./category/notes.html ">Notes</a>
	    <li >
	      <a href="./category/pearls.html ">Pearls</a>
	    <li >
	      <a href="./category/viewpoint.html ">Viewpoint</a>
	</ul>
      </div>
    </div> <!-- End of fluid row-->
  </div>   <!-- End of Container-->
</header>
    
<div class="container">
    <div class="content">
    <div class="row-fluid">

        <div class="span10">
        

        

    <div class='row-fluid''>
        <div class="article-title span9">
            <a href="./zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-ii.html"><h1>自然语言处理(序章):我爱自然语言处理(II)</h1></a>
        </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>二 06 五月 2014 </p>

<p style="text-align: left;">
Filed under <a href="./category/pearls.html">Pearls</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="./tag/nlp.html">NLP</a> </p>
<p>
</p>
      </div>
      <div class="article-content span8">
	<p>本文紧接上一篇<a href="http://www.qingyuanxingsi.com/zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i.html">自然语言处理(序章):我爱自然语言处理(I)</a>,由于文章篇幅过长导致编辑器响应速度变慢,所以将其拆分为两篇,本文即为第二部分。(<strong>本博文引用内容版权属我爱自然语言博客作者及其引用文章作者,特此再次声明</strong>)。</p>
<h1>正态分布的前世今生</h1>
<hr />
<blockquote>
<p>神说，要有正态分布，就有了正态分布。</p>
<p>神看正态分布是好的，就让随机误差就服从了正态分布。</p>
</blockquote>
<h2>正态分布</h2>
<p>学过基础统计学的同学大都对正态分布非常熟悉。这个钟型的分布曲线不但形状优雅，其密度函数写成数学表达式:</p>
<p>\begin{equation}
f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{equation}</p>
<p>也非常具有数学的美感。其标准化后的概率密度函数:</p>
<p>\begin{equation}
f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}
\end{equation}</p>
<p>更加的简洁漂亮,两个最重要的数学常量 $\pi$,$e$都出现在了公式之中。在我个人的审美之中，它也属于 top-N 的最美丽的数学公式之一，如果有人问我数理统计领域哪个公式最能让人感觉到上帝的存在，那我一定投正态分布的票。因为这个分布戴着神秘的面纱，在自然界中无处不在，让你在纷繁芜杂的数据背后看到隐隐的秩序。</p>
<p><img alt="Normal_Curve" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/normal_curve_zpse8660e23.png" /></p>
<p>正态分布又通常被称为高斯分布，在科学领域，冠名权那是一个很高的荣誉。去过德国的兄弟们还会发现，德国的钢镚和10马克的纸币上都留有高斯的头像和正态密度曲线。正态分布被冠名高斯分布，我们也容易认为是高斯发现了正态分布，其实不然，不过高斯对于正态分布的历史地位的确立是起到了决定性的作用。</p>
<p><img alt="10dm" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/10dm_with_gauss_curve_zps561d08ca.jpg" /></p>
<p><img alt="10dm_detail" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/10dm_with_gauss_curve_detail-300x217_zpsd38f3d9e.jpg" /></p>
<p><img alt="20_mark" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/20-mark-gauss_zpsd6e55205.jpg" /></p>
<p>正态曲线虽然看上去很美，却不是一拍脑袋就能想到的。我在本科学习数理统计的时候，课本一上来介绍正态分布就给出密度分布函数，却从来不说明这个分布函数是通过什么原理推导出来的。所以我一直搞不明白数学家当年是怎么找到这个概率分布曲线的，又是怎么发现误差服从这个奇妙的分布的。直到我读研究生的时候我的导师给我介绍了陈希儒院士的《数理统计简史》这本书，看了之后才了解了正态分布曲线从发现到被人们重视进而广泛应用，也是经过了几百年的历史。</p>
<p>正态分布的这段历史是很精彩的，我们通过讲几个故事来揭开她的神秘面纱。</p>
<h2>邂逅</h2>
<hr />
<p>正态曲线的首次发现第一个故事和概率论的发展密切相关，主角是棣莫弗(De Moivre) 和拉普拉斯(Laplace)。拉普拉斯是个大科学家，被称为法国的牛顿；棣莫弗名气可能不算很大，不过大家应该应该都熟悉这个名字，因为我们在高中数学学复数的时候都学过棣莫弗定理:</p>
<p>\begin{equation}
(cosθ+isinθ)^n=cos(nθ)+isin(nθ)
\end{equation}</p>
<p>古典概率论发源于赌博，惠更斯、帕斯卡、费马、贝努力都是古典概率的奠基人，他们那会研究的概率问题大都来自赌桌上，最早的概率论问题是赌徒梅累在1654年向帕斯卡提出的如何分赌金的问题。统计学中的总体均值之所以被称为期望(Expectation), 就是源自惠更斯、帕斯卡这些人研究平均情况下一个赌徒在赌桌上可以期望自己赢得多少钱。</p>
<p>有一天一个哥们，也许是个赌徒，向棣莫弗提了一个和赌博相关的一个问题：A,B两人在赌场里赌博，A，B各自的获胜概率是$p,q=1−p$,赌$n$局，若 A 赢的局数$X&gt;np$, 则A付给赌场$X−np$元，否则B付给赌场$np−X$元。 问赌场挣钱的期望值是多少。</p>
<p>问题并不复杂， 本质上是一个二项分布，最后求出的理论结果是:</p>
<p>\begin{equation}
2npq \ b(n, p, np)
\end{equation}</p>
<p>其中$b(n,p,i)=C_n^ip^iq^{n−i}$是常见的二项概率。 但是对具体的 n, 要把这个理论结果实际计算出数值结果可不容易， 因为其中的二项公式中有组合数.这就驱动De Moivre寻找近似计算的方法计算。</p>
<p>与此相关联的另一个问题，是遵从二项分布的随机变量$X \sim\ B(n,p)$, 求$X$落在二项分布中心点一定范围的概率$P_d=P(|X–np|≤d)$</p>
<p>对于$p=1/2$的情形， 棣莫弗做了一些计算并得到了一些近似结果，但是还不够漂亮，幸运的是棣莫弗和Stirling 处在同一个时代， 而且二人之间有联系，Stirling公式是在数学分析中必学的一个重要公式(事实上Stirling 公式的形式其实是棣莫弗最先发现的，但是 Stirling 改进了公式:</p>
<p>\begin{equation}
n! \sim \sqrt{2\pi n} (\frac{n}{e})^{n}
\end{equation}</p>
<p>1733 年，棣莫弗很快利用 Stirling 公式进行计算并取得了重要的进展。考虑$n$是偶数的情形，令二项概率</p>
<p>\begin{equation}
b(i) = b(n, \frac{1}{2}, i) = \binom{n}{i}(\frac{1}{2})^n
\end{equation}</p>
<p>通过 Stirling 公式做一些简单的计算容易得到:</p>
<p>\begin{equation}
\displaystyle b(\frac{n}{2}) \sim \sqrt{\frac{2}{\pi n}}
\end{equation}</p>
<p>\begin{equation}
\displaystyle \frac{b(\frac{n}{2}+d)}{b(\frac{n}{2})} \sim e^{-\frac{2d^2}{n}}
\end{equation}</p>
<p>于是有:</p>
<p>\begin{equation}
\displaystyle b(\frac{n}{2}+d) \sim \frac{2}{\sqrt{2 \pi n}}e^{-\frac{2d^2}{n}}
\end{equation}</p>
<p>使用上式的结果，并在二项概率累加求和的过程中近似的使用定积分代替求和，很容易就能得到:</p>
<p>\begin{equation}
\displaystyle P(|\frac{X}{n} – \frac{1}{2}| \le \frac{c}{\sqrt{n}} ) \sim\int_{-2c}^{2c} \frac{1}{\sqrt{2\pi}} e^{-x^2/2} dx
\end{equation}</p>
<blockquote>
<p><strong>NOTE:以上两个公式没看懂,求大神指教。</strong></p>
</blockquote>
<p>看，正态分布的密度函数的形式在积分公式中出现了！这也就是我们在数理统计课本上学到的二项分布的极限分布是正态分布。以上只是讨论了$p=1/2$的情形， 棣莫弗也对 p≠1/2做了一些计算，后来拉普拉斯对 $p \neq 1/2$的情况做了更多的分析，并把二项分布的正态近似推广到了任意$p$的情况。 这是第一次正态密度函数被数学家勾画出来，而且是以二项分布的极限分布的形式被推导出来的。 熟悉基础概率统计的同学们都知道这个结果其实叫棣莫弗-拉普拉斯中心极限定理。</p>
<p><strong>[De Moivre-Laplace 中心极限定理]</strong></p>
<p>设随机变量$X_n(n=1,2,⋯)$服从参数为$p$的二项分布，则对任意的$x$, 恒有:</p>
<p>\begin{equation}
\displaystyle\lim_{n\rightarrow\infty}P{ \frac{X_n – np}{\sqrt{np(1-p)}} \le x }=\int_{-\infty}^x \frac{1}{\sqrt{2\pi}} e^{\frac{-t^2}{2}}dt
\end{equation}</p>
<p>我们在大学学习数理统计的时候，学习的过程都是先学习了正态分布，然后才学习中心极限定理。而学习到正态分布的时候，直接就描述了其概率密度的数学形式，虽然数学上很漂亮，但是当时很容易困惑数学家们是如何凭空就找到这个分布的。读了陈希孺的《数理统计学简史》之后，我才明白正态分布的密度形式首次发现是在棣莫弗-拉普拉斯的中心极限定理中。数学家研究数学问题的进程很少是按照我们数学课本的安排顺序推进的，现代的数学课本都是按照数学内在的逻辑进行组织编排的，虽然逻辑结构上严谨优美，却把数学问题研究的历史痕迹抹得一干二净。DNA 双螺旋结构的发现者之一 Waston 在他的名著《DNA 双螺旋》序言中说：“科学的发现很少会像门外汉所想象的一样，按照直接了当合乎逻辑的方式进行的。”</p>
<p>棣莫弗出他的发现后40年（大约是 1770),拉普拉斯建立了中心极限定理较一般的形式，中心极限定理后续又被其它数学家们推广到了其它任意分布的情形，而不限于二项分布。后续的统计学家发现，一系列的重要统计量，在样本量$N$趋于无穷的时候， 其极限分布都有正态的形式， 这构成了数理统计学中大样本理论的基础。</p>
<p>棣莫弗在二项分布的计算中瞥见了正态曲线的模样，不过他并没有能展现这个曲线的美妙之处。棣莫弗的这个工作当时并没有引起人们足够的重视，原因在于棣莫弗不是个统计学家，从未从统计学的角度去考虑其工作的意义。 正态分布(当时也没有被命名为正态分布) 在当时也只是以极限分布的形式出现，并没有在统计学，尤其是误差分析中发挥作用。这也就是正态分布最终没有被冠名棣莫弗分布的重要原因。 那高斯做了啥工作导致统计学家把正态分布的这顶桂冠戴在了他的头上呢？这先得从最小二乘法的发展说起。
<strong>[本部分待续]</strong></p>
<h1>MCMC 和 Gibbs Sampling</h1>
<hr />
<h2>随机模拟</h2>
<p>随机模拟(或者统计模拟)方法有一个很酷的别名是蒙特卡罗方法(Monte Carlo Simulation)。这个方法的发展始于20世纪40年代，和原子弹制造的曼哈顿计划密切相关，当时的几个大牛，包括乌拉姆、冯.诺依曼、费米、费曼、Nicholas Metropolis，在美国洛斯阿拉莫斯国家实验室研究裂变物质的中子连锁反应的时候，开始使用统计模拟的方法,并在最早的计算机上进行编程实现。</p>
<p><img alt="Simulation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/simulation_zpsfd333536.jpg" /></p>
<p>现代的统计模拟方法最早由数学家乌拉姆提出，被Metropolis命名为蒙特卡罗方法，蒙特卡罗是著名的赌场，赌博总是和统计密切关联的，所以这个命名风趣而贴切，很快被大家广泛接受。被不过据说费米之前就已经在实验中使用了，但是没有发表。说起蒙特卡罗方法的源头，可以追溯到18世纪，布丰当年用于计算π的著名的投针实验就是蒙特卡罗模拟实验。统计采样的方法其实数学家们很早就知道，但是在计算机出现以前，随机数生成的成本很高，所以该方法也没有实用价值。随着计算机技术在二十世纪后半叶的迅猛发展，随机模拟技术很快进入实用阶段。对那些用确定算法不可行或不可能解决的问题，蒙特卡罗方法常常为人们带来希望。</p>
<p><img alt="monte-carlo-simulation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/monte-carlo-simulation_zpsd57f8e88.jpg" /></p>
<p>统计模拟中有一个重要的问题就是给定一个概率分布$p(x)$，我们如何在计算机中生成它的样本。一般而言均匀分布 $Uniform(0,1)$的样本是相对容易生成的。通过线性同余发生器可以生成伪随机数，我们用确定性算法生成$[0,1]$之间的伪随机数序列后，这些序列的各种统计指标和均匀分布$Uniform(0,1)$的理论计算结果非常接近。这样的伪随机序列就有比较好的统计性质，可以被当成真实的随机数使用。</p>
<p><img alt="sampling" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sampling_zpsb8ae4169.png" /></p>
<p>而我们常见的概率分布，无论是连续的还是离散的分布，都可以基于$Uniform(0,1)$的样本生成。例如正态分布可以通过著名的<strong>Box-Muller</strong>变换得到</p>
<blockquote>
<p><strong>[Box-Muller变换]</strong></p>
<p>如果随机变量$U_1$,$U_2$独立且$U_1,U_2 \sim\ Uniform[0,1]$,</p>
<p>\begin{equation}
\begin{split} 
Z_0 &amp; = \sqrt{-2\ln U_1} cos(2\pi U_2) \\ 
Z_1 &amp; = \sqrt{-2\ln U_1} sin(2\pi U_2) 
\end{split}
\end{equation}</p>
</blockquote>
<p>则$Z_0,Z_1$独立且服从标准正态分布。</p>
<p>其它几个著名的连续分布，包括指数分布、Gamma分布、t分布、F分布、Beta分布、Dirichlet分布等等,也都可以通过类似的数学变换得到；离散的分布通过均匀分布更加容易生成。更多的统计分布如何通过均匀分布的变换生成出来，大家可以参考统计计算的书，其中 Sheldon M. Ross 的<strong>《统计模拟》</strong>是写得非常通俗易懂的一本。</p>
<p>不过我们并不是总是这么幸运的，当$p(x)$的形式很复杂，或者$p(x)$是个高维的分布的时候，样本的生成就可能很困难了。 譬如有如下的情况:</p>
<ul>
<li>$p(x) = \frac{\tilde{p}(x)}{\int \tilde{p}(x) dx}$,而$\tilde{p}(x)$我们是可以计算的，但是底下的积分式无法显式计算。</li>
<li>$p(x,y)$是一个二维的分布函数，这个函数本身计算很困难，但是条件分布$p(x|y),p(y|x)$的计算相对简单;如果$p(x)$是高维的，这种情形就更加明显。</li>
</ul>
<p>此时就需要使用一些更加复杂的随机模拟的方法来生成样本。而本节中将要重点介绍的 MCMC(Markov Chain Monte Carlo) 和 Gibbs Sampling算法就是最常用的一种，这两个方法在现代贝叶斯分析中被广泛使用。要了解这两个算法，我们首先要对马氏链的平稳分布的性质有基本的认识。</p>
<h2>马氏链及其平稳分布</h2>
<p>马氏链的数学定义很简单:</p>
<p>\begin{equation}
P(X_{t+1}=x|X_t, X_{t-1}, \cdots) =P(X_{t+1}=x|X_t)
\end{equation}</p>
<p>也就是状态转移的概率只依赖于前一个状态。</p>
<p>我们先来看马氏链的一个具体的例子。社会学家经常把人按其经济状况分成3类：下层(lower-class)、中层(middle-class)、上层(upper-class)，我们用1,2,3分别代表这三个阶层。社会学家们发现决定一个人的收入阶层的最重要的因素就是其父母的收入阶层。如果一个人的收入属于下层类别，那么他的孩子属于下层收入的概率是 0.65, 属于中层收入的概率是 0.28, 属于上层收入的概率是 0.07。事实上，从父代到子代，收入阶层的变化的转移概率如下:</p>
<p><img alt="table-1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/table-1_zps3d0d323d.jpg" /></p>
<p><img alt="markov-transition" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/markov-transition_zps8213ffd9.png" /></p>
<p>使用矩阵的表示方式，转移概率矩阵记为:</p>
<p>\begin{equation}
P=\left[
\begin{array}{cc}
0.65 &amp; 0.28 &amp; 0.07 \\ 
0.15 &amp; 0.67 &amp; 0.18 \\ 
0.12 &amp; 0.36 &amp; 0.52 \\ 
\end{array}
\right]
\end{equation}</p>
<p>假设当前这一代人处在下层、中层、上层的人的比例是概率分布向量 $\pi_0=[\pi_0(1),\pi_0(2),\pi_0(3)]$，那么他们的子女的分布比例将是$\pi_1=\pi_0P$, 他们的孙子代的分布比例将是 $\pi_2=\pi_1P=\pi_0P^2$, ……, 第n代子孙的收入分布比例将是$\pi_n=\pi_{n−1}P=\pi_0P^n$。</p>
<p>假设初始概率分布为$\pi_0=[0.21,0.68,0.11]$，则我们可以计算前$n$代人的分布状况如下</p>
<p><img alt="table-2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/table-2_zps47ffb526.jpg" /></p>
<p>我们发现从第7代人开始，这个分布就稳定不变了，这个是偶然的吗？我们换一个初始概率分布$\pi_0=[0.75,0.15,0.1]$.试试看，继续计算前$n$代人的分布状况如下</p>
<p><img alt="table-3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/table-3_zps3b41fb58.jpg" /></p>
<p>我们发现，到第9代人的时候, 分布又收敛了。最为奇特的是，两次给定不同的初始概率分布，最终都收敛到概率分布 $\pi=[0.286,0.489,0.225]$，也就是说收敛的行为和初始概率分布$\pi_0$无关。这说明这个收敛行为主要是由概率转移矩阵$P$决定的。我们计算一下$P^n$.</p>
<p>\begin{equation}
P^{20} = P^{21} = \cdots = P^{100} = \cdots = 
\begin{bmatrix} 
0.286 &amp; 0.489 &amp; 0.225 \\
0.286 &amp; 0.489 &amp; 0.225 \\ 
0.286 &amp; 0.489 &amp; 0.225 \\ 
\end{bmatrix}
\end{equation}</p>
<p>我们发现，当$n$足够大的时候，这个$P^n$矩阵的每一行都是稳定地收敛到$\pi=[0.286,0.489,0.225]$这个概率分布。自然的，这个收敛现象并非是我们这个马氏链独有的，而是绝大多数马氏链的共同行为，关于马氏链的收敛我们有如下漂亮的定理：</p>
<blockquote>
<p><strong>马氏链定理</strong></p>
<p>如果一个非周期马氏链具有转移概率矩阵$P$,且它的任何两个状态是连通的，那么$\lim_{n\rightarrow\infty}P_{ij}^n$存在且与$i$无关，记$\lim_{n\rightarrow\infty}P_{ij}^n = \pi(j)$, 我们有:</p>
<p>\begin{equation}
\begin{split}
\lim_{n \rightarrow \infty} P^n =\begin{bmatrix} 
\pi(1) &amp; \pi(2) &amp; \cdots &amp; \pi(j) &amp; \cdots \\ 
\pi(1) &amp; \pi(2) &amp; \cdots &amp; \pi(j) &amp; \cdots \\ 
\cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\ 
\pi(1) &amp; \pi(2) &amp; \cdots &amp; \pi(j) &amp; \cdots \\ 
\cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\ 
\end{bmatrix}    \\
\pi(j) = \sum_{i=0}^{\infty}\pi(i)P_{ij} \\
\end{split}
\end{equation}</p>
</blockquote>
<p>$\pi$是方程$\pi P=\pi$的唯一非负解。其中,</p>
<p>\begin{equation}
\pi = [\pi(1), \pi(2), \cdots, \pi(j),\cdots ], \quad \sum_{i=0}^{\infty} \pi_i = 1
\end{equation}</p>
<p>$\pi$称为马氏链的平稳分布。</p>
<p>这个马氏链的收敛定理非常重要，<strong>所有的 MCMC(Markov Chain Monte Carlo) 方法都是以这个定理作为理论基础的</strong>。 定理的证明相对复杂，一般的随机过程课本中也不给证明，所以我们就不用纠结它的证明了，直接用这个定理的结论就好了。我们对这个定理的内容做一些解释说明：</p>
<ol>
<li>该定理中马氏链的状态不要求有限，可以是有无穷多个的；</li>
<li>定理中的“非周期“这个概念我们不打算解释了，因为我们遇到的绝大多数马氏链都是非周期的；</li>
<li>两个状态$i,j$是连通并非指$i$可以直接一步转移到$j$($P_{ij}&gt;0)$,而是指$i$可以通过有限的$n$步转移到达$j$($P^n_{ij}&gt;0$)。马氏链的任何两个状态是连通的含义是指存在一个$n$,使得矩阵$P^n$中的任何一个元素的数值都大于零。</li>
<li>我们用$X_i$表示在马氏链上跳转第$i$步后所处的状态，如果$\lim_{n\rightarrow\infty}P_{ij}^n=\pi(j)$存在，很容易证明以上定理的第二个结论。由于</li>
</ol>
<p>\begin{equation}
\begin{split} 
P(X_{n+1}=j) &amp; = \sum_{i=0}^\infty P(X_n=i) P(X_{n+1}=j|X_n=i) \\
&amp; = \sum_{i=0}^\infty P(X_n=i) P_{ij} 
\end{split}
\end{equation}</p>
<p>上式两边取极限就得到$\pi(j) = \sum_{i=0}^{\infty}\pi(i)P_{ij}$.</p>
<p>从初始概率分布$\pi_0$出发，我们在马氏链上做状态转移，记$X_i$的概率分布为$\pi_i$, 则有:</p>
<p>\begin{equation}
\begin{split} 
X_0 &amp; \sim \pi_0(x) \ 
X_i &amp; \sim \pi_i(x), \quad\quad \pi_i(x) = \pi_{i-1}(x)P = \pi_0(x)P^n 
\end{split}
\end{equation}</p>
<p>由马氏链收敛的定理, 概率分布$\pi_i(x)$将收敛到平稳分布$\pi_(x)$。假设到第$n$步的时候马氏链收敛，则有</p>
<p>\begin{equation}
\begin{split} 
X_0 &amp; \sim \pi_0(x) \\ 
X_1 &amp; \sim \pi_1(x) \\ 
&amp; \cdots \\ 
X_n &amp; \sim \pi_n(x)=\pi(x) \\ 
X_{n+1} &amp; \sim \pi(x) \\ 
X_{n+2}&amp; \sim \pi(x) \\ 
&amp; \cdots 
\end{split}
\end{equation}</p>
<p>所以$X_n,X_{n+1},X_{n+2},\cdots \sim \pi(x)$都是同分布的随机变量，当然他们并不独立。如果我们从一个具体的初始状态$x_0$开始,沿着马氏链按照概率转移矩阵做跳转，那么我们得到一个转移序列$x_0,x_1,x_2,\cdots,x_n,x_{n+1},\cdots$, 由于马氏链的收敛行为，$x_n,x_{n+1},...$ 都将是平稳分布$\pi(x)$的样本。</p>
<h1>参考文献</h1>
<hr /><script type= "text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>
 
	<a class="btn btn-mini xsmall" href="./zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-ii.html">
          <i class="icon-comment"></i> Comment </a>
	<hr />
      </div>
      
    </div>
    

 
        



    <div class='row-fluid'>
      <div class='article-title span9'> 
        <a href="./xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html"><h1>小小收藏夹[持续更新中]</h1></a>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>一 05 五月 2014 </p>

<p style="text-align: left;">
Filed under <a href="./category/pearls.html">Pearls</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="./tag/suan-fa.html">算法</a> <a href="./tag/fun.html">Fun</a> <a href="./tag/staff.html">Staff</a> <a href="./tag/shou-cang-jia.html">收藏夹</a> <a href="./tag/bloom-filter.html">Bloom Filter</a> <a href="./tag/b-trees.html">B Trees</a> <a href="./tag/data-structure.html">Data Structure</a> <a href="./tag/algorithm.html">Algorithm</a> <a href="./tag/pgm.html">PGM</a> </p>
<p>
</p>
      </div>
      <div class="summary span8">
	<p>主要收集一些还不错的东东(持续更新)。</p> 
	<a class="btn btn-mini xsmall" href="./xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html">
          <i class="icon-plus-sign"></i> Read More </a>
	<hr />
      </div>
      
    </div>
      

 
        



    <div class='row-fluid'>
      <div class='article-title span9'> 
        <a href="./zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i.html"><h1>自然语言处理(序章):我爱自然语言处理(I)</h1></a>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>一 05 五月 2014 </p>

<p style="text-align: left;">
Filed under <a href="./category/pearls.html">Pearls</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="./tag/nlp.html">NLP</a> </p>
<p>
</p>
      </div>
      <div class="summary span8">
	<p>昨天浏览了一下<a href="http://www.52nlp.cn">我爱自然语言处理</a>站点上的全部文章,然后基本过滤下来自己感兴趣的90篇左右的文章,逐篇浏览后以下为个人的一个基本总结。</p> 
	<a class="btn btn-mini xsmall" href="./zi-ran-yu-yan-chu-li-xu-zhang-wo-ai-zi-ran-yu-yan-chu-li-i.html">
          <i class="icon-plus-sign"></i> Read More </a>
	<hr />
      </div>
      
    </div>
      

 
        



    <div class='row-fluid'>
      <div class='article-title span9'> 
        <a href="./ji-qi-xue-xi-shi-yi-iida-za-hui.html"><h1>机器学习拾遗(II):大杂烩</h1></a>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>五 02 五月 2014 </p>

<p style="text-align: left;">
Filed under <a href="./category/notes.html">Notes</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="./tag/machine-learning.html">Machine Learning</a> <a href="./tag/gaussian-models.html">Gaussian Models</a> <a href="./tag/linear-models.html">Linear Models</a> <a href="./tag/notes.html">Notes</a> </p>
<p>
</p>
      </div>
      <div class="summary span8">
	<p>阅读<em>Machine Learning:A Probabilistic Perspective</em>一书关于上述部分的若干章节并对之前未理解的部分内容进行梳理。</p> 
	<a class="btn btn-mini xsmall" href="./ji-qi-xue-xi-shi-yi-iida-za-hui.html">
          <i class="icon-plus-sign"></i> Read More </a>
	<hr />
      </div>
      
    </div>
      

 
        



    <div class='row-fluid'>
      <div class='article-title span9'> 
        <a href="./dang-zui-jin-lin-yu-dao-lsh.html"><h1>当最近邻遇到LSH</h1></a>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>四 01 五月 2014 </p>

<p style="text-align: left;">
Filed under <a href="./category/pearls.html">Pearls</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="./tag/suan-fa.html">算法</a> <a href="./tag/algorithm.html">Algorithm</a> <a href="./tag/lsh.html">LSH</a> <a href="./tag/ju-bu-min-gan-ha-xi-suan-fa.html">局部敏感哈希算法</a> </p>
<p>
</p>
      </div>
      <div class="summary span8">
	<p>本文主要介绍局部敏感哈希算法。</p> 
	<a class="btn btn-mini xsmall" href="./dang-zui-jin-lin-yu-dao-lsh.html">
          <i class="icon-plus-sign"></i> Read More </a>
	<hr />
      </div>
      
    </div>
      
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="./index.html">1</a></li>
    <li class=""><a href="./index2.html">2</a></li>
    <li class=""><a href="./index3.html">3</a></li>
    <li class=""><a href="./index4.html">4</a></li>

    <li class="next"><a href="./index2.html">Next &rarr;</a></li>

</ul>
</div>
 
  
        </div>
        
        
    </div>     </div> </div>

<!--footer-->
<div class="container">
  <div class="well" style="background-color: #E9EFF6">
    <div id="blog-footer">
      <div class="row-fluid">
	<div class="social span2" align="center" id="socialist">
	  <ul class="nav nav-list">
	    <li class="nav-header">
	      Social
	    </li>
	    <li><a href="https://github.com/qingyuanxingsi"><i class="icon-Github" style="color: #1f334b"></i>Github</a></li>

	  </ul>
	</div>
        <div class="links span2" align="center">
          <ul class="nav nav-list">
            <li class="nav-header"> 
              Links
            </li>
            
            <li><a href="http://freemind.pluskid.org">Pluskid</a></li>
            <li><a href="https://github.com/julycoding/The-Art-Of-Programming-By-July">结构之法 算法之道</a></li>
            <li><a href="http://www.nosqlnotes.net/">NOSQL Notes</a></li>
            <li><a href="http://diaorui.net/">数学之美</a></li>
            <li><a href="http://licstar.net/">让博客飞(A BLOG WITH FUN)</a></li>
            <li><a href="http://www.xperseverance.net/blogs/">持之以恒</a></li>
            <li><a href="http://ibillxia.github.io/">Bill's Blog</a></li>
          </ul>
        </div>
	<div class="site-nav span2" align="center">
          <ul class="nav nav-list" id="site-links">
            <li class="nav-header"> 
              Site
            </li>
            <li><a href="."><i class="icon-home" style="color: #1f334b">
                </i>Home</a></li>
            <li><a href="./archives.html"><i class="icon-list" style="color: #1f334b">
                </i>Archives</a></li>
	    <li><a href="./tags.html"><i class="icon-tags" style="color: #1f334b">
                </i>Tags</a></li>
	    
            <li><a href="./" rel="alternate">
                <i class="icon-rss-sign" style="color: #1f334b"></i>
                Atom Feed</a></li>
	  </ul>

        </div>

      </div> <!--end of fluid row-->
    </div> <!--end of blog-footer-->
    <hr />
    <p align="center"><a href=".">苹果的味道</a>
      &copy; qingyuanxingsi
    Powered by <a href="github.com/getpelican/pelican">Pelican</a> and
        <a href="https://twitter.github.com/bootstrap">Twitter Bootstrap</a>. 
        Icons by <a href="http://fortawesome.github.com/Font-Awesome">Font Awesome</a> and 
        <a href="http://gregoryloucas.github.com/Font-Awesome-More">Font Awesome More</a></p>

  </div> <!--end of well -->
</div> <!--end of container -->

<!--/footer-->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
<script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/js/bootstrap.min.js"></script>


<script>var _gaq=[['_setAccount','UA-48582273-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>

</body>
</html>