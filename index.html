<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>苹果的味道</title>
    <meta name="description" content="">
    <meta name="author" content="qingyuanxingsi">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="./theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.1.1/css/bootstrap.no-icons.min.css" rel="stylesheet">
    <link href="./theme/local.css" rel="stylesheet">
    <link href="./theme/pygments.css" rel="stylesheet">
    <link href="./theme/font-awesome.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Gudea:400,400italic|Alegreya+SC' rel='stylesheet' type='text/css'>
</head>

<body>
<header class="blog-header">
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
	<a href="." class="brand">苹果的味道</a>
      </div>

      <div class="span3" id="blog-nav">
	<ul class="nav nav-pills pull-right">
            <li><a href="./pages/about.html">About</a></li>
	    <li >
	      <a href="./category/distributed-system.html ">Distributed System</a>
	    <li >
	      <a href="./category/life.html ">Life</a>
	    <li >
	      <a href="./category/machine-learning.html ">Machine Learning</a>
	    <li >
	      <a href="./category/pearls.html ">Pearls</a>
	    <li >
	      <a href="./category/viewpoint.html ">Viewpoint</a>
	</ul>
      </div>
    </div> <!-- End of fluid row-->
  </div>   <!-- End of Container-->
</header>
    
<div class="container">
    <div class="content">
    <div class="row-fluid">

        <div class="span10">
        

        

    <div class='row-fluid''>
        <div class="article-title span9">
            <a href="./ji-qi-xue-xi-xi-lie-vilatent-linear-models.html"><h1>机器学习系列(VI):Latent Linear Models</h1></a>
        </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>四 24 四月 2014 </p>

<p style="text-align: left;">
Filed under <a href="./category/machine-learning.html">Machine Learning</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="./tag/machine-learning.html">Machine Learning</a> <a href="./tag/latent-linear-models.html">Latent Linear Models</a> <a href="./tag/factor-analysis.html">Factor Analysis</a> <a href="./tag/em.html">EM</a> <a href="./tag/pca.html">PCA</a> <a href="./tag/ica.html">ICA</a> </p>
<p>
</p>
      </div>
      <div class="article-content span8">
	<h1>因子分析</h1>
<hr />
<h2>问题</h2>
<p>当样本个数$m$远远大于其特征个数$n$时，这样不管是进行回归、聚类等都没有太大的问题。然而当训练样例个数$m$太小，甚至$m \ll n$的时候，使用梯度下降法进行回归时，如果初值不同，得到的参数结果会有很大偏差（因为方程数小于参数个数）。另外，如果使用多元高斯分布(Multivariate Gaussian distribution)对数据进行拟合时，也会有问题。让我们来演算一下，看看会有什么问题：</p>
<p>多元高斯分布的参数估计公式如下:</p>
<p>\begin{equation}
\begin{split}
\mu &amp;= {1 \over m}\sum_{i=1}^m x^{(i)} \\
\Sigma &amp;= {1 \over m}\sum_{i=1}^m (x^{(i)}-\mu)(x^{(i)}-\mu)^T
\end{split}
\end{equation}</p>
<p>其中,$x^{(i)}$表示样例，共有$m$个，每个样例$n$个特征，因此$\mu$是$n$维向量，$\Sigma$是$n*n$协方差矩阵。</p>
<p><strong>当$m \ll n$时，我们会发现$\Sigma$是奇异阵($|\Sigma|=0$)</strong>，也就是说$\Sigma^{-1}$不存在，没办法拟合出多元高斯分布了，确切的说是我们估计不出来$\Sigma$。</p>
<p>如果我们仍然想用多元高斯分布来估计样本，那怎么办呢？</p>
<h3>限制协方差矩阵</h3>
<p>当没有足够的数据去估计$\Sigma$时，那么只能对模型参数进行一定假设，之前我们想估计出完全的$\Sigma$(矩阵中的全部元素），现在我们假设$\Sigma$就是对角阵（各特征间相互独立），那么我们只需要计算每个特征的方差即可，最后的$\Sigma$只有对角线上的元素不为0.</p>
<p>\begin{equation}
\Sigma_{jj} = {1 \over m}\sum_{i=1}^m (x_j^{(i)}-\mu_j)^2
\end{equation}</p>
<p>回想我们之前在<strong>Gaussian Models</strong>一文中讨论过的二维多元高斯分布的几何特性，在平面上的投影是个椭圆，中心点由$\mu$决定，椭圆的形状由$\Sigma$决定。$\Sigma$如果变成对角阵，就意味着椭圆的两个轴都和坐标轴平行了。</p>
<p><img alt="Cov Matrix" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/cov_matrix_zps81794a47.jpg" /></p>
<p>如果我们想对$\Sigma$进一步限制的话，可以假设对角线上的元素都是等值的。</p>
<p>\begin{equation}
\Sigma = \sigma^2 I
\end{equation}</p>
<p>其中:</p>
<p>\begin{equation}
\sigma^2 = \frac{1}{mn}\sum_{j=1}^n \sum_{i=1}^m (x_j^{(i)}-\mu_j)^2
\end{equation}</p>
<p>也就是上一步对角线上元素的均值，反映到二维高斯分布图上就是椭圆变成圆。</p>
<p>当我们要估计出完整的$\Sigma$时，我们需要$m&gt;=n+1$才能保证在最大似然估计下得出的$\Sigma$是非奇异的。然而在上面的任何一种假设限定条件下，只要$m&gt;=2$都可以估计出限定的$\Sigma$。</p>
<p>这样做的缺点也是显然易见的，我们认为特征间独立，这个假设太强。接下来，我们给出一种称为因子分析的方法，使用更多的参数来分析特征间的关系，并且不需要计算一个完整的$\Sigma$。</p>
<h2>因子分析</h2>
<p>下面通过一个简单例子，来引出因子分析背后的思想。</p>
<p>因子分析的实质是认为$m$个$n$维特征的训练样例$x^{(i)}$的产生过程如下：</p>
<ul>
<li>首先在一个$k$维的空间中按照多元高斯分布生成$m$个$z^{(i)}$（$k$维向量),即</li>
</ul>
<p>\begin{equation}
z^{(i)} \sim\ N(0,I)
\end{equation}</p>
<ul>
<li>然后存在一个变换矩阵$\Lambda \in R^{n*k}$，将$z^{(i)}$映射到$n$维空间中，即</li>
</ul>
<p>\begin{equation}
\Lambda z^{(i)}
\end{equation}</p>
<p>因为$z^{(i)}$的均值是0，映射后仍然是0。</p>
<ul>
<li>然后将$\Lambda z^{(i)}$加上一个均值$\mu$（$n$维），即</li>
</ul>
<p>\begin{equation}
\mu + \Lambda z^{(i)}
\end{equation}</p>
<p>对应的意义是将变换后的$\Lambda z^{(i)}$（$n$维向量）移动到样本$x^{(i)}$的中心点$\mu$。</p>
<ul>
<li>由于真实样例$x^{(i)}$与上述模型生成的有误差，因此我们继续加上误差$\epsilon$($n$维向量),而且$\epsilon$符合多元高斯分布，即</li>
</ul>
<p>\begin{equation}
\begin{split}
\epsilon \sim\ N(0,\mathbf \Psi) \\
\mu + \Lambda z^{(i)} + \epsilon 
\end{split}
\end{equation}</p>
<ul>
<li>最后的结果认为是真实的训练样例$x^{(i)}$的生成公式</li>
</ul>
<p>\begin{equation}
x^{(i)} = \mu + \Lambda z^{(i)} + \epsilon 
\end{equation}</p>
<p>让我们使用一种直观方法来解释上述过程：</p>
<p>假设我们有$m=5$个2维的样本点$x^{(i)}$（两个特征），如下：</p>
<p><img alt="Original Data" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/original_data_zps9626b246.png" /></p>
<p>那么按照因子分析的理解，样本点的生成过程如下：</p>
<ul>
<li>我们首先认为在1维空间（这里$k=1$），存在着按正态分布生成的$m$个点$z^{(i)}$，如下:</li>
</ul>
<p><img alt="Demo_1" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_1_zps3e189697.png" /></p>
<p>均值为0，方差为1。</p>
<ul>
<li>然后使用某个$\Lambda=(a,b)^T$将一维的$z$映射到2维，图形表示如下：</li>
</ul>
<p><img alt="Demo_2" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_2_zps166db3d8.png" /></p>
<ul>
<li>之后加上$\mu\ (\mu_1,\mu_2)^T$，即将所有点的横坐标移动$\mu_1$，纵坐标移动$\mu_2$，将直线移到一个位置，使得直线过点$\mu$，原始左边轴的原点现在为$\mu$(红色点)。</li>
</ul>
<p><img alt="Demo_3" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_3_zps178e1a6a.png" /></p>
<p>然而，样本点不可能这么规则，在模型上会有一定偏差，因此我们需要将上步生成的点做一些扰动（误差），扰动$\epsilon \sim\ N(0,\mathbf \Psi)$。</p>
<ul>
<li>加入扰动后，我们得到黑色样本$x^{(i)}$如下：</li>
</ul>
<p><img alt="Demo_4" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/demo_4_zps0cc40309.png" /></p>
<ul>
<li>其中由于$z$和$\epsilon$的均值都为0，因此$\mu$也是原始样本点（黑色点）的均值。</li>
</ul>
<p>由以上的直观分析，我们知道了<strong>因子分析其实就是认为高维样本点实际上是由低维样本点经过高斯分布、线性变换、误差扰动生成的，因此高维数据可以使用低维来表示</strong>。</p>
<h2>因子分析模型</h2>
<p>上面的过程是从隐含随机变量$z$经过变换和误差扰动来得到观测到的样本点。其中$z$被称为因子，是低维的。</p>
<p>我们将式子再列一遍如下：</p>
<p>\begin{equation}
\begin{split}
z &amp;\sim\ N(0,I)   \\
\epsilon &amp;\sim\ N(0,\mathbf \Psi) \\
x &amp;= \mu + \Lambda z + \epsilon
\end{split}
\end{equation}</p>
<p>其中误差$\epsilon$和$z$是独立的。</p>
<p>下面使用的因子分析表示方法是矩阵表示法，在参考资料中给出了一些其他的表示方法，如果不明白矩阵表示法，可以参考其他资料。</p>
<p>矩阵表示法认为$z$和$x$联合符合多元高斯分布，如下:</p>
<p>\begin{equation}
\left[
\begin{array}{cc}
z \\
x
\end{array}
\right]\sim\ N(\mu_{zx},\Sigma)
\end{equation}</p>
<p>求$\mu_{zx}$之前需要求$E[x]$</p>
<p>\begin{equation}
\begin{split}
E[x] &amp;= E[\mu + \Lambda z + \epsilon] \\
&amp;= \mu + \Lambda E[z] + E[\epsilon] \\
&amp;= \mu
\end{split}
\end{equation}</p>
<p>我们已知$E[z]=0$，因此:</p>
<p>\begin{equation}
\mu_{zx} = \left[
\begin{array}{cc}
0 \\
\mu
\end{array}
\right]
\end{equation}</p>
<p>下一步是计算$\Sigma$，其中$\Sigma_{zz} = Cov(z) = I$.接着求$\Sigma_{zx}$</p>
<p>\begin{equation}
\begin{split}
E[(z-E[z])(x-E[x])^T] &amp;= E[z(\mu + \Lambda z + \epsilon - \mu)^T]   \\
&amp;= E[zz^T]\Lambda^T + E[z\epsilon^T] \\
&amp;= \Lambda^T
\end{split}
\end{equation}</p>
<p>这个过程中利用了$z$和$\epsilon$独立假设($E[z\epsilon^T] = E[z]E[\epsilon^T]=0$)。并将$\Lambda$看作已知变量。</p>
<p>接着求$\Sigma_{xx}$</p>
<p>\begin{equation}
\begin{split}
E[(x-E(x))(x-E[x])^T] &amp;= E[(\mu+\Lambda z+\epsilon -\mu)(\mu+\Lambda z+\epsilon -\mu)^T]  \\
&amp;= E[\Lambda zz^T \Lambda^T+ \epsilon z^T \Lambda^T+\Lambda z \epsilon^T+\epsilon \epsilon^T] \\
&amp;= \Lambda E[zz^T] \Lambda^T + E[\epsilon \epsilon^T]  \\
&amp;= \Lambda \Lambda^T + \mathbf \Psi
\end{split}
\end{equation}</p>
<p>然后得出联合分布的最终形式:</p>
<p>\begin{equation}
\left[
\begin{array}{cc}
z \\
x 
\end{array}
\right] \sim\
N\left(
\left[
\begin{array}{cc}
0 \\
\mu
\end{array}
\right],
\left[
\begin{array}{cc}
I &amp; \Lambda^T \\
\Lambda &amp; \Lambda \Lambda^T + \mathbf \Psi
\end{array}
\right]
\right)
\end{equation}</p>
<p>从上式中可以看出$x$的边缘分布为$x \sim\ N(\mu,\Lambda \Lambda^T + \mathbf \Psi)$.</p>
<p>那么对样本${x^{(i);i=1,...,m}}$进行最大似然估计</p>
<p>\begin{equation}
\ell(\mu,\Lambda,\mathbf \Psi) = log \prod_{i=1}^m \frac{1}{(2\pi)^{n/2} |\Lambda \Lambda^T+\mathbf \Psi|}exp(-{1 \over 2}(x^{(i)}-\mu)^T(\Lambda \Lambda^T + \mathbf \Psi)^{-1} (x^{(i)}-\mu))
\end{equation}</p>
<p>然后对各个参数求偏导数不就得到各个参数的值了么？可惜我们得不到closed-form。想想也是，如果能得到，还干嘛将$z$和$x$放在一起求联合分布呢。根据之前对参数估计的理解，在有隐含变量$z$时，我们可以考虑使用EM来进行估计。</p>
<h2>因子分析的EM估计</h2>
<h1>参考文献</h1>
<hr />
<ol>
<li><a href="http://www.cnblogs.com/jerrylead/archive/2011/05/11/2043317.html">因子分析（Factor Analysis)</a></li>
</ol><script type= "text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>
 
	<a class="btn btn-mini xsmall" href="./ji-qi-xue-xi-xi-lie-vilatent-linear-models.html">
          <i class="icon-comment"></i> Comment </a>
	<hr />
      </div>
      
    </div>
    

 
        



    <div class='row-fluid'>
      <div class='article-title span9'> 
        <a href="./xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html"><h1>小小收藏夹[持续更新中]</h1></a>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>四 24 四月 2014 </p>

<p style="text-align: left;">
Filed under <a href="./category/pearls.html">Pearls</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="./tag/suan-fa.html">算法</a> <a href="./tag/fun.html">Fun</a> <a href="./tag/staff.html">Staff</a> <a href="./tag/shou-cang-jia.html">收藏夹</a> <a href="./tag/bloom-filter.html">Bloom Filter</a> <a href="./tag/b-trees.html">B Trees</a> <a href="./tag/data-structure.html">Data Structure</a> <a href="./tag/algorithm.html">Algorithm</a> <a href="./tag/pgm.html">PGM</a> </p>
<p>
</p>
      </div>
      <div class="summary span8">
	<p>主要收集一些还不错的东东(持续更新)。</p> 
	<a class="btn btn-mini xsmall" href="./xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html">
          <i class="icon-plus-sign"></i> Read More </a>
	<hr />
      </div>
      
    </div>
      

 
        



    <div class='row-fluid'>
      <div class='article-title span9'> 
        <a href="./ji-qi-xue-xi-xi-lie-v-generalized-linear-models-and-the-exponential-family.html"><h1>机器学习系列(V): Generalized linear models and the exponential family</h1></a>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>三 23 四月 2014 </p>

<p style="text-align: left;">
Filed under <a href="./category/machine-learning.html">Machine Learning</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="./tag/machine-learning.html">Machine Learning</a> <a href="./tag/exponential-family.html">Exponential Family</a> <a href="./tag/generalized-linear-models.html">Generalized Linear Models</a> </p>
<p>
</p>
      </div>
      <div class="summary span8">
	<p>本文主要介绍广义线性模型以及指数分布族。</p> 
	<a class="btn btn-mini xsmall" href="./ji-qi-xue-xi-xi-lie-v-generalized-linear-models-and-the-exponential-family.html">
          <i class="icon-plus-sign"></i> Read More </a>
	<hr />
      </div>
      
    </div>
      

 
        



    <div class='row-fluid'>
      <div class='article-title span9'> 
        <a href="./play-with-cardinality-estimation.html"><h1>Play With Cardinality Estimation</h1></a>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>一 14 四月 2014 </p>

<p style="text-align: left;">
Filed under <a href="./category/pearls.html">Pearls</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="./tag/ji-shu-gu-ji.html">基数估计</a> <a href="./tag/cardinality-estimation.html">Cardinality Estimation</a> <a href="./tag/big-data.html">Big Data</a> </p>
<p>
</p>
      </div>
      <div class="summary span8">
	<p>本文主要简介一下基数估计的基础知识并简介几种Cadinality Estimation算法。</p> 
	<a class="btn btn-mini xsmall" href="./play-with-cardinality-estimation.html">
          <i class="icon-plus-sign"></i> Read More </a>
	<hr />
      </div>
      
    </div>
      

 
        



    <div class='row-fluid'>
      <div class='article-title span9'> 
        <a href="./ji-qi-xue-xi-wai-chuan-zhi-deep-learningisparse-autoencoder.html"><h1>机器学习外传之Deep Learning(I):Sparse Autoencoder</h1></a>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>日 13 四月 2014 </p>

<p style="text-align: left;">
Filed under <a href="./category/machine-learning.html">Machine Learning</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="./tag/deep-learning.html">Deep Learning</a> <a href="./tag/machine-learning.html">Machine Learning</a> <a href="./tag/back-propagation.html">Back Propagation</a> <a href="./tag/bp-neural-network.html">BP Neural Network</a> <a href="./tag/sparse-autoencoder.html">Sparse Autoencoder</a> </p>
<p>
</p>
      </div>
      <div class="summary span8">
	<p>本文简要介绍一下Deep Learning是什么以及其基本使用场景;在此基础上,我们会简要介绍一下BP神经网络以及Back Propagation反向传播算法。接下来在充分理解Back Propagation的基础之上,我们介绍一下Sparse Autoencoder稀疏自编码器。</p> 
	<a class="btn btn-mini xsmall" href="./ji-qi-xue-xi-wai-chuan-zhi-deep-learningisparse-autoencoder.html">
          <i class="icon-plus-sign"></i> Read More </a>
	<hr />
      </div>
      
    </div>
      
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="./index.html">1</a></li>
    <li class=""><a href="./index2.html">2</a></li>
    <li class=""><a href="./index3.html">3</a></li>

    <li class="next"><a href="./index2.html">Next &rarr;</a></li>

</ul>
</div>
 
  
        </div>
        
        
    </div>     </div> </div>

<!--footer-->
<div class="container">
  <div class="well" style="background-color: #E9EFF6">
    <div id="blog-footer">
      <div class="row-fluid">
	<div class="social span2" align="center" id="socialist">
	  <ul class="nav nav-list">
	    <li class="nav-header">
	      Social
	    </li>
	    <li><a href="https://github.com/qingyuanxingsi"><i class="icon-Github" style="color: #1f334b"></i>Github</a></li>

	  </ul>
	</div>
        <div class="links span2" align="center">
          <ul class="nav nav-list">
            <li class="nav-header"> 
              Links
            </li>
            
            <li><a href="http://freemind.pluskid.org">Pluskid</a></li>
            <li><a href="https://github.com/julycoding/The-Art-Of-Programming-By-July">结构之法 算法之道</a></li>
            <li><a href="http://www.nosqlnotes.net/">NOSQL Notes</a></li>
            <li><a href="http://diaorui.net/">数学之美</a></li>
            <li><a href="http://licstar.net/">让博客飞(A BLOG WITH FUN)</a></li>
            <li><a href="http://www.xperseverance.net/blogs/">持之以恒</a></li>
            <li><a href="http://ibillxia.github.io/">Bill's Blog</a></li>
          </ul>
        </div>
	<div class="site-nav span2" align="center">
          <ul class="nav nav-list" id="site-links">
            <li class="nav-header"> 
              Site
            </li>
            <li><a href="."><i class="icon-home" style="color: #1f334b">
                </i>Home</a></li>
            <li><a href="./archives.html"><i class="icon-list" style="color: #1f334b">
                </i>Archives</a></li>
	    <li><a href="./tags.html"><i class="icon-tags" style="color: #1f334b">
                </i>Tags</a></li>
	    
            <li><a href="./" rel="alternate">
                <i class="icon-rss-sign" style="color: #1f334b"></i>
                Atom Feed</a></li>
	  </ul>

        </div>

      </div> <!--end of fluid row-->
    </div> <!--end of blog-footer-->
    <hr />
    <p align="center"><a href=".">苹果的味道</a>
      &copy; qingyuanxingsi
    Powered by <a href="github.com/getpelican/pelican">Pelican</a> and
        <a href="https://twitter.github.com/bootstrap">Twitter Bootstrap</a>. 
        Icons by <a href="http://fortawesome.github.com/Font-Awesome">Font Awesome</a> and 
        <a href="http://gregoryloucas.github.com/Font-Awesome-More">Font Awesome More</a></p>

  </div> <!--end of well -->
</div> <!--end of container -->

<!--/footer-->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
<script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/js/bootstrap.min.js"></script>


<script>var _gaq=[['_setAccount','UA-48582273-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>

</body>
</html>