<!DOCTYPE html>
<html lang="en">
<head>
	<title>机器学习系列(V):聚类大观园后记 | 苹果的味道</title>
	<meta charset="utf-8" />
    <meta name="author" content="qingyuanxingsi">
	<link rel="stylesheet" href="../theme/css/franticworld.css" type="text/css" />
	<link href="../theme/css/pygments.css" rel="stylesheet">
</head>
<body background="../theme/img/pattern.png">
		<div class="nav-banner">
		<a href="..">苹果的味道</a>
		</div>
		
		<div class="content">

<div class="metabox">
	<p class="metaday">102</p>
	<p class="metayear">2014</p>
	<p class="metacategory">Machine Learning</p>
</div>
<div class="arcticlecontentbox">
	<div class="articlecontent">
		<a class="articletitle" href="/ji-qi-xue-xi-xi-lie-vju-lei-da-guan-yuan-hou-ji.html " >机器学习系列(V):聚类大观园后记</a>
		<p>在上一篇<a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-ivju-lei-da-guan-yuan.html">机器学习系列(IV):聚类大观园</a>中,我们主要介绍了一些聚类的基本概念以及一些常用的聚类算法,本文中我们主要介绍一下Dirichlet Process(有了这个工具之后,不再由人工选取聚类数目$K$值就成为了可能,想想就各种高端大气上档次啊),此外我们对基于特征的聚类方法做一个简要的介绍。</p>
<h1 id="dirichlet-distribution">Dirichlet Distribution</h1>
<hr />
<p>在正式介绍Dirichlet Process之前,我想我们有必要了解一下Dirichlet分布的基础知识。</p>
<p>Dirichlet分布可以看做是分布之上的分布。如何理解这句话，我们可以先举个例子：假设我们有一个骰子，其有六面，分别为<em>{1,2,3,4,5,6}</em>。现在我们做了10000次投掷的实验，得到的实验结果是六面分别出现了<em>{2000,2000,2000,2000,1000,1000}</em>次，如果用每一面出现的次数与试验总数的比值估计这个面出现的概率，则我们得到六面出现的概率，分别为<em>{0.2,0.2,0.2,0.2,0.1,0.1}</em>。现在，我们还不满足，我们想要做10000次试验，每次试验中我们都投掷骰子10000次。我们想知道，出现这样的情况使得我们认为，骰子六面出现概率为<em>{0.2,0.2,0.2,0.2,0.1,0.1}</em>的概率是多少（说不定下次试验统计得到的概率为<em>{0.1, 0.1, 0.2, 0.2, 0.2, 0.2}</em>这样了）。这样我们就在思考骰子六面出现概率分布这样的分布之上的分布。而这样一个分布就是Dirichlet分布。</p>
<p>我们之前在<a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html">机器学习系列(II):Generative models for discrete data</a>一篇中曾经提到过Conjugate Prior(共轭先验分布）的概念,而采用和Likelihood相对应的共轭先验分布能在很大程度上简化计算，因此在计算Posterior时,共轭先验分布被广泛采用。同时,我们在那一篇里也提到其实贝塔分布是二项分布的共轭先验分布，而实际上<strong>狄利克雷分布则是多项分布的共轭先验分布</strong>。(<code>这个世界很奇妙,对吧?!!!</code>)</p>
<p>下面正式进入狄利克雷分布介绍，首先说一下这个多项分布的参数$\mu$。在二项分布里，参数$\mu$就是抛硬币取某一面的概率，因为伯努利分布的状态空间只有<em>{0,1}</em>。但是在多项分布里，因为状态空间有$K$个取值，因此$\mu$变成了向量$\vec{\mu} = (\mu_1,...,\mu_k)^T$。多项分布的likelihood函数形式是$\prod\limits_{k=1}^{K}\mu_k^{m_k}$，因此就像选择二项分布的共轭先验贝塔函数时那样，狄利克雷分布的函数形式应该如下：</p>
<p>\begin{equation}
p(\mu|\alpha)\propto\prod\limits_{k=1}^{K}\mu_k^{\alpha_k-1}
\end{equation}</p>
<p>上式中，$\sum_k\mu_k=1$,$\vec{\alpha}=(\alpha_1,~…,~\alpha_k)^T$是狄利克雷分布的参数。最后把上式归一化成为真正的狄利克雷分布：</p>
<p>\begin{equation}
Dir(\mu|\alpha)=\frac{\Gamma(\alpha_0)}{\Gamma(\alpha_1)…\Gamma(\alpha_k)}\prod\limits_{k=1}^{K}\mu_k^{\alpha_k-1}
\end{equation}</p>
<p>其中$\alpha_0=\sum\limits_{k=1}^{K}\alpha_k$。这个函数跟贝塔分布有点像（取$K=2$时就是Beta分布）。跟多项分布也有点像。就像Beta分布那样，狄利克雷分布就是它所对应的后验多项分布的参数$\vec{\mu}$ 的分布，只不过$\mu$是一个向量，下图是当$\vec{\mu}=(\mu_1,\mu_2,\mu_3$)时，即只有三个值时狄利克雷概率密度函数的例子。其中中间那个图的三角形表示一个平放的Simplex，三角形三个顶点分别表示$\vec{\mu} =(1,0,0)$,$\vec{\mu} =(0,1,0)$和$\vec{\mu}=(0,0,1)$，因此三角形中间部分的任意一个点就是$\vec{\mu}$的一个取值，纵轴就是这个$\vec{\mu}$的Simplex上的概率密度值(PDF)。</p>
<p><img alt="Dirichlet Distribution Example" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/PRML_Dir_Distr_Example_zpsa5d1cf1e.png" /></p>
<p>对于参数$\vec{\mu}$的估计时,由<strong>Posterior=Prior*Likelihood</strong>知,</p>
<p>\begin{equation}
p(\mathbf{\mu}|D,\mathbf{\alpha})\propto(D|\mathbf{\mu})p(\mathbf{\mu}|\mathbf{\alpha})\propto\prod\limits_{k=1}^{K}\mu_k^{\alpha_k+m_k-1}
\end{equation}</p>
<p>从这个形式可以看出，后验也是狄利克雷分布。类似于贝塔分布归一化后验的方法，我们把这个后验归一化一下，得到:</p>
<p>\begin{equation}
\begin{split}
p(\mathbf{\mu}|D,\mathbf{\alpha}) &amp;=Dir(\mathbf{\mu}|\mathbf{\alpha}+\mathbf{m}) \\
&amp;=\frac{\Gamma(\alpha_0+N)}{\Gamma(\alpha_1+m_1)…\Gamma(\alpha_K+m_K)}\prod\limits_{k=1}^{K}\mu_k^{\alpha_k+m_k-1}
\end{split}
\end{equation}</p>
<h1 id="_1">参考文献</h1>
<hr />
<ol>
<li><a href="http://www.xperseverance.net/blogs/2012/03/510/">The Dirichlet Distribution 狄利克雷分布 (PRML 2.2.1)</a></li>
<li>Machine Learning:A Probabilistic Perspective(<em>Chapter 25</em>)</li>
</ol><script type= "text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>

	</div>
	
	<div class="comments">
	<div id="disqus_thread"></div>
	<script type="text/javascript">
	   var disqus_identifier = "ji-qi-xue-xi-xi-lie-vju-lei-da-guan-yuan-hou-ji.html";
	   (function() {
			var dsq = document.createElement('script');
			dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = 'http://qingyuanxingsi.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] ||
			 document.getElementsByTagName('body')[0]).appendChild(dsq);
	  })();
	</script>
	</div>
	       
</div>

		</div>
		
		<div class="sidebar">
<div class="sidebarcategory">
<h4>Categories</h4>
	<li><a href="../category/distributed-system.html">Distributed System</a></li>
	<li><a href="../category/life.html">Life</a></li>
	<li><a href="../category/machine-learning.html">Machine Learning</a></li>
	<li><a href="../category/notes.html">Notes</a></li>
	<li><a href="../category/pearls.html">Pearls</a></li>
	<li><a href="../category/viewpoint.html">Viewpoint</a></li>
</div>

<div class="sidebarpages">
	<h4>Pages</h4>
		<li><a href="../pages/about.html">About</a></li>
</div>

<div class="sidebarblogroll">
<h4>Blogroll</h4>
    <li><a href="http://freemind.pluskid.org" target="_blank">Pluskid</a></li>
    <li><a href="https://github.com/julycoding/The-Art-Of-Programming-By-July" target="_blank">结构之法 算法之道</a></li>
    <li><a href="http://www.nosqlnotes.net/" target="_blank">NOSQL Notes</a></li>
    <li><a href="http://diaorui.net/" target="_blank">数学之美</a></li>
    <li><a href="http://licstar.net/" target="_blank">让博客飞(A BLOG WITH FUN)</a></li>
    <li><a href="http://www.xperseverance.net/blogs/" target="_blank">持之以恒</a></li>
    <li><a href="http://ibillxia.github.io/" target="_blank">Bill's Blog</a></li>
    <li><a href="http://malagis.com/" target="_blank">麻辣GIS</a></li>
    <li><a href="http://skyoung.github.io" target="_blank">Skyoung</a></li>
</div>

<div class="sidebarsocial">
<h4>Social</h4>
    <li><a href="https://github.com/qingyuanxingsi">Github</a></li>
</div>		</div>
		
		<footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>,
				Theme by <a href="http://frantic1048.com/">Frantic1048</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>