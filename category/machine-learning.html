<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Doodle World</title>
    <meta name="description" content="">
    <meta name="author" content="qingyuanxingsi">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="/theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.1.1/css/bootstrap.no-icons.min.css" rel="stylesheet">
    <link href="/theme/local.css" rel="stylesheet">
    <link href="/theme/pygments.css" rel="stylesheet">
    <link href="/theme/font-awesome.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Gudea:400,400italic|Alegreya+SC' rel='stylesheet' type='text/css'>
</head>

<body>
<header class="blog-header">
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
	<a href="" class="brand">Doodle World</a>
      </div>

      <div class="span3" id="blog-nav">
	<ul class="nav nav-pills pull-right">
            <li><a href="/pages/about.html">About</a></li>
	    <li >
	      <a href="/category/distributed-system.html ">Distributed System</a>
	    <li >
	      <a href="/category/life.html ">Life</a>
	    <li  class="active" >
	      <a href="/category/machine-learning.html ">Machine Learning</a>
	    <li >
	      <a href="/category/viewpoint.html ">Viewpoint</a>
	</ul>
      </div>
    </div> <!-- End of fluid row-->
  </div>   <!-- End of Container-->
</header>
    
<div class="container">
    <div class="content">
    <div class="row-fluid">

        <div class="span10">
        

        

    <div class='row-fluid''>
        <div class="article-title span9">
            <a href="/ji-qi-xue-xi-xi-lie-iiigaussian-models.html"><h1>机器学习系列(III):Gaussian Models</h1></a>
        </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>六 15 三月 2014 </p>

<p style="text-align: left;">
Filed under <a href="/category/machine-learning.html">Machine Learning</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="/tag/machine-learning.html">Machine Learning</a> <a href="/tag/gaussian-models.html">Gaussian Models</a> <a href="/tag/generative-models.html">Generative Models</a> </p>
<p>
</p>
      </div>
      <div class="article-content span8">
	<p>在<a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html">上一篇</a>中我们着重介绍了对于离散数据的生成模型，紧接上一篇，本篇我们介绍对于离散数据的生成模型。好吧,废话我们就不多说了,直接进入正文。</p>
<h1>MLE for MVN</h1>
<hr>
<h2>Basics about MVN</h2>
<p>谈到离散分布,我们很自然地就会想到高斯分布,从小学到现在，印象中第一个走入我脑海中的看着比较高端大气上档次的就是Gaussian分布了。这次我们的重点便完全集中在Gaussian分布之上了,在正式讨论之前，我们先介绍一些关于Gaussian分布的基础知识吧。</p>
<p>在$D$维空间中,MVN(Multivariate Normal)多变量正态分布的概率分布函数具有如下形式:</p>
<p>\begin{equation}
N(x|\mu,\Sigma) \triangleq \frac{1}{(2\pi)^{D/2}det(\Sigma)^{1/2}} exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]
\end{equation}</p>
<p>上式中的指数部分是$x$与$\mu$之间的<a href="http://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis距离</a>。为了更好地理解这个量,我们对$\Sigma$做特征值分解,即$\Sigma = U \Lambda U^T$,其中$U$是一正交阵,满足$U^TU=I$,而$\Lambda$是特征值矩阵。</p>
<p>通过特征值分解,我们有:</p>
<p>\begin{equation}
\Sigma^{-1} = U^{-T}\Lambda^{-1}U^{-1} = U\Lambda^{-1}U^T = \sum_{i=1}^{D}\frac{1}{\lambda_i}u_iu_i^T
\end{equation}</p>
<p>其中,$u_i$是$U$的第$i$列。因此Mahalanobis距离可被改写为:</p>
<p>\begin{equation}
(x-\mu)^T\Sigma^{-1}(x-\mu) = (x-\mu)^T (\sum_{i=1}^{D}\frac{1}{\lambda_i}u_iu_i^T) (x-\mu) 
                            = \sum_{i=1}^{D} \frac{1}{\lambda_i}(x-\mu)^T u_iu_i^T (x-\mu) 
                            = \sum_{i=1}^{D} \frac{y_i^2}{x_i}
\end{equation}</p>
<p>其中,$y_i \triangleq u_i^T(x-\mu)$。另2维空间中的椭圆方程为:</p>
<p>$$\frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2} = 1$$</p>
<p>因此我们可知Gaussian概率密度的等高线沿着椭圆分布,特征向量决定椭圆的朝向,而特征值则决定椭圆有多<code>“椭”</code>。一般来说，如果我们将坐标系移动$\mu$,然后按$U$旋转，此时的欧拉距离即为Mahalanobis距离。</p>
<h2>MLE for MVN</h2>
<p>以下我们给出MVN参数的MLE(极大似然估计)的证明:</p>
<blockquote>
<p>Theorem 3.1 若我们获取的$N$个独立同分布的样本$x_i \sim\ N(x|\mu,\Sigma)$,则关于$\mu$以及$\Sigma$的极大似然分布如下:
  \begin{equation}
  \hat{\mu}<em mle="mle">{mle} = \frac{1}{N}\sum x_i \triangleq \bar{x}
  \end{equation}
  \begin{equation}
  \hat{\Sigma}</em> = \frac{1}{N} \sum (x_i-\bar{x})(x_i-\bar{x})^T = \frac{1}{N}(\sum_{i=1}^{N}x_ix_i^T) - \bar{x}\bar{x}^T
  \end{equation}</p>
</blockquote>
<p>我们不加证明地给出如下公式组:</p>
<blockquote>
<p>\begin{equation}
  \frac{\partial(b^Ta)}{\partial a} = b  \
  \frac{\partial(a^TAa)}{\partial a} = (A+A^T)a \
  \frac{\partial}{\partial A} tr(BA) = B^T  \
  \frac{\partial}{\partial A} log |A| = A^{-T} \
  tr(ABC) = tr(CAB) = tr(BCA)
  \end{equation}</p>
</blockquote>
<p>最后一个等式称为迹的循环置换性质(cyclic permutation property)。利用这个性质,我们使用<code>trace trick</code>可以得到下式:</p>
<p>$$x^TAx = tr(x^TAx) = tr(xx^TA) = tr(Axx^T)$$</p>
<p>证明:</p>
<p>对数似然函数为:</p>
<p>\begin{equation}
l(\mu.\Sigma) = log p(D|\mu,\Sigma) = \frac{N}{2} log |\Lambda| - \frac{1}{2} \sum_{i=1}^{N} (x_i-\mu)^T \Lambda (x_i-\mu)
\end{equation}</p>
<p>其中，$\Lambda = \Sigma^{-1}$为精度矩阵。另$y_i=x_i-\mu$并利用链式法则有:</p>
<p>\begin{equation}
\frac{\partial}{\partial \mu}(x_i-\mu)^T \Sigma^{-1} (x_i-\mu) = \frac {\partial}{\partial y_i} y_i^T \Sigma^{-1} y_i \frac{\partial y_i}{\partial \mu}=-(\Sigma^{-T}+\Sigma^{-1})y_i
\end{equation}</p>
<p>即:</p>
<p>\begin{equation}
\frac{\partial}{\partial \mu} l(\mu,\Sigma) = -\frac{1}{2} \sum_{i=1}^{N} -2\Sigma^{-1}(x_i-\mu) = 0
\end{equation}</p>
<p>故有:</p>
<p>\begin{equation}
\hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} x_i = \bar{x}
\end{equation}</p>
<p>即最大似然均值即为经验均值。</p>
<p>利用trace trick我们重写对数似然函数为:</p>
<p>\begin{equation}
l(\Lambda) = \frac{N}{2} log |\Lambda| - \frac{1}{2} \sum_{i} tr[(x_i-\mu)(x_i-\mu)^T \Lambda]
           = \frac{N}{2} log |\Lambda| - \frac{1}{2} tr[S_u\Lambda]
\end{equation}</p>
<p>其中，$S_u \triangleq \sum_{i=1}^{N} (x_i-\mu)(x_i-\mu)^T$,业界尊称其为分散度矩阵(<code>Scatter Matrix</code>),以后我们聊LDA的时候会再次碰到。对$\Lambda$求偏导有:</p>
<p>\begin{equation}
\frac{\partial l(\Lambda)}{\partial \Lambda} = \frac{N}{2}\Lambda^{-T} - \frac{1}{2} S_u^{T} = 0
\end{equation}</p>
<p>得:</p>
<p>\begin{equation}
\hat{\Sigma} = \frac{1}{N} \sum_{i=1}^{N} (x_i-\mu)(x_i-\mu)^T
\end{equation}</p>
<p>证毕。</p>
<h1>Gaussian Discriminant Analysis</h1>
<hr>
<p>我们在上一篇中提到了Naive Bayes方法,其实质无非是估计在每一类下特定的样本出现的概率，进而我们可以把该特定样本分配给概率值最大的那个类。而对于连续数据而言，其实质其实也是一样的，每一个MVN(我们可以看做一类或者一个Component)都可能生成一些数据，我们估计在每一个Component下生成特定样本的概率，然后把该特定样本分配给概率值最大的那个Component即可。即我们可以定义如下的条件分布:</p>
<p>\begin{equation}
p(x|y=c,\theta) = N(x|\mu_c,\Sigma_c)
\end{equation}</p>
<p>上述模型即为高斯判别分析(Gaussian Discriminant Analysis,GDA)(<code>注意,该模型为生成模型，而不是判别模型</code>)。如果$\Sigma_c$是对角阵，即所有的特征都是独立的时，该模型等同于Naive Bayes.</p>
<h2>QDA</h2>
<p>在上式中带入高斯密度函数的定义，则有:</p>
<p>\begin{equation}
p(y=c|x,\theta) = \frac{\pi_c |2\pi\Sigma_c|^{-1 \over 2}exp[-{1 \over 2}(x-\mu_c)^T\Sigma_C^{-1}(x-\mu_c)]}{\sum_{c\prime}\pi_{c\prime} |2\pi\Sigma_{c\prime}|^{-1 \over 2}exp[-{1 \over 2}(x-\mu_{c\prime})^T\Sigma_{c\prime}^{-1}(x-\mu_{c\prime})}
\end{equation}</p>
<p>上式中$\pi$为各个Component的先验概率分布。根据上式得到的模型则称为Quadratic Discriminant Analysis(QDA).以下给出在2类以及2类情形下可能的决策边界形状,如下图所示:</p>
<p><img alt="Decision Boundary" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/decision_boundary_zps289df588.jpeg"></p>
<h2>Linear Discriminant Analysis(LDA)</h2>
<p>当各个Gaussian Component的协方差矩阵相同时，我们进一步有:</p>
<p>\begin{equation}
p(y=c|x,\theta) \propto \pi_c exp[\mu_c^T\Sigma^{-1}x-{1 \over 2}x^T\Sigma^{-1}x-{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c]
= exp[\mu_c^T\Sigma^{-1}x-{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c]exp[-{1 \over 2}x^T\Sigma^{-1}x]
\end{equation}</p>
<p>上式中$exp[-{1 \over 2}x^T\Sigma^{-1}x]$是独立于$c$的，分子分母相除抵消到此项。</p>
<p>令:</p>
<p>\begin{equation}
\gamma_c = -{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c
\end{equation}
\begin{equation}
\beta_c = \Sigma^{-1}\mu_c
\end{equation}</p>
<p>于是有:</p>
<p>\begin{equation}
p(y=c|x,\theta) = \frac{e^{\beta_c^Tx+\gamma_c}}{\sum_{c\prime}e^{\beta_{c\prime}^Tx+\gamma_{c\prime}}}=S(\eta)_c
\end{equation}</p>
<p>其中$\eta = [\beta_1^Tx+\gamma_1,...,\beta_C^Tx+\gamma_c]$,$S$为softmax函数(类似于max函数,故得此名),定义如下:</p>
<p>\begin{equation}
S(\eta)<em c_prime="c\prime">c = \frac{e^{\eta_c}}{\sum e^{\eta</em>}}
\end{equation}</p>
<p>若将$\eta_c$除以一个常数(temperature),当$T\to 0$时，我们有:</p>
<p><img alt="Boltzmann Distribution" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/boltzman_distribution_zpsa00cdbf0.jpeg"></p>
<p>换句话说，当温度很低时，分布集中在概率最大的那个状态上，而当温度高时，所有的状态呈现均匀分布。</p>
<blockquote>
<p>NOTE:该术语来自于统计物理学，在统计物理学中，人们更倾向于使用波尔兹曼分布（Boltzmann distribution）一词。</p>
</blockquote>
<p>关于<code>式18</code>有一个有趣的性质，即对该式取log,我们则会得到一个关于$x$的线性方程。因此对于任意两类之间的决策边界将会是一条直线，据此该模型也被称为线性判别分析(Linear Discriminant Analysis,LDA)。而且对于二分类问题，我们可以得到:</p>
<p>\begin{equation}
p(y=c|x,\theta) = p(y=c\prime|x,\theta)
\end{equation}</p>
<p>\begin{equation}
\beta_c^Tx+\gamma_c = \beta_{c\prime}^Tx+\gamma_{c\prime}
\end{equation}</p>
<p>\begin{equation}
x^T(\beta_{c\prime}-\beta_c) = \gamma_{c\prime}-\gamma_c
\end{equation}</p>
<h2>Two-class LDA</h2>
<p>为了加深对以上等式的理解，对于二分的情况，我们做如下说明:</p>
<p>\begin{equation}
p(y=1|x,\theta) = \frac{e^{\beta_1^Tx+\gamma_1}}{e^{\beta_1^Tx+\gamma_1}+e^{\beta_0^Tx+\gamma_0}} = \frac{1}{1+e^{(\beta_0-\beta_1)^Tx+(\gamma_0-\gamma_1)}}=sigm((\beta_1-\beta_0)^Tx+(\gamma_1-\gamma_0))
\end{equation}</p>
<p>其中,$sigm(\eta)$代表<a href="http://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid函数</a>。</p>
<p>现有:</p>
<p>\begin{equation}
\gamma_1-\gamma_0 = -{1 \over 2}\mu_1^T\Sigma^{-1}\mu_1+{1 \over 2}\mu_0^T\Sigma^{-1}\mu_0+log(\pi_1/\pi_0)=-{1 \over 2}(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1+\mu_0)+log(\pi_1/\pi_0)
\end{equation}</p>
<p>因此若我们另:</p>
<p>\begin{equation}
\omega = \beta_1-\beta_0 = \Sigma^{-1}(\mu_1-\mu_0)
\end{equation}</p>
<p>\begin{equation}
x_0 = {1 \over 2}(\mu_1+\mu_0)-(\mu_1-\mu_0)\frac{log(\pi_1/\pi_0)}{(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0)}
\end{equation}</p>
<p>则有$\omega^Tx_0 = -(\gamma_1-\gamma_0)$,即:</p>
<p>\begin{equation}
p(y=1|x,\theta) = sigm(\omega^T(x-x_0))
\end{equation}</p>
<p>因此最后的决策规则很简单:将$x$平移$x_0$,然后投影到$\omega$上，通过结果是正还是负决定它到底属于哪一类。</p>
<p><img alt="Two class LDA" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/2_class_lda_zps98b80132.jpeg"></p>
<p>当$\Sigma = \sigma^2I$时，$\omega$与$\mu_1-\mu_0$同向。这时我们只需要判断投影点离$\mu_1$和$\mu_0$中的那个点近。当它们的先验概率$\pi_1 = \pi_0$时，投影点位于其中点；当$\pi_1&gt;\pi_0$时，则$x_0$越趋近于$\mu_0$,直线的更大部分先验地属于类1;反之亦然。<sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1-back"><a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1" class="simple-footnote" title="LDA算法可能导致overfitting,具体解决方法请参阅Machine Learning:a probabilistic perspective一书4.2.*部分">1</a></sup></p>
<h1>Inference in joint Gaussian distributions</h1>
<hr>
<p>给定联合概率分布$p(x_1,x_2)$,如果我们能够计算边际概率分布$p(x1)$以及条件概率分布$p(x_1|x_2)$想必是极好的而且是及有用的。以下我们给出具体的推导过程并给出一具体实例:</p>
<blockquote>
<p>Theorem 3.2 假定$x=(x_1,x_2)$服从联合高斯分布,且参数如下:
\begin{equation}
\mu = \left(
        \begin{array}{ccc}
        \mu_1 \
        \mu_2
        \end{array}
      \right)
\end{equation}
\begin{equation}
\Sigma = \left(
        \begin{array}{ccc}
        \Sigma_{11} &amp; \Sigma_{12} \
        \Sigma_{21} &amp; \Sigma_{22}
        \end{array}
      \right)
\end{equation}
则我们可以得到如下边际概率分布:
\begin{equation}
p(x_1) = N(x_1|\mu_1,\Sigma_{11})  \
p(x_2) = N(x_2|\mu_2,\Sigma_{22})
\end{equation}
另其后验条件分布为:
\begin{equation}
p(x_1|x_2) = N(x_1|\mu_{1|2},\Sigma_{1|2}) \
\mu_{1|2} = \mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2) \
=\mu_1-\Lambda_{11}^{-1}\Lambda_{12}(x_2-\mu_2) \
=\Sigma_{1|2}(\Lambda_{11}\mu_1-\Lambda_{12}(x_2-\mu_2)) \
\Sigma_{1|2} = \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}=\Lambda_{11}^{-1}
\end{equation}</p>
</blockquote>
<h2>Proof</h2>
<h3>Inverse of a partioned matrix using Schur complements</h3>
<blockquote>
<p>Theorem 3.3 分块矩阵求逆
给定分块矩阵如下:
\begin{equation}
M=\left(
    \begin{array}{ccc}
    E &amp; F \
    G &amp; H
    \end{array}
  \right)
\end{equation}
其中$E$和$H$均为可逆矩阵，于是我们有:
\begin{equation}
M^{-1} = \left(
           \begin{array}{ccc}
            (M/H)^{-1} &amp; (M/H)^{-1}FH^{-1} \
            -H^{-1}G(M/H)^{-1} &amp; H^{-1}+H^{-1}G(M/H)^{-1}FH^{-1}
           \end{array}
         \right)        <br>
\end{equation}
其中,$M/H \triangleq E-FH^{-1}G$,也称$M$关于$H$的<em>Schur complement</em>.上式称为分块矩阵求逆公式。</p>
</blockquote>
<h1>Linear Gaussian Systems</h1>
<hr>
<h1>Inferring the parameters of an MVN</h1>
<hr><script type="text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>
<ol class="simple-footnotes"><li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1">LDA算法可能导致overfitting,具体解决方法请参阅Machine Learning:a probabilistic perspective一书4.2.*部分 <a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1-back" class="simple-footnote-back">↩</a></li></ol> 
	<a class="btn btn-mini xsmall" href="/ji-qi-xue-xi-xi-lie-iiigaussian-models.html">
          <i class="icon-comment"></i> Comment </a>
	<hr />
      </div>
      
    </div>
    

 
        



    <div class='row-fluid'>
      <div class='article-title span9'> 
        <a href="/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html"><h1>机器学习系列(II):Generative models for discrete data</h1></a>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>二 04 三月 2014 </p>

<p style="text-align: left;">
Filed under <a href="/category/machine-learning.html">Machine Learning</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="/tag/machine-learning.html">Machine Learning</a> <a href="/tag/classfication.html">Classfication</a> <a href="/tag/generative-models.html">Generative Models</a> <a href="/tag/mutual-information.html">Mutual Information</a> </p>
<p>
</p>
      </div>
      <div class="summary span8">
	<p>本文为机器学习系列第二篇，主要研究一下离散数据生成模型。我们会介绍两种模型，一为Dirichlet-multinomial model,一为朴素贝叶斯分类器（Naive Bayes Classfier，NBC),最后我们介绍一下关于互信息的基本知识以及将其用于特征选择。</p> 
	<a class="btn btn-mini xsmall" href="/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html">
          <i class="icon-plus-sign"></i> Read More </a>
	<hr />
      </div>
      
    </div>
      

 
        



    <div class='row-fluid'>
      <div class='article-title span9'> 
        <a href="/Decision Tree.html"><h1>机器学习系列(I):决策树算法</h1></a>
      </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>一 03 三月 2014 </p>

<p style="text-align: left;">
Filed under <a href="/category/machine-learning.html">Machine Learning</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="/tag/machine-learning.html">Machine Learning</a> </p>
<p>
</p>
      </div>
      <div class="summary span8">
	<p>本文作为本博客的第一篇博文，简单说明了创建这个博客的目的；同时作为机器学习系列的第一篇，对决策树中的ID3和C4.5算法进行了梳理，对于ID3算法，我们给出了Python源码实现。</p> 
	<a class="btn btn-mini xsmall" href="/Decision Tree.html">
          <i class="icon-plus-sign"></i> Read More </a>
	<hr />
      </div>
      
    </div>
      
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="/category/machine-learning.html">1</a></li>

    <li class="next disabled"><a href="#">&rarr; Next</a></li>

</ul>
</div>
 
  
        </div>
        
        
    </div>     </div> </div>

<!--footer-->
<div class="container">
  <div class="well" style="background-color: #E9EFF6">
    <div id="blog-footer">
      <div class="row-fluid">
	<div class="social span2" align="center" id="socialist">
	  <ul class="nav nav-list">
	    <li class="nav-header">
	      Social
	    </li>
	    <li><a href="https://github.com/qingyuanxingsi"><i class="icon-Github" style="color: #1f334b"></i>Github</a></li>

	  </ul>
	</div>
        <div class="links span2" align="center">
          <ul class="nav nav-list">
            <li class="nav-header"> 
              Links
            </li>
            
            <li><a href="http://freemind.pluskid.org">Pluskid</a></li>
            <li><a href="https://github.com/julycoding/The-Art-Of-Programming-By-July">结构之法 算法之道</a></li>
            <li><a href="http://www.nosqlnotes.net/">NOSQL Notes</a></li>
          </ul>
        </div>
	<div class="site-nav span2" align="center">
          <ul class="nav nav-list" id="site-links">
            <li class="nav-header"> 
              Site
            </li>
            <li><a href=""><i class="icon-home" style="color: #1f334b">
                </i>Home</a></li>
            <li><a href="/archives.html"><i class="icon-list" style="color: #1f334b">
                </i>Archives</a></li>
	    <li><a href="/tags.html"><i class="icon-tags" style="color: #1f334b">
                </i>Tags</a></li>
	    
            <li><a href="/" rel="alternate">
                <i class="icon-rss-sign" style="color: #1f334b"></i>
                Atom Feed</a></li>
	  </ul>

        </div>

      </div> <!--end of fluid row-->
    </div> <!--end of blog-footer-->
    <hr />
    <p align="center"><a href="">Doodle World</a>
      &copy; qingyuanxingsi
    Powered by <a href="github.com/getpelican/pelican">Pelican</a> and
        <a href="https://twitter.github.com/bootstrap">Twitter Bootstrap</a>. 
        Icons by <a href="http://fortawesome.github.com/Font-Awesome">Font Awesome</a> and 
        <a href="http://gregoryloucas.github.com/Font-Awesome-More">Font Awesome More</a></p>

  </div> <!--end of well -->
</div> <!--end of container -->

<!--/footer-->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
<script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/js/bootstrap.min.js"></script>


<script>var _gaq=[['_setAccount','UA-48582273-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>

</body>
</html>