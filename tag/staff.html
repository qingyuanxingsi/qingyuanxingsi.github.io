<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>苹果的味道</title>
    <meta name="description" content="">
    <meta name="author" content="qingyuanxingsi">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="../theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.1.1/css/bootstrap.no-icons.min.css" rel="stylesheet">
    <link href="../theme/local.css" rel="stylesheet">
    <link href="../theme/pygments.css" rel="stylesheet">
    <link href="../theme/font-awesome.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Gudea:400,400italic|Alegreya+SC' rel='stylesheet' type='text/css'>
</head>

<body>
<header class="blog-header">
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
	<a href=".." class="brand">苹果的味道</a>
      </div>

      <div class="span3" id="blog-nav">
	<ul class="nav nav-pills pull-right">
            <li><a href="../pages/about.html">About</a></li>
	    <li >
	      <a href="../category/computer-vision.html ">Computer Vision</a>
	    <li >
	      <a href="../category/distributed-system.html ">Distributed System</a>
	    <li >
	      <a href="../category/life.html ">Life</a>
	    <li >
	      <a href="../category/machine-learning.html ">Machine Learning</a>
	    <li >
	      <a href="../category/notes.html ">Notes</a>
	    <li >
	      <a href="../category/pearls.html ">Pearls</a>
	    <li >
	      <a href="../category/viewpoint.html ">Viewpoint</a>
	</ul>
      </div>
    </div> <!-- End of fluid row-->
  </div>   <!-- End of Container-->
</header>
    
<div class="container">
    <div class="content">
    <div class="row-fluid">

        <div class="span10">
        

        

    <div class='row-fluid''>
        <div class="article-title span9">
            <a href="../xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html"><h1>小小收藏夹[持续更新中]</h1></a>
        </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>二 20 五月 2014 </p>

<p style="text-align: left;">
Filed under <a href="../category/pearls.html">Pearls</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="../tag/suan-fa.html">算法</a> <a href="../tag/fun.html">Fun</a> <a href="../tag/staff.html">Staff</a> <a href="../tag/shou-cang-jia.html">收藏夹</a> <a href="../tag/bloom-filter.html">Bloom Filter</a> <a href="../tag/b-trees.html">B Trees</a> <a href="../tag/data-structure.html">Data Structure</a> <a href="../tag/algorithm.html">Algorithm</a> <a href="../tag/pgm.html">PGM</a> </p>
<p>
</p>
      </div>
      <div class="article-content span8">
	<h1>NLP<sup id="sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1-back"><a class="simple-footnote" href="#sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1" title="Natural Language Processing">1</a></sup></h1>
<hr>
<ul>
<li>Relation Extration<ul>
<li>Hand-written approach more suitable for structured data,such as a telephone book,Facebook or eBay;</li>
<li>Supervised Method;得到所有的命名实体组,使用一个分类器(<em>features</em>)判断它们是否是关联的,如果是,则使用第二个分类器判断它们之间的关联关系具体是什么; </li>
<li>Semi-Supervised(Relation Bootstrapping/Distant Supervised Learning) and unsupervised methods(Open Information Extraction);Strapping方法感觉很巧妙,个人很喜欢;</li>
</ul>
</li>
</ul>
<h1>SVM</h1>
<hr>
<p>对于SVM这么高端大气上档次的东西,当然要单独列出来。今天其他东西实在看不下去了,所以把Pluskid之前写的一系列讲SVM的文章再挖出来看看。以下是目录以及对每篇的简单说明:</p>
<ul>
<li><a href="http://blog.pluskid.org/?p=632">支持向量机: Maximum Margin Classifier</a>;文中主要介绍了两个距离,<em>Functional Margin</em> $\hat{\gamma}$和<em>Geometrical Margin</em> $\tilde{\gamma}$.它们之间满足$\hat{\gamma} = ||w||\tilde{\gamma}$.我们固定$\hat{\gamma} = 1$,通过最大化$\frac{1}{||w||}$来得到<strong>Maximum Margin Classifier</strong>.</li>
<li><a href="http://blog.pluskid.org/?p=682">支持向量机: Support Vector</a>;简要介绍了Support Vector是指什么,另外对线性可分的情况利用Duality进行了推导并得出了两个比较重要的结论:<ul>
<li>对新点的预测只需要计算与训练点之间的内积即可;</li>
<li>非支持向量不参与模型的计算过程之中。</li>
</ul>
</li>
<li><a href="http://blog.pluskid.org/?p=685">支持向量机: Kernel</a>;<strong>Kernel</strong>的基本思想。</li>
<li><a href="http://blog.pluskid.org/?p=692">支持向量机：Outliers</a>;通过引入松弛变量处理Outliers,而实际上最后的优化形式只是加上$\alpha_i \leq C$的限制。</li>
<li><a href="http://blog.pluskid.org/?p=696">支持向量机：Numerical Optimization</a>;以非常通俗易懂的方式介绍了一下<strong>SMO(Sequential Minimal Optimization)</strong>,赞一个。</li>
</ul>
<h1>Deep Learning</h1>
<hr>
<ul>
<li>稀疏编码(<em>Sparse Coding</em>)</li>
</ul>
<p><a href="http://blog.csdn.net/zouxy09/article/details/8777094">Deep Learning（深度学习）学习笔记整理系列之（五)</a></p>
<ul>
<li>Deep Learning总结</li>
</ul>
<p><a href="http://blog.csdn.net/zouxy09/article/details/8782018">Deep Learning（深度学习）学习笔记整理系列之（八)</a></p>
<ul>
<li><strong>受限Boltzmann机</strong></li>
</ul>
<p><a href="http://pan.baidu.com/s/1gdh3P1x">A Brief Introduction to Restricted Boltzmann Machine</a></p>
<h1>Pocket</h1>
<hr>
<ul>
<li><a href="http://v.youku.com/v_show/id_XNzAwMTI0MDgw.html">罗辑思维 2014：右派为什么这么横 10</a>;视频主要介绍了保守主义的三个特征,同时分析了人们在面临选择的时候的不同思维方式,个人觉得这一点很有借鉴意义，建议一看!</li>
<li><a href="http://v.youku.com/v_show/id_XNzAzMTkyNDky.html">罗辑思维 2014：迷茫时代的明白人 11</a>;活在当下。</li>
</ul>
<h1>算法</h1>
<hr>
<ul>
<li>今天看了一下网上流传的传说中的高大上的所谓的<code>十大海量数据处理算法</code>,看了一下,实际上没有什么东西,唯独<strong>Bloom Filter</strong>看着还挺好玩的,所以以下给出一个通俗易懂的链接。</li>
</ul>
<p><a href="http://www.cnblogs.com/heaad/archive/2011/01/02/1924195.html">那些优雅的数据结构(1) : BloomFilter——大规模数据处理利器</a></p>
<ul>
<li>
<p>在过去的若干年里,有一个心结一直萦绕在我的心头挥之不去,它存在于我的脑海里，我的梦里，我的歌声里,TA就是<strong>B树</strong>(好吧,其实是因为没有机会好好地研究一下它啦)。以下给出两个链接,它们主要介绍了B树的基本概念,性质以及针对B树的插入、删除操作,两个PPT还是相当直观的,应该能够比较直观地了解B树这个数据结构!</p>
<ul>
<li><a href="http://cecs.wright.edu/~tkprasad/courses/cs707/L04-X-B-Trees.ppt">B-Trees</a>;</li>
<li><a href="http://zh.scribd.com/doc/18210/B-TREE-TUTORIAL-PPT">B TREE TUTORIAL PPT</a>.</li>
</ul>
</li>
<li>
<p>今天又重新看了一下这个写的很不错的<strong>A*算法</strong>,恩,这篇文章想来是极好的。</p>
</li>
</ul>
<p><a href="http://www.raywenderlich.com/zh-hans/21503/a%E6%98%9F%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95%E4%BB%8B%E7%BB%8D">A星寻路算法介绍</a></p>
<ul>
<li>复习一下之前做智能提示时用到的<strong>Trie Tree</strong>.</li>
</ul>
<p><a href="https://www.cs.umd.edu/class/fall2005/cmsc132/lecs/lec29.ppt">Indexed Search Tree (Trie) - Computer Science Department</a></p>
<ul>
<li>看了一下传说中的数据挖掘十大算法,好像就<strong>Apriori算法</strong>不是特别熟吧,所以重新看了一遍;个人觉得如果我早出生若干年,这种程度的算法也是能想出来的吧(我指思想).好吧,我认为着重要理解的有如下两点:<ul>
<li>Support;其实也就是某种组合在所有Transaction中出现的频度。</li>
<li>Confidence;当生成关联规则$A\to B$时,有$confidence = \frac{Count(A,B)}{Count(A)}$,背后的Intuition就是如果我买了$A$,大概会有多大的可能买$B$.</li>
</ul>
</li>
</ul>
<p><a href="http://www.cs.sjsu.edu/faculty/lee/cs157b/Gaurang%20Negandhi--Apriori%20Algorithm%20Presentation.ppt">Apriori Algorithm Review for Finals</a></p>
<ul>
<li>最近需要对1G的文本数据进行处理,所以想了解一下现行的分布式计算框架的应用场景,从而选择合适的框架用于这个任务,期间看到以下两篇文章写的很不错,特此摘录。</li>
</ul>
<p><a href="http://langyu.iteye.com/blog/1407194">对实时分析与离线分析的思考</a>
<a href="http://langyu.iteye.com/blog/1407194">对实时分析与离线分析的思考(二)</a></p>
<ul>
<li>最近脑子总是不断宕机,宕机了就什么也看不了了,刚看了一篇一个人讲自己怎么学习算法的博文,感觉还不错(只是看了作者的经历,他看过的那些书还没来得及看)。以下给出链接:</li>
</ul>
<p><a href="http://zh.lucida.me/blog/on-learning-algorithms/">我的算法学习之路</a></p>
<h1>Machine Learning</h1>
<hr>
<ul>
<li>看的有点累了,不想看<em>EM</em>算法复杂的数学公式推导了,所以找到之前看过的一篇,回顾一下,等以后想看了再详细介绍<em>Mixture Models</em>和<em>EM</em>算法吧!</li>
</ul>
<p><a href="http://blog.pluskid.org/?p=39">漫谈 Clustering (3): Gaussian Mixture Model</a></p>
<ul>
<li>近期为了理解卷积,于是到处找资料,无意中发现了这一篇神一般的理解。(<strong>墙裂推荐</strong>)</li>
</ul>
<p><a href="http://www.guokr.com/post/342476/">关于卷积的一个血腥的讲解，看完给跪了</a></p>
<ul>
<li>PCA数据预处理<em>Whitening</em></li>
</ul>
<blockquote>
<p>在使用PCA前需要对数据进行预处理，首先是均值化，即对每个特征维，都减掉该维的平均值，然后就是将不同维的数据范围归一化到同一范围，方法一般都是<strong>除以最大值</strong>。但是比较奇怪的是，在对自然图像进行均值处理时并不是不是减去该维的平均值，而是减去这张图片本身的平均值。因为PCA的预处理是按照不同应用场合来定的。</p>
<p>自然图像指的是人眼经常看见的图像，其符合某些统计特征。一般实际过程中，只要是拿正常相机拍的，没有加入很多人工创作进去的图片都可以叫做是自然图片，因为很多算法对这些图片的输入类型还是比较鲁棒的。在对自然图像进行学习时，其实不需要太关注对图像做方差归一化，因为自然图像每一部分的统计特征都相似，只需做均值为0化就ok了。不过对其它的图片进行训练时，比如首先字识别等，就需要进行方差归一化了。</p>
<p>有一个观点需要注意，那就是<strong>PCA并不能阻止过拟合现象</strong>。表明上看PCA是降维了，因为在同样多的训练样本数据下，其特征数变少了，应该是更不容易产生过拟合现象。但是在实际操作过程中，这个方法阻止过拟合现象效果很小，主要还是通过<strong>规则项</strong>来进行阻止过拟合的。</p>
<p><strong>Whitening</strong>：</p>
<p><strong>Whitening</strong>的目的是去掉数据之间的相关联度，是很多算法进行预处理的步骤。比如说当训练图片数据时，由于图片中相邻像素值有一定的关联，所以很多信息是冗余的。这时候去相关的操作就可以采用白化操作。数据的Whitening必须满足两个条件：</p>
<ul>
<li>不同特征间相关性最小，接近0；</li>
<li>所有特征的方差相等（不一定为1）。</li>
</ul>
<p>常见的白化操作有PCA whitening和ZCA whitening。</p>
<p><em>PCA whitening</em>是指将数据x经过PCA降维为z后，可以看出z中每一维是独立的，满足whitening白化的第一个条件，这是只需要将z中的每一维都除以标准差就得到了每一维的方差为1，也就是说方差相等。公式为：</p>
<p>\begin{equation}
x_{PCAwhite,i} = \frac{x_{rot,i}}{\sqrt{\lambda_i}}
\end{equation}</p>
<p><em>ZCA whitening</em>是指数据x先经过PCA变换为z，但是并不降维，因为这里是把所有的成分都选进去了。这是也同样满足whtienning的第一个条件，特征间相互独立。然后同样进行方差为1的操作，最后将得到的矩阵左乘一个特征向量矩阵U即可。ZCA whitening公式为：</p>
<p>\begin{equation}
x_{ZCAwhite} = Ux_{PCAwhite}
\end{equation}</p>
<p>参考<a href="http://www.cnblogs.com/tornadomeet/archive/2013/03/21/2973231.html">Deep learning：十(PCA和whitening)</a></p>
</blockquote>
<ul>
<li>最近一直在看<strong>高斯过程</strong>,挺难理解的,好吧,咱们慢慢来,先给个链接。</li>
</ul>
<p><a href="http://www.eurandom.tue.nl/events/workshops/2010/YESIV/Prog-Abstr_files/Ghahramani-lecture2.pdf">Introduction to Gaussian Process</a>
* 今天看自然语言处理Standford公开课的时候看到最大熵模型(Maximum Entropy Models),视频讲的实在太罗嗦了,在网上找了找,下面这个PPT貌似还挺不错的。(原始PPT有部分错误,以下网盘共享文件是部分修正后版本,可能还会有错误,欢迎指出)</p>
<p><a href="http://pan.baidu.com/s/1gdze7h5">Maximum Entropy Model</a></p>
<ul>
<li>
<p>EM/pLSA/LDA的一些参考资料</p>
<ul>
<li>Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA, 2006.</li>
<li>Gregor Heinrich. Parameter estimation for text analysis. Technical report, 2004.</li>
<li>Wayne Xin Zhao, Note for pLSA and LDA, Technical report, 2011.</li>
<li>CX Zhai, A note on the expectation-maximization (em) algorithm 2007</li>
<li>Qiaozhu Mei, A Note on EM Algorithm for Probabilistic Latent Semantic Analysis 2008</li>
<li>Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008.</li>
<li>Freddy Chong Tat Chua. Dimensionality reduction and clustering of text documents.Technical report, 2009.</li>
</ul>
</li>
<li>
<p>LDA算法伪代码实现参考</p>
<ul>
<li><a href="http://www.arbylon.net/projects/">Gregor Heinrich’s LDA-J</a></li>
<li><a href="http://www.gatsby.ucl.ac.uk/teaching/courses/ml1-2008.html">Yee Whye Teh’s Gibbs LDA Matlab codes</a></li>
<li><a href="http://psiexp.ss.uci.edu/research/programs_data/toolbox.htm">Mark Steyvers and Tom Griffiths’s topic modeling matlab toolbox</a></li>
<li><a href="http://gibbslda.sourceforge.net/">GibbsLDA++</a></li>
</ul>
</li>
<li>
<p>A <strong>Very</strong> Brief Introduction about Semi-supervised Learning.</p>
</li>
</ul>
<p><a href="http://pan.baidu.com/s/1pJqGSSN">SEMI_SUPERVISED LEARNING</a></p>
<ul>
<li><a href="http://www.chawenti.com/articles/12497.html">一种用来确定K-Means算法类别数的方法</a>;<strong>平均质心距离加权平均值</strong>。</li>
</ul>
<h1>PGM</h1>
<hr>
<p>以下给出讲解PGM比较深入浅出的一系列Lecture Slides。</p>
<h2>PART I:Introduction to PGM</h2>
<ul>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-1-Introduction-Combined.pdf">Introduction and Overview</a>;主要介绍了PGM的背景以及Factor的基础知识。</li>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Bayes-Nets.pdf">Bayesian Network Fundamentals</a>;简要介绍了什么是Bayesian Network、Reasoning Patterns以及Influence Flow.最后简要介绍了一下Naive Bayes Classifier.</li>
</ul>
<p><img alt="Naive Bayes Classifier" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/naive_bayes_model_zps09771da2.png"></p>
<ul>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Template-Models.pdf">Template Models</a>;主要介绍了Template Models,包括Bayesian Network(HMM)以及Plate Models;</li>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-CPDs.pdf">Structured CPDs</a>;介绍了几种CPD表示的其他常见形式,包括:<ul>
<li>Deterministic CPDs</li>
<li>Tree-structured CPDs</li>
<li>Logistic CPDs &amp; generalizations</li>
<li>Noisy OR/AND</li>
<li>Linear Gaussian &amp; generalizations</li>
</ul>
</li>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-2-Representation-Markov-Nets.pdf">Markov Network Fundamentals</a>;本部分涵盖的内容有Markov Network,General Gibbs Distribution,CRF,Log-Linear Models.(<strong>Logistic Models is a simple CRF;CRF does not need to concern about the correlation between features!</strong>)</li>
</ul>
<h2>PART II:PGM Inference</h2>
<ul>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Variable-Elimination.pdf">Variable Elimination</a>;简要介绍了如何在Bayesian Network以及Ｍarkov Network中执行VE算法;接着对其复杂度进行了分析;最后从图的视角重新审视了一下VE算法.</li>
</ul>
<p><img alt="Variable Elimination" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/variable_elimination_zps6fdc76a6.png"></p>
<ul>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Belief-Propagation.pdf">Belief Propagation(I)</a> &amp; <a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Belief-Propagation.pdf">Belief Propagation(II)</a>;其基本内容如下:<ul>
<li>Belief Propagation算法基本流程;</li>
<li>Cluster Graph的基本性质(<code>BP does poorly when we have strong correlations!</code>);</li>
<li>BP算法的基本性质;</li>
<li>Clique Tree Algorithm.</li>
</ul>
</li>
</ul>
<p><img alt="Belief Propagation" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/belief_propogation_zps866416cc.png"></p>
<ul>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-MAP.pdf">MAP Estimation</a>;关于MAP Inference的基础知识。</li>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-3-Sampling.pdf">Sampling Methods</a>;Basic Sampling Methods.</li>
</ul>
<h2>PART III:PGM Learning</h2>
<ul>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Parameters-Part-1.pdf">Learning: Parameter Estimation, Part 1</a> &amp; <a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Parameters-Part-2.pdf">Learning: Parameter Estimation, Part 2</a>;Parameter Estimation for BN and MN;</li>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-BN-Structures.pdf">Structure Learning</a>;</li>
<li><a href="http://spark-university.s3.amazonaws.com/stanford-pgm/slides/Section-5-Learning-Incomplete-Data.pdf">Learning With Incomplete Data</a></li>
</ul>
<blockquote>
<p>NOTE:该课程网址见<a href="https://class.coursera.org/pgm-003">PGM</a>.</p>
<p><em>TODO Board</em>:</p>
<ul>
<li>Ising Model</li>
<li>Dual Decomposition</li>
<li>Decision Making</li>
<li>Bayesian Scores</li>
<li>Learning With Incomplete Data</li>
<li>Lassos</li>
<li>凸QP</li>
<li>Duality</li>
<li>KKT条件</li>
<li>支持向量机番外篇I:<a href="http://blog.pluskid.org/?p=702">支持向量机：Duality</a></li>
<li>支持向量机番外篇II:<a href="http://blog.pluskid.org/?p=723">支持向量机：Kernel II</a></li>
<li>Apriori算法细节</li>
<li><strong>pLSA算法EM算法求解</strong></li>
</ul>
</blockquote><script type="text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>
<ol class="simple-footnotes"><li id="sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1"><a href="https://class.coursera.org/nlp/lecture">Natural Language Processing</a> <a class="simple-footnote-back" href="#sf-xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong-1-back">↩</a></li></ol> 
	<a class="btn btn-mini xsmall" href="../xiao-xiao-shou-cang-jia-chi-xu-geng-xin-zhong.html">
          <i class="icon-comment"></i> Comment </a>
	<hr />
      </div>
      
    </div>
    
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="../tag/staff.html">1</a></li>

    <li class="next disabled"><a href="#">&rarr; Next</a></li>

</ul>
</div>
 
  
        </div>
        
        
    </div>     </div> </div>

<!--footer-->
<div class="container">
  <div class="well" style="background-color: #E9EFF6">
    <div id="blog-footer">
      <div class="row-fluid">
	<div class="social span2" align="center" id="socialist">
	  <ul class="nav nav-list">
	    <li class="nav-header">
	      Social
	    </li>
	    <li><a href="https://github.com/qingyuanxingsi"><i class="icon-Github" style="color: #1f334b"></i>Github</a></li>

	  </ul>
	</div>
        <div class="links span2" align="center">
          <ul class="nav nav-list">
            <li class="nav-header"> 
              Links
            </li>
            
            <li><a href="http://freemind.pluskid.org">Pluskid</a></li>
            <li><a href="https://github.com/julycoding/The-Art-Of-Programming-By-July">结构之法 算法之道</a></li>
            <li><a href="http://www.nosqlnotes.net/">NOSQL Notes</a></li>
            <li><a href="http://diaorui.net/">数学之美</a></li>
            <li><a href="http://licstar.net/">让博客飞(A BLOG WITH FUN)</a></li>
            <li><a href="http://www.xperseverance.net/blogs/">持之以恒</a></li>
            <li><a href="http://ibillxia.github.io/">Bill's Blog</a></li>
            <li><a href="http://malagis.com/">麻辣GIS</a></li>
          </ul>
        </div>
	<div class="site-nav span2" align="center">
          <ul class="nav nav-list" id="site-links">
            <li class="nav-header"> 
              Site
            </li>
            <li><a href=".."><i class="icon-home" style="color: #1f334b">
                </i>Home</a></li>
            <li><a href="../archives.html"><i class="icon-list" style="color: #1f334b">
                </i>Archives</a></li>
	    <li><a href="../tags.html"><i class="icon-tags" style="color: #1f334b">
                </i>Tags</a></li>
	    
            <li><a href="../" rel="alternate">
                <i class="icon-rss-sign" style="color: #1f334b"></i>
                Atom Feed</a></li>
	  </ul>

        </div>

      </div> <!--end of fluid row-->
    </div> <!--end of blog-footer-->
    <hr />
    <p align="center"><a href="..">苹果的味道</a>
      &copy; qingyuanxingsi
    Powered by <a href="github.com/getpelican/pelican">Pelican</a> and
        <a href="https://twitter.github.com/bootstrap">Twitter Bootstrap</a>. 
        Icons by <a href="http://fortawesome.github.com/Font-Awesome">Font Awesome</a> and 
        <a href="http://gregoryloucas.github.com/Font-Awesome-More">Font Awesome More</a></p>

  </div> <!--end of well -->
</div> <!--end of container -->

<!--/footer-->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
<script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/js/bootstrap.min.js"></script>


<script>var _gaq=[['_setAccount','UA-48582273-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>

</body>
</html>