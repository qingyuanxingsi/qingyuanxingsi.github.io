<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Doodle World</title>
    <meta name="description" content="">
    <meta name="author" content="qingyuanxingsi">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="http://www.qingyuanxingsi.com/theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="//netdna.bootstrapcdn.com/twitter-bootstrap/2.1.1/css/bootstrap.no-icons.min.css" rel="stylesheet">
    <link href="http://www.qingyuanxingsi.com/theme/local.css" rel="stylesheet">
    <link href="http://www.qingyuanxingsi.com/theme/pygments.css" rel="stylesheet">
    <link href="http://www.qingyuanxingsi.com/theme/font-awesome.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Gudea:400,400italic|Alegreya+SC' rel='stylesheet' type='text/css'>
</head>

<body>
<header class="blog-header">
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
	<a href="http://www.qingyuanxingsi.com" class="brand">Doodle World</a>
      </div>

      <div class="span3" id="blog-nav">
	<ul class="nav nav-pills pull-right">
            <li><a href="http://www.qingyuanxingsi.com/pages/about.html">About</a></li>
	    <li >
	      <a href="http://www.qingyuanxingsi.com/category/distributed-system.html ">Distributed System</a>
	    <li >
	      <a href="http://www.qingyuanxingsi.com/category/life.html ">Life</a>
	    <li >
	      <a href="http://www.qingyuanxingsi.com/category/machine-learning.html ">Machine Learning</a>
	    <li >
	      <a href="http://www.qingyuanxingsi.com/category/viewpoint.html ">Viewpoint</a>
	</ul>
      </div>
    </div> <!-- End of fluid row-->
  </div>   <!-- End of Container-->
</header>
    
<div class="container">
    <div class="content">
    <div class="row-fluid">

        <div class="span10">
        

        

    <div class='row-fluid''>
        <div class="article-title span9">
            <a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iiigaussian-models.html"><h1>机器学习系列(III):Gaussian Models</h1></a>
        </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>六 15 三月 2014 </p>

<p style="text-align: left;">
Filed under <a href="http://www.qingyuanxingsi.com/category/machine-learning.html">Machine Learning</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="http://www.qingyuanxingsi.com/tag/machine-learning.html">Machine Learning</a> <a href="http://www.qingyuanxingsi.com/tag/gaussian-models.html">Gaussian Models</a> <a href="http://www.qingyuanxingsi.com/tag/generative-models.html">Generative Models</a> </p>
<p>
</p>
      </div>
      <div class="article-content span8">
	<p>在<a href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iigenerative-models-for-discrete-data.html">上一篇</a>中我们着重介绍了对于离散数据的生成模型，紧接上一篇，本篇我们介绍对于离散数据的生成模型。好吧,废话我们就不多说了,直接进入正文。</p>
<h1>MLE for MVN</h1>
<hr>
<h2>Basics about MVN</h2>
<p>谈到离散分布,我们很自然地就会想到高斯分布,从小学到现在，印象中第一个走入我脑海中的看着比较高端大气上档次的就是Gaussian分布了。这次我们的重点便完全集中在Gaussian分布之上了,在正式讨论之前，我们先介绍一些关于Gaussian分布的基础知识吧。</p>
<p>在$D$维空间中,MVN(Multivariate Normal)多变量正态分布的概率分布函数具有如下形式:</p>
<p>\begin{equation}
N(x|\mu,\Sigma) \triangleq \frac{1}{(2\pi)^{D/2}det(\Sigma)^{1/2}} exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]
\end{equation}</p>
<p>上式中的指数部分是$x$与$\mu$之间的<a href="http://en.wikipedia.org/wiki/Mahalanobis_distance">Mahalanobis距离</a>。为了更好地理解这个量,我们对$\Sigma$做特征值分解,即$\Sigma = U \Lambda U^T$,其中$U$是一正交阵,满足$U^TU=I$,而$\Lambda$是特征值矩阵。</p>
<p>通过特征值分解,我们有:</p>
<p>\begin{equation}
\Sigma^{-1} = U^{-T}\Lambda^{-1}U^{-1} = U\Lambda^{-1}U^T = \sum_{i=1}^{D}\frac{1}{\lambda_i}u_iu_i^T
\end{equation}</p>
<p>其中,$u_i$是$U$的第$i$列。因此Mahalanobis距离可被改写为:</p>
<p>\begin{equation}
\begin{split}
(x-\mu)^T\Sigma^{-1}(x-\mu) &amp;= (x-\mu)^T (\sum_{i=1}^{D}\frac{1}{\lambda_i}u_iu_i^T) (x-\mu)   \\
                            &amp;= \sum_{i=1}^{D} \frac{1}{\lambda_i}(x-\mu)^T u_iu_i^T (x-\mu)    \\
                            &amp;= \sum_{i=1}^{D} \frac{y_i^2}{x_i}                                \\
\end{split}
\end{equation}</p>
<p>其中,$y_i \triangleq u_i^T(x-\mu)$。另2维空间中的椭圆方程为:</p>
<p>$$\frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2} = 1$$</p>
<p>因此我们可知Gaussian概率密度的等高线沿着椭圆分布,特征向量决定椭圆的朝向,而特征值则决定椭圆有多<code>“椭”</code>。一般来说，如果我们将坐标系移动$\mu$,然后按$U$旋转，此时的欧拉距离即为Mahalanobis距离。</p>
<h2>MLE for MVN</h2>
<p>以下我们给出MVN参数的MLE(极大似然估计)的证明:</p>
<blockquote>
<p>Theorem 3.1 若我们获取的$N$个独立同分布的样本$x_i \sim\ N(x|\mu,\Sigma)$,则关于$\mu$以及$\Sigma$的极大似然分布如下:
  \begin{equation}
  \hat{\mu}<em mle="mle">{mle} = \frac{1}{N}\sum x_i \triangleq \bar{x}
  \end{equation}
  \begin{equation}
  \hat{\Sigma}</em> = \frac{1}{N} \sum (x_i-\bar{x})(x_i-\bar{x})^T = \frac{1}{N}(\sum_{i=1}^{N}x_ix_i^T) - \bar{x}\bar{x}^T
  \end{equation}</p>
</blockquote>
<p>我们不加证明地给出如下公式组:</p>
<blockquote>
<p>\begin{equation}
  \begin{split}
  &amp;\frac{\partial(b^Ta)}{\partial a} = b  \\
  &amp;\frac{\partial(a^TAa)}{\partial a} = (A+A^T)a \\
  &amp;\frac{\partial}{\partial A} tr(BA) = B^T  \\
  &amp;\frac{\partial}{\partial A} log |A| = A^{-T} \\
  &amp;tr(ABC) = tr(CAB) = tr(BCA)
  \end{split}
  \end{equation}</p>
</blockquote>
<p>最后一个等式称为迹的循环置换性质(cyclic permutation property)。利用这个性质,我们使用<code>trace trick</code>可以得到下式:</p>
<p>$$x^TAx = tr(x^TAx) = tr(xx^TA) = tr(Axx^T)$$</p>
<p>证明:</p>
<p>对数似然函数为:</p>
<p>\begin{equation}
l(\mu,\Sigma) = log p(D|\mu,\Sigma) = \frac{N}{2} log |\Lambda| - \frac{1}{2} \sum_{i=1}^{N} (x_i-\mu)^T \Lambda (x_i-\mu)
\end{equation}</p>
<p>其中，$\Lambda = \Sigma^{-1}$为精度矩阵。另$y_i=x_i-\mu$并利用链式法则有:</p>
<p>\begin{equation}
\frac{\partial}{\partial \mu}(x_i-\mu)^T \Sigma^{-1} (x_i-\mu) = \frac {\partial}{\partial y_i} y_i^T \Sigma^{-1} y_i \frac{\partial y_i}{\partial \mu}=-(\Sigma^{-T}+\Sigma^{-1})y_i
\end{equation}</p>
<p>即:</p>
<p>\begin{equation}
\frac{\partial}{\partial \mu} l(\mu,\Sigma) = -\frac{1}{2} \sum_{i=1}^{N} -2\Sigma^{-1}(x_i-\mu) = 0
\end{equation}</p>
<p>故有:</p>
<p>\begin{equation}
\hat{\mu} = \frac{1}{N} \sum_{i=1}^{N} x_i = \bar{x}
\end{equation}</p>
<p>即最大似然均值即为经验均值。</p>
<p>利用trace trick我们重写对数似然函数为:</p>
<p>\begin{equation}
l(\Lambda) = \frac{N}{2} log |\Lambda| - \frac{1}{2} \sum_{i} tr[(x_i-\mu)(x_i-\mu)^T \Lambda]
           = \frac{N}{2} log |\Lambda| - \frac{1}{2} tr[S_u\Lambda]
\end{equation}</p>
<p>其中，$S_u \triangleq \sum_{i=1}^{N} (x_i-\mu)(x_i-\mu)^T$,业界尊称其为分散度矩阵(<code>Scatter Matrix</code>),以后我们聊LDA的时候会再次碰到。对$\Lambda$求偏导有:</p>
<p>\begin{equation}
\frac{\partial l(\Lambda)}{\partial \Lambda} = \frac{N}{2}\Lambda^{-T} - \frac{1}{2} S_u^{T} = 0
\end{equation}</p>
<p>得:</p>
<p>\begin{equation}
\hat{\Sigma} = \frac{1}{N} \sum_{i=1}^{N} (x_i-\mu)(x_i-\mu)^T
\end{equation}</p>
<p>证毕。</p>
<h1>Gaussian Discriminant Analysis</h1>
<hr>
<p>我们在上一篇中提到了Naive Bayes方法,其实质无非是估计在每一类下特定的样本出现的概率，进而我们可以把该特定样本分配给概率值最大的那个类。而对于连续数据而言，其实质其实也是一样的，每一个MVN(我们可以看做一类或者一个Component)都可能生成一些数据，我们估计在每一个Component下生成特定样本的概率，然后把该特定样本分配给概率值最大的那个Component即可。即我们可以定义如下的条件分布:</p>
<p>\begin{equation}
p(x|y=c,\theta) = N(x|\mu_c,\Sigma_c)
\end{equation}</p>
<p>上述模型即为高斯判别分析(Gaussian Discriminant Analysis,GDA)(<code>注意,该模型为生成模型，而不是判别模型</code>)。如果$\Sigma_c$是对角阵，即所有的特征都是独立的时，该模型等同于Naive Bayes.</p>
<h2>QDA</h2>
<p>在上式中带入高斯密度函数的定义，则有:</p>
<p>\begin{equation}
p(y=c|x,\theta) = \frac{\pi_c |2\pi\Sigma_c|^{-1 \over 2}exp[-{1 \over 2}(x-\mu_c)^T\Sigma_C^{-1}(x-\mu_c)]}{\sum_{c\prime}\pi_{c\prime} |2\pi\Sigma_{c\prime}|^{-1 \over 2}exp[-{1 \over 2}(x-\mu_{c\prime})^T\Sigma_{c\prime}^{-1}(x-\mu_{c\prime})]}
\end{equation}</p>
<p>上式中$\pi$为各个Component的先验概率分布。根据上式得到的模型则称为Quadratic Discriminant Analysis(QDA).以下给出在2类以及2类情形下可能的决策边界形状,如下图所示:</p>
<p><img alt="Decision Boundary" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/decision_boundary_zps289df588.jpeg"></p>
<h2>Linear Discriminant Analysis(LDA)</h2>
<p>当各个Gaussian Component的协方差矩阵相同时，我们进一步有:</p>
<p>\begin{equation}
p(y=c|x,\theta) \propto \pi_c exp[\mu_c^T\Sigma^{-1}x-{1 \over 2}x^T\Sigma^{-1}x-{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c]
= exp[\mu_c^T\Sigma^{-1}x-{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c]exp[-{1 \over 2}x^T\Sigma^{-1}x]
\end{equation}</p>
<p>上式中$exp[-{1 \over 2}x^T\Sigma^{-1}x]$是独立于$c$的，分子分母相除抵消到此项。</p>
<p>令:</p>
<p>\begin{equation}
\gamma_c = -{1 \over 2}\mu_c^T\Sigma^{-1}\mu_c+log\pi_c
\end{equation}
\begin{equation}
\beta_c = \Sigma^{-1}\mu_c
\end{equation}</p>
<p>于是有:</p>
<p>\begin{equation}
p(y=c|x,\theta) = \frac{e^{\beta_c^Tx+\gamma_c}}{\sum_{c\prime}e^{\beta_{c\prime}^Tx+\gamma_{c\prime}}}=S(\eta)_c
\end{equation}</p>
<p>其中$\eta = [\beta_1^Tx+\gamma_1,...,\beta_C^Tx+\gamma_c]$,$S$为softmax函数(类似于max函数,故得此名),定义如下:</p>
<p>\begin{equation}
S(\eta)<em c_prime="c\prime">c = \frac{e^{\eta_c}}{\sum e^{\eta</em>}}
\end{equation}</p>
<p>若将$\eta_c$除以一个常数(temperature),当$T\to 0$时，我们有:</p>
<p>\begin{equation}
S(\eta/T)<em c_prime="c\prime">c=
\left{                     <br>
\begin{array}{lll}    <br>
1.0 &amp; c=argmax</em> \eta_{c \prime}   \\
0.0 &amp; otherwise 
\end{array}   <br>
\right.                    <br>
\end{equation}</p>
<p>换句话说，当温度很低时，分布集中在概率最大的那个状态上，而当温度高时，所有的状态呈现均匀分布。</p>
<blockquote>
<p>NOTE:该术语来自于统计物理学，在统计物理学中，人们更倾向于使用波尔兹曼分布（Boltzmann distribution）一词。</p>
</blockquote>
<p>关于<code>式18</code>有一个有趣的性质，即对该式取log,我们则会得到一个关于$x$的线性方程。因此对于任意两类之间的决策边界将会是一条直线，据此该模型也被称为线性判别分析(Linear Discriminant Analysis,LDA)。而且对于二分类问题，我们可以得到:</p>
<p>\begin{equation}
p(y=c|x,\theta) = p(y=c\prime|x,\theta)
\end{equation}</p>
<p>\begin{equation}
\beta_c^Tx+\gamma_c = \beta_{c\prime}^Tx+\gamma_{c\prime}
\end{equation}</p>
<p>\begin{equation}
x^T(\beta_{c\prime}-\beta_c) = \gamma_{c\prime}-\gamma_c
\end{equation}</p>
<h2>Two-class LDA</h2>
<p>为了加深对以上等式的理解，对于二分的情况，我们做如下说明:</p>
<p>\begin{equation}
p(y=1|x,\theta) = \frac{e^{\beta_1^Tx+\gamma_1}}{e^{\beta_1^Tx+\gamma_1}+e^{\beta_0^Tx+\gamma_0}} = \frac{1}{1+e^{(\beta_0-\beta_1)^Tx+(\gamma_0-\gamma_1)}}=sigm((\beta_1-\beta_0)^Tx+(\gamma_1-\gamma_0))
\end{equation}</p>
<p>其中,$sigm(\eta)$代表<a href="http://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid函数</a>。</p>
<p>现有:</p>
<p>\begin{equation}
\gamma_1-\gamma_0 = -{1 \over 2}\mu_1^T\Sigma^{-1}\mu_1+{1 \over 2}\mu_0^T\Sigma^{-1}\mu_0+log(\pi_1/\pi_0)=-{1 \over 2}(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1+\mu_0)+log(\pi_1/\pi_0)
\end{equation}</p>
<p>因此若我们另:</p>
<p>\begin{equation}
\omega = \beta_1-\beta_0 = \Sigma^{-1}(\mu_1-\mu_0)
\end{equation}</p>
<p>\begin{equation}
x_0 = {1 \over 2}(\mu_1+\mu_0)-(\mu_1-\mu_0)\frac{log(\pi_1/\pi_0)}{(\mu_1-\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0)}
\end{equation}</p>
<p>则有$\omega^Tx_0 = -(\gamma_1-\gamma_0)$,即:</p>
<p>\begin{equation}
p(y=1|x,\theta) = sigm(\omega^T(x-x_0))
\end{equation}</p>
<p>因此最后的决策规则很简单:将$x$平移$x_0$,然后投影到$\omega$上，通过结果是正还是负决定它到底属于哪一类。</p>
<p><img alt="Two class LDA" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/2_class_lda_zps98b80132.jpeg"></p>
<p>当$\Sigma = \sigma^2I$时，$\omega$与$\mu_1-\mu_0$同向。这时我们只需要判断投影点离$\mu_1$和$\mu_0$中的那个点近。当它们的先验概率$\pi_1 = \pi_0$时，投影点位于其中点；当$\pi_1&gt;\pi_0$时，则$x_0$越趋近于$\mu_0$,直线的更大部分先验地属于类1;反之亦然。<sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1-back"><a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1" class="simple-footnote" title="LDA算法可能导致overfitting,具体解决方法请参阅Machine Learning:a probabilistic perspective一书4.2.*部分">1</a></sup></p>
<h1>Inference in joint Gaussian distributions</h1>
<hr>
<p>给定联合概率分布$p(x_1,x_2)$,如果我们能够计算边际概率分布$p(x1)$以及条件概率分布$p(x_1|x_2)$想必是极好的而且是及有用的。以下我们仅给出结论,下式表明<strong>如果两变量符合联合高斯分布，则它们的边际分布以及条件分布也都是高斯分布</strong>。</p>
<blockquote>
<p>Theorem 3.2<sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2-back"><a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2" class="simple-footnote" title="具体证明请参考ML:APP一书4.4.4.3一节">2</a></sup> 假定$x=(x_1,x_2)$服从联合高斯分布,且参数如下:
\begin{equation}
\mu = \left(
        \begin{array}{ccc}
        \mu_1 \\
        \mu_2
        \end{array}
      \right)
\end{equation}
\begin{equation}
\Sigma = \left(
        \begin{array}{ccc}
        \Sigma_{11} &amp; \Sigma_{12} \\
        \Sigma_{21} &amp; \Sigma_{22}
        \end{array}
      \right)
\end{equation}
则我们可以得到如下边际概率分布:
\begin{equation}
p(x_1) = N(x_1|\mu_1,\Sigma_{11})  \\
p(x_2) = N(x_2|\mu_2,\Sigma_{22})
\end{equation}
另其后验条件分布为:
\begin{equation}
p(x_1|x_2) = N(x_1|\mu_{1|2},\Sigma_{1|2})
\end{equation}
\begin{equation}
\begin{split}
\mu_{1|2} &amp;= \mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2) \\
&amp;=\mu_1-\Lambda_{11}^{-1}\Lambda_{12}(x_2-\mu_2) \\
&amp;=\Sigma_{1|2}(\Lambda_{11}\mu_1-\Lambda_{12}(x_2-\mu_2)) \\
\end{split}
\end{equation}
\begin{equation}
\Sigma_{1|2} = \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}=\Lambda_{11}^{-1}
\end{equation}</p>
</blockquote>
<h1>Linear Gaussian Systems</h1>
<hr>
<p>给定两变量，$x$和$y$.令$x \in R^{D_x}$为一隐含变量,$y \in R^{D_y}$为关于$x$的包含噪声的观察值。此外，我们假定存在如下prior和likelihood:
\begin{equation}
\begin{split}
p(x) &amp;= N(x|\mu_x,\Sigma_x) \\
p(y|x) &amp;= N(y|Ax+b,\Sigma_y)
\end{split}
\end{equation}
上式即称为<em>Linear Gaussian System</em>。此时我们有:</p>
<blockquote>
<p>Theorem 3.3<sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3-back"><a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3" class="simple-footnote" title="证明请参考ML:APP一书4.4.3节">3</a></sup> 给定一Linear Gaussian System.其后验分布$p(x|y)$具有如下形式:
\begin{equation}
\begin{split}
p(x|y) &amp;= N(x|\mu_{x|y},\Sigma_{x|y}) \\
\Sigma_{x|y}^{-1} &amp;= \Sigma_x^{-1}+A^T\Sigma_y^{-1}A  \\
\mu_{x|y} &amp;= \Sigma_{x|y}[A^T\Sigma_y^{-1}(y-b)+\Sigma_x^{-1}\mu_x] 
\end{split}
\end{equation}</p>
</blockquote>
<h2>Inferring an unknown vector from noisy measurements</h2>
<p>下面我们举一个简单的例子以进一步说明Linear Gaussian System:
现有$N$个观测向量,$y_i \sim\ N(x,\Sigma_y)$,prior服从高斯分布$x \sim\ N(\mu_0,\Sigma_0)$.令$A=I,b=0$,此外，我们采用$\bar{y}$作为我们的有效估计值,其精度为$N\Sigma_y^{-1},$我们有:
\begin{equation}
\begin{split}
p(x|y_1,...,y_N) &amp;= N(x|\mu_N,\Sigma_N) \\
\Sigma_N^{-1} &amp;= \Sigma_{0}^{-1}+N\Sigma_{y}^{-1} \\
\mu_N &amp;= \Sigma_N(\Sigma_y^{-1}(N\bar{y})+\Sigma_0^{-1}\mu_0)
\end{split}
\end{equation}
为了更直观地解释以上模型,如下图所示:</p>
<p><img alt="Radar Blips" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/radar_blips_zpsfd1a04d1.png"></p>
<p>我们可以x视为2维空间中一个物体的真实位置(但我们并不知道),例如一枚导弹或者一架飞机,$y_i$则是我们的观测值(含噪声),可以视为雷达上的一些点。当我们得到越来越多的点时，我们就能够更好地进行定位。<sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4-back"><a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4" class="simple-footnote" title="另外一种方法为Kalman Filter Algorithm">4</a></sup></p>
<p>现假定我们有多个测量设备，且我们想利用多个设备的观测值进行估计，这种方法称为<code>sensor fusion</code>.如果我们有具有不同方差的多组观测值，那么posterior将会是它们的加权平均。如下图所示:</p>
<p><img alt="Sensor Fusion" src="http://i1302.photobucket.com/albums/ag136/qingyuanxingsi/blog/sensor_fusion_zps4ad8202f.png"></p>
<p>我们采用不带任何信息的关于$x$的先验分布，即$p(x)=N(\mu_0,\Sigma_0)=N(0,10^10I_2)$,我们得到2个观测值,$y_1 \sim\ N(x,\Sigma_{y,1}$,$y_2 \sim\ N(x,\Sigma_{y,2})$,我们需要计算$p(x|y_1,y_2)$.</p>
<p>如上图(a),我们设定$\Sigma_{y,1} = \Sigma_{y,2} = 0.01I_2$,因此两个传感器都相当可靠，posterior mean即位于两个观测值中间；如上图(b)，我们设定$\Sigma_{y,1}=0.05I_2$且$\Sigma_{y,2}=0.01I_2$,因此传感器2比传感器1可靠，此时posterior mean更靠近于$y_2$;如上图(c),我们有:
\begin{equation}
\Sigma_{y,1} = 0.01
\left(
\begin{array}{cc}
10 &amp; 1 \\
1  &amp; 1
\end{array}
\right),
\Sigma_{y,2} = 0.01
\left(
\begin{array}{cc}
1 &amp; 1 \\
1  &amp; 10
\end{array}
\right)
\end{equation}</p>
<p>从上式我们不难看出，传感器1在第2个分量上更可靠，传感器2在第1个分量上也更可靠。此时，posterior mean采用传感器1的第二分量以及传感器1的第二分量。</p>
<blockquote>
<p>NOTE:当sensor测量精度未知时，我们则需要它们的测量精度也进行估计。</p>
</blockquote>
<h1>The Wishart Distribution</h1>
<hr>
<blockquote>
<p>在多变量统计学中,Wishart分布是继高斯分布后最重要且最有用的模型。  ------Press</p>
</blockquote>
<p>既然Press他老人家都说了Wishart分布很重要，而且我们下一部分会用到它，那么我们就必须得介绍介绍它了。(它主要被用来Model关于协方差矩阵的不确定性)</p>
<h2>Wishart Distribution</h2>
<p>Wishart概率密度函数具有如下形式:
\begin{equation}
Wi(\Lambda|S,\nu)=\frac{1}{Z_{Wi}}|\Lambda|^{(\nu-D-1)/2}exp(-{1 \over 2}tr(\Lambda S^{-1}))
\end{equation}</p>
<p>其中,$\nu$为自由度，$S$为缩放矩阵。其归一项具有如下形式:(<code>很恐怖，对吧!</code>)
\begin{equation}
Z_{Wi}=2^{\nu D/2}\Gamma_D(\nu/2)|S|^{\nu/2}
\end{equation}</p>
<p>其中,$\Gamma_D(a)$为多变量Gamma函数:
\begin{equation}
\Gamma_D(x) = \pi^{D(D-1)/4}\prod_{i=1}^{D} \Gamma(x+(1-i)/2)
\end{equation}</p>
<p>于是有$\Gamma_1(a)=\Gamma(a)$且有:
\begin{equation}
\Gamma_D(\nu_0/2)=\prod_{i=1}^{D} \Gamma(\frac{\nu_0+1-i}{2})
\end{equation}</p>
<p><strong>仅当$\nu&gt;D-1$时归一项存在</strong>。</p>
<p>其实Wishart分布和Gaussian分布是有联系的。具体而言，令$x_i \sim\ N(0,\Sigma)$,则离散度矩阵$S=\sum_{i=1}^{N}x_ix_i^T$服从Wishart分布,且$S \sim\ Wi(\Sigma,1)$。于是有$E(S)=N\Sigma$.</p>
<p>更一般地，我们可以证明$Wi(S,\nu$的mean和mode具有如下形式:
\begin{equation}
mean=\nu S,mode=(\nu-D-1)S
\end{equation}</p>
<p><strong>仅当$\nu&gt;D+1$时mode存在</strong>。</p>
<p>当$D=1$时，Wishart分布退化为Gamma分布，且有:
\begin{equation}
Wi(\lambda|s^{-1},v) = Ga(\lambda|{\nu \over 2},{s \over 2})
\end{equation}</p>
<h2>Inverse Wishart Distribution</h2>
<p>若$\Sigma^{-1} \sim\ Wi(S,\nu)$,则$\Sigma \sim\ IW(S^{-1},\nu+D+1)$,其中$IW$为逆Wishart分布。当$\nu&gt;D-1$且$S \succ 0$时,我们有:
\begin{equation}
\begin{split}
IW(\Sigma|S,\nu) &amp;= \frac{1}{Z_{IW}}|\Sigma|^{-(\nu+D+1)/2}exp(-{1 \over 2}tr(S^{-1}\Sigma^{-1})) \\
Z_{IW} = |S|^{-\nu/2}2^{\nu D/2}\Gamma_D(\nu/2)
\end{split}
\end{equation}</p>
<p>此外我们可以证明逆Wishart分布具有如下性质:
\begin{equation}
mean=\frac{S^{-1}}{\nu-D-1},mode=\frac{S^{-1}}{\nu+D+1}
\end{equation}</p>
<p>当$D=1$时,它们退化为逆Gamma分布:
\begin{equation}
IW(\sigma^2|S^{-1},\nu)=IG(\sigma^2|\nu/2,S/2)
\end{equation}</p>
<h1>Inferring the parameters of an MVN<sup id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5-back"><a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5" class="simple-footnote" title="本部分部分参考Regularized Gaussian Covariance Estimation">5</a></sup></h1>
<hr><script type="text/javascript">
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
</script>
<ol class="simple-footnotes"><li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1">LDA算法可能导致overfitting,具体解决方法请参阅Machine Learning:a probabilistic perspective一书4.2.*部分 <a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-1-back" class="simple-footnote-back">↩</a></li><li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2">具体证明请参考ML:APP一书4.4.4.3一节 <a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-2-back" class="simple-footnote-back">↩</a></li><li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3">证明请参考ML:APP一书4.4.3节 <a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-3-back" class="simple-footnote-back">↩</a></li><li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4">另外一种方法为Kalman Filter Algorithm <a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-4-back" class="simple-footnote-back">↩</a></li><li id="sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5">本部分部分参考<a href="http://freemind.pluskid.org/machine-learning/regularized-gaussian-covariance-estimation/">Regularized Gaussian Covariance Estimation</a> <a href="#sf-ji-qi-xue-xi-xi-lie-iiigaussian-models-5-back" class="simple-footnote-back">↩</a></li></ol> 
	<a class="btn btn-mini xsmall" href="http://www.qingyuanxingsi.com/ji-qi-xue-xi-xi-lie-iiigaussian-models.html">
          <i class="icon-comment"></i> Comment </a>
	<hr />
      </div>
      
    </div>
    
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="http://www.qingyuanxingsi.com/tag/gaussian-models.html">1</a></li>

    <li class="next disabled"><a href="#">&rarr; Next</a></li>

</ul>
</div>
 
  
        </div>
        
        
    </div>     </div> </div>

<!--footer-->
<div class="container">
  <div class="well" style="background-color: #E9EFF6">
    <div id="blog-footer">
      <div class="row-fluid">
	<div class="social span2" align="center" id="socialist">
	  <ul class="nav nav-list">
	    <li class="nav-header">
	      Social
	    </li>
	    <li><a href="https://github.com/qingyuanxingsi"><i class="icon-Github" style="color: #1f334b"></i>Github</a></li>

	  </ul>
	</div>
        <div class="links span2" align="center">
          <ul class="nav nav-list">
            <li class="nav-header"> 
              Links
            </li>
            
            <li><a href="http://freemind.pluskid.org">Pluskid</a></li>
            <li><a href="https://github.com/julycoding/The-Art-Of-Programming-By-July">结构之法 算法之道</a></li>
            <li><a href="http://www.nosqlnotes.net/">NOSQL Notes</a></li>
          </ul>
        </div>
	<div class="site-nav span2" align="center">
          <ul class="nav nav-list" id="site-links">
            <li class="nav-header"> 
              Site
            </li>
            <li><a href="http://www.qingyuanxingsi.com"><i class="icon-home" style="color: #1f334b">
                </i>Home</a></li>
            <li><a href="http://www.qingyuanxingsi.com/archives.html"><i class="icon-list" style="color: #1f334b">
                </i>Archives</a></li>
	    <li><a href="http://www.qingyuanxingsi.com/tags.html"><i class="icon-tags" style="color: #1f334b">
                </i>Tags</a></li>
	    
            <li><a href="http://www.qingyuanxingsi.com/" rel="alternate">
                <i class="icon-rss-sign" style="color: #1f334b"></i>
                Atom Feed</a></li>
	  </ul>

        </div>

      </div> <!--end of fluid row-->
    </div> <!--end of blog-footer-->
    <hr />
    <p align="center"><a href="http://www.qingyuanxingsi.com">Doodle World</a>
      &copy; qingyuanxingsi
    Powered by <a href="github.com/getpelican/pelican">Pelican</a> and
        <a href="https://twitter.github.com/bootstrap">Twitter Bootstrap</a>. 
        Icons by <a href="http://fortawesome.github.com/Font-Awesome">Font Awesome</a> and 
        <a href="http://gregoryloucas.github.com/Font-Awesome-More">Font Awesome More</a></p>

  </div> <!--end of well -->
</div> <!--end of container -->

<!--/footer-->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
<script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/js/bootstrap.min.js"></script>


<script>var _gaq=[['_setAccount','UA-48582273-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>

</body>
</html>